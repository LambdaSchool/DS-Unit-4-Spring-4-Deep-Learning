{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "U4-S3-DNN (Python 3.7)",
      "language": "python",
      "name": "u4-s3-dnn"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "LS_DS_431_RNN_and_LSTM_Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Neha-kumari31/DS-Unit-4-Sprint-3-Deep-Learning/blob/master/LS_DS_431_RNN_and_LSTM_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfBcksZToZ6L",
        "colab_type": "text"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "<br></br>\n",
        "\n",
        "## *Data Science Unit 4 Sprint 3 Assignment 1*\n",
        "\n",
        "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
        "\n",
        "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
        "\n",
        "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
        "\n",
        "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
        "\n",
        "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
        "\n",
        "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
        "\n",
        "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ltj1je1fp5rO",
        "colab": {}
      },
      "source": [
        "# TODO - Words, words, mere words, no matter from the heart.\n",
        "#Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) \n",
        "#to generate, and returns generated text of that size."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRPWQ8bhetJg",
        "colab_type": "code",
        "outputId": "ac194ba5-0ecc-4412-9b46-24df4c267325",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "import sys\n",
        "\n",
        "if not sys.warnoptions:\n",
        "    import warnings\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.callbacks import LambdaCallback\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "import os\n",
        "import textwrap\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "np.random.seed(45)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1u7J1o00etLd",
        "colab_type": "code",
        "outputId": "348d37a3-2ae8-49dd-a7ff-d893a74353ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "url = \"https://www.gutenberg.org/files/100/100-0.txt\"\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', url)\n",
        "raw_text = open(path_to_file, 'r', encoding='utf-8').read()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.gutenberg.org/files/100/100-0.txt\n",
            "5783552/5777367 [==============================] - 6s 1us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nAOAtLiENsX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "be07a482-72e5-4bfd-beb3-119ff5a0e2a8"
      },
      "source": [
        "print(raw_text[:1500])\n",
        "print(f'Length of the text: {len(raw_text)}')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "﻿\n",
            "Project Gutenberg’s The Complete Works of William Shakespeare, by William\n",
            "Shakespeare\n",
            "\n",
            "This eBook is for the use of anyone anywhere in the United States and\n",
            "most other parts of the world at no cost and with almost no restrictions\n",
            "whatsoever.  You may copy it, give it away or re-use it under the terms\n",
            "of the Project Gutenberg License included with this eBook or online at\n",
            "www.gutenberg.org.  If you are not located in the United States, you’ll\n",
            "have to check the laws of the country where you are located before using\n",
            "this ebook.\n",
            "\n",
            "\n",
            "Title: The Complete Works of William Shakespeare\n",
            "\n",
            "Author: William Shakespeare\n",
            "\n",
            "Release Date: January 1994 [EBook #100]\n",
            "Last Updated: November 7, 2019\n",
            "\n",
            "Language: English\n",
            "\n",
            "Character set encoding: UTF-8\n",
            "\n",
            "*** START OF THIS PROJECT GUTENBERG EBOOK THE COMPLETE WORKS OF WILLIAM SHAKESPEARE ***\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "The Complete Works of William Shakespeare\n",
            "\n",
            "\n",
            "\n",
            "by William Shakespeare\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "      Contents\n",
            "\n",
            "\n",
            "\n",
            "               THE SONNETS\n",
            "\n",
            "               ALL’S WELL THAT ENDS WELL\n",
            "\n",
            "               THE TRAGEDY OF ANTONY AND CLEOPATRA\n",
            "\n",
            "               AS YOU LIKE IT\n",
            "\n",
            "               THE COMEDY OF ERRORS\n",
            "\n",
            "               THE TRAGEDY OF CORIOLANUS\n",
            "\n",
            "               CYMBELINE\n",
            "\n",
            "               THE TRAGEDY OF HAMLET, PRINCE OF DENMARK\n",
            "\n",
            "               THE FIRST PART OF KING HENRY THE FOURTH\n",
            "\n",
            "               THE SECOND PART OF KING HENRY THE FOURTH\n",
            "\n",
            "               THE LIFE OF KING HENRY THE FIFTH\n",
            "\n",
            "               THE FIRST PART OF HENRY THE SIXTH\n",
            "\n",
            "               THE SECOND PART OF KING HENRY THE SIXTH\n",
            "\n",
            "\n",
            "Length of the text: 5573152\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmgngApcFYyl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "d7d47c05-abec-4777-e414-0a1238598a78"
      },
      "source": [
        "def pre_process(text):\n",
        "    text = text[900:-21100]\n",
        "    text = text.replace(\"\\r\", \"\")\n",
        "    text = \" \".join(text.split())\n",
        "    print (f'Length of text: {len(text)} characters long')\n",
        "    return text\n",
        "text = pre_process(raw_text)\n",
        "text[:100]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 5258116 characters long\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Contents THE SONNETS ALL’S WELL THAT ENDS WELL THE TRAGEDY OF ANTONY AND CLEOPATRA AS YOU LIKE IT TH'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6cPIff3etOm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create mapping of unique chars to integers\n",
        "chars = sorted(set(raw_text))\n",
        "\n",
        "char_int = {c:i for i, c in enumerate(chars)}\n",
        "int_char = {i:c for i, c in enumerate(chars)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryMdBEMQetRd",
        "colab_type": "code",
        "outputId": "10a05e9f-2525-4d27-9441-2f87bd79aadd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "len(chars)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "107"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSNdWqbxlsoE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "41c9d109-a56e-4c8a-f9a0-4ccc54ce282a"
      },
      "source": [
        "chars"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\t',\n",
              " '\\n',\n",
              " ' ',\n",
              " '!',\n",
              " '\"',\n",
              " '#',\n",
              " '$',\n",
              " '%',\n",
              " '&',\n",
              " \"'\",\n",
              " '(',\n",
              " ')',\n",
              " '*',\n",
              " ',',\n",
              " '-',\n",
              " '.',\n",
              " '/',\n",
              " '0',\n",
              " '1',\n",
              " '2',\n",
              " '3',\n",
              " '4',\n",
              " '5',\n",
              " '6',\n",
              " '7',\n",
              " '8',\n",
              " '9',\n",
              " ':',\n",
              " ';',\n",
              " '?',\n",
              " '@',\n",
              " 'A',\n",
              " 'B',\n",
              " 'C',\n",
              " 'D',\n",
              " 'E',\n",
              " 'F',\n",
              " 'G',\n",
              " 'H',\n",
              " 'I',\n",
              " 'J',\n",
              " 'K',\n",
              " 'L',\n",
              " 'M',\n",
              " 'N',\n",
              " 'O',\n",
              " 'P',\n",
              " 'Q',\n",
              " 'R',\n",
              " 'S',\n",
              " 'T',\n",
              " 'U',\n",
              " 'V',\n",
              " 'W',\n",
              " 'X',\n",
              " 'Y',\n",
              " 'Z',\n",
              " '[',\n",
              " '\\\\',\n",
              " ']',\n",
              " '_',\n",
              " '`',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'k',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'w',\n",
              " 'x',\n",
              " 'y',\n",
              " 'z',\n",
              " '|',\n",
              " '}',\n",
              " 'Æ',\n",
              " 'É',\n",
              " 'à',\n",
              " 'â',\n",
              " 'æ',\n",
              " 'ç',\n",
              " 'è',\n",
              " 'é',\n",
              " 'ê',\n",
              " 'î',\n",
              " 'œ',\n",
              " '—',\n",
              " '‘',\n",
              " '’',\n",
              " '“',\n",
              " '”',\n",
              " '\\ufeff']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ku5MDhnWmqUZ",
        "colab_type": "code",
        "outputId": "6b6ae7d7-160a-41ae-f21b-2f9fc502d778",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Create the sequence data\n",
        "maxlen = 40\n",
        "step = 5\n",
        "\n",
        "encoded = [char_int[c] for c in raw_text]\n",
        "\n",
        "sequences = [] # Each element is 40 chars long\n",
        "next_char = [] # One element for each sequence\n",
        "\n",
        "for i in range(0, len(encoded) - maxlen, step):\n",
        "    \n",
        "    sequences.append(encoded[i : i + maxlen])\n",
        "    next_char.append(encoded[i + maxlen])\n",
        "    \n",
        "print('sequences: ', len(sequences))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sequences:  1114623\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "il2PjDUYmqXa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "outputId": "cf604e6b-ba5e-454f-c7fb-c04ee0a3ff18"
      },
      "source": [
        "sequences[0]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[106,\n",
              " 1,\n",
              " 46,\n",
              " 79,\n",
              " 76,\n",
              " 71,\n",
              " 66,\n",
              " 64,\n",
              " 81,\n",
              " 2,\n",
              " 37,\n",
              " 82,\n",
              " 81,\n",
              " 66,\n",
              " 75,\n",
              " 63,\n",
              " 66,\n",
              " 79,\n",
              " 68,\n",
              " 103,\n",
              " 80,\n",
              " 2,\n",
              " 50,\n",
              " 69,\n",
              " 66,\n",
              " 2,\n",
              " 33,\n",
              " 76,\n",
              " 74,\n",
              " 77,\n",
              " 73,\n",
              " 66,\n",
              " 81,\n",
              " 66,\n",
              " 2,\n",
              " 53,\n",
              " 76,\n",
              " 79,\n",
              " 72,\n",
              " 80]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veBQohIAmqgL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create x & y\n",
        "x = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sequences),len(chars)), dtype=np.bool)\n",
        "\n",
        "for i, sequence in enumerate(sequences):\n",
        "    for t, char in enumerate(sequence):\n",
        "        x[i,t,char] = 1\n",
        "        \n",
        "    y[i, next_char[i]] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvAxxZE6mqjQ",
        "colab_type": "code",
        "outputId": "c4211721-9b63-4750-d151-891e2f450676",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Number of sequences, sequence length, unique charactersin that sequence\n",
        "x.shape, y.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1114623, 40, 107), (1114623, 107))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjCF-OfjSI7z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "3271ddb3-e163-49d4-9b59-cf2c0fae4af2"
      },
      "source": [
        "len(chars)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "107"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtJ5fEZwtoah",
        "colab_type": "text"
      },
      "source": [
        "###Create Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTzrOhUOsuDE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model1(X, y):\n",
        "    # Define the LSTM model\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(256, input_shape=(x.shape[1], x.shape[2])))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(y.shape[1], activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "    return model\n",
        "\n",
        "\n",
        "model1 = create_model1(x, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfJZJYZVTIKc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(preds):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / 1\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IU276D-9TP1l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def on_epoch_end(epoch, _):\n",
        "    # Function invoked at end of each epoch. Prints generated text.\n",
        "    \n",
        "    print()\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        "    \n",
        "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "    \n",
        "    generated = ''\n",
        "    \n",
        "    # sentence = text[start_index: start_index + maxlen]\n",
        "    sentence = 'How much mother love her kids '\n",
        "    generated += sentence\n",
        "    \n",
        "    print('----- Generating with seed: \"' + sentence + '\"')\n",
        "    sys.stdout.write(generated)\n",
        "    \n",
        "    for i in range(400):\n",
        "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "        for t, char in enumerate(sentence):\n",
        "            x_pred[0, t, char_int[char]] = 1\n",
        "            \n",
        "        preds = model.predict(x_pred, verbose=0)[0]\n",
        "        next_index = sample(preds)\n",
        "        next_char = int_char[next_index]\n",
        "        \n",
        "        sentence = sentence[1:] + next_char\n",
        "        \n",
        "        sys.stdout.write(next_char)\n",
        "        sys.stdout.flush()\n",
        "    print()\n",
        "\n",
        "\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qreQ9fZcVgP2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "outputId": "ed1877a5-a315-4082-8817-91c4a7a95fe3"
      },
      "source": [
        "# fit the model\n",
        "model1.fit(x, y,\n",
        "          batch_size=32,\n",
        "          epochs=25,\n",
        "          callbacks=[print_callback])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "34830/34832 [============================>.] - ETA: 0s - loss: 1.9019\n",
            "----- Generating text after Epoch: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-2307b25ef240>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m           callbacks=[print_callback])\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    874\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 876\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    877\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m       \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-8668131da85b>\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(epoch, _)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'----- Generating text after Epoch: %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mstart_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmaxlen\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mgenerated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'random' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxWSFI3-ttaz",
        "colab_type": "text"
      },
      "source": [
        "###Add Callback"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szTFGZszt8fV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install wandb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGQ4xkWosuHJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "\n",
        "wandb.init(project=\"angus\")\n",
        "\n",
        "# define the checkpoint\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_directory = './training_checkpoints'\n",
        "\n",
        "if not os.path.exists(checkpoint_directory ):\n",
        "    os.makedirs(checkpoint_directory)\n",
        "    print(f'New directory created at $:->{checkpoint_directory}')\n",
        "file_name =\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint_filename = os.path.join(checkpoint_directory, file_name)\n",
        "checkpoint_callback = ModelCheckpoint(checkpoint_filename, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "\n",
        "callbacks_list = [checkpoint_callback, WandbCallback()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJg04krNsuKO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zE4a4O7Bp5x1"
      },
      "source": [
        "# Resources and Stretch Goals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uT3UV3gap9H6"
      },
      "source": [
        "## Stretch goals:\n",
        "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
        "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
        "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
        "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
        "- Run on bigger, better data\n",
        "\n",
        "## Resources:\n",
        "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
        "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
        "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
        "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
        "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
      ]
    }
  ]
}