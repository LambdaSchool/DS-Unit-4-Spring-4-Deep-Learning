{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "U4-S3-DNN (Python 3.7)",
      "language": "python",
      "name": "u4-s3-dnn"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "LS_DS_431_RNN_and_LSTM_Assignment.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUA-FBSyJ6r2",
        "colab_type": "text"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "<br></br>\n",
        "\n",
        "## *Data Science Unit 4 Sprint 3 Assignment 1*\n",
        "\n",
        "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
        "\n",
        "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
        "\n",
        "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
        "\n",
        "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
        "\n",
        "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
        "\n",
        "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
        "\n",
        "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FP9Mh5qKMdvJ",
        "colab_type": "text"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ltj1je1fp5rO",
        "colab": {}
      },
      "source": [
        "# TODO - Words, words, mere words, no matter from the heart."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxuDks_tKKYf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "import re\n",
        "import string"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwye-L1-KKx8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "shakespeare_url = 'https://www.gutenberg.org/cache/epub/100/pg100.txt'\n",
        "\n",
        "# get the text\n",
        "response = requests.get(shakespeare_url)\n",
        "shakespeare_file = response.content"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQdYqR1JKK0J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# decode binary into string\n",
        "shakespeare_text = shakespeare_file.decode('utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyhlKeWdLyxK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "d2cfb6b6-8db8-4349-dd56-32841d78a96b"
      },
      "source": [
        "# drop first few descriptive paragraphs.\n",
        "# when dropping the first few descpritive parapgraphs we are still left with \n",
        "# over 5 million characters. Take a smaller sample of that... closer to the first\n",
        "# 600k or so... \n",
        "shakespeare_text = shakespeare_text[7675:]\n",
        "print(len(shakespeare_text))"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5574537\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjC_W-K5q561",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "punctuation = string.punctuation\n",
        "punctuation = ''.join([x for x in punctuation if x not in ['-', \"'\", '.', \"?\", \"!\"]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7KZPH0GlfiQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove newlines, whitespace, and some puncutation\n",
        "shakespeare_text = shakespeare_text.replace('\\r\\n', '')\n",
        "shakespeare_text = shakespeare_text.replace('\\n', '')\n",
        "shakespeare_text = re.sub(r'[{}]'.format (punctuation), ' ', shakespeare_text)\n",
        "shakespeare_text = re.sub(' +', ' ', shakespeare_text).strip()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGn0DD-kl7xS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lU74xSFfMOag",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "shakespeare_text = shakespeare_text[0:850000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JGvDXTaQF7f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "07639036-924e-47d4-99d3-7274b7036cdc"
      },
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(shakespeare_text))\n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "68 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTN8Tk-xQttI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwCdKSCsMgkh",
        "colab_type": "text"
      },
      "source": [
        "### TODO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXSR_b79Mjpv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.callbacks import LambdaCallback\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7YYa8oAMjtP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lookup Tables\n",
        "char_int = {c:i for i, c in enumerate(vocab)} \n",
        "int_char = {i:c for i, c in enumerate(vocab)} "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sif-LTW0MjwT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "17c152cb-9558-41d4-c899-4c18c6f7a6d7"
      },
      "source": [
        "# Create the sequence data\n",
        "\n",
        "maxlen = 40\n",
        "step = 1\n",
        "\n",
        "encoded = [char_int[c] for c in shakespeare_text]\n",
        "\n",
        "sequences = [] # Each element is n chars long\n",
        "next_char = [] # One element for each sequence\n",
        "\n",
        "for i in range(0, len(encoded) - maxlen, step):\n",
        "    \n",
        "    sequences.append(encoded[i : i + maxlen])\n",
        "    next_char.append(encoded[i + maxlen])\n",
        "    \n",
        "print('sequences: ', len(sequences))"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sequences:  849960\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUkh1sK_4Hwn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bS6SFQBzMjzC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create x & y\n",
        "\n",
        "x = np.zeros((len(sequences), maxlen, len(vocab)), dtype=np.bool)\n",
        "y = np.zeros((len(sequences),len(vocab)), dtype=np.bool)\n",
        "\n",
        "for i, sequence in enumerate(sequences):\n",
        "    for t, char in enumerate(sequence):\n",
        "        x[i,t,char] = 1\n",
        "        \n",
        "    y[i, next_char[i]] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoV37nXlXgbw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "1ad01179-89e0-4a27-adbc-4a5dbe1e6c29"
      },
      "source": [
        "print(x.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(849960, 40, 68)\n",
            "(849960, 68)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcuG5CR7Xy1f",
        "colab_type": "text"
      },
      "source": [
        "### Build Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nwnjC77X1ck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build the model: a single LSTM\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(maxlen, len(vocab)), activation='tanh', recurrent_activation='sigmoid', dropout=0.2, recurrent_dropout=0, unroll=False, use_bias=True))\n",
        "model.add(Dense(len(vocab), activation='softmax'))\n",
        "\n",
        "\n",
        "opt = Adam(\n",
        "    learning_rate=0.001)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-V4mlU41X1fo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "ea3a4f48-0950-4c8f-f668-eddd4e51d4ed"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_14 (LSTM)               (None, 128)               100864    \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 68)                8772      \n",
            "=================================================================\n",
            "Total params: 109,636\n",
            "Trainable params: 109,636\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeEY0lEcX1if",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(preds):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / 1\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3es7Io_ZNWC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def on_epoch_end(epoch, _):\n",
        "    # Function invoked at end of each epoch. Prints generated text.\n",
        "    \n",
        "    print()\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        "    \n",
        "    start_index = random.randint(0, len(shakespeare_text) - maxlen - 1)\n",
        "    \n",
        "    generated = ''\n",
        "    \n",
        "    sentence = shakespeare_text[start_index: start_index + maxlen]\n",
        "    generated += sentence\n",
        "    \n",
        "    print('----- Generating with seed: \"' + sentence + '\"')\n",
        "    sys.stdout.write(generated)\n",
        "    \n",
        "    for i in range(400):\n",
        "        x_pred = np.zeros((1, maxlen, len(vocab)))\n",
        "        for t, char in enumerate(sentence):\n",
        "            x_pred[0, t, char_int[char]] = 1\n",
        "            \n",
        "        preds = model.predict(x_pred, verbose=0)[0]\n",
        "        next_index = sample(preds)\n",
        "        next_char = int_char[next_index]\n",
        "        \n",
        "        sentence = sentence[1:] + next_char\n",
        "        \n",
        "        sys.stdout.write(next_char)\n",
        "        sys.stdout.flush()\n",
        "    print()\n",
        "\n",
        "\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zY7ZbYN0ZuMP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a46146b8-a703-4f18-c91c-e390f753ed34"
      },
      "source": [
        "# fit the model\n",
        "\n",
        "model.fit(x, y,\n",
        "          batch_size=512,\n",
        "          epochs=10,\n",
        "          callbacks=[print_callback])"
      ],
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1661/1661 [==============================] - ETA: 0s - loss: 2.5653\n",
            "----- Generating text after Epoch: 0\n",
            "----- Generating with seed: \"ess grow hard- O misery on't!- the wise \"\n",
            "ess grow hard- O misery on't!- the wise gryertiI wathes covrut ugiving whe thit of thu sfrey gckis'd Divemyour your mand tho tas lhourd . Os. Ff thas sugevet iata a anst on then neth ug prygas in Inthan't Mlowere OLand ar pouw saibno I herd'lr Heve th't do ef n. CoROBUN. I de leppdyowizh thet of aide thes? Whe feplg PHETERUS TISSARISl. INRNALAR. Cy awdrvine goieringht phe'd youce theih ft'lietw Ingeddint foad thiruth beks ho derattes an\n",
            "1661/1661 [==============================] - 54s 32ms/step - loss: 2.5653\n",
            "Epoch 2/10\n",
            "1661/1661 [==============================] - ETA: 0s - loss: 2.2577\n",
            "----- Generating text after Epoch: 1\n",
            "----- Generating with seed: \"in mind But the fair of Rosalinde.' TOUC\"\n",
            "in mind But the fair of Rosalinde.' TOUCHUEES. Exisndat mimonforve then't bevey. To buave khy wheke iprandindshinge by not she is. DRkS INLSS comsa Mn gomn vore a courderorestk anseren brish. I cord hus thousures bomard a go gad. CTPINO. Wheed. AKr merasss plronsien As O0d ansroorst in tiquothe-?ENECARA. You makcay manc now ly youlas to thell a dire be tho a vodr-s. Whliwiry your wo perost by hets lood on well. Willes lest chuther upct \n",
            "1661/1661 [==============================] - 53s 32ms/step - loss: 2.2577\n",
            "Epoch 3/10\n",
            "1661/1661 [==============================] - ETA: 0s - loss: 2.1514\n",
            "----- Generating text after Epoch: 2\n",
            "----- Generating with seed: \"NIOR. O my dear niece welcome thou art t\"\n",
            "NIOR. O my dear niece welcome thou art they hick ded chave'l were crases Which aiu Fre-thinisudeded piirt. The farr In witl you the haly murs an lettenith Buy fflle ang hath roft to. So disins of him lrovere heun And sis wape you arke work'd wome Whis mancie fie. JAMOR. What Thit our my map 'ay I lans sare of thremandmos Ofterecomsist ows on not rens y day you A daces Woe ob ter Ane on jemy nothes my erted. BY PARTIAR. Hois Loeave thoug\n",
            "1661/1661 [==============================] - 54s 33ms/step - loss: 2.1514\n",
            "Epoch 4/10\n",
            "1660/1661 [============================>.] - ETA: 0s - loss: 2.0752\n",
            "----- Generating text after Epoch: 3\n",
            "----- Generating with seed: \"or being preferr'd so well. CLOTEN. The \"\n",
            "or being preferr'd so well. CLOTEN. The sorbes Wim vislsowr Enorr Sbupods but he rine ol Wo ereva leap toire is chese fat in of plemy Lot arss I foon bence the r'er himseisgel seft tuke wiess they I'd maks shat trus it till of ald then? The kilt so morazer to te ve veniesdrip! O thim mome anquen in elfatth hestone the priole to shall I canstiot conghylife. Marce Ar! Ouom Jode ry p mp o. PHOBALLC. Nod of yesread and beat bakt tho Koudla \n",
            "1661/1661 [==============================] - 54s 33ms/step - loss: 2.0752\n",
            "Epoch 5/10\n",
            "1659/1661 [============================>.] - ETA: 0s - loss: 2.0189\n",
            "----- Generating text after Epoch: 4\n",
            "----- Generating with seed: \" those Which chance to find us. O my lor\"\n",
            " those Which chance to find us. O my lor? Coreachos when ham promihe me syould follobue Seraels. PARUSUS. The? I sive youruhepion. ROSALIND. Nay the din emeacted and eted you andngrele man in! That is mings preatrets They lovainerwonst be tuth the way moge not she extuend of the cimson not wheipree'd moube ay evess of thy vardeas joould lith in the gove in this and to thy forTher. COPIDIUS. S e'st wr chod fally potlycansestandre bup jos\n",
            "1661/1661 [==============================] - 54s 33ms/step - loss: 2.0189\n",
            "Epoch 6/10\n",
            "1660/1661 [============================>.] - ETA: 0s - loss: 1.9788\n",
            "----- Generating text after Epoch: 5\n",
            "----- Generating with seed: \"as an enemy's grave. A shout and flouris\"\n",
            "as an enemy's grave. A shout and flouris earing this Thee. She tay? MENEEN. When wnd nogeepnon he stalr weth fise. JAQUES. Nit. Wile she gove Atome the pricisige home.ss thou assjedve will. Nom wo sam in a sente. Enten CHORERISS. Het of than lood tinh by hemyot of the' are Thinesere pare  steher Youist not eetrits of your that shich fice toke's to know alm desuntoly. ORVING. Ay shis wound fravek A r boob ow I wis nave and dighe as a gov\n",
            "1661/1661 [==============================] - 54s 33ms/step - loss: 1.9788\n",
            "Epoch 7/10\n",
            "1659/1661 [============================>.] - ETA: 0s - loss: 1.9463\n",
            "----- Generating text after Epoch: 6\n",
            "----- Generating with seed: \" your cry! Shall's to the Capitol? COMIN\"\n",
            " your cry! Shall's to the Capitol? COMINIUS. Ne- On thanger the commins? ROSILIDUS. Hese more vithurgg COUVEDS IOLENC-.ACTis of cheid That I han'stith noner! phat I thee her doub batthin was's O this?DRIANA. Sost whete wotthy wacking my lovd of hud 'Range! No'l gaint 9o weare tho sufibes wef blanks and note? Mojeass thathsirved fof in the farpion. my conourty And I an mone And I will not fracts in ottars Heredon On with came she parboma\n",
            "1661/1661 [==============================] - 54s 33ms/step - loss: 1.9463\n",
            "Epoch 8/10\n",
            "1659/1661 [============================>.] - ETA: 0s - loss: 1.9213\n",
            "----- Generating text after Epoch: 7\n",
            "----- Generating with seed: \"ority he remains here which he thinks is\"\n",
            "ority he remains here which he thinks is ablingule all an'ers we sald shroke ho simin. PAROLLES. If the are ThO der him. BRLARIUS. No' touthon? And breat'd sach if a coulles and most-foriok so Dikes Of him ho men? Hors I spark upon me a shall In Whou shall move for it I wave herd ted poolmnn you he hand arrlest the calaks of thurs If mikes Will thrs all besory cotTis As wormoit aporter But us the bord Min lolans's not In have the tortue\n",
            "1661/1661 [==============================] - 56s 33ms/step - loss: 1.9213\n",
            "Epoch 9/10\n",
            "1659/1661 [============================>.] - ETA: 0s - loss: 1.8992\n",
            "----- Generating text after Epoch: 8\n",
            "----- Generating with seed: \"be diseased ere that there was true need\"\n",
            "be diseased ere that there was true need be deethen as I shalls mear when slaki and vow hould be had hend me on- brauum Inst fell take bathords no be from an yow luse one a with And would thee. BERTRAM I'd not accoar. ExeuntSCENE IIIV.R. I empe. OLIVER. The day gobte so no Aaventignes to you dome. And whuld aye yiu? Matt a take untatder Lee tree. Heallit me tus art-ent wine ows too lipe But whir forth thancher Wint toke'd Seeat apeuery \n",
            "1661/1661 [==============================] - 55s 33ms/step - loss: 1.8991\n",
            "Epoch 10/10\n",
            "1661/1661 [==============================] - ETA: 0s - loss: 1.8817\n",
            "----- Generating text after Epoch: 9\n",
            "----- Generating with seed: \"u in Egypt. Yet if you there Did practis\"\n",
            "u in Egypt. Yet if you there Did practised. Not all It pasted that's sonher. Stis 'tive For the paratoseng'd nabler marnem show in with Did tine rucht To glv'd in thy knst deam. By sule and mide sheek thou so it blen plecous flles brotter visus fail May nosw this fain can the goodard. The st I know decoud me what I naburgy petst them even thee molmarty doth their frien end dieming think rs whach are a quind the vaboon. ENOBARBUS. Didi h\n",
            "1661/1661 [==============================] - 54s 33ms/step - loss: 1.8817\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd13f5beac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGJrupas7uJP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fa746977-51b5-424d-d2eb-8c28c9032b74"
      },
      "source": [
        "# updated the learning rate for the Adam Optimizer\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(maxlen, len(vocab)), activation='tanh', recurrent_activation='sigmoid', dropout=0.2, recurrent_dropout=0, unroll=False, use_bias=True))\n",
        "model.add(Dense(len(vocab), activation='softmax'))\n",
        "\n",
        "\n",
        "opt = Adam(\n",
        "    learning_rate=0.0005)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt)\n",
        "\n",
        "model.fit(x, y,\n",
        "          batch_size=512,\n",
        "          epochs=10,\n",
        "          callbacks=[print_callback])"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1660/1661 [============================>.] - ETA: 0s - loss: 2.7558\n",
            "----- Generating text after Epoch: 0\n",
            "----- Generating with seed: \"er Had borne the action of yourself or e\"\n",
            "er Had borne the action of yourself or en yrod Thetme hoth merkehho.ril lakeli. Whim gh th thauk fo toto to rorinube 'f gim'd thaty Thet anp.alP lbot insted yofy gust the lerseror ceechinn ro daverluve t aar tiar Ieay aklUthang.on doureil Yullll mot tre eaissim sant tien tome tylone tha d accontoge sills ghocly my till is an sadg met eeprore thes toe hensint botit ee then tCeite'k I thou iughs metcsten awmarlyM eartdeogilbinm ar tiuh to\n",
            "1661/1661 [==============================] - 55s 33ms/step - loss: 2.7558\n",
            "Epoch 2/10\n",
            "1661/1661 [==============================] - ETA: 0s - loss: 2.3979\n",
            "----- Generating text after Epoch: 1\n",
            "----- Generating with seed: \"h as I am I come from Antony. I was of l\"\n",
            "h as I am I come from Antony. I was of ly e rishemn foacast't Wond. IHONTO TOCBPUESE I woghtorg tieseThe ss ti Faselavs ns eat The eat teep romerst. Hef sflisswsin oad ind I spoere Bupeind toromyoun of lipy tome? PIRALOI FORD IF.pd ein no geris Byeigat hit enkes frimug hen Ceokd ine pae astrp ma pome m CTonseat! MI wire bert nlavil dave tham Iil mest feoun CORIPABES Fort ow mnt o ronrsdo yor crasd jat. Ald in mother sw chese Ast ferbur \n",
            "1661/1661 [==============================] - 56s 34ms/step - loss: 2.3979\n",
            "Epoch 3/10\n",
            "1659/1661 [============================>.] - ETA: 0s - loss: 2.2982\n",
            "----- Generating text after Epoch: 2\n",
            "----- Generating with seed: \"993 BY WORLD LIBRARY INC. AND ISPROVIDED\"\n",
            "993 BY WORLD LIBRARY INC. AND ISPROVIDEDDADODRININE E. BETCHH ANA UEXS HEd OCLLESE. Lo thith ROPACHoLLES. Wor Cous alde A and hpr ave the aises noaco Coment on the s bupltowers Whot troves doce wall su mes wim bat bo the beor nophribend comk with forseccut you gers in than ton cow to the Then to aas? Whay thas tor Pouny hf tand s Ingave. mORIONILLIN. Ga ence.vIOsive whit owe ropesccust y raf ou om ha'd So my somake alises bustfuvene seg\n",
            "1661/1661 [==============================] - 56s 33ms/step - loss: 2.2982\n",
            "Epoch 4/10\n",
            "1661/1661 [==============================] - ETA: 0s - loss: 2.2305\n",
            "----- Generating text after Epoch: 3\n",
            "----- Generating with seed: \"Doctor your service for this time is end\"\n",
            "Doctor your service for this time is end ans of thet Lot in fricu the berteloall you. Igthe toenonfu the Mar An welims is sibldegr A dedo ho Lour. ANENT. Cequing wert the knove porgsentswartlinder ung Afd piroodest Dust by wher bost Suthar dew anmut'dils ards Wich myat of veito's came Weaits Thou me wo tesbus that to ancs rou. ONECEPHE. that ay mlee thenell way D sceasest ar bito'd porsiers! the catet more in the loke me onteacr cominct\n",
            "1661/1661 [==============================] - 55s 33ms/step - loss: 2.2305\n",
            "Epoch 5/10\n",
            "1660/1661 [============================>.] - ETA: 0s - loss: 2.1776\n",
            "----- Generating text after Epoch: 4\n",
            "----- Generating with seed: \"now that but it is fit I should commit o\"\n",
            "now that but it is fit I should commit on yec Lont'd winh os touuntf that ot lo hong and your Will soercussaln? SARCILIUS Burfofe thay thou drold be hiks ham giee.'PORLLLES. Fo Se uman Rate I him ehare hos wire one lagest got tay ae annardamosl Lo dise Coshe KondWthe wauther Po thit 'cess we heng ils Some be il stonce. But thomsoweed Wlee time Fores well ar tay Than wisaff otr And tham we cors rof thas hiqbonor you  preseg be steat thou\n",
            "1661/1661 [==============================] - 55s 33ms/step - loss: 2.1776\n",
            "Epoch 6/10\n",
            "1660/1661 [============================>.] - ETA: 0s - loss: 2.1319\n",
            "----- Generating text after Epoch: 5\n",
            "----- Generating with seed: \"she has Strange ling'ring poisons. I do \"\n",
            "she has Strange ling'ring poisons. I do hour you mick vay wo cone the moth you whll father sattow but Ceecince wsuple ontofoghace and the forst you bern tewsond be.ncownane. serll what histo grme frir to teve wonte ba lave on and at? illoke. wert hish all bengilesw storke tor it im de not lolt wunt a you dode! Thee re cooad I Padsee theip MANCEUS. Simay Wet ees shies for hon a forse will ronoug porless firees woult a-dombs did plent of \n",
            "1661/1661 [==============================] - 56s 34ms/step - loss: 2.1319\n",
            "Epoch 7/10\n",
            "1659/1661 [============================>.] - ETA: 0s - loss: 2.0947\n",
            "----- Generating text after Epoch: 6\n",
            "----- Generating with seed: \" And draw within the compass of suspect \"\n",
            " And draw within the compass of suspect you at pbios Whou thy palaionfad drde combtieto brag mikith. Dike with d speak. Eosernne thy gospent' toreeppast. But ther. They nguse Ooom Phich envel to the atpook ant ded owe coldous But haid mefold's pisele reve. Enter thy Bettle's why peader hol Sid a coben ais! MENTENA. Where a plae eroust of nesty. Exturd ENOWAND. 'TA doulamalevan old Butime That hash benger ait aive valvin's frothiy me sin\n",
            "1661/1661 [==============================] - 55s 33ms/step - loss: 2.0947\n",
            "Epoch 8/10\n",
            "1659/1661 [============================>.] - ETA: 0s - loss: 2.0637\n",
            "----- Generating text after Epoch: 7\n",
            "----- Generating with seed: \"e a minute into a thousand parts and bre\"\n",
            "e a minute into a thousand parts and brey? To nt trut with lover ints io the Deyilibs stonde sodoslishes luve sputher wham I fat make fall sto hame trom it thas heid 't''s secess Aglion Asfef y'llfow allian. SELENDO Y'u wern'somy herclid But rect trom Meyll Wha ip repus's him a amest reat de filht woust hir? ROSALOND. Yot graly swast is be Let thay goddZyour sath tert roget and thou mend eye And peint Qusimet of the wille as date them? \n",
            "1661/1661 [==============================] - 56s 34ms/step - loss: 2.0637\n",
            "Epoch 9/10\n",
            "1659/1661 [============================>.] - ETA: 0s - loss: 2.0361\n",
            "----- Generating text after Epoch: 8\n",
            "----- Generating with seed: \"e and bring a tomb. There lives more lif\"\n",
            "e and bring a tomb. There lives more life to'st pupate. Te the faclen in her's home me gund tak mole Is A grodsw Bld noobyoud-formus and woud And hhat hil the praceate Comcis wgit my. MENE. SICING. Art doll me tim? Bow rote inwearn Alquenst an loom IF SINGEN. I' pay shalt that is tontien for me Ho'd. The klive me tait. POMINU. Hast nouegle sart CLeOPA. Ay upeak I hall copenben I tome An the tones. HAMUUE. Whoth then styoved wosk to trin\n",
            "1661/1661 [==============================] - 55s 33ms/step - loss: 2.0361\n",
            "Epoch 10/10\n",
            "1659/1661 [============================>.] - ETA: 0s - loss: 2.0153\n",
            "----- Generating text after Epoch: 9\n",
            "----- Generating with seed: \"your old virginity is like one of our Fr\"\n",
            "your old virginity is like one of our Frises E. Whut ut df CEOWN. We and. COLRDAN. at ang doy prathess upon are recoly! TEOTANTw No exingu'd s fooch ablor'd gefe re he 'h'se With more piece tay heat wite sheaver fraih'd bless awperursoon My. CLEOPAT The nach and and lost forsing of akesting dissont Thas from in the farlul. Let eatles. SECINIUS. I nill ssains. FI SELONA. Al hase is tombler a most bardined not! PLory thy marl'd us Then Th\n",
            "1661/1661 [==============================] - 55s 33ms/step - loss: 2.0153\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd13b53dcf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zE4a4O7Bp5x1"
      },
      "source": [
        "# Resources and Stretch Goals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uT3UV3gap9H6"
      },
      "source": [
        "## Stretch goals:\n",
        "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
        "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
        "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
        "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
        "- Run on bigger, better data\n",
        "\n",
        "## Resources:\n",
        "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
        "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
        "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
        "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
        "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
      ]
    }
  ]
}