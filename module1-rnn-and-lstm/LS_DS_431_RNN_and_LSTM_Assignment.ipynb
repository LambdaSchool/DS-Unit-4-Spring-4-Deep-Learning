{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 3 Assignment 1*\n",
    "\n",
    "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
    "\n",
    "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
    "\n",
    "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
    "\n",
    "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
    "\n",
    "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
    "\n",
    "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
    "\n",
    "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ltj1je1fp5rO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = ['THE SONNETS', 'WELL THAT ENDS WELL', 'THE TRAGEDY OF ANTONY AND CLEOPATRA',\n",
    "          'AS YOU LIKE IT', 'THE COMEDY OF ERRORS', 'THE TRAGEDY OF CORIOLANUS', 'CYMBELINE',\n",
    "          'THE TRAGEDY OF HAMLET', 'PRINCE OF DENMARK', 'THE FIRST PART OF KING HENRY THE FOURTH',\n",
    "          'THE SECOND PART OF KING HENRY THE FOURTH', 'THE LIFE OF KING HENRY',\n",
    "          'THE FIRST PART OF HENRY THE SIXTH', 'THE SECOND PART OF KING HENRY THE SIXTH',\n",
    "          'THE THIRD PART OF KING HENRY THE SIXTH', 'KING HENRY THE EIGHTH', 'KING JOHN',\n",
    "          'THE TRAGEDY OF JULIUS CAESAR', 'THE TRAGEDY OF KING LEAR', 'LABOUR\\\\xe2\\\\x80\\\\x99S LOST',\n",
    "         'MACBETH', 'MEASURE FOR MEASURE', 'THE MERCHANT OF VENICE',\n",
    "          'THE MERRY WIVES OF WINDSOR', 'A MIDSUMMER NIGHT','MUCH ADO ABOUT NOTHING',\n",
    "          'OTHELLO, THE MOOR OF VENICE','MOOR OF VENICE', 'PERICLES, PRINCE OF TYRE', 'KING RICHARD THE SECOND',\n",
    "          'KING RICHARD THE THIRD', 'THE TRAGEDY OF ROMEO AND JULIET', 'THE TAMING OF THE SHREW',\n",
    "          'THE TEMPEST', 'THE LIFE OF TIMON OF ATHENS', 'THE TRAGEDY OF TITUS ANDRONICUS',\n",
    "          'THE HISTORY OF TROILUS AND CRESSIDA', 'TWELFTH NIGHT',\n",
    "          'THE TWO GENTLEMEN OF VERONA', 'THE TWO NOBLE KINSMEN', 'THE WINTER',\n",
    "          'A LOVER', 'THE PASSIONATE PILGRIM', 'THE PHOENIX AND THE TURTLE',\n",
    "          'THE RAPE OF LUCRECE', 'VENUS AND ADONIS']\n",
    "txt = txt[txt.find('VENUS AND ADONIS')+len('VENUS AND ADONIS'):]\n",
    "first_part = \"\"\n",
    "indices = []\n",
    "for search in splits:\n",
    "    index = txt.find(\"%s\" % search)\n",
    "    indices.append(index)\n",
    "    continue\n",
    "    if index != -1:\n",
    "        first_part = txt[:index]\n",
    "        break\n",
    "\n",
    "parts = [txt[indices[i]:indices[i+1]] for i in range(len(indices)-1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zE4a4O7Bp5x1"
   },
   "source": [
    "# Resources and Stretch Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uT3UV3gap9H6"
   },
   "source": [
    "## Stretch goals:\n",
    "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
    "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
    "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
    "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
    "- Run on bigger, better data\n",
    "\n",
    "## Resources:\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
    "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
    "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
    "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
    "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
