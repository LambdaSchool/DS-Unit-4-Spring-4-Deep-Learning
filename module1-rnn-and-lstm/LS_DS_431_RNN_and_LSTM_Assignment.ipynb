{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 3 Assignment 1*\n",
    "\n",
    "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
    "\n",
    "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
    "\n",
    "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
    "\n",
    "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
    "\n",
    "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
    "\n",
    "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
    "\n",
    "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit 4 sprint 3.1\n",
    "from tensorflow.keras.callbacks import LambdaCallback, EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import requests\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.gutenberg.org/files/100/100-0.txt\"\n",
    "\n",
    "r = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.encoding = r.apparent_encoding\n",
    "\n",
    "data = r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\r\\nProject Gutenberg’s The Complete Works of William Shakespeare, by William\\r\\nShakespeare\\r\\n\\r\\nThis eBo'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.split('\\r\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip the Table of Contents\n",
    "data = data[135:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "toc = [l.strip() for l in data[44:130:2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['For where is she so fair whose uneared womb',\n",
       " 'Or who is he so fond will be the tomb',\n",
       " 'Thou art thy mother’s glass and she in thee',\n",
       " 'So thou through windows of thine age shalt see,',\n",
       " 'But if thou live remembered not to be,',\n",
       " '',\n",
       " '4',\n",
       " 'Unthrifty loveliness why dost thou spend,',\n",
       " 'Nature’s bequest gives nothing but doth lend,',\n",
       " 'Then beauteous niggard why dost thou abuse,',\n",
       " 'Profitless usurer why dost thou use',\n",
       " 'For having traffic with thy self alone,',\n",
       " 'Then how when nature calls thee to be gone,',\n",
       " 'Thy unused beauty must be tombed with thee,',\n",
       " '',\n",
       " '5',\n",
       " 'Those hours that with gentle work did frame',\n",
       " 'Will play the tyrants to the very same,',\n",
       " 'For never-resting time leads summer on',\n",
       " 'Sap checked with frost and lusty leaves quite gone,',\n",
       " 'Then were not summer’s distillation left',\n",
       " 'Beauty’s effect with beauty were bereft,',\n",
       " 'But flowers distilled though they with winter meet,',\n",
       " '',\n",
       " '6',\n",
       " 'Then let not winter’s ragged hand deface,',\n",
       " 'Make sweet some vial; treasure thou some place,',\n",
       " 'That use is not forbidden usury,',\n",
       " 'That’s for thy self to breed another thee,',\n",
       " 'Ten times thy self were happier than thou art,',\n",
       " 'Then what could death do if thou shouldst depart,',\n",
       " 'Be not self-willed for thou art much too fair,',\n",
       " '',\n",
       " '7',\n",
       " 'Lo in the orient when the gracious light',\n",
       " 'Doth homage to his new-appearing sight,',\n",
       " 'And having climbed the steep-up heavenly hill,',\n",
       " 'Yet mortal looks adore his beauty still,',\n",
       " 'But when from highmost pitch with weary car,',\n",
       " 'The eyes (fore duteous) now converted are',\n",
       " 'So thou, thy self out-going in thy noon:',\n",
       " '',\n",
       " '8']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = {id_:{'title':title, 'start':-99} for id_,title in enumerate(toc)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing Titles\n",
    "locations[9]['title'] = 'THE LIFE OF KING HENRY V'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "43",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-b17799167a04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'start'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mlocations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'end'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 43"
     ]
    }
   ],
   "source": [
    "# Start \n",
    "for e,i in enumerate(data):\n",
    "    for t,title in enumerate(toc):\n",
    "        if title in i:\n",
    "            locations[t].update({'start':e})\n",
    "            \n",
    "# End            \n",
    "for title in toc:\n",
    "    \n",
    "    t = 0\n",
    "    \n",
    "    while t < len(toc):\n",
    "        print(t)\n",
    "        end = locations[t+1]['start'] - 1\n",
    "        locations[t]['end'] = end\n",
    "        t += 1\n",
    "\n",
    "    # Last One\n",
    "    locations[t]['end'] = len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'THE LIFE OF KING HENRY V', 'start': 62, 'end': 63}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'title': 'For where is she so fair whose uneared womb',\n",
       "  'start': 44,\n",
       "  'end': 45},\n",
       " 1: {'title': 'Or who is he so fond will be the tomb', 'start': 46, 'end': 47},\n",
       " 2: {'title': 'Thou art thy mother’s glass and she in thee',\n",
       "  'start': 48,\n",
       "  'end': 49},\n",
       " 3: {'title': 'So thou through windows of thine age shalt see,',\n",
       "  'start': 50,\n",
       "  'end': 51},\n",
       " 4: {'title': 'But if thou live remembered not to be,',\n",
       "  'start': 52,\n",
       "  'end': 166766},\n",
       " 5: {'title': '', 'start': 166767, 'end': 166702},\n",
       " 6: {'title': '4', 'start': 166703, 'end': 57},\n",
       " 7: {'title': 'Unthrifty loveliness why dost thou spend,',\n",
       "  'start': 58,\n",
       "  'end': 59},\n",
       " 8: {'title': 'Nature’s bequest gives nothing but doth lend,',\n",
       "  'start': 60,\n",
       "  'end': 61},\n",
       " 9: {'title': 'THE LIFE OF KING HENRY V', 'start': 62, 'end': 63},\n",
       " 10: {'title': 'Profitless usurer why dost thou use', 'start': 64, 'end': 65},\n",
       " 11: {'title': 'For having traffic with thy self alone,',\n",
       "  'start': 66,\n",
       "  'end': 67},\n",
       " 12: {'title': 'Then how when nature calls thee to be gone,',\n",
       "  'start': 68,\n",
       "  'end': 69},\n",
       " 13: {'title': 'Thy unused beauty must be tombed with thee,',\n",
       "  'start': 70,\n",
       "  'end': 166766},\n",
       " 14: {'title': '', 'start': 166767, 'end': 166736},\n",
       " 15: {'title': '5', 'start': 166737, 'end': 75},\n",
       " 16: {'title': 'Those hours that with gentle work did frame',\n",
       "  'start': 76,\n",
       "  'end': 77},\n",
       " 17: {'title': 'Will play the tyrants to the very same,',\n",
       "  'start': 78,\n",
       "  'end': 79},\n",
       " 18: {'title': 'For never-resting time leads summer on',\n",
       "  'start': 80,\n",
       "  'end': 81},\n",
       " 19: {'title': 'Sap checked with frost and lusty leaves quite gone,',\n",
       "  'start': 82,\n",
       "  'end': 83},\n",
       " 20: {'title': 'Then were not summer’s distillation left',\n",
       "  'start': 84,\n",
       "  'end': 85},\n",
       " 21: {'title': 'Beauty’s effect with beauty were bereft,',\n",
       "  'start': 86,\n",
       "  'end': 87},\n",
       " 22: {'title': 'But flowers distilled though they with winter meet,',\n",
       "  'start': 88,\n",
       "  'end': 166766},\n",
       " 23: {'title': '', 'start': 166767, 'end': 166694},\n",
       " 24: {'title': '6', 'start': 166695, 'end': 93},\n",
       " 25: {'title': 'Then let not winter’s ragged hand deface,',\n",
       "  'start': 94,\n",
       "  'end': 95},\n",
       " 26: {'title': 'Make sweet some vial; treasure thou some place,',\n",
       "  'start': 96,\n",
       "  'end': 97},\n",
       " 27: {'title': 'That use is not forbidden usury,', 'start': 98, 'end': 99},\n",
       " 28: {'title': 'That’s for thy self to breed another thee,',\n",
       "  'start': 100,\n",
       "  'end': 101},\n",
       " 29: {'title': 'Ten times thy self were happier than thou art,',\n",
       "  'start': 102,\n",
       "  'end': 103},\n",
       " 30: {'title': 'Then what could death do if thou shouldst depart,',\n",
       "  'start': 104,\n",
       "  'end': 105},\n",
       " 31: {'title': 'Be not self-willed for thou art much too fair,',\n",
       "  'start': 106,\n",
       "  'end': 166766},\n",
       " 32: {'title': '', 'start': 166767, 'end': 166694},\n",
       " 33: {'title': '7', 'start': 166695, 'end': 111},\n",
       " 34: {'title': 'Lo in the orient when the gracious light',\n",
       "  'start': 112,\n",
       "  'end': 113},\n",
       " 35: {'title': 'Doth homage to his new-appearing sight,',\n",
       "  'start': 114,\n",
       "  'end': 115},\n",
       " 36: {'title': 'And having climbed the steep-up heavenly hill,',\n",
       "  'start': 116,\n",
       "  'end': 117},\n",
       " 37: {'title': 'Yet mortal looks adore his beauty still,',\n",
       "  'start': 118,\n",
       "  'end': 119},\n",
       " 38: {'title': 'But when from highmost pitch with weary car,',\n",
       "  'start': 120,\n",
       "  'end': 121},\n",
       " 39: {'title': 'The eyes (fore duteous) now converted are',\n",
       "  'start': 122,\n",
       "  'end': 123},\n",
       " 40: {'title': 'So thou, thy self out-going in thy noon:',\n",
       "  'start': 124,\n",
       "  'end': 166766},\n",
       " 41: {'title': '', 'start': 166767, 'end': 166694},\n",
       " 42: {'title': '8', 'start': 166695}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2777\n"
     ]
    }
   ],
   "source": [
    "for e, i in enumerate(data):\n",
    "    \n",
    "    if \"ALL’S WELL THAT ENDS WELL\" in i:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'THE SONNETS'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide b/w plays and sonets\n",
    "sonets = data[:2776]\n",
    "plays = data[2777:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'THE SONNETS'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_lines(lst_ln):\n",
    "    clean = []\n",
    "    \n",
    "    for ln in lst_ln: \n",
    "        \n",
    "        if len(ln) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            pct = len(ln.strip(' ')) / len(ln)\n",
    "\n",
    "            if pct >= .5:\n",
    "                clean.append(ln.lstrip())\n",
    "\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# May Not be Needed\n",
    "sonets = long_lines(sonets)\n",
    "plays = long_lines(plays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Encoding\n",
    "\n",
    "This is just a start, and is not complete yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(set(\"\\r\\n\".join(plays).split()))\n",
    "words = [line.split() for line in plays]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75585"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character Encoding\n",
    "\n",
    "Using the technique shown in lecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our corpus contains 63 unique characters.\n"
     ]
    }
   ],
   "source": [
    "text = '\\r\\n'.join(sonets)\n",
    "\n",
    "chars = list(set(text))\n",
    "\n",
    "char_int = {c:i for i,c in enumerate(chars)}\n",
    "int_char = {i:c for i,c in enumerate(chars)}\n",
    "\n",
    "print(f\"Our corpus contains {len(chars)} unique characters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences: 95690\n"
     ]
    }
   ],
   "source": [
    "# Create the Sequence Data\n",
    "\n",
    "maxlen = 150\n",
    "step = 1\n",
    "\n",
    "encoded = [char_int[c] for c in text]\n",
    "\n",
    "sequences = [] # Each element is 40 characters long\n",
    "next_chars = [] # One element for each sequence\n",
    "\n",
    "for i in range(0, len(encoded) - maxlen, step):\n",
    "    sequences.append(encoded[i : i + maxlen])\n",
    "    next_chars.append(encoded[i + maxlen])\n",
    "    \n",
    "print('sequences:', len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Specify x & y\n",
    "\n",
    "x = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sequences), len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, char in enumerate(sequence):\n",
    "        x[i,t,char] = 1\n",
    "        \n",
    "    y[i, next_chars[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95690, 63)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95690, 150, 63)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model: a single LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(maxlen, len(chars)), dropout=0.2))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='nadam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 256)               327680    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 63)                16191     \n",
      "=================================================================\n",
      "Total params: 343,871\n",
      "Trainable params: 343,871\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / 1\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    \n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "    \n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    \n",
    "    generated = ''\n",
    "    \n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "    \n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "    \n",
    "    for i in range(400):\n",
    "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_int[char]] = 1\n",
    "            \n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds)\n",
    "        next_char = int_char[next_index]\n",
    "        \n",
    "        sentence = sentence[1:] + next_char\n",
    "        \n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print()\n",
    "\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 3.1648\n",
      "----- Generating text after Epoch: 0\n",
      "----- Generating with seed: \" love doth live,\n",
      "Hung with the trophies of my lovers gone,\n",
      "Who all their parts of me to thee did give,\n",
      "That due of many, now is thine alone.\n",
      "Their\"\n",
      " love doth live,\n",
      "Hung with the trophies of my lovers gone,\n",
      "Who all their parts of me to thee did give,\n",
      "That due of many, now is thine alone.\n",
      " heirbrnwttdhade tmk ehswita m et hhagahohsTiwonattnennrse tltoesho sortwfsy  .nser  ar, yfentre ihiedwbltsegyx\n",
      "r ule  m t t eteehd eair fdriiwTetho\n",
      "eftnr irish’hw eo ,ev  ) ad f, s,icwews \n",
      "o\n",
      "Inmnrtrr bii og iiee\n",
      "lilecuallo U  .ohoi. hea a   atoeh arm h  m ae\n",
      " dsihe soo aetY\n",
      ")ermnsh arram lt teto e winrsI vhseiofe n s eae  rnatreshausheiahrit awowseto  ne e\n",
      "eTestosm o taeohyn ditnmg;hayresril\n",
      "75/75 [==============================] - 250s 3s/step - loss: 3.1648 - val_loss: 3.0161\n",
      "Epoch 2/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 2.8362\n",
      "----- Generating text after Epoch: 1\n",
      "----- Generating with seed: \"l were minded so, the times should cease,\n",
      "And threescore year would make the world away:\n",
      "Let those whom nature hath not made for store,\n",
      "Harsh, feat\"\n",
      "l were minded so, the times should cease,\n",
      "And threescore year would make the world away:\n",
      "Let those whom nature hath not made for store,\n",
      "harsh, featkitod shonh ftidhfui’wt s,riret shretByeordtn neudsiaeesl hhbreaA tilpihis o‘!,hoAwjsellng s fftiettpvder  cs yo  surrmw ,hecseu fTl dbe\n",
      "fy oormgibe eokltinn yeichmp el tiheso,oHteicrbth gin sou  hiegbth mme  c ghke k, tdaslhbeleguhtgefgeik foral ihdrean  doo rot  ips yorgtu pis niyw,otoi io at  dotg moi   op rdpintdot dhqe!ii tnttt webdd\n",
      "ibe ned laIa. i:orresy moapfl whetg hilr are r ovg \n",
      "75/75 [==============================] - 227s 3s/step - loss: 2.8362 - val_loss: 2.5819\n",
      "Epoch 3/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 2.5269\n",
      "----- Generating text after Epoch: 2\n",
      "----- Generating with seed: \", flesh stays no farther reason,\n",
      "But rising at thy name doth point out thee,\n",
      "As his triumphant prize, proud of this pride,\n",
      "He is contented thy poor\"\n",
      ", flesh stays no farther reason,\n",
      "But rising at thy name doth point out thee,\n",
      "As his triumphant prize, proud of this pride,\n",
      "He is contented thy poor soaise fonespis bhe oilq ,aleuate eou eno  horebe\n",
      " uher. we iy notd yonf iptusovtnghepdir nonr’ordonvlnsate tyousuro,\n",
      "en manmes any censdeslethe holes adtenthar thostry thasd win wiresarcef ve tgaou dogves sunmoo.nmdeent aulouttf th de, bayve tefeaund risliseesfawt’lpy.\n",
      "\n",
      "Rheg zqlu , fov  sellOry \n",
      "a thu d onkenkat moaldtadencit inns uea o mt use bend font thazhtrselo lanlecezO tu lorcet nored\n",
      "75/75 [==============================] - 239s 3s/step - loss: 2.5269 - val_loss: 2.3648\n",
      "Epoch 4/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 2.4010\n",
      "----- Generating text after Epoch: 3\n",
      "----- Generating with seed: \" that cannot write to thee,\n",
      "When thou thy self dost give invention light?\n",
      "Be thou the tenth Muse, ten times more in worth\n",
      "Than those old nine which\"\n",
      " that cannot write to thee,\n",
      "When thou thy self dost give invention light?\n",
      "Be thou the tenth Muse, ten times more in worth\n",
      "Than those old nine which.\n",
      "Rh singkemestyashanicaimebnt. teu teithscour’y-winpyosestrore Iowrdsmero hilg ancy d df re y earmytTrs grit lifce.rchit rhat thane ert,\n",
      "Ahiro mes mermsy Ithy wefesm:\n",
      "tham brouveihy s.nvt ,rrobubtrssoe mo calgopy :fdser.\n",
      "Isbeado fovuinldliyt runt.\n",
      "dOsti ffreat ye murhamn boo eee,dandhs my ir leeat tyowlmand’ssreneee hashht ct mand gidpPooth m colntat dsos wothy ingrssban fang my warke be\n",
      "75/75 [==============================] - 251s 3s/step - loss: 2.4010 - val_loss: 2.2543\n",
      "Epoch 5/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 2.3215\n",
      "----- Generating text after Epoch: 4\n",
      "----- Generating with seed: \"ut yet like prayers divine,\n",
      "I must each day say o’er the very same,\n",
      "Counting no old thing old, thou mine, I thine,\n",
      "Even as when first I hallowed th\"\n",
      "ut yet like prayers divine,\n",
      "I must each day say o’er the very same,\n",
      "Counting no old thing old, thou mine, I thine,\n",
      "Even as when first I hallowed th dpect,\n",
      "Foketettinninl ntie deaglykyasedand lit settiedc\n",
      "Soomfpidilt huvis’l tere vy mesfom danci\n",
      ",\n",
      "Sedestnls,\n",
      "khthere peples ahelparn-tront:\n",
      "Wh th chichet aireat sorenngt\n",
      "franc oimed Beae  ougreeistest,\n",
      "N!nsk, thut moxeeeodres buu toll sheaall?t\n",
      "Oet\n",
      "aftkening art dey pith h at,\n",
      "Olthiwessy nglmobououlld seseansa sfal fileeat\n",
      "\n",
      "otoththord?y ialektise thecrevenmere some,\n",
      "Sereillvatd by ghe,\n",
      "75/75 [==============================] - 250s 3s/step - loss: 2.3215 - val_loss: 2.1904\n",
      "Epoch 6/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 2.2664\n",
      "----- Generating text after Epoch: 5\n",
      "----- Generating with seed: \"y of our old acquaintance tell.\n",
      "For thee, against my self I’ll vow debate,\n",
      "For I must ne’er love him whom thou dost hate.\n",
      "Then hate me when thou wi\"\n",
      "y of our old acquaintance tell.\n",
      "For thee, against my self I’ll vow debate,\n",
      "For I must ne’er love him whom thou dost hate.\n",
      "Then hate me when thou witeeiTh tit my mocl merfiminw se thet,\n",
      "Nit augo gfyka intee pmachelly geathhuy wath theas,\n",
      "Begu depilly’s by me canpate roungt mit, watte touP thys erriwn\n",
      "Aist nmkey bictil bmive tayou sre\n",
      "heraot urexckendeith whed res aky,\n",
      "def titinn fout selit pe tin gathes oraig thes srive menyon awesasy hf thev yef resa\n",
      "tdy fetig af bemp eeAmy weald ne loneen (myas giyeut ls e,e\n",
      "Fin weat eoupt oveamy o\n",
      "75/75 [==============================] - 261s 3s/step - loss: 2.2664 - val_loss: 2.1255\n",
      "Epoch 7/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 2.2200\n",
      "----- Generating text after Epoch: 6\n",
      "----- Generating with seed: \"otley to the view,\n",
      "Gored mine own thoughts, sold cheap what is most dear,\n",
      "Made old offences of affections new.\n",
      "Most true it is, that I have looked \"\n",
      "otley to the view,\n",
      "Gored mine own thoughts, sold cheap what is most dear,\n",
      "Made old offences of affections new.\n",
      "Most true it is, that I have looked sukk ords prewin\n",
      "d, arvene, Whet dle tosimg,ind dudy,\n",
      "hve thollft(id eave or iuctis, then bus ing thou oud int.\n",
      "Ssthatt fuchong thale inedreedove forpede oo  ove \n",
      "oh fov hy fare geoves  ie toth Inr,\n",
      "Simh teicr foo mend toan sha toun thruse\n",
      "rlI blid,\n",
      "Th a dal fhecaw thame hor ste teoullbecus sosell.\n",
      "I ver? ceaite se wor ead angeoren’s dses ancose,\n",
      "Gn curtay dour oy ther rs br tho goal stgete\n",
      "75/75 [==============================] - 239s 3s/step - loss: 2.2200 - val_loss: 2.0827\n",
      "Epoch 8/100\n",
      "14/75 [====>.........................] - ETA: 2:57 - loss: 2.1977"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-265fe4419ae1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m           callbacks=[print_callback, \n\u001b[1;32m      9\u001b[0m                      \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_delta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.02\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                      tensorboard_callback])\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/U4-S1-NLP/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/U4-S1-NLP/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/U4-S1-NLP/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/U4-S1-NLP/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/U4-S1-NLP/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/U4-S1-NLP/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/U4-S1-NLP/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/U4-S1-NLP/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/U4-S1-NLP/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=1024,\n",
    "          validation_split=.2,\n",
    "          epochs=100,\n",
    "          callbacks=[print_callback, \n",
    "                     EarlyStopping(min_delta=.02, monitor='val_loss', patience=10),\n",
    "                     tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zE4a4O7Bp5x1"
   },
   "source": [
    "# Resources and Stretch Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uT3UV3gap9H6"
   },
   "source": [
    "## Stretch goals:\n",
    "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
    "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
    "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
    "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
    "- Run on bigger, better data\n",
    "\n",
    "## Resources:\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
    "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
    "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
    "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
    "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "U4-S1-NLP (Python3)",
   "language": "python",
   "name": "u4-s1-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
