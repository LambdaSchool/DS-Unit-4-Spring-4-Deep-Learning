{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 3 Assignment 1*\n",
    "\n",
    "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
    "\n",
    "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
    "\n",
    "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
    "\n",
    "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
    "\n",
    "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
    "\n",
    "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
    "\n",
    "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ltj1je1fp5rO"
   },
   "outputs": [],
   "source": [
    "# Loading in some Borges. Shakespeare is boring!\n",
    "\n",
    "with open(f'./borges/borges_collected_fictions.txt', 'r', encoding='utf8') as f:\n",
    "            data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PENGUIN BOOKS\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "COLLECTED FICTIONS\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Jorge Luis Borges was born in Buenos Aires in 1899 and was educated in Europe. One of the most widely acclaimed writers of our time, he published many collections of poems, essays, and short stories before his death in Geneva in June 1986. In 1961 Borges shared the International Publishers’ prize with Samuel Beckett. The Ingram Merrill Foundation granted him its Annual Literary Award in 1966 for his “outstanding contribution to literature.” In 1971 Colu\n"
     ]
    }
   ],
   "source": [
    "print(data[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(text):\n",
    "    '''cleans the text of escape chars and the like.'''\n",
    "    stripped = text.strip(' ')\n",
    "    lowered = stripped.lower()\n",
    "    cleaned = re.sub(r\"[^a-zA-Z ^0-9 \\. , ']\", ' ', lowered)\n",
    "    despaced = cleaned.strip(' ')\n",
    "    return despaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the data\n",
    "clean_data = cleaner(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'penguin books      collected fictions      jorge luis borges was born in buenos aires in 1899 and was educated in europe. one of the most widely acclaimed writers of our time, he published many collections of poems, essays, and short stories before his death in geneva in june 1986. in 1961 borges shared the international publishers  prize with samuel beckett. the ingram merrill foundation granted '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that things look alright\n",
    "clean_data[:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26646"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a list of all unique words, check how many\n",
    "wordlist = clean_data.split(' ')\n",
    "wordset = list(set(clean_data.split(' ')))\n",
    "len(wordset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of words below a certain frequency\n",
    "freqdict = {word:wordlist.count(word) for word in wordset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3723"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqdict['i']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "smaller_list = [ word for word in wordlist if (freqdict[word] >= 10) and (word != '') ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2246"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(smaller_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_wordset = list(set(smaller_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode words as ints\n",
    "\n",
    "words_to_ints = {w:i for i,w in enumerate(small_wordset)}\n",
    "ints_to_words = {i:w for i,w in enumerate(small_wordset)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1399"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_to_ints['and']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Sequence Data\n",
    "\n",
    "maxlen = 40\n",
    "step = 5\n",
    "\n",
    "encoded = [words_to_ints[w] for w in smaller_list]\n",
    "\n",
    "sequences = []\n",
    "next_words = []\n",
    "\n",
    "for ii in range(0, len(encoded)-maxlen, step):\n",
    "    sequences.append(encoded[ii : ii+maxlen])\n",
    "    next_words.append(encoded[ii+maxlen])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172530"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(smaller_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify x & y\n",
    "\n",
    "x = np.zeros((len(sequences), maxlen, len(small_wordset)), dtype=np.bool)\n",
    "y = np.zeros((len(sequences), len(small_wordset)), dtype=np.bool)\n",
    "\n",
    "for ii, sequence in enumerate(sequences):\n",
    "    for t, word in enumerate(sequence):\n",
    "        x[ii,t,word] = 1\n",
    "        \n",
    "    y[ii, next_words[ii]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34498, 2246)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model: a single LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(small_wordset))))\n",
    "model.add(Dense(len(small_wordset), activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(seed, num_words, diversity):\n",
    "    clean_seed = cleaner(seed)\n",
    "    seed_list = clean_seed.split(' ')\n",
    "    \n",
    "    maxlen = 40\n",
    "    step = 1\n",
    "\n",
    "    encoded = [words_to_ints[w] for w in seed_list]\n",
    "    to_pad = 40 - len(encoded)\n",
    "    \n",
    "    if len(encoded) < 40:\n",
    "        zeros = [0] * to_pad\n",
    "        sequence = zeros + encoded\n",
    "    elif len(encoded) > 40:\n",
    "        sequence = encoded[-40:]\n",
    "    else:\n",
    "        sequence = encoded\n",
    "        \n",
    "\n",
    "    for ii in range(0,num_words):\n",
    "        \n",
    "        sequence_last40 = sequence[-40:]\n",
    "        \n",
    "        x_pred = np.zeros((1, maxlen, len(small_wordset)))\n",
    "        for t, word in enumerate(sequence_last40):\n",
    "            x_pred[0, t, word] = 1.\n",
    "\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_word = sample(preds, diversity)\n",
    "\n",
    "        sequence.append(next_word)\n",
    "        \n",
    "    output = [ints_to_words[w] for w in sequence][to_pad:]\n",
    "    str_out = ' '.join(output)\n",
    "    return str_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(smaller_list) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        \n",
    "        input_list = smaller_list[start_index: start_index + maxlen]\n",
    "        input_sentence = ' '.join(input_list)\n",
    "        print('----- Generating with seed: \"' + input_sentence + '\"')\n",
    "        output = generate_text(input_sentence, 20, diversity)\n",
    "\n",
    "            \n",
    "        print(output)\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 34498 samples\n",
      "Epoch 1/20\n",
      "34448/34498 [============================>.] - ETA: 0s - loss: 5.8778\n",
      "----- Generating text after Epoch: 0\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"of the boys that were aside to make way for him, and like we were all the first of pure eye all eyes opened out like a and but that about to last. in the just behind those first the\"\n",
      "of the boys that were aside to make way for him, and like we were all the first of pure eye all eyes opened out like a and but that about to last. in the just behind those first the of the of the of the of the of the of the of the of the of the of the\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"of the boys that were aside to make way for him, and like we were all the first of pure eye all eyes opened out like a and but that about to last. in the just behind those first the\"\n",
      "of the boys that were aside to make way for him, and like we were all the first of pure eye all eyes opened out like a and but that about to last. in the just behind those first the of of the of the the of the of the of the but of the are of the feel the\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"of the boys that were aside to make way for him, and like we were all the first of pure eye all eyes opened out like a and but that about to last. in the just behind those first the\"\n",
      "of the boys that were aside to make way for him, and like we were all the first of pure eye all eyes opened out like a and but that about to last. in the just behind those first the added of with which and of the out of the under the else the bogle single as a he is\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"of the boys that were aside to make way for him, and like we were all the first of pure eye all eyes opened out like a and but that about to last. in the just behind those first the\"\n",
      "of the boys that were aside to make way for him, and like we were all the first of pure eye all eyes opened out like a and but that about to last. in the just behind those first the english far fierro day, upon with men back. its myself, late our from arrived certainty and i filled with without\n",
      "34498/34498 [==============================] - 35s 1ms/sample - loss: 5.8773\n",
      "Epoch 2/20\n",
      "34464/34498 [============================>.] - ETA: 0s - loss: 5.5257\n",
      "----- Generating text after Epoch: 1\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"on the light. i took out the impossible book and turned its pages. on one, i saw an of a there was a number in the corner of the page i don't remember now what it was raised to the\"\n",
      "on the light. i took out the impossible book and turned its pages. on one, i saw an of a there was a number in the corner of the page i don't remember now what it was raised to the of the of the of the of the of the of the of the of the of the of the\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"on the light. i took out the impossible book and turned its pages. on one, i saw an of a there was a number in the corner of the page i don't remember now what it was raised to the\"\n",
      "on the light. i took out the impossible book and turned its pages. on one, i saw an of a there was a number in the corner of the page i don't remember now what it was raised to the of the of the and by the of the of the and we saw the of the of the of\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"on the light. i took out the impossible book and turned its pages. on one, i saw an of a there was a number in the corner of the page i don't remember now what it was raised to the\"\n",
      "on the light. i took out the impossible book and turned its pages. on one, i saw an of a there was a number in the corner of the page i don't remember now what it was raised to the brick death of the place the of the of want the of on the from of the first of her\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"on the light. i took out the impossible book and turned its pages. on one, i saw an of a there was a number in the corner of the page i don't remember now what it was raised to the\"\n",
      "on the light. i took out the impossible book and turned its pages. on one, i saw an of a there was a number in the corner of the page i don't remember now what it was raised to the so crossed my espinosa was his room. myself el that garden was a your then many are few stories of\n",
      "34498/34498 [==============================] - 32s 924us/sample - loss: 5.5253\n",
      "Epoch 3/20\n",
      "34480/34498 [============================>.] - ETA: 0s - loss: 5.2312\n",
      "----- Generating text after Epoch: 2\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"the of in the last volume of the she found this it is written in an english that with this is a of the i as i recall, my began in a garden in hundred in the time of the\"\n",
      "the of in the last volume of the she found this it is written in an english that with this is a of the i as i recall, my began in a garden in hundred in the time of the of the of the of the of the of the of the of the of the of the of the\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"the of in the last volume of the she found this it is written in an english that with this is a of the i as i recall, my began in a garden in hundred in the time of the\"\n",
      "the of in the last volume of the she found this it is written in an english that with this is a of the i as i recall, my began in a garden in hundred in the time of the the of the were and the of the and the of the and the of the and in the of\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"the of in the last volume of the she found this it is written in an english that with this is a of the i as i recall, my began in a garden in hundred in the time of the\"\n",
      "the of in the last volume of the she found this it is written in an english that with this is a of the i as i recall, my began in a garden in hundred in the time of the of 3 the century of than cover i never runs sort but it is giving to it. it have is\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"the of in the last volume of the she found this it is written in an english that with this is a of the i as i recall, my began in a garden in hundred in the time of the\"\n",
      "the of in the last volume of the she found this it is written in an english that with this is a of the i as i recall, my began in a garden in hundred in the time of the was the who house ever more at darkness high ways less first line ancient after add truth don or that\n",
      "34498/34498 [==============================] - 32s 920us/sample - loss: 5.2312\n",
      "Epoch 4/20\n",
      "34464/34498 [============================>.] - ETA: 0s - loss: 4.9725\n",
      "----- Generating text after Epoch: 3\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"darkness, unwin heard from his lips the story of ibn death. what well may be my memory, dunraven is ibn hakam on the at he was followed by a black man with a the first black man and the first\"\n",
      "darkness, unwin heard from his lips the story of ibn death. what well may be my memory, dunraven is ibn hakam on the at he was followed by a black man with a the first black man and the first of the of the of the of the of the of the and the of the and the of the\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"darkness, unwin heard from his lips the story of ibn death. what well may be my memory, dunraven is ibn hakam on the at he was followed by a black man with a the first black man and the first\"\n",
      "darkness, unwin heard from his lips the story of ibn death. what well may be my memory, dunraven is ibn hakam on the at he was followed by a black man with a the first black man and the first the of the of the of the of the of the of the he was a of and and a\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"darkness, unwin heard from his lips the story of ibn death. what well may be my memory, dunraven is ibn hakam on the at he was followed by a black man with a the first black man and the first\"\n",
      "darkness, unwin heard from his lips the story of ibn death. what well may be my memory, dunraven is ibn hakam on the at he was followed by a black man with a the first black man and the first brodie's volumes with his he was not to because surprised the and al outskirts in the of a pulled the\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"darkness, unwin heard from his lips the story of ibn death. what well may be my memory, dunraven is ibn hakam on the at he was followed by a black man with a the first black man and the first\"\n",
      "darkness, unwin heard from his lips the story of ibn death. what well may be my memory, dunraven is ibn hakam on the at he was followed by a black man with a the first black man and the first heads and is all the it was alejandro of a little and , the suddenly in remained deal then out\n",
      "34498/34498 [==============================] - 33s 968us/sample - loss: 4.9716\n",
      "Epoch 5/20\n",
      "34496/34498 [============================>.] - ETA: 0s - loss: 4.7064\n",
      "----- Generating text after Epoch: 4\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"and set off on a and silent it had the were and it was sometime around five in the morning when they finally reached n. there, they woke up the of a and offered to sell her the deal was\"\n",
      "and set off on a and silent it had the were and it was sometime around five in the morning when they finally reached n. there, they woke up the of a and offered to sell her the deal was a of the and the of the and the of the and the of the and the of the and\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"and set off on a and silent it had the were and it was sometime around five in the morning when they finally reached n. there, they woke up the of a and offered to sell her the deal was\"\n",
      "and set off on a and silent it had the were and it was sometime around five in the morning when they finally reached n. there, they woke up the of a and offered to sell her the deal was to him and and he saw his he was a in a miracle men again and the then the of\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"and set off on a and silent it had the were and it was sometime around five in the morning when they finally reached n. there, they woke up the of a and offered to sell her the deal was\"\n",
      "and set off on a and silent it had the were and it was sometime around five in the morning when they finally reached n. there, they woke up the of a and offered to sell her the deal was better at the queen he day, on the the hatred abandoned this than the which had lazarus discovered that is\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"and set off on a and silent it had the were and it was sometime around five in the morning when they finally reached n. there, they woke up the of a and offered to sell her the deal was\"\n",
      "and set off on a and silent it had the were and it was sometime around five in the morning when they finally reached n. there, they woke up the of a and offered to sell her the deal was after a then, those writing by his thought from for the if should all make sang particularly another, with and\n",
      "34498/34498 [==============================] - 33s 951us/sample - loss: 4.7063\n",
      "Epoch 6/20\n",
      "34496/34498 [============================>.] - ETA: 0s - loss: 4.4295\n",
      "----- Generating text after Epoch: 5\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"well it's it seems to me, to imagine him the entire treasure than taking the time to part of it. perhaps no odd coins were found lying about because there were no coins left perhaps the had a treasure which,\"\n",
      "well it's it seems to me, to imagine him the entire treasure than taking the time to part of it. perhaps no odd coins were found lying about because there were no coins left perhaps the had a treasure which, like the and the of the and the of the and the of the and the of the and the\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"well it's it seems to me, to imagine him the entire treasure than taking the time to part of it. perhaps no odd coins were found lying about because there were no coins left perhaps the had a treasure which,\"\n",
      "well it's it seems to me, to imagine him the entire treasure than taking the time to part of it. perhaps no odd coins were found lying about because there were no coins left perhaps the had a treasure which, perhaps the the world was in the the of the and the of the and the of the and the\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"well it's it seems to me, to imagine him the entire treasure than taking the time to part of it. perhaps no odd coins were found lying about because there were no coins left perhaps the had a treasure which,\"\n",
      "well it's it seems to me, to imagine him the entire treasure than taking the time to part of it. perhaps no odd coins were found lying about because there were no coins left perhaps the had a treasure which, memorable do all if i cannot plains neither allowed to the least kept the half homer i will have perceive\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"well it's it seems to me, to imagine him the entire treasure than taking the time to part of it. perhaps no odd coins were found lying about because there were no coins left perhaps the had a treasure which,\"\n",
      "well it's it seems to me, to imagine him the entire treasure than taking the time to part of it. perhaps no odd coins were found lying about because there were no coins left perhaps the had a treasure which, le spite dahlmann say, ever past against and don horse, to my there, to soldiers . truly five under him\n",
      "34498/34498 [==============================] - 33s 944us/sample - loss: 4.4296\n",
      "Epoch 7/20\n",
      "34480/34498 [============================>.] - ETA: 0s - loss: 4.1404\n",
      "----- Generating text after Epoch: 6\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"the real and with the dreamed or the rather, was one of the shapes the dream it seemed impossible that the earth should be anything but and forms of every hundred steps a tower cut the air, to the their\"\n",
      "the real and with the dreamed or the rather, was one of the shapes the dream it seemed impossible that the earth should be anything but and forms of every hundred steps a tower cut the air, to the their and the the the the the man was to the the of the the the the the the the the\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"the real and with the dreamed or the rather, was one of the shapes the dream it seemed impossible that the earth should be anything but and forms of every hundred steps a tower cut the air, to the their\"\n",
      "the real and with the dreamed or the rather, was one of the shapes the dream it seemed impossible that the earth should be anything but and forms of every hundred steps a tower cut the air, to the their and not the work of the and the of the and the of the and the of the the of\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"the real and with the dreamed or the rather, was one of the shapes the dream it seemed impossible that the earth should be anything but and forms of every hundred steps a tower cut the air, to the their\"\n",
      "the real and with the dreamed or the rather, was one of the shapes the dream it seemed impossible that the earth should be anything but and forms of every hundred steps a tower cut the air, to the their and they from the long 3 with its since our were in fact many there was in face and that\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"the real and with the dreamed or the rather, was one of the shapes the dream it seemed impossible that the earth should be anything but and forms of every hundred steps a tower cut the air, to the their\"\n",
      "the real and with the dreamed or the rather, was one of the shapes the dream it seemed impossible that the earth should be anything but and forms of every hundred steps a tower cut the air, to the their shadow i set riding near saying the of stories man the to memory i am to the going to the\n",
      "34498/34498 [==============================] - 32s 934us/sample - loss: 4.1407\n",
      "Epoch 8/20\n",
      "34480/34498 [============================>.] - ETA: 0s - loss: 3.8328- ETA: 1s - loss: 3\n",
      "----- Generating text after Epoch: 7\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"write it, you will think that you're a tale of and it won't be either it will be many years from now. he stopped talking i realized that he had died. in a way, i died with him in i\"\n",
      "write it, you will think that you're a tale of and it won't be either it will be many years from now. he stopped talking i realized that he had died. in a way, i died with him in i was a of a and a of the and a of the men whose was the black and and the\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"write it, you will think that you're a tale of and it won't be either it will be many years from now. he stopped talking i realized that he had died. in a way, i died with him in i\"\n",
      "write it, you will think that you're a tale of and it won't be either it will be many years from now. he stopped talking i realized that he had died. in a way, i died with him in i was a few of the house i from my i have been surprised in my i turned out a of\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"write it, you will think that you're a tale of and it won't be either it will be many years from now. he stopped talking i realized that he had died. in a way, i died with him in i\"\n",
      "write it, you will think that you're a tale of and it won't be either it will be many years from now. he stopped talking i realized that he had died. in a way, i died with him in i fell the night one of my long got come of april on my own horses, nothing it, you go with\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"write it, you will think that you're a tale of and it won't be either it will be many years from now. he stopped talking i realized that he had died. in a way, i died with him in i\"\n",
      "write it, you will think that you're a tale of and it won't be either it will be many years from now. he stopped talking i realized that he had died. in a way, i died with him in i with that air, a woman and between the life, of long immediately his speaks large on the he'd sense first\n",
      "34498/34498 [==============================] - 33s 951us/sample - loss: 3.8327\n",
      "Epoch 9/20\n",
      "21520/34498 [=================>............] - ETA: 11s - loss: 3.4738\n",
      "----- Generating text after Epoch: 8\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"die, at any moment i may into that which is unknown to me, and still i dream these dreams of my that subject i got from and i sensed that the of name was a not some of i was\"\n",
      "die, at any moment i may into that which is unknown to me, and still i dream these dreams of my that subject i got from and i sensed that the of name was a not some of i was a in a in a in a a few ago, i don't i know i am almost a dream. it\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"die, at any moment i may into that which is unknown to me, and still i dream these dreams of my that subject i got from and i sensed that the of name was a not some of i was\"\n",
      "die, at any moment i may into that which is unknown to me, and still i dream these dreams of my that subject i got from and i sensed that the of name was a not some of i was in a of a and i i recall i felt that was almost in the i have the story the\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"die, at any moment i may into that which is unknown to me, and still i dream these dreams of my that subject i got from and i sensed that the of name was a not some of i was\"\n",
      "die, at any moment i may into that which is unknown to me, and still i dream these dreams of my that subject i got from and i sensed that the of name was a not some of i was able to speak in but i numbers left entire and with the i shall possess the explanation for him you\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"die, at any moment i may into that which is unknown to me, and still i dream these dreams of my that subject i got from and i sensed that the of name was a not some of i was\"\n",
      "die, at any moment i may into that which is unknown to me, and still i dream these dreams of my that subject i got from and i sensed that the of name was a not some of i was horse, a no one when capable the bit and some of them money days that were crossed to gathered and\n",
      "21584/34498 [=================>............] - ETA: 12s - loss: 3.4730"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-f66d6547f574>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m           callbacks=[print_callback])\n\u001b[0m",
      "\u001b[1;32m~\\.virtualenvs\\DS-Unit-4-Sprint-3-Deep-Learning-sI3BcHf7\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\.virtualenvs\\DS-Unit-4-Sprint-3-Deep-Learning-sI3BcHf7\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.virtualenvs\\DS-Unit-4-Sprint-3-Deep-Learning-sI3BcHf7\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.virtualenvs\\DS-Unit-4-Sprint-3-Deep-Learning-sI3BcHf7\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.virtualenvs\\DS-Unit-4-Sprint-3-Deep-Learning-sI3BcHf7\\lib\\site-packages\\tensorflow_core\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.virtualenvs\\DS-Unit-4-Sprint-3-Deep-Learning-sI3BcHf7\\lib\\site-packages\\tensorflow_core\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.virtualenvs\\DS-Unit-4-Sprint-3-Deep-Learning-sI3BcHf7\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36m_non_none_constant_value\u001b[1;34m(v)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_non_none_constant_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m   \u001b[0mconstant_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant_value\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mconstant_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.virtualenvs\\DS-Unit-4-Sprint-3-Deep-Learning-sI3BcHf7\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_util.py\u001b[0m in \u001b[0;36mconstant_value\u001b[1;34m(tensor, partial)\u001b[0m\n\u001b[0;32m    820\u001b[0m   \"\"\"\n\u001b[0;32m    821\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 822\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    823\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.virtualenvs\\DS-Unit-4-Sprint-3-Deep-Learning-sI3BcHf7\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    940\u001b[0m     \"\"\"\n\u001b[0;32m    941\u001b[0m     \u001b[1;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 942\u001b[1;33m     \u001b[0mmaybe_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    943\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.virtualenvs\\DS-Unit-4-Sprint-3-Deep-Learning-sI3BcHf7\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    906\u001b[0m     \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 908\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    909\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    910\u001b[0m       \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(x, y,\n",
    "          batch_size=16,\n",
    "          epochs=20,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the knife of the gaucho lay in the sand j which is to yet borges. everyone can the way of the and then the work of the page of the century, the of the still a in the men poets by the man who was in the volume, the of the translation of the secret and the are word'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text('the knife of the gaucho lay in the sand',50, 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zE4a4O7Bp5x1"
   },
   "source": [
    "# Resources and Stretch Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uT3UV3gap9H6"
   },
   "source": [
    "## Stretch goals:\n",
    "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
    "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
    "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
    "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
    "- Run on bigger, better data\n",
    "\n",
    "## Resources:\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
    "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
    "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
    "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
    "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "U4S3, Deep Learning 2",
   "language": "python",
   "name": "u4s3-deep_learning_20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
