{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 3 Assignment 1*\n",
    "\n",
    "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
    "\n",
    "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
    "\n",
    "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
    "\n",
    "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
    "\n",
    "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
    "\n",
    "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
    "\n",
    "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'Data/100-0.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open(path, 'r', encoding='utf8') as f:\n",
    "    data.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' THE SONNETS                     1  From fairest creatures we desire increase, That thereby beauty’s rose might never die, But as the riper should by time decease, His tender heir might bear his memory: But thou contracted to thine own bright eyes, Feed’st thy light’s flame with self-substantial fuel, Making a famine where abundance lies, Thy self thy foe, to thy sweet self too cruel: Thou that art now the world’s fresh ornament, And only herald to the gaudy spring, Within thine own bud buriest '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [re.sub('\\ufeff', ' ', i) for i in data]\n",
    "data = [re.sub('\\n', ' ', i) for i in data]\n",
    "data = [re.sub(r\"\\'\", \"'\", i) for i in data]\n",
    "data[0][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' THE SONNETS                     1  From fairest creatures we desire increase, That thereby beauty’s rose might never die, But as the riper should by time decease, His tender heir might bear his memory: But thou contracted to thine own bright eyes, Feed’st thy light’s flame with self-substantial fuel, Making a famine where abundance lies, Thy self thy foe, to thy sweet self too cruel: Thou that art now the world’s fresh ornament, And only herald to the gaudy spring, Within thine own bud buriest '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "giant_string = ' '.join(data)\n",
    "giant_string[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = list(set(giant_string))\n",
    "\n",
    "char_int = {c:i for i,c in enumerate(chars)}\n",
    "int_char = {i:c for i,c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_char = int_char\n",
    "char_indices = char_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences:  1096509\n"
     ]
    }
   ],
   "source": [
    "maxlen = 40\n",
    "step = 5\n",
    "\n",
    "encoded = [char_int[c] for c in giant_string]\n",
    "\n",
    "sequences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(encoded) - maxlen, step):\n",
    "    sequences.append(encoded[i: i + maxlen])\n",
    "    next_chars.append(encoded[i + maxlen])\n",
    "\n",
    "print('sequences: ', len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sequences), len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, char in enumerate(sequence):\n",
    "        x[i, t, char] = 1\n",
    "\n",
    "    y[i, next_chars[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1096509, 40, 99), (1096509, 99))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = giant_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "optimizer = RMSprop(learning_rate=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen)\n",
    "    for diversity in [0.2, 0.5, 0.75, 1.0]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(500):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1096192/1096509 [============================>.] - ETA: 0s - loss: 1.7003\n",
      "----- Generating text after Epoch: 0\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"t a verdict?   ALL. No more talking on't\"\n",
      "t a verdict?   ALL. No more talking on't the words the words     That the compand and the words the sweet the with of the word.     The good the wittle shall be such be the words     And the wittle the witch and be the would be my lord.     And a prince the what is my soul and the word of the words     That the would speak of the words and the words     That shall be so such be the word of the word.     The would be should be the would be a prince of the way the weach a boy.     The would be such my lord and the witting the way the wo\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"t a verdict?   ALL. No more talking on't\"\n",
      "t a verdict?   ALL. No more talking on't and fair bear the word.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
      "----- diversity: 0.75\n",
      "----- Generating with seed: \"t a verdict?   ALL. No more talking on't\"\n",
      "t a verdict?   ALL. No more talking on't.  She you are much and this bestill under thee.   EYLLAIUS. Speak and that the trisction of you have drink,     And me death hath stroke and the well,     And     What is the wood a being Greasures,     And lies you must have we hear you proteer’s full;     And gain my hears to be in the Diving,     My brother ale not to graign with the weach thy fears.   WARWICK. And to do my hearning father of a man.     Went castle be he shall be aloud all our faction.   PROTEunisul and Masters.     To be a \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"t a verdict?   ALL. No more talking on't\"\n",
      "t a verdict?   ALL. No more talking on'ty, and sluel horsesoo morions!  NORT. Marry name well, be sun     Let     That the intrume or 'tis a false he     That witnimser? O peapation, the Enterraf Sir Caunely hands. Wher brusurant,     When my how, be provereat min’s Carrs of in         extruction,     That deayant with madam on his actir’s abply, Their cheeked in Rowards, for what and as is crow nether stripces. Com’d hailcher! The Speakt the will.  FALSTAFF. What us you happ own,     In well the withstrys be lively     Weet When they\n",
      "1096509/1096509 [==============================] - 244s 223us/sample - loss: 1.7002\n",
      "Epoch 2/4\n",
      "1096192/1096509 [============================>.] - ETA: 0s - loss: 1.5727\n",
      "----- Generating text after Epoch: 1\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"mind to it.  BASSANIO. There’s more depe\"\n",
      "mind to it.  BASSANIO. There’s more depent the court the fairs     The hand of the streage of the ground     The speak the friends the strange the strange     The groan and the the strength the strange     The strange the strange the strange the strange     The strange of the thing the ground the strange     The streath the foul the strange the strange     The land of the man shall be the strange that the strange     The thing the strange the star'd the straight the court and the court the man to the stroke the proud of the strange.  \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"mind to it.  BASSANIO. There’s more depe\"\n",
      "mind to it.  BASSANIO. There’s more depert.     The thing in the Parters all the arm.     Where is she will not should the strong strange The King     The earlian than you have not be her foul of his speak     But     Here shall be the strange to the the use the marking and the blood,     To at her may that love shall fall the court and disposity, When the worthy report of his dead of my stranges.     The thought the blood grace.     My proud of the spection in princely                                                                  \n",
      "----- diversity: 0.75\n",
      "----- Generating with seed: \"mind to it.  BASSANIO. There’s more depe\"\n",
      "mind to it.  BASSANIO. There’s more depert.   BEROLIUS. It is not the grian passions and the sun Durstersefell than be sun their astain’d that     Mock that he hath my revence of thy graves. be a shoudd heap the word of some dargas.     Where with my lord, and the reward may Have goodn a acking the blooding should have no woron As have should these much which is the thought     Foo the hall her Sillian That the look.     The such a fleat used this offite for me what is proof the man bard.     Not I do be so, you aspent thee, denement \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"mind to it.  BASSANIO. There’s more depe\"\n",
      "mind to it.  BASSANIO. There’s more depertiur not be thy content.     Thought the matter fail haste with nothing! Grough thus say ashied     Thou hast ashap'd And distune for his why aut I am she had sun. The trie. How gentle and in the bed or Last     God!   [To raph'd Sir.]  Have calized in from them in nockerio?  DESDEMONA. By his commanods. I'll the nature mands sweet the groancon. I do that on! Hell a busy the dialf Tunkividg may the death; A prepur'd in fliedon on the near.   CORIAN. To shall say howst new hungulles pardre, Hout\n",
      "1096509/1096509 [==============================] - 245s 223us/sample - loss: 1.5727\n",
      "Epoch 3/4\n",
      "1096192/1096509 [============================>.] - ETA: 0s - loss: 1.5293\n",
      "----- Generating text after Epoch: 2\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"STARD. I am more bound to you than your \"\n",
      "STARD. I am more bound to you than your lord.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"STARD. I am more bound to you than your \"\n",
      "STARD. I am more bound to you than your lord, I will be stand                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
      "----- diversity: 0.75\n",
      "----- Generating with seed: \"STARD. I am more bound to you than your \"\n",
      "STARD. I am more bound to you than your falls.  or ciens     Will perided sorry         And let on him how is the dead;     So they are them, he is longs and York that with drumit,     Not yet surms in my lovelt of a moneyes that I was a mind; in like a son, and Alone.  BENTROER. God a word-wal'd so     and a risonian parts, to know you can shall and stand all over doth make.  SCENE II. Poor Lace in Outide WOuntember. MARCIUS. I cannot be proach to love, be sir,     The start of you have we hand and we may long beyess up the swift it \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"STARD. I am more bound to you than your \"\n",
      "STARD. I am more bound to you than your shake, And skils follous counnate in our Flound the fail regicouss and hope Lonterrapon fear upon yet and in the earth   ANS GOGER. Well those driunge of any are dayled death.   GLOUCESTER. Sir, go     Bread soversame ity often 'emate, The Macbers of her not change To Dirmit false indo as I says? Ay.   SIRIA. Somet stand you what here?  COUNTES. Yea, you in do milgerin sake cansure, Edway? , debent deveng'dness. Away, heaventenes!  PUCELLA. He providected good war look   ISADIUS. He was whow let\n",
      "1096509/1096509 [==============================] - 244s 222us/sample - loss: 1.5293\n",
      "Epoch 4/4\n",
      "1096192/1096509 [============================>.] - ETA: 0s - loss: 1.5055\n",
      "----- Generating text after Epoch: 3\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"e, And tell his wife that, being lunatic\"\n",
      "e, And tell his wife that, being lunatic with the son to the world     And the strife to make the faith of the faiths of the strange.     The stand to the to the world to make the fair and the son of the strange.     The sail to the son to the strange to the stand of the words     That we have the strange to the stand to the stand.     The season to the fall and the stand of the strange.     Now thou did he be the stand to the son of the strange to the soul to the world     The stand to the still to heard the strange to stand to stand\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"e, And tell his wife that, being lunatic\"\n",
      "e, And tell his wife that, being lunatic sum,     And there this seems and the beguaser and tender the tribune him.     Sir, I were not by the King that in my father, The words, and hath mear like a good by the dividess     The man; and for the life is the more leve.                                                                                                                                                                                                                                                                                 \n",
      "----- diversity: 0.75\n",
      "----- Generating with seed: \"e, And tell his wife that, being lunatic\"\n",
      "e, And tell his wife that, being lunatic joy him,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:4: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " brook me for him, But in her almost for her did, and host to so be I began, Lood you was sent; and the fury the part for the fee.     Let it on me she res 'twist him of thy press.     In man save me for the soldiers and time To wounding good weard to pearly of thy rances.     Marry, and no was wear his frow.   CLEOPH. To go of the applamon of shadow and you have him? O Farewell, have progressies of his where’st no best decain,     In thy hearty parts of men in the ground.   BENT. Then \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"e, And tell his wife that, being lunatic\"\n",
      "e, And tell his wife that, being lunatic wretch a’en, When she did well sawy to laugh and boy'd and weep. If you shall ge:     And never hold hy after for lands, wrong     All pand, a Adices is the wonding of city,     Like this daughter of this else; Te! I was her is laption we defecuid to him,     keep !   The bagions kill shalt the judging to this?   TITUS. [Myselp, the PISCOOO? ARVITbian that Gentlefock.]  ’RACHINES. Knatk? I disbervers the follows, can in you, time To sure he cenvy     Poins not of you are pardieste re- You a tru\n",
      "1096509/1096509 [==============================] - 244s 222us/sample - loss: 1.5055\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb2d067cc50>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y,\n",
    "          batch_size=512,\n",
    "          epochs=4,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zE4a4O7Bp5x1"
   },
   "source": [
    "# Resources and Stretch Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uT3UV3gap9H6"
   },
   "source": [
    "## Stretch goals:\n",
    "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
    "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
    "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
    "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
    "- Run on bigger, better data\n",
    "\n",
    "## Resources:\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
    "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
    "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
    "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
    "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
