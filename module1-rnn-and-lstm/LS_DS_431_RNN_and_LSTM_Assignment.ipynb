{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 3 Assignment 1*\n",
    "\n",
    "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
    "\n",
    "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
    "\n",
    "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
    "\n",
    "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
    "\n",
    "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
    "\n",
    "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
    "\n",
    "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ltj1je1fp5rO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "import newspaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "will = urllib.request.urlopen('https://www.gutenberg.org/files/100/100-0.txt').read()[2966:]\n",
    "sonnet = will[0:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sonnet = sonnet.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From fairest creatures we desire increase,\n",
      "That thereby beauty’s rose might never die,\n",
      "But as the riper should by time decease,\n",
      "His tender heir might bear his memory:\n",
      "But thou contracted to thine own bright eyes,\n",
      "Feed’st thy light’s flame with self-substantial fuel,\n",
      "Making a famine where abundance lies,\n",
      "Thy self thy foe, to thy sweet self too cruel:\n",
      "Thou that art now the world’s fresh ornament,\n",
      "And only herald to the gaudy spring,\n",
      "Within thine own bud buriest thy content,\n",
      "And, tender churl, mak’st waste in niggarding:\n",
      "  Pity the world, or else this glutton be,\n",
      "  To eat the world’s due, by the grave and thee.\n",
      "\n",
      "\n",
      "                    2\n",
      "\n",
      "When forty winters shall besiege thy brow,\n",
      "And dig deep trenches in thy beauty’s field,\n",
      "Thy youth’s proud livery so gazed on now,\n",
      "Will be a tattered weed of small worth held:\n",
      "Then being asked, where all thy beauty lies,\n",
      "Where all the treasure of thy lusty days;\n",
      "To say, within thine own deep sunken eyes,\n",
      "Were an all-eating shame, and thriftless praise.\n",
      "How much more praise deserv’d thy beauty’s use,\n",
      "If thou couldst answer ‘This fair child of mine\n",
      "Shall sum my count, and make my old excuse,’\n",
      "Proving his beauty by succession thine.\n",
      "  This were to be new made when thou art old,\n",
      "  And see thy blood warm when thou feel’st it cold.\n",
      "\n",
      "\n",
      "                    3\n",
      "\n",
      "Look in thy glass and tell the face thou viewest,\n",
      "Now is the time that face should form another,\n",
      "Whose fresh repair if now thou not renewest,\n",
      "Thou dost beguile the world, unbless some mother.\n",
      "For where is she so fair whose uneared womb\n",
      "Disdains the tillage of thy husbandry?\n",
      "Or who is he so fond will be the tomb\n",
      "Of his self-love to stop posterity?\n",
      "Thou art thy mother’s glass and she in thee\n",
      "Calls back the lovely April of her prime,\n",
      "So thou through windows of thine age shalt see,\n",
      "Despite of wrinkles this thy golden time.\n",
      "  But if thou live remembered not to be,\n",
      "  Die single and thine image dies with thee.\n",
      "\n",
      "\n",
      "                    4\n",
      "\n",
      "Unthrifty loveliness why dost thou spend,\n",
      "Upon thy self thy beauty’s legacy?\n",
      "Nature’s bequest gives nothing but doth lend,\n",
      "And being frank she lends to those are free:\n",
      "Then beauteous niggard why dost thou abuse,\n",
      "The bounteous largess given thee to give?\n",
      "Profitless usurer why dost thou use\n",
      "So great a sum of sums yet canst not live?\n",
      "For having traffic with thy self alone,\n",
      "Thou of thy self thy sweet self dost deceive,\n",
      "Then how when nature calls thee to be gone,\n",
      "What acceptable audit canst thou leave?\n",
      "  Thy unused beauty must be tombed with thee,\n",
      "  Which used lives th’ executor to be.\n",
      "\n",
      "\n",
      "                    5\n",
      "\n",
      "Those hours that with gentle work did frame\n",
      "The lovely gaze where every eye doth dwell\n",
      "Will play the tyrants to the very same,\n",
      "And that unfair which fairly doth excel:\n",
      "For never-resting time leads summer on\n",
      "To hideous winter and confounds him there,\n",
      "Sap checked with frost and lusty leaves quite gone,\n",
      "Beauty o’er-snowed and bareness eve\n"
     ]
    }
   ],
   "source": [
    "# test.strip()\n",
    "print(sonnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique characters :  56\n",
      "txt_data_size :  2964\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "chars = list(set(sonnet)) # split and remove duplicate characters. convert to list.\n",
    "\n",
    "num_chars = len(chars) # the number of unique characters\n",
    "txt_data_size = len(sonnet)\n",
    "\n",
    "print(\"unique characters : \", num_chars)\n",
    "print(\"txt_data_size : \", txt_data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 0, 'z': 1, ';': 2, 'e': 3, 'p': 4, 'U': 5, 'I': 6, 'm': 7, 'N': 8, 'w': 9, 'k': 10, 'b': 11, 'M': 12, ',': 13, 'x': 14, 'L': 15, '2': 16, 'l': 17, '5': 18, '4': 19, ' ': 20, 'f': 21, 'd': 22, '?': 23, 'n': 24, 'u': 25, '-': 26, '\\r': 27, 'g': 28, 'F': 29, '.': 30, 'B': 31, 'i': 32, 't': 33, 'r': 34, 'c': 35, 'D': 36, 'v': 37, '‘': 38, 'W': 39, 'S': 40, 'T': 41, 'y': 42, ':': 43, 'P': 44, 'C': 45, 'O': 46, '3': 47, 'o': 48, 'q': 49, '’': 50, 's': 51, 'h': 52, 'H': 53, '\\n': 54, 'a': 55}\n",
      "----------------------------------------------------\n",
      "{0: 'A', 1: 'z', 2: ';', 3: 'e', 4: 'p', 5: 'U', 6: 'I', 7: 'm', 8: 'N', 9: 'w', 10: 'k', 11: 'b', 12: 'M', 13: ',', 14: 'x', 15: 'L', 16: '2', 17: 'l', 18: '5', 19: '4', 20: ' ', 21: 'f', 22: 'd', 23: '?', 24: 'n', 25: 'u', 26: '-', 27: '\\r', 28: 'g', 29: 'F', 30: '.', 31: 'B', 32: 'i', 33: 't', 34: 'r', 35: 'c', 36: 'D', 37: 'v', 38: '‘', 39: 'W', 40: 'S', 41: 'T', 42: 'y', 43: ':', 44: 'P', 45: 'C', 46: 'O', 47: '3', 48: 'o', 49: 'q', 50: '’', 51: 's', 52: 'h', 53: 'H', 54: '\\n', 55: 'a'}\n",
      "----------------------------------------------------\n",
      "[29, 34, 48, 7, 20, 21, 55, 32, 34, 3, 51, 33, 20, 35, 34, 3, 55, 33, 25, 34, 3, 51, 20, 9, 3, 20, 22, 3, 51, 32, 34, 3, 20, 32, 24, 35, 34, 3, 55, 51, 3, 13, 27, 54, 41, 52, 55, 33, 20, 33, 52, 3, 34, 3, 11, 42, 20, 11, 3, 55, 25, 33, 42, 50, 51, 20, 34, 48, 51, 3, 20, 7, 32, 28, 52, 33, 20, 24, 3, 37, 3, 34, 20, 22, 32, 3, 13, 27, 54, 31, 25, 33, 20, 55, 51, 20, 33, 52, 3, 20, 34, 32, 4, 3, 34, 20, 51, 52, 48, 25, 17, 22, 20, 11, 42, 20, 33, 32, 7, 3, 20, 22, 3, 35, 3, 55, 51, 3, 13, 27, 54, 53, 32, 51, 20, 33, 3, 24, 22, 3, 34, 20, 52, 3, 32, 34, 20, 7, 32, 28, 52, 33, 20, 11, 3, 55, 34, 20, 52, 32, 51, 20, 7, 3, 7, 48, 34, 42, 43, 27, 54, 31, 25, 33, 20, 33, 52, 48, 25, 20, 35, 48, 24, 33, 34, 55, 35, 33, 3, 22, 20, 33, 48, 20, 33, 52, 32, 24, 3, 20, 48, 9, 24, 20, 11, 34, 32, 28, 52, 33, 20, 3, 42, 3, 51, 13, 27, 54, 29, 3, 3, 22, 50, 51, 33, 20, 33, 52, 42, 20, 17, 32, 28, 52, 33, 50, 51, 20, 21, 17, 55, 7, 3, 20, 9, 32, 33, 52, 20, 51, 3, 17, 21, 26, 51, 25, 11, 51, 33, 55, 24, 33, 32, 55, 17, 20, 21, 25, 3, 17, 13, 27, 54, 12, 55, 10, 32, 24, 28, 20, 55, 20, 21, 55, 7, 32, 24, 3, 20, 9, 52, 3, 34, 3, 20, 55, 11, 25, 24, 22, 55, 24, 35, 3, 20, 17, 32, 3, 51, 13, 27, 54, 41, 52, 42, 20, 51, 3, 17, 21, 20, 33, 52, 42, 20, 21, 48, 3, 13, 20, 33, 48, 20, 33, 52, 42, 20, 51, 9, 3, 3, 33, 20, 51, 3, 17, 21, 20, 33, 48, 48, 20, 35, 34, 25, 3, 17, 43, 27, 54, 41, 52, 48, 25, 20, 33, 52, 55, 33, 20, 55, 34, 33, 20, 24, 48, 9, 20, 33, 52, 3, 20, 9, 48, 34, 17, 22, 50, 51, 20, 21, 34, 3, 51, 52, 20, 48, 34, 24, 55, 7, 3, 24, 33, 13, 27, 54, 0, 24, 22, 20, 48, 24, 17, 42, 20, 52, 3, 34, 55, 17, 22, 20, 33, 48, 20, 33, 52, 3, 20, 28, 55, 25, 22, 42, 20, 51, 4, 34, 32, 24, 28, 13, 27, 54, 39, 32, 33, 52, 32, 24, 20, 33, 52, 32, 24, 3, 20, 48, 9, 24, 20, 11, 25, 22, 20, 11, 25, 34, 32, 3, 51, 33, 20, 33, 52, 42, 20, 35, 48, 24, 33, 3, 24, 33, 13, 27, 54, 0, 24, 22, 13, 20, 33, 3, 24, 22, 3, 34, 20, 35, 52, 25, 34, 17, 13, 20, 7, 55, 10, 50, 51, 33, 20, 9, 55, 51, 33, 3, 20, 32, 24, 20, 24, 32, 28, 28, 55, 34, 22, 32, 24, 28, 43, 27, 54, 20, 20, 44, 32, 33, 42, 20, 33, 52, 3, 20, 9, 48, 34, 17, 22, 13, 20, 48, 34, 20, 3, 17, 51, 3, 20, 33, 52, 32, 51, 20, 28, 17, 25, 33, 33, 48, 24, 20, 11, 3, 13, 27, 54, 20, 20, 41, 48, 20, 3, 55, 33, 20, 33, 52, 3, 20, 9, 48, 34, 17, 22, 50, 51, 20, 22, 25, 3, 13, 20, 11, 42, 20, 33, 52, 3, 20, 28, 34, 55, 37, 3, 20, 55, 24, 22, 20, 33, 52, 3, 3, 30, 27, 54, 27, 54, 27, 54, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 16, 27, 54, 27, 54, 39, 52, 3, 24, 20, 21, 48, 34, 33, 42, 20, 9, 32, 24, 33, 3, 34, 51, 20, 51, 52, 55, 17, 17, 20, 11, 3, 51, 32, 3, 28, 3, 20, 33, 52, 42, 20, 11, 34, 48, 9, 13, 27, 54, 0, 24, 22, 20, 22, 32, 28, 20, 22, 3, 3, 4, 20, 33, 34, 3, 24, 35, 52, 3, 51, 20, 32, 24, 20, 33, 52, 42, 20, 11, 3, 55, 25, 33, 42, 50, 51, 20, 21, 32, 3, 17, 22, 13, 27, 54, 41, 52, 42, 20, 42, 48, 25, 33, 52, 50, 51, 20, 4, 34, 48, 25, 22, 20, 17, 32, 37, 3, 34, 42, 20, 51, 48, 20, 28, 55, 1, 3, 22, 20, 48, 24, 20, 24, 48, 9, 13, 27, 54, 39, 32, 17, 17, 20, 11, 3, 20, 55, 20, 33, 55, 33, 33, 3, 34, 3, 22, 20, 9, 3, 3, 22, 20, 48, 21, 20, 51, 7, 55, 17, 17, 20, 9, 48, 34, 33, 52, 20, 52, 3, 17, 22, 43, 27, 54, 41, 52, 3, 24, 20, 11, 3, 32, 24, 28, 20, 55, 51, 10, 3, 22, 13, 20, 9, 52, 3, 34, 3, 20, 55, 17, 17, 20, 33, 52, 42, 20, 11, 3, 55, 25, 33, 42, 20, 17, 32, 3, 51, 13, 27, 54, 39, 52, 3, 34, 3, 20, 55, 17, 17, 20, 33, 52, 3, 20, 33, 34, 3, 55, 51, 25, 34, 3, 20, 48, 21, 20, 33, 52, 42, 20, 17, 25, 51, 33, 42, 20, 22, 55, 42, 51, 2, 27, 54, 41, 48, 20, 51, 55, 42, 13, 20, 9, 32, 33, 52, 32, 24, 20, 33, 52, 32, 24, 3, 20, 48, 9, 24, 20, 22, 3, 3, 4, 20, 51, 25, 24, 10, 3, 24, 20, 3, 42, 3, 51, 13, 27, 54, 39, 3, 34, 3, 20, 55, 24, 20, 55, 17, 17, 26, 3, 55, 33, 32, 24, 28, 20, 51, 52, 55, 7, 3, 13, 20, 55, 24, 22, 20, 33, 52, 34, 32, 21, 33, 17, 3, 51, 51, 20, 4, 34, 55, 32, 51, 3, 30, 27, 54, 53, 48, 9, 20, 7, 25, 35, 52, 20, 7, 48, 34, 3, 20, 4, 34, 55, 32, 51, 3, 20, 22, 3, 51, 3, 34, 37, 50, 22, 20, 33, 52, 42, 20, 11, 3, 55, 25, 33, 42, 50, 51, 20, 25, 51, 3, 13, 27, 54, 6, 21, 20, 33, 52, 48, 25, 20, 35, 48, 25, 17, 22, 51, 33, 20, 55, 24, 51, 9, 3, 34, 20, 38, 41, 52, 32, 51, 20, 21, 55, 32, 34, 20, 35, 52, 32, 17, 22, 20, 48, 21, 20, 7, 32, 24, 3, 27, 54, 40, 52, 55, 17, 17, 20, 51, 25, 7, 20, 7, 42, 20, 35, 48, 25, 24, 33, 13, 20, 55, 24, 22, 20, 7, 55, 10, 3, 20, 7, 42, 20, 48, 17, 22, 20, 3, 14, 35, 25, 51, 3, 13, 50, 27, 54, 44, 34, 48, 37, 32, 24, 28, 20, 52, 32, 51, 20, 11, 3, 55, 25, 33, 42, 20, 11, 42, 20, 51, 25, 35, 35, 3, 51, 51, 32, 48, 24, 20, 33, 52, 32, 24, 3, 30, 27, 54, 20, 20, 41, 52, 32, 51, 20, 9, 3, 34, 3, 20, 33, 48, 20, 11, 3, 20, 24, 3, 9, 20, 7, 55, 22, 3, 20, 9, 52, 3, 24, 20, 33, 52, 48, 25, 20, 55, 34, 33, 20, 48, 17, 22, 13, 27, 54, 20, 20, 0, 24, 22, 20, 51, 3, 3, 20, 33, 52, 42, 20, 11, 17, 48, 48, 22, 20, 9, 55, 34, 7, 20, 9, 52, 3, 24, 20, 33, 52, 48, 25, 20, 21, 3, 3, 17, 50, 51, 33, 20, 32, 33, 20, 35, 48, 17, 22, 30, 27, 54, 27, 54, 27, 54, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 47, 27, 54, 27, 54, 15, 48, 48, 10, 20, 32, 24, 20, 33, 52, 42, 20, 28, 17, 55, 51, 51, 20, 55, 24, 22, 20, 33, 3, 17, 17, 20, 33, 52, 3, 20, 21, 55, 35, 3, 20, 33, 52, 48, 25, 20, 37, 32, 3, 9, 3, 51, 33, 13, 27, 54, 8, 48, 9, 20, 32, 51, 20, 33, 52, 3, 20, 33, 32, 7, 3, 20, 33, 52, 55, 33, 20, 21, 55, 35, 3, 20, 51, 52, 48, 25, 17, 22, 20, 21, 48, 34, 7, 20, 55, 24, 48, 33, 52, 3, 34, 13, 27, 54, 39, 52, 48, 51, 3, 20, 21, 34, 3, 51, 52, 20, 34, 3, 4, 55, 32, 34, 20, 32, 21, 20, 24, 48, 9, 20, 33, 52, 48, 25, 20, 24, 48, 33, 20, 34, 3, 24, 3, 9, 3, 51, 33, 13, 27, 54, 41, 52, 48, 25, 20, 22, 48, 51, 33, 20, 11, 3, 28, 25, 32, 17, 3, 20, 33, 52, 3, 20, 9, 48, 34, 17, 22, 13, 20, 25, 24, 11, 17, 3, 51, 51, 20, 51, 48, 7, 3, 20, 7, 48, 33, 52, 3, 34, 30, 27, 54, 29, 48, 34, 20, 9, 52, 3, 34, 3, 20, 32, 51, 20, 51, 52, 3, 20, 51, 48, 20, 21, 55, 32, 34, 20, 9, 52, 48, 51, 3, 20, 25, 24, 3, 55, 34, 3, 22, 20, 9, 48, 7, 11, 27, 54, 36, 32, 51, 22, 55, 32, 24, 51, 20, 33, 52, 3, 20, 33, 32, 17, 17, 55, 28, 3, 20, 48, 21, 20, 33, 52, 42, 20, 52, 25, 51, 11, 55, 24, 22, 34, 42, 23, 27, 54, 46, 34, 20, 9, 52, 48, 20, 32, 51, 20, 52, 3, 20, 51, 48, 20, 21, 48, 24, 22, 20, 9, 32, 17, 17, 20, 11, 3, 20, 33, 52, 3, 20, 33, 48, 7, 11, 27, 54, 46, 21, 20, 52, 32, 51, 20, 51, 3, 17, 21, 26, 17, 48, 37, 3, 20, 33, 48, 20, 51, 33, 48, 4, 20, 4, 48, 51, 33, 3, 34, 32, 33, 42, 23, 27, 54, 41, 52, 48, 25, 20, 55, 34, 33, 20, 33, 52, 42, 20, 7, 48, 33, 52, 3, 34, 50, 51, 20, 28, 17, 55, 51, 51, 20, 55, 24, 22, 20, 51, 52, 3, 20, 32, 24, 20, 33, 52, 3, 3, 27, 54, 45, 55, 17, 17, 51, 20, 11, 55, 35, 10, 20, 33, 52, 3, 20, 17, 48, 37, 3, 17, 42, 20, 0, 4, 34, 32, 17, 20, 48, 21, 20, 52, 3, 34, 20, 4, 34, 32, 7, 3, 13, 27, 54, 40, 48, 20, 33, 52, 48, 25, 20, 33, 52, 34, 48, 25, 28, 52, 20, 9, 32, 24, 22, 48, 9, 51, 20, 48, 21, 20, 33, 52, 32, 24, 3, 20, 55, 28, 3, 20, 51, 52, 55, 17, 33, 20, 51, 3, 3, 13, 27, 54, 36, 3, 51, 4, 32, 33, 3, 20, 48, 21, 20, 9, 34, 32, 24, 10, 17, 3, 51, 20, 33, 52, 32, 51, 20, 33, 52, 42, 20, 28, 48, 17, 22, 3, 24, 20, 33, 32, 7, 3, 30, 27, 54, 20, 20, 31, 25, 33, 20, 32, 21, 20, 33, 52, 48, 25, 20, 17, 32, 37, 3, 20, 34, 3, 7, 3, 7, 11, 3, 34, 3, 22, 20, 24, 48, 33, 20, 33, 48, 20, 11, 3, 13, 27, 54, 20, 20, 36, 32, 3, 20, 51, 32, 24, 28, 17, 3, 20, 55, 24, 22, 20, 33, 52, 32, 24, 3, 20, 32, 7, 55, 28, 3, 20, 22, 32, 3, 51, 20, 9, 32, 33, 52, 20, 33, 52, 3, 3, 30, 27, 54, 27, 54, 27, 54, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 19, 27, 54, 27, 54, 5, 24, 33, 52, 34, 32, 21, 33, 42, 20, 17, 48, 37, 3, 17, 32, 24, 3, 51, 51, 20, 9, 52, 42, 20, 22, 48, 51, 33, 20, 33, 52, 48, 25, 20, 51, 4, 3, 24, 22, 13, 27, 54, 5, 4, 48, 24, 20, 33, 52, 42, 20, 51, 3, 17, 21, 20, 33, 52, 42, 20, 11, 3, 55, 25, 33, 42, 50, 51, 20, 17, 3, 28, 55, 35, 42, 23, 27, 54, 8, 55, 33, 25, 34, 3, 50, 51, 20, 11, 3, 49, 25, 3, 51, 33, 20, 28, 32, 37, 3, 51, 20, 24, 48, 33, 52, 32, 24, 28, 20, 11, 25, 33, 20, 22, 48, 33, 52, 20, 17, 3, 24, 22, 13, 27, 54, 0, 24, 22, 20, 11, 3, 32, 24, 28, 20, 21, 34, 55, 24, 10, 20, 51, 52, 3, 20, 17, 3, 24, 22, 51, 20, 33, 48, 20, 33, 52, 48, 51, 3, 20, 55, 34, 3, 20, 21, 34, 3, 3, 43, 27, 54, 41, 52, 3, 24, 20, 11, 3, 55, 25, 33, 3, 48, 25, 51, 20, 24, 32, 28, 28, 55, 34, 22, 20, 9, 52, 42, 20, 22, 48, 51, 33, 20, 33, 52, 48, 25, 20, 55, 11, 25, 51, 3, 13, 27, 54, 41, 52, 3, 20, 11, 48, 25, 24, 33, 3, 48, 25, 51, 20, 17, 55, 34, 28, 3, 51, 51, 20, 28, 32, 37, 3, 24, 20, 33, 52, 3, 3, 20, 33, 48, 20, 28, 32, 37, 3, 23, 27, 54, 44, 34, 48, 21, 32, 33, 17, 3, 51, 51, 20, 25, 51, 25, 34, 3, 34, 20, 9, 52, 42, 20, 22, 48, 51, 33, 20, 33, 52, 48, 25, 20, 25, 51, 3, 27, 54, 40, 48, 20, 28, 34, 3, 55, 33, 20, 55, 20, 51, 25, 7, 20, 48, 21, 20, 51, 25, 7, 51, 20, 42, 3, 33, 20, 35, 55, 24, 51, 33, 20, 24, 48, 33, 20, 17, 32, 37, 3, 23, 27, 54, 29, 48, 34, 20, 52, 55, 37, 32, 24, 28, 20, 33, 34, 55, 21, 21, 32, 35, 20, 9, 32, 33, 52, 20, 33, 52, 42, 20, 51, 3, 17, 21, 20, 55, 17, 48, 24, 3, 13, 27, 54, 41, 52, 48, 25, 20, 48, 21, 20, 33, 52, 42, 20, 51, 3, 17, 21, 20, 33, 52, 42, 20, 51, 9, 3, 3, 33, 20, 51, 3, 17, 21, 20, 22, 48, 51, 33, 20, 22, 3, 35, 3, 32, 37, 3, 13, 27, 54, 41, 52, 3, 24, 20, 52, 48, 9, 20, 9, 52, 3, 24, 20, 24, 55, 33, 25, 34, 3, 20, 35, 55, 17, 17, 51, 20, 33, 52, 3, 3, 20, 33, 48, 20, 11, 3, 20, 28, 48, 24, 3, 13, 27, 54, 39, 52, 55, 33, 20, 55, 35, 35, 3, 4, 33, 55, 11, 17, 3, 20, 55, 25, 22, 32, 33, 20, 35, 55, 24, 51, 33, 20, 33, 52, 48, 25, 20, 17, 3, 55, 37, 3, 23, 27, 54, 20, 20, 41, 52, 42, 20, 25, 24, 25, 51, 3, 22, 20, 11, 3, 55, 25, 33, 42, 20, 7, 25, 51, 33, 20, 11, 3, 20, 33, 48, 7, 11, 3, 22, 20, 9, 32, 33, 52, 20, 33, 52, 3, 3, 13, 27, 54, 20, 20, 39, 52, 32, 35, 52, 20, 25, 51, 3, 22, 20, 17, 32, 37, 3, 51, 20, 33, 52, 50, 20, 3, 14, 3, 35, 25, 33, 48, 34, 20, 33, 48, 20, 11, 3, 30, 27, 54, 27, 54, 27, 54, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 18, 27, 54, 27, 54, 41, 52, 48, 51, 3, 20, 52, 48, 25, 34, 51, 20, 33, 52, 55, 33, 20, 9, 32, 33, 52, 20, 28, 3, 24, 33, 17, 3, 20, 9, 48, 34, 10, 20, 22, 32, 22, 20, 21, 34, 55, 7, 3, 27, 54, 41, 52, 3, 20, 17, 48, 37, 3, 17, 42, 20, 28, 55, 1, 3, 20, 9, 52, 3, 34, 3, 20, 3, 37, 3, 34, 42, 20, 3, 42, 3, 20, 22, 48, 33, 52, 20, 22, 9, 3, 17, 17, 27, 54, 39, 32, 17, 17, 20, 4, 17, 55, 42, 20, 33, 52, 3, 20, 33, 42, 34, 55, 24, 33, 51, 20, 33, 48, 20, 33, 52, 3, 20, 37, 3, 34, 42, 20, 51, 55, 7, 3, 13, 27, 54, 0, 24, 22, 20, 33, 52, 55, 33, 20, 25, 24, 21, 55, 32, 34, 20, 9, 52, 32, 35, 52, 20, 21, 55, 32, 34, 17, 42, 20, 22, 48, 33, 52, 20, 3, 14, 35, 3, 17, 43, 27, 54, 29, 48, 34, 20, 24, 3, 37, 3, 34, 26, 34, 3, 51, 33, 32, 24, 28, 20, 33, 32, 7, 3, 20, 17, 3, 55, 22, 51, 20, 51, 25, 7, 7, 3, 34, 20, 48, 24, 27, 54, 41, 48, 20, 52, 32, 22, 3, 48, 25, 51, 20, 9, 32, 24, 33, 3, 34, 20, 55, 24, 22, 20, 35, 48, 24, 21, 48, 25, 24, 22, 51, 20, 52, 32, 7, 20, 33, 52, 3, 34, 3, 13, 27, 54, 40, 55, 4, 20, 35, 52, 3, 35, 10, 3, 22, 20, 9, 32, 33, 52, 20, 21, 34, 48, 51, 33, 20, 55, 24, 22, 20, 17, 25, 51, 33, 42, 20, 17, 3, 55, 37, 3, 51, 20, 49, 25, 32, 33, 3, 20, 28, 48, 24, 3, 13, 27, 54, 31, 3, 55, 25, 33, 42, 20, 48, 50, 3, 34, 26, 51, 24, 48, 9, 3, 22, 20, 55, 24, 22, 20, 11, 55, 34, 3, 24, 3, 51, 51, 20, 3, 37, 3]\n",
      "----------------------------------------------------\n",
      "data length :  2964\n"
     ]
    }
   ],
   "source": [
    "# one hot encode\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars)) # \"enumerate\" retruns index and value. Convert it to dictionary\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "print(char_to_int)\n",
    "print(\"----------------------------------------------------\")\n",
    "print(int_to_char)\n",
    "print(\"----------------------------------------------------\")\n",
    "# integer encode input data\n",
    "integer_encoded = [char_to_int[i] for i in sonnet] # \"integer_encoded\" is a list which has a sequence converted from an original data to integers.\n",
    "print(integer_encoded)\n",
    "print(\"----------------------------------------------------\")\n",
    "print(\"data length : \", len(integer_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "iteration = 1000\n",
    "sequence_length = 40\n",
    "batch_size = round((txt_data_size /sequence_length)+0.5) # = math.ceil\n",
    "hidden_size = 500  # size of hidden layer of neurons.  \n",
    "learning_rate = 1e-1\n",
    "\n",
    "\n",
    "# model parameters\n",
    "\n",
    "W_xh = np.random.randn(hidden_size, num_chars)*0.01     # weight input -> hidden. \n",
    "W_hh = np.random.randn(hidden_size, hidden_size)*0.01   # weight hidden -> hidden\n",
    "W_hy = np.random.randn(num_chars, hidden_size)*0.01     # weight hidden -> output\n",
    "\n",
    "b_h = np.zeros((hidden_size, 1)) # hidden bias\n",
    "b_y = np.zeros((num_chars, 1)) # output bias\n",
    "\n",
    "h_prev = np.zeros((hidden_size,1)) # h_(t-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardprop(inputs, targets, h_prev):\n",
    "        \n",
    "    # Since the RNN receives the sequence, the weights are not updated during one sequence.\n",
    "    xs, hs, ys, ps = {}, {}, {}, {} # dictionary\n",
    "    hs[-1] = np.copy(h_prev) # Copy previous hidden state vector to -1 key value.\n",
    "    loss = 0 # loss initialization\n",
    "    \n",
    "    for t in range(len(inputs)): # t is a \"time step\" and is used as a key(dic).  \n",
    "        \n",
    "        xs[t] = np.zeros((num_chars,1)) \n",
    "        xs[t][inputs[t]] = 1\n",
    "        hs[t] = np.tanh(np.dot(W_xh, xs[t]) + np.dot(W_hh, hs[t-1]) + b_h) # hidden state. \n",
    "        ys[t] = np.dot(W_hy, hs[t]) + b_y # unnormalized log probabilities for next chars\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars. \n",
    "        \n",
    "        # Softmax. -> The sum of probabilities is 1 even without the exp() function, but all of the elements are positive through the exp() function.\n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss). Efficient and simple code\n",
    "\n",
    "#         y_class = np.zeros((num_chars, 1)) \n",
    "#         y_class[targets[t]] =1\n",
    "#         loss += np.sum(y_class*(-np.log(ps[t]))) # softmax (cross-entropy loss)        \n",
    "\n",
    "    return loss, ps, hs, xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(ps, inputs, hs, xs, targets):\n",
    "\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy) # make all zero matrices.\n",
    "    dbh, dby = np.zeros_like(b_h), np.zeros_like(b_y)\n",
    "    dhnext = np.zeros_like(hs[0]) # (hidden_size,1) \n",
    "\n",
    "    # reversed\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t]) # shape (num_chars,1).  \"dy\" means \"dloss/dy\"\n",
    "        dy[targets[t]] -= 1 # backprop into y. After taking the soft max in the input vector, subtract 1 from the value of the element corresponding to the correct label.\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy \n",
    "        dh = np.dot(W_hy.T, dy) + dhnext # backprop into h. \n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity #tanh'(x) = 1-tanh^2(x)\n",
    "        dbh += dhraw\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        dhnext = np.dot(W_hh.T, dhraw)\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]: \n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients.  \n",
    "    \n",
    "    return dWxh, dWhh, dWhy, dbh, dby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 82.974313\n",
      "iter 100, loss: 5.980449\n",
      "iter 200, loss: 4.782816\n",
      "iter 300, loss: 4.686887\n",
      "iter 400, loss: 4.779900\n",
      "iter 500, loss: 3.847413\n",
      "iter 600, loss: 3.245488\n",
      "iter 700, loss: 3.128234\n",
      "iter 800, loss: 2.979261\n",
      "iter 900, loss: 2.807586\n",
      "CPU times: user 1h 18min 22s, sys: 53.8 s, total: 1h 19min 16s\n",
      "Wall time: 42min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data_pointer = 0\n",
    "\n",
    "# memory variables for Adagrad\n",
    "mWxh, mWhh, mWhy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy)\n",
    "mbh, mby = np.zeros_like(b_h), np.zeros_like(b_y) \n",
    "\n",
    "for i in range(iteration):\n",
    "    h_prev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    data_pointer = 0 # go from start of data\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        \n",
    "        inputs = [char_to_int[ch] for ch in sonnet[data_pointer:data_pointer+sequence_length]]\n",
    "        targets = [char_to_int[ch] for ch in sonnet[data_pointer+1:data_pointer+sequence_length+1]] # t+1        \n",
    "            \n",
    "        if (data_pointer+sequence_length+1 >= len(sonnet) and b == batch_size-1): # processing of the last part of the input data. \n",
    "#             targets.append(char_to_int[txt_data[0]])   # When the data doesn't fit, add the first char to the back.\n",
    "            targets.append(char_to_int[\" \"])   # When the data doesn't fit, add space(\" \") to the back.\n",
    "\n",
    "\n",
    "        # forward\n",
    "        loss, ps, hs, xs = forwardprop(inputs, targets, h_prev)\n",
    "#         print(loss)\n",
    "    \n",
    "        # backward\n",
    "        dWxh, dWhh, dWhy, dbh, dby = backprop(ps, inputs, hs, xs, targets) \n",
    "        \n",
    "        \n",
    "    # perform parameter update with Adagrad\n",
    "        for param, dparam, mem in zip([W_xh, W_hh, W_hy, b_h, b_y], \n",
    "                                    [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                    [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "            mem += dparam * dparam # elementwise\n",
    "            param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update      \n",
    "    \n",
    "        data_pointer += sequence_length # move data pointer\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        print ('iter %d, loss: %f' % (i, loss)) # print progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_char, length):\n",
    "    x = np.zeros((num_chars, 1)) \n",
    "    x[char_to_int[test_char]] = 1\n",
    "    ixes = []\n",
    "    h = np.zeros((hidden_size,1))\n",
    "\n",
    "    for t in range(length):\n",
    "        h = np.tanh(np.dot(W_xh, x) + np.dot(W_hh, h) + b_h) \n",
    "        y = np.dot(W_hy, h) + b_y\n",
    "        p = np.exp(y) / np.sum(np.exp(y)) \n",
    "        ix = np.random.choice(range(num_chars), p=p.ravel()) # ravel -> rank0\n",
    "        # \"ix\" is a list of indexes selected according to the soft max probability.\n",
    "        x = np.zeros((num_chars, 1)) # init\n",
    "        x[ix] = 1 \n",
    "        ixes.append(ix) # list\n",
    "    txt = test_char + ''.join(int_to_char[i] for i in ixes)\n",
    "    print ('----\\n %s \\n----' % (txt, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " e gong afissese pin toredeave,\n",
      "\n",
      "Thene geftherequgexteef qurect ith’st’st st  nacist onty fose nivease mafes atese oivd thelh thoust?\n",
      "Thimy’s dalve thece feneloneroused gouty mp’sd\n",
      "Bucamvcy hemd begy alleare aed mer thalfeve,\n",
      "\n",
      "\n",
      "Wiwerrre oind wille.\n",
      "Ofse oilerilerocheae.\n",
      "aand,\n",
      "Fe gt\n",
      "\n",
      "Tonk wanf one,\n",
      "Buthac mort ald of thy thives fafty bllmond,\n",
      "Thy laausunkAofon’senesurh muterafty thrnt’seselary?\n",
      "andew toeu:\n",
      "angeuthefhe lndew the ath cied thensnine oteeailate’ thlve mouze of leald ane  \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "predict('e', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zE4a4O7Bp5x1"
   },
   "source": [
    "# Resources and Stretch Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uT3UV3gap9H6"
   },
   "source": [
    "## Stretch goals:\n",
    "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
    "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
    "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
    "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
    "- Run on bigger, better data\n",
    "\n",
    "## Resources:\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
    "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
    "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
    "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
    "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
