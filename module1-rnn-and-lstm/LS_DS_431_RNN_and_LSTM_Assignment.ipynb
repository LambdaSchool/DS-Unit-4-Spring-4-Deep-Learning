{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 3 Assignment 1*\n",
    "\n",
    "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
    "\n",
    "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
    "\n",
    "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
    "\n",
    "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
    "\n",
    "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
    "\n",
    "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
    "\n",
    "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ltj1je1fp5rO"
   },
   "outputs": [],
   "source": [
    "# TODO - Words, words, mere words, no matter from the heart.\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import requests\n",
    "\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://www.gutenberg.org/files/100/100-0.txt')\n",
    "data = response.text\n",
    "\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ï»¿\\r\\nProject Gutenbergâ\\x80\\x99s The Complete Works of William Shakespeare, by William\\r\\nShakespeare\\r\\n\\r\\nThis eBook is for the use of anyone anywhere in the United States and\\r\\nmost other parts of the world at no cost and with almost no restrictions\\r\\nwhatsoever.  You may copy it, give it away or re-use it under the terms\\r\\nof the Project Gutenberg License included with this eBook or online at\\r\\nwww.gutenberg.org.  If you are not located in the United States, youâ\\x80\\x99ll\\r\\nhave to check the laws of the country where you are located before using\\r\\nthis ebook.\\r\\n\\r\\n\\r\\nTitle: The Complete Works of William Shakespeare\\r\\n\\r\\nAuthor: William Shakespeare\\r\\n\\r\\nRelease Date: January 1994 [EBook #100]\\r\\nLast Updated: November 7, 2019\\r\\n\\r\\nLanguage: English\\r\\n\\r\\nCharacter set encoding: UTF-8\\r\\n\\r\\n*** START OF THIS PROJECT GUTENBERG EBOOK THE COMPLETE WORKS OF WILLIAM SHAKESPEARE ***\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nThe Complete Works of William Shakespeare\\r\\n\\r\\n\\r\\n\\r\\nby William Shakespeare\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n      Contents\\r\\n\\r\\n\\r\\n\\r\\n               THE SONNETS\\r\\n\\r\\n               ALLâ\\x80\\x99S WELL THAT ENDS WELL\\r\\n\\r\\n               THE TRAGEDY OF ANTONY AND CLEOPATRA\\r\\n\\r\\n               AS YOU LIKE IT\\r\\n\\r\\n               THE COMEDY OF ERRORS\\r\\n\\r\\n               THE TRAGEDY OF CORIOLANUS\\r\\n\\r\\n               CYMBELINE\\r\\n\\r\\n               THE TRAGEDY OF HAMLET, PRINCE OF DENMARK\\r\\n\\r\\n               THE FIRST PART OF KING HENRY THE FOURTH\\r\\n\\r\\n               THE SECOND PART OF KING HENRY THE FOURTH\\r\\n\\r\\n               THE LIFE OF KING HENRY THE FIFTH\\r\\n\\r\\n               THE FIRST PART OF HENRY THE SIXTH\\r\\n\\r\\n               THE SECOND PART OF KING HENRY THE SIXTH\\r\\n\\r\\n               THE THIRD PART OF KING HENRY THE SIXTH\\r\\n\\r\\n               KING HENRY THE EIGHTH\\r\\n\\r\\n               KING JOHN\\r\\n\\r\\n               THE TRAGEDY OF JULIUS CAESAR\\r\\n\\r\\n               THE TRAGEDY OF KING LEAR\\r\\n\\r\\n               LOVEâ\\x80\\x99S LABOURâ\\x80\\x99S LOST\\r\\n\\r\\n               THE TRAGEDY OF MACBETH\\r\\n\\r\\n               MEASURE FOR MEASURE\\r\\n\\r\\n               THE MERCHANT OF VENICE\\r\\n\\r\\n               THE MERRY WIVES OF WINDSOR\\r\\n\\r\\n               A MIDSUMMER NIGHTâ\\x80\\x99S DREAM\\r\\n\\r\\n               MUCH ADO ABOUT NOTHING\\r\\n\\r\\n               THE TRAGEDY OF OTHELLO, MOOR OF VENICE\\r\\n\\r\\n               PERICLES, PRINCE OF TYRE\\r\\n\\r\\n               KING RICHARD THE SECOND\\r\\n\\r\\n               KING RICHARD THE THIRD\\r\\n\\r\\n               THE TRAGEDY OF ROMEO AND JULIET\\r\\n\\r\\n               THE TAMING OF THE SHREW\\r\\n\\r\\n               THE TEMPEST\\r\\n\\r\\n               THE LIFE OF TIMON OF ATHENS\\r\\n\\r\\n               THE TRAGEDY OF TITUS ANDRONICUS\\r\\n\\r\\n               THE HISTORY OF TROILUS AND CRESSIDA\\r\\n\\r\\n               TWELFTH NIGHT; OR, WHAT YOU WILL\\r\\n\\r\\n               THE TWO GENTLEMEN OF VERONA\\r\\n\\r\\n               THE TWO NOBLE KINSMEN\\r\\n\\r\\n               THE WINTERâ\\x80\\x99S TALE\\r\\n\\r\\n               A LOVERâ\\x80\\x99S COMPLAINT\\r\\n\\r\\n               THE PASSIONATE PILGRIM\\r\\n\\r\\n               THE PHOENIX AND THE TURTLE\\r\\n\\r\\n               THE RAPE OF LUCRECE\\r\\n\\r\\n               VENUS AND ADONIS\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nTHE SONNETS\\r\\n\\r\\n                    '"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head, text = data.split('1\\r\\n\\r\\n', 1)\n",
    "head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From fairest creatures we desire increase,\\r\\nThat thereby beautyâ\\x80\\x99s rose might never die,\\r\\nBut as the riper should by time decease,\\r\\nHis tender heir might bear his memory:\\r\\nBut thou contracted to thine own bright eyes,\\r\\nFeedâ\\x80\\x99st thy lightâ\\x80\\x99s flame with self-substantial fuel,\\r\\nMaking a famine where abundance lies,\\r\\nThy self thy foe, to thy sweet self too cruel:\\r\\nThou that art now the worldâ\\x80\\x99s fresh ornament,\\r\\nAnd only herald to the gaudy spring,\\r\\nWithin thine own bud buriest thy content,\\r\\nAnd, tender churl, makâ\\x80\\x99st waste in niggarding:\\r\\n  Pity the world, or else this glutton be,\\r\\n  To eat the worldâ\\x80\\x99s due, by the grave and thee.\\r\\n\\r\\n\\r\\n                    2\\r\\n\\r\\nWhen forty winters shall besiege thy brow,\\r\\nAnd dig deep trenches in thy beautyâ\\x80\\x99s field,\\r\\nThy youthâ\\x80\\x99s proud livery so gazed on now,\\r\\nWill be a tattered weed of small worth held:\\r\\nThen being asked, where all thy beauty lies,\\r\\nWhere all the treasure of thy lusty days;\\r\\nTo say, within thine own deep sunken eyes,\\r\\nWere an all-eating shame, and thriftless praise.\\r\\nHow much more praise deservâ\\x80\\x99d thy beautyâ\\x80\\x99s use,\\r\\nIf thou couldst answer â\\x80\\x98This fair child of mine\\r\\nShall sum my count, and make my old excuse,â\\x80\\x99\\r\\nProving his'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:1200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5774402"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From fairest creatures we desire increase, That thereby beauty's rose might never die, But as the riper should by time decease, His tender heir might bear his memory: But thou contracted to thine own bright eyes, Feed'st thy light's flame with self-substantial fuel, Making a famine where abundance lies, Thy self thy foe, to thy sweet self too cruel: Thou that art now the world's fresh ornament, And only herald to the gaudy spring, Within thine own bud buriest thy content, And, tender churl, mak'st waste in niggarding:   Pity the world, or else this glutton be,   To eat the world's due, by the grave and thee.                       2  When forty winters shall besiege thy brow, And dig deep trenches in thy beauty's field, Thy youth's proud livery so gazed on now, Will be a tattered weed of small worth held: Then being asked, where all thy beauty lies, Where all the treasure of thy lusty days; To say, within thine own deep sunken eyes, Were an all-eating shame, and thriftless praise. How much more praise deserv'd thy beauty's use, If thou couldst answer 'This fair child of mine Shall sum my count, and make my old excuse,' Proving his beauty by succession thine.   This were to be new made when thou art old,   And see thy blood warm when thou feel'st it cold.                       3  Look in thy glass and tell the face thou viewest, Now is the time that face should form another, Whose fresh repair if now thou not renewest, Thou dost beguile the world, unbless some mother. For where is she so fair whose uneared womb Disdains the tillage of thy husbandry? Or who is he so fond will be the tomb Of his self-love to stop posterity? Thou art thy mother's glass and she in thee Calls back the lovely April of her prime, So thou through windows of thine age shalt see, Despite of wrinkles this thy golden time.   But if thou live remembered not to be,   Die single and thine image dies with thee.                       4  Unthrifty loveliness why dost thou spend, Upon thy self thy beauty's legacy? Nature's bequest gives nothing but doth lend, And being frank she lends to those are free: Then beauteous niggard why dost thou abuse, The bounteous largess given thee to give? Profitless usurer why dost thou use So great a sum of sums yet canst not live? For having traffic with thy self alone, Thou of thy self thy sweet self dost deceive, Then how when nature calls thee to be gone, Wh\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text.replace('\\r\\n', ' ')\n",
    "text = text.replace('â\\x80\\x99', \"'\")\n",
    "text = text.replace('â\\x80\\x98', \"'\")\n",
    "text[:2400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = list(set(text))\n",
    "char_int = {c:i for i,c in enumerate(chars)}\n",
    "int_char = {i:c for i,c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences:  557497\n"
     ]
    }
   ],
   "source": [
    "# Create the sequence data\n",
    "\n",
    "maxlen = 20\n",
    "step = 10\n",
    "\n",
    "encoded = [char_int[c] for c in text]\n",
    "\n",
    "sequences = [] # Each element is 40 chars long\n",
    "next_char = [] # One element for each sequence\n",
    "\n",
    "for i in range(0, len(encoded) - maxlen, step):\n",
    "    \n",
    "    sequences.append(encoded[i : i + maxlen])\n",
    "    next_char.append(encoded[i + maxlen])\n",
    "    \n",
    "print('sequences: ', len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[47, 81, 84, 16, 9, 8, 29, 69, 81, 90, 97, 39, 9, 17, 81, 90, 29, 39, 40, 81]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create x & y\n",
    "\n",
    "x = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sequences), len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, char in enumerate(sequence):\n",
    "        x[i,t,char] = 1\n",
    "        \n",
    "    y[i, next_char[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((557497, 20, 106), (557497, 106))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 64)                43776     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 106)               6890      \n",
      "=================================================================\n",
      "Total params: 50,666\n",
      "Trainable params: 50,666\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build the model: a single LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / 1\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    \n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "    \n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    \n",
    "    generated = ''\n",
    "    \n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "    \n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "    \n",
    "    for i in range(400):\n",
    "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_int[char]] = 1\n",
    "            \n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds)\n",
    "        next_char = int_char[next_index]\n",
    "        \n",
    "        sentence = sentence[1:] + next_char\n",
    "        \n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print()\n",
    "\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 557497 samples\n",
      "557440/557497 [============================>.] - ETA: 0s - loss: 2.2481\n",
      "----- Generating text after Epoch: 0\n",
      "----- Generating with seed: \"ere shed?   AUMERLE.\"\n",
      "ere shed?   AUMERLE. Dest's and I thit my witrever And Is to hore. aove     Corsh yo'g 'tiye mus sire hom; to as tear hers, to fortles and Sight burth a thoun his woll ow, shour' no bovert. That have not that dit on; Sup nace the last thay diwe?    ELkex a fack fortone Hedmees, I Did Poy lom on,     Expen hat I bat, an sthy omte.     Whand;     to [noag it the Perige?  Cokn dytt must'd I kneth yap liber, the gom not \n",
      "557497/557497 [==============================] - 331s 593us/sample - loss: 2.2481\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2620d41d748>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "\n",
    "model.fit(x, y,\n",
    "         batch_size=32,\n",
    "         epochs=1,\n",
    "         callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zE4a4O7Bp5x1"
   },
   "source": [
    "# Resources and Stretch Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uT3UV3gap9H6"
   },
   "source": [
    "## Stretch goals:\n",
    "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
    "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
    "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
    "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
    "- Run on bigger, better data\n",
    "\n",
    "## Resources:\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
    "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
    "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
    "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
    "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "U4-S2-NN (Python3)",
   "language": "python",
   "name": "u4-s2-nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
