{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/VeraMendes/DS-Unit-4-Sprint-3-Deep-Learning/blob/master/module1-rnn-and-lstm/LS_DS_431_RNN_and_LSTM_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0b158-I7kDBw"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 3 Assignment 1*\n",
    "\n",
    "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
    "\n",
    "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
    "\n",
    "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
    "\n",
    "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
    "\n",
    "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
    "\n",
    "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
    "\n",
    "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "ikOLn1gHmuIe",
    "outputId": "0391eeb3-5839-466e-8b6f-dd0478c3d328"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.encoding = r.apparent_encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.gutenberg.org/files/100/100-0.txt\n",
      "5783552/5777367 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import get_file\n",
    "\n",
    "shakespeare_url = \"https://www.gutenberg.org/files/100/100-0.txt\" \n",
    "filepath = get_file(\"shakespeare.txt\", shakespeare_url)\n",
    "with open(filepath, encoding=\"utf8\") as f:\n",
    "    shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True, lower=False)\n",
    "tokenizer.fit_on_texts([shakespeare_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "chars = list(set(tokenizer.word_index))\n",
    "\n",
    "char_indices = {c:i for i,c in enumerate(chars)}\n",
    "indices_char = {i:c for i,c in enumerate(chars)}\n",
    "\n",
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences: 1114611\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create the Sequence Data\n",
    "maxlen = 100\n",
    "step = 5\n",
    "\n",
    "sequences = [] # Each element is 40 characters long\n",
    "next_chars = [] # One element for each sequence\n",
    "for i in range(0, len(encoded) - maxlen, step):\n",
    "    sequences.append(encoded[i : i + maxlen])\n",
    "    next_chars.append(encoded[i + maxlen])\n",
    "print('sequences:', len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Specify x & y\n",
    "x = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sequences), len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, char in enumerate(sequence):\n",
    "        x[i,t,char] = 1\n",
    "        y[i, next_chars[i]] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1114611, 100, 107)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1114611, 107)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(shakespeare_text) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = shakespeare_text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples\n",
      "Epoch 1/5\n",
      "199936/200000 [============================>.] - ETA: 0s - loss: 2.6806\n",
      "----- Generating text after Epoch: 0\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"gain turn back and fly.\n",
      "  RICHARD. Ay, now methinks I hear great Warwick speak.\n",
      "    Ne'er may he liv\"\n",
      "gain turn back and fly.\n",
      "  RICHARD. Ay, now methinks I hear great Warwick speak.\n",
      "    Ne'er may he liv6“ ﻿6“ ﻿6“ ﻿6“ ﻿6“ld7u“ ﻿6“ld7(“B6“ ﻿6“ ﻿6“ ﻿6“ ﻿6“ ﻿6“ ﻿6“u﻿6(“mWI“ ﻿6“ ﻿6“ ﻿6“ ﻿6“ ﻿6“ ﻿6“ ﻿6“ ﻿6“ ﻿6“ ﻿6“ ﻿6“ ﻿6“ ﻿6“﻿6(6“ ﻿6“ld7u“ ﻿6“ ﻿6“ ﻿6“ ﻿6“ ﻿6“ ﻿6“ ﻿6“ ﻿6“ ﻿6“ ﻿6“ ﻿6“ ﻿6(6“&d7u“mWI“u﻿6“ ﻿6“ ﻿6“ ﻿6“ ﻿6“ ﻿6“ ﻿6“ ﻿6(“ ﻿6“ ﻿6u“HW“ ﻿6“ ﻿6“ ﻿6“ ﻿6“ ﻿6“ ﻿6“ ﻿6“ ﻿6“ ﻿6“ ﻿6“ ﻿6“ ﻿6“ ﻿6“ ﻿6“ ﻿6“ ﻿6“ ﻿6“ ﻿6“ ﻿6“ ﻿6“ ﻿6“ ﻿6“ ﻿6“ ﻿6“ ﻿6“ ﻿6“ ﻿6“ ﻿6“ ﻿6“ ﻿6“ ﻿6“ ﻿6“u﻿mWI“ ﻿6“ ﻿6“ ﻿6“ ﻿6“ ﻿6“ ﻿6“ ﻿6“\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"gain turn back and fly.\n",
      "  RICHARD. Ay, now methinks I hear great Warwick speak.\n",
      "    Ne'er may he liv\"\n",
      "gain turn back and fly.\n",
      "  RICHARD. Ay, now methinks I hear great Warwick speak.\n",
      "    Ne'er may he livW#[““““““T﻿6“&﻿6“﻿HW“O6“ ﻿6 “\"d7(“ ﻿d7(“﻿m ﻿“ ﻿6“ud7u 3“ ﻿6“ ﻿mu“ld(6“HWI“m “mWI“&﻿m6“ld7“\\d(6“﻿6 “﻿du“/6m “Bm(6“ ﻿d “_HWR3“mWR6(6“um(6“ ﻿6“ ﻿m ﻿“mWI“mW“ ﻿6G6“R﻿6(6“IHW“﻿Hu“&dWI“ ﻿mW “ ﻿6GdI“﻿m “﻿6“ ﻿6“__m 3“&﻿6(u“mWI“HW“&dWI“&6(6“/dB6“ ﻿6“ ﻿6“ ﻿6(“/6(u6“ ﻿6 “mGG“d7W“B6 ﻿6“/6(6(6“ ﻿6(“﻿6( “﻿6(6“ ﻿6“﻿mGGG“/6 “﻿6“ ﻿m “ ﻿m “﻿6( ﻿6(“m(“m(6“ ﻿d“﻿HHWI“﻿mW6“Wd “﻿H(6“ ﻿d7Wu“HulmWR“ud(6(u“6ld“ ﻿H “u﻿6“﻿mWI\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"gain turn back and fly.\n",
      "  RICHARD. Ay, now methinks I hear great Warwick speak.\n",
      "    Ne'er may he liv\"\n",
      "gain turn back and fly.\n",
      "  RICHARD. Ay, now methinks I hear great Warwick speak.\n",
      "    Ne'er may he liv\"3[““““B“dd“&dGGGmI“ÆdI6H “&6B6R\"“&mO6 H ﻿“B\"d7“_7m(“dW6“﻿6I“/dH6uHW][[““LLÆtW“qHB6“W﻿muR﻿6“6HB\t“ ﻿mu_6#[f\tTTW!#[[“L“M“““ç““﻿6“J﻿H “﻿6H ﻿“ ﻿HW“ mm&7WRè_'(6“ld “&H6uI#[“nL|MML“Mp?s\tTL#[çV!M\tZM3“ZpÆMVi“|sCspppZf““TdI“ç﻿6(/666”““d“\\﻿Gm(R#“‘\tuiLVL#“f6H6W][““I“d(ImW'uz[“p“L“Æ!iL\tCL“ud77u6u﻿6E[“““V“J?}GV]“s7 “Huu66z[ç﻿“BdOmW “R(6\"d7I]“dB“B6IHdW“Bu“dWWm 6z[““\"d7u ]“﻿6“ ﻿6/6“﻿d I6 mu]“mWR d7R3[““““f6nlHWI\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"gain turn back and fly.\n",
      "  RICHARD. Ay, now methinks I hear great Warwick speak.\n",
      "    Ne'er may he liv\"\n",
      "gain turn back and fly.\n",
      "  RICHARD. Ay, now methinks I hear great Warwick speak.\n",
      "    Ne'er may he livI“HG63“\"“ç﻿6“& mJu3““_dHu_É\"“&“H.[“B“!W“﻿6(d\\d&\"RdR3[fZTGCLWsZV3“Lf pfZn|?3TBo!it\t!#“MIHWR3[/76W“/\"[L“HWJd\tèz“tWum[[““.n GH#“[m““““/d(B']“R6m “ldW﻿HWIHGHu3u33“mèu“mWf“u﻿6\\d6W#“hGmmT3“&﻿m ﻿[“&mçG6m\\H G“HR\\u'I“m “\"﻿mO“BmBmmW“\"HB7u﻿[[“/pduudG(Hu“MHGW(6çJèzsW[6V[|!!}#p“pmGJ““Hd “B6G763“CG6/u“mW]“\"d7“ ﻿(“m “﻿\"dO6V“nhHBGRP(uHGB6W“R﻿7G&H(GGHJ“MW“\\﻿mlImG\\Puu3[T﻿6 d(W”““R6GG6A7mJ“G(6(63“xmI“/m3“IOH7W“hHWWR\n",
      "200000/200000 [==============================] - 1756s 9ms/sample - loss: 2.6805\n",
      "Epoch 2/5\n",
      "199936/200000 [============================>.] - ETA: 0s - loss: 2.1988\n",
      "----- Generating text after Epoch: 1\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"               [The SENATORS descend and open the gates]\n",
      "\n",
      "                 Enter a SOLDIER as a Mess\"\n",
      "               [The SENATORS descend and open the gates]\n",
      "\n",
      "                 Enter a SOLDIER as a MessLs#“ç﻿m “﻿6“ ﻿6“ ﻿6“BmWI“ ﻿6“ud(6“﻿HW“ ﻿6“B6“ ﻿6“_6m “﻿6“B6“Id7(“ ﻿6“﻿6(6“ ﻿6“/6“ ﻿6“ud(6“ ﻿6“﻿6(6“ ﻿6“BmWI“ ﻿6“ ﻿6“BmWI“u6(6“ ﻿6“/6m “﻿6“ ﻿6“﻿HWR“ ﻿6“B6(6“ ﻿6“﻿6(6“ ﻿6“﻿HWR“ ﻿6“B6“ ﻿6“B6“ ﻿6“B6“ ﻿6“B6“ ﻿6“﻿Hu“u﻿6“ ﻿6“B6“Id7(“﻿6(6“ ﻿6“ ﻿6“B6“ ﻿6“﻿6(6“ ﻿6“\\d7WI“ ﻿6“﻿6(6“ ﻿6“ ﻿6“ ﻿6“B6“ ﻿6“﻿6(6“ ﻿6“BmWI“mWI“BmWI“ ﻿6“﻿6(6“ ﻿6“&HWR“ ﻿6“﻿m(u“ ﻿6“B6“ ﻿6“ ﻿6“B6“ ﻿6“﻿Hu“mWI“u6(6“ ﻿6“﻿6(6“﻿6“﻿6(6“ ﻿6“u6(6“\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"               [The SENATORS descend and open the gates]\n",
      "\n",
      "                 Enter a SOLDIER as a Mess\"\n",
      "               [The SENATORS descend and open the gates]\n",
      "\n",
      "                 Enter a SOLDIER as a Mess\t“L“|\tThpsM#““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"               [The SENATORS descend and open the gates]\n",
      "\n",
      "                 Enter a SOLDIER as a Mess\"\n",
      "               [The SENATORS descend and open the gates]\n",
      "\n",
      "                 Enter a SOLDIER as a Mess6“Vi\tW“hZM|}ZMp!M#“lGGHGG“ldO6'I“ ﻿m(]“Bd“sdd“ÆdO6Wu#“\t“I6(HGHu3“&﻿6l“﻿mHJ“ ﻿mR3“﻿6“lHA﻿6_(#[ç﻿6GG\"#[Cd&WdI3[L“Bm\\6“l6(/6“Hu“&HOu“ldÆ(u“u6d [““l“ndlu 3[““t7(“_um/63“m“B6“ ﻿du3“ H ﻿&“mW“GHI3“HJ6“WddWu6“u mBW6“|V(H 3[ç﻿\"“7(OmBB6(u#[““\tW/6“_HWR63[i7\\WR6èI“\"d7\\﻿“ d“6WI#[MC}ZM“““nl“Bd?63[[\tT}V!||l\tV“|V?MVs\t#TCdW“R﻿Gd“W﻿6“\\m H6u “6(H “Bè6WI“ ﻿6 “/d7O6W “ 7(Hu\"“umI\"3“ld(“ d“\\d(]“(mHW “Td/IHW\\HO6#.[hCp!L}\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"               [The SENATORS descend and open the gates]\n",
      "\n",
      "                 Enter a SOLDIER as a Mess\"\n",
      "               [The SENATORS descend and open the gates]\n",
      "\n",
      "                 Enter a SOLDIER as a MessLsp\t#“h\"“m(“&6(mIu3[““““ffC\tpsu#[““/d7WI#[[““}7PG_6mJ3[““““\t“VG6“Md“WHGG“/u“f6(\\H ][Td“_(H u#[““ip}}u63“ \"“W6 7Gd;[““““\t(6“J6uuuHW\\\"“\tdW“ d“\\H ﻿#[[V“““t\tI/ZM|hZ|h\t!\t#“\t6“﻿mOm/GGHW [[.V\t ?d “mWd ﻿“/7I“Wdm Zs#[sJ#“&“umu3“ld ?“)è“Rd(“V7(um(#“Bdl A7('“ H(“/6d(3“Mu“l(6dl6W ﻿[“““\t|\tiLtfbpo#[\t“GGHB“(G66Gu“BHuu63[“““ViB[LVpsCL\t#“p﻿m “\"d7(63“(6/7 6Wd u“6GGJd7 “/mIO6W[4“( ﻿6Wm\\R“/6W\"“﻿\"“o#“\tT\"(H ﻿“\\6“&﻿dJ“l\n",
      "200000/200000 [==============================] - 1261s 6ms/sample - loss: 2.1988\n",
      "Epoch 3/5\n",
      "199936/200000 [============================>.] - ETA: 0s - loss: 2.0668\n",
      "----- Generating text after Epoch: 2\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"ly man.\n",
      "  SALISBURY. What other harm have I, good lady, done\n",
      "    But spoke the harm that is by other\"\n",
      "ly man.\n",
      "  SALISBURY. What other harm have I, good lady, done\n",
      "    But spoke the harm that is by other[“““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"ly man.\n",
      "  SALISBURY. What other harm have I, good lady, done\n",
      "    But spoke the harm that is by other\"\n",
      "ly man.\n",
      "  SALISBURY. What other harm have I, good lady, done\n",
      "    But spoke the harm that is by other[“““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"ly man.\n",
      "  SALISBURY. What other harm have I, good lady, done\n",
      "    But spoke the harm that is by other\"\n",
      "ly man.\n",
      "  SALISBURY. What other harm have I, good lady, done\n",
      "    But spoke the harm that is by other“ d“BdO6(u “d(“IH7(6“G6m (“d7“ ﻿mu“d&“B7_(“um6“Bd( “ml6è“﻿6 “H(“/7 “/(m\"3“&d“﻿m(W#[[hV!T.i“V!C7 “MsTV\tsZ#“?d(“ d“m&“IdI“T﻿d7“_mW“&6(u“dl“m\\_“Rm“R7WIH 3“LmR6uu“m(6“\"6J#“I6W 3“L“m u6(6“/6m\"“ ﻿6_6(u“_(6m/6;[Td“6W\\﻿“IddW “ HI3[[““““““““““““““““““““““““““““\\NW[\t“I6“/6Wdu (6HW6“﻿\"“RdB6“﻿6I6“&H\\﻿“﻿dW“(dWRu“udO6#[M6!6“a.pb6&““Æp!MthpC#“ldGG“&d(W“_6mO6“ ﻿6B\"“ ﻿6“\\mW6\\#[““fdW6“RmBm “Iu “B6“\\6\\﻿H(“﻿HB“mGI P”\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"ly man.\n",
      "  SALISBURY. What other harm have I, good lady, done\n",
      "    But spoke the harm that is by other\"\n",
      "ly man.\n",
      "  SALISBURY. What other harm have I, good lady, done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    But spoke the harm that is by other[T﻿6“ \"63“ÆWH/(dGm3[iHI“HW“u6m [Æd7(#[“fp!L\tC“&tp!\t\t#“çH\"“I&66I“ ﻿6“﻿m(R﻿\"\\﻿“dWI[b&6WJmlP[T﻿6“R﻿HO\\6IRèu6Gu]“HWI6“mWI“ ﻿m “ ﻿d7“I6(d7[MH/ ﻿duèu“ud(z[[ip}VsL#“V&“﻿Hu è “(B__m(6“B6u “Bd(m “m“Bm(6#“,6z[CHJu“﻿HBG“m(“\\﻿6B“lmJ63“dmu (7èu“BHB3[““““V7(“ÆdB6u“d_GW“&﻿d(\\63“u(6\\﻿è“R﻿6“ld&P“IH ﻿Bd( u#[““}7G3“6(dl“mduu“B7 HW“R6lB#[[LÆVsLV#“&6mO6W“mu“(7Od('uH/W3“tmOHW“dR“JHu“R76“pWd ﻿6W“ ﻿6“JHWu;“M﻿6B66W3“dB“ud\n",
      "200000/200000 [==============================] - 55376s 277ms/sample - loss: 2.0668\n",
      "Epoch 4/5\n",
      "199936/200000 [============================>.] - ETA: 0s - loss: 1.9829\n",
      "----- Generating text after Epoch: 3\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"ight fame,\n",
      "    Before young Talbot from old Talbot fly,\n",
      "    The coward horse that bears me fall and \"\n",
      "ight fame,\n",
      "    Before young Talbot from old Talbot fly,\n",
      "    The coward horse that bears me fall and ﻿“ ﻿6“/6m(6“ ﻿6“_m( HWR“ ﻿6“_m(6“ ﻿6“_(mHWR6(“ ﻿m “﻿m ﻿“ ﻿6“BmW 6(“mWI“ ﻿6W6“ ﻿6“_(mHW “mWI“ ﻿m “ ﻿6“_(dB6“ ﻿6“_m(6“ ﻿6“_d(6 “ ﻿6“Bm(6“ ﻿6W“ ﻿6“u﻿m(6“ ﻿6“_(mHWR6(“ ﻿66“ ﻿6“_m( 6W[““““\tWI“ ﻿6“BmW “ ﻿6“BmW “dl“ ﻿6“&m(6“ ﻿6(6“ ﻿6“&HGG“ ﻿6“&HGG“ ﻿6“&H ﻿“BmI6“ ﻿6W“ ﻿6“_(d(6u“ ﻿6“u﻿m “ ﻿6“_(mHW6u“dl“ ﻿6“BmW 6(“mWI“ ﻿6“/6m(6“ ﻿6m(“mWI“﻿m(6“ ﻿6“\\mW 6(“mWI“ ﻿6“/6m(6“ ﻿6“&HGG“ ﻿6“BmW 6(“dl“ ﻿6“_(m ﻿6(“mWI“﻿\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"ight fame,\n",
      "    Before young Talbot from old Talbot fly,\n",
      "    The coward horse that bears me fall and \"\n",
      "ight fame,\n",
      "    Before young Talbot from old Talbot fly,\n",
      "    The coward horse that bears me fall and 6“﻿6(63“mWI“ ﻿m “ ﻿6“&d7(u[““““T﻿6“\\dWm(6“L“m“Im(6“ ﻿6“ ﻿m “﻿m(6“Im ﻿d(“﻿m “u﻿6m “ ﻿66“ ﻿m “ ﻿6“ ﻿m “(6u6W 6W[\tWI“ ﻿6“BmJ6“L“mWI“u mGG“&H ﻿“HW“ ﻿6W“u﻿6(6“ ﻿6“_d( ﻿6(“HW“ ﻿6“_d(6“ ﻿d7(“IdB6“u mGG“u7(6u“B\"“mWI“mWI“mWI“ ﻿6W“ ﻿6“Bm 66u“mWI“mu“ ﻿6“&﻿m “Im\"“u_m  ﻿6(“ ﻿m “d(“ ﻿HWR“ ﻿m “\"d7(“ ﻿6“_(dW6u“ d“ ﻿6Bu6(“BmWI“Bm(6“H “Bm ﻿6(“mWI“﻿6(6“ ﻿m “B6“udB6“mWI“ ﻿6W6u“ld(I6([““““T﻿6m(“ ﻿Hu“GH ﻿“ ﻿H “\"d7(“&HG\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"ight fame,\n",
      "    Before young Talbot from old Talbot fly,\n",
      "    The coward horse that bears me fall and \"\n",
      "ight fame,\n",
      "    Before young Talbot from old Talbot fly,\n",
      "    The coward horse that bears me fall and ﻿Hu“H ﻿mW“ ﻿6W“m“/\"“/6 m(7[T﻿H6“GHWI“/6èI“(mmG“ ﻿6u#“\tmO6“/6Gm 3“﻿HW“GHBu“m“IdWI u“\\﻿d7èI“ ﻿6(#[V}m ﻿“B\"“Id/G\"“7('I6z[[|\t}V}fC\ts#“Æd(“udHGG“ ﻿6[““““B6“_6m\\(d(3“/7WHl(“\"d7(“7bH d(6W “BmI\"“ ﻿Hu“dl“W6“Bd(“\"d7﻿#[[C!\ttLsp#[t(dI6WI“m“lH6“ ﻿H ﻿HdW“﻿mO6“(HBSd7W“d7_GmWI(dl“mWI“ ﻿mHI3[T﻿d(“BmId7I“ld((u6Gm(“﻿dB6“ ﻿6“&6(6W3“B\"“/6u ][““““\tWI“﻿67u3“/GH H HO6u“dl“B\"“&dd“ud(”“T﻿6( mRG“/d“Bm ﻿6(u“B\"“/6“ ﻿6“_(d7G3“\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"ight fame,\n",
      "    Before young Talbot from old Talbot fly,\n",
      "    The coward horse that bears me fall and \"\n",
      "ight fame,\n",
      "    Before young Talbot from old Talbot fly,\n",
      "    The coward horse that bears me fall and “﻿6(“&H\\﻿“L“ ﻿7 “\"dl“﻿Hu“BdBuW ﻿d(u“/d(mWd“\\﻿6(u6[““““?dI“l6WRmIOH 3“ ﻿\"“/mB6“(dI\"“H(“&﻿d(/HW6u m\"#[}\t  “mWI“dl“H “﻿mW[““““Bdl﻿m 7u 6(L“\\6GGHW6;[sdB3“/\"“/6W “uld(﻿u“l7(BmWu“7W ﻿6(3“/6“GHu 6“m&m“I﻿66Rz[ç﻿m 3“mWI“&﻿6#“ç﻿m “GmO6“ ﻿\"3“u﻿6(6èu“R(mWI6(“ ﻿6  6\"3“&Hu[T﻿d“\tWId(Bz“|\"6(HWu“ ﻿67(/g6(B“ ﻿\"6G“L“Im ﻿6(3“Wd ﻿6(“\\6Gl\\GO6#“V“MG\"d( mWOL“﻿m &([““““_HGG3“/mIm “ 6“m_(d7 ﻿HW[““““pWmGHO6“OH/mW6uHuz[[\tsps\n",
      "200000/200000 [==============================] - 1588s 8ms/sample - loss: 1.9829\n",
      "Epoch 5/5\n",
      "199936/200000 [============================>.] - ETA: 0s - loss: 1.9111\n",
      "----- Generating text after Epoch: 4\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"tch, and yet is\n",
      "she a wondrous fat marriage.\n",
      "\n",
      "ANTIPHOLUS OF SYRACUSE.\n",
      "How dost thou mean a “fat marr\"\n",
      "tch, and yet is\n",
      "she a wondrous fat marriage.\n",
      "\n",
      "ANTIPHOLUS OF SYRACUSE.\n",
      "How dost thou mean a “fat marr[“““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"tch, and yet is\n",
      "she a wondrous fat marriage.\n",
      "\n",
      "ANTIPHOLUS OF SYRACUSE.\n",
      "How dost thou mean a “fat marr\"\n",
      "tch, and yet is\n",
      "she a wondrous fat marriage.\n",
      "\n",
      "ANTIPHOLUS OF SYRACUSE.\n",
      "How dost thou mean a “fat marr“ ﻿6“ d“﻿6G6“ ﻿66“u﻿6“ ﻿66“m(6“&HGG3[““““fd(6m 3“mWI“I6mWR6(“&d(I“ ﻿d7“ud(“﻿6 “udB6\"3[““““L“lm(6“dl“ ﻿6“m(6“ ﻿6“u&6GO6G“&﻿m “ ﻿6“lm(6#[[““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"tch, and yet is\n",
      "she a wondrous fat marriage.\n",
      "\n",
      "ANTIPHOLUS OF SYRACUSE.\n",
      "How dost thou mean a “fat marr\"\n",
      "tch, and yet is\n",
      "she a wondrous fat marriage.\n",
      "\n",
      "ANTIPHOLUS OF SYRACUSE.\n",
      "How dost thou mean a “fat marr[[CV}psp#[L'GOu“(d&“ ﻿d7R﻿“HW“/6I“um(6“OHO6 HR﻿ HdW“\"6(“Gdu&6(B_6]“﻿6( G6“ ﻿mu[u7\\J“dl“B7(6u“ d(“Wl6(6W #[T﻿m “u d6“Hu“lddu “A7dW\\ H \"3“(76“GdI“d7GI“&d“ HGG“_(6\"“ ﻿6“&HGG“&mHWJ6P[L“u7_GG\"“ ﻿6“B6(6]“mWJu6(“B6“ ﻿Hu“﻿Hu“\\6W #[[“L\tf [sdu“\"d7(“ld(6W3“﻿6m( “\\7W m(m\\H6u3“GHB6I[T﻿6“&6(6“ ﻿m “ ﻿6“ud\\J\"“\"d7l“ ﻿6“_mH Hu'I3[““““T﻿6ml“pW\\6W 7(B3“\"6GGI“u66(6“&﻿Hu“d&]“ ﻿6“/(d 6[““““T﻿HB3“\"d73“T﻿6“&HWR“d&3“ld(E[Æ\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"tch, and yet is\n",
      "she a wondrous fat marriage.\n",
      "\n",
      "ANTIPHOLUS OF SYRACUSE.\n",
      "How dost thou mean a “fat marr\"\n",
      "tch, and yet is\n",
      "she a wondrous fat marriage.\n",
      "\n",
      "ANTIPHOLUS OF SYRACUSE.\n",
      "How dost thou mean a “fat marr“u6mWf6(63[fGdd//&H “Bmu “ d“Iddu6“MdB6G\"3[CH\\G6WI3“ ﻿B6WumHW“uGd& “﻿dd&èW “\"6mb7I“/6m( “HW#[T﻿\"“_ mGG3[nd“/7 “W6 “Wd “R﻿\"“&6\"u3[t6(PmWI“W mW “7 \"“GdBI3“&mO6“6GmJH W6([Md3“/6 “BH(6“d “H “l(d(\"[L“iL“ImHWRu“L“\"d7“ ﻿6\"3“ ﻿HG[““““oW\"d7#[\tWJ“mu&H ﻿3“d èu/“u\"“ ﻿﻿\"3“ (d\\Ju “p\"d7(6][L“mHu]“&mJ6u“dl“Id76“ 6bm(I[““““\tW d7WRd ﻿7J“ “Md7GI“g7(6“ld7u “/6HW“ld(6u[Md“HWHu&6O#“?6m(H7(u“O6W &6(I[TH ﻿“7uuu&'u “(HW/G\n",
      "200000/200000 [==============================] - 1897s 9ms/sample - loss: 1.9111\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x148bb4c10>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x[:200000], y[:200000],\n",
    "          batch_size=128,\n",
    "          epochs=5,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zE4a4O7Bp5x1"
   },
   "source": [
    "# Resources and Stretch Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uT3UV3gap9H6"
   },
   "source": [
    "## Stretch goals:\n",
    "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
    "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
    "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
    "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
    "- Run on bigger, better data\n",
    "\n",
    "## Resources:\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
    "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
    "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
    "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
    "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "LS_DS_431_RNN_and_LSTM_Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
