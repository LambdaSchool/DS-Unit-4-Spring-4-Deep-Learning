{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 3 Assignment 1*\n",
    "\n",
    "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
    "\n",
    "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
    "\n",
    "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
    "\n",
    "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
    "\n",
    "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
    "\n",
    "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
    "\n",
    "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ltj1je1fp5rO"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "target_url = 'https://www.gutenberg.org/files/100/100-0.txt'\n",
    "request = requests.get(target_url)\n",
    "data = request.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create char2int and int2char dictionaries\n",
    "sample = data.replace('\\n', '').replace('\\r','')[3000:500000]\n",
    "chars = list(set(sample))\n",
    "\n",
    "char_int = {c:i for i,c in enumerate(chars)}\n",
    "int_char = {i:c for i,c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences: 165654\n"
     ]
    }
   ],
   "source": [
    "#Create sequence data\n",
    "maxlen = 40\n",
    "step = 3\n",
    "#encode each character in sample text with matching integer\n",
    "encoded = [char_int[c] for c in sample]\n",
    "\n",
    "sequences = [] #Each element will be 40 chars long\n",
    "next_chars = [] #the next char after each sequence\n",
    "\n",
    "for i in range(0, len(encoded) - maxlen, step):\n",
    "    sequences.append(encoded[i:i+maxlen])\n",
    "    next_chars.append(encoded[i+maxlen])\n",
    "    \n",
    "print('sequences:', len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index of 40-char sequences by index of characters by the characters themselves\n",
    "X = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool) \n",
    "# index of 40-char sequences by the single next character\n",
    "y = np.zeros((len(sequences), len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, char in enumerate(sequence):\n",
    "        X[i, t, char] = 1\n",
    "    y[i, next_chars[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((165654, 40, 87), (165654, 87))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\tally\\.virtualenvs\\DS-Unit-4-Sprint-2-Neural-Networks-psAJyakQ\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_f(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(sample) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = sample[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_int[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample_f(preds, diversity)\n",
    "            next_char = int_char[next_index]\n",
    "\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "165632/165654 [============================>.] - ETA: 0s - loss: 2.0588\n",
      "----- Generating text after Epoch: 0\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"sun; that he that hath    learned no wit\"\n",
      "sun; that he that hath    learned no with the fort the fore the will the will she the worl dome the sore the will do the sore the ment the ment the have a more the word of the were the beat the hore the will and of the will and of the sere the fore the will the have the eres and the sore the will of the will the will will whe have the wore the will he will of the sear of the will the will will and in the fore the fore the lood the love \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"sun; that he that hath    learned no wit\"\n",
      "sun; that he that hath    learned no with my wall for of come to that a the four peet the now the menener.                                                                                                                                                                                                                                                                                                                                              \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"sun; that he that hath    learned no wit\"\n",
      "sun; that he that hath    learned no with    Apreantterse, TuarWhevee whan of wich belles, ander buce that year harss, ofser mesh.BERARV. Wham lliomen viqull sher , meck I    'ad if be blot? I Saak here four fith,Whis dede is ford    ann thatenbenes,    Wow coom shave de prote. Be. At But o    So chintut in dinger awe you wist thou: be world morg;I loued, wheble the sond,Daghily hase To minker.ROLINTOr Saquincâm,The womempre. CoNTONDA\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"sun; that he that hath    learned no wit\"\n",
      "sun; that he that hath    learned no with with ad wisthen I fooke. ON O cis; bn wors me teoughe wem'? CLEOPATOYeGber he shone cat:   Meate hade hest wiluko rurtaid 'der, youlble. HoBdu?  Bure. I Fxectosty?  SOULIER. Nodlun! Dad unmen! eine?    I Ga ston leot yourm!  not alimr,Suct?! He.    thew isvurt,_streen guop rthe cen.  Sersotsejr.FerpatethBink thoueThue to thou lave es Ehenicie alf ham, bad head you verin, Lowtloonellfoten diges g\n",
      "165654/165654 [==============================] - 235s 1ms/sample - loss: 2.0587\n",
      "Epoch 2/5\n",
      "165632/165654 [============================>.] - ETA: 0s - loss: 1.9598\n",
      "----- Generating text after Epoch: 1\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"one._                                   \"\n",
      "one._                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"one._                                   \"\n",
      "one._                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"one._                                   \"\n",
      "one._                                      TtEleXsSe TRANAIUS you, sert thMERESTELO. Whe hough ghere womght;    And and,Thes nomy stounds oun es concind cortug,Serpeoe For litt lowe that ialt )ave to frollise, the ciols. [_om youn    Orun. JAon my thich om prace afp illiriss over, o cisth's peliiath, thou his sim the hate pach'd to clack aty; ttrate, Os mare blipede. If cownold traok, be uplord!    Un thou sik! fortuli, them tou mart, n\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"one._                                   \"\n",
      "one._                                    Mver kleee mytting ust on bulint well ent git that maked lamnee le aadales Lnoud:   Norsushifed 1eAdsize thee; in hem. Noil'd:    Pepoubbus ingabloauts theer baldos; Berbeingld miteBtuly  fonth't, you sould at beace ablow roceaveliane not in Tounot    Her thabâre; eave   Lokn it ax, coun to he love hike CuLEarend! RoI?HGORLDES.Whanto hoo pild here thouabent,  MBEgRABES. Pandeak aod have feflefo\n",
      "165654/165654 [==============================] - 231s 1ms/sample - loss: 1.9598\n",
      "Epoch 3/5\n",
      "165632/165654 [============================>.] - ETA: 0s - loss: 1.8882- ETA: 2s \n",
      "----- Generating text after Epoch: 2\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \" yet I know him a notorious liar,Think h\"\n",
      " yet I know him a notorious liar,Think his and the here the sing the have the will the will the have the have the will the seart the have the have the will the have the will the will the have the have the worth the will the will the have the will the will the world the parter the with the here the will and the have that a ment the will the will the have the shall the will the forth the bear her shath the prome the here the forth the hav\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" yet I know him a notorious liar,Think h\"\n",
      " yet I know him a notorious liar,Think her sill the were of a the greas offer,    I will do hour make him her and is and the hear the mereped to that for and that the will thou here the hand of not and    With shand of for the mand and the port the dees of his have were my port him is the plowe the songe,    And what have the pave the what his love to the    And lates to the have'd of her she deest to the for the fitht are come thee sar\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" yet I know him a notorious liar,Think h\"\n",
      " yet I know him a notorious liar,Think hay your woll this deesh, so falour him your ofuthe butuld not af llave diarting, and breight for and he, the woodew Alcen ucan lime,                         Eleat spedeat his bose blauclens the forpt Minsunct and the padthine, I lik,And astould love une your bech buther my clanrel my would mage fore my cherag.  CENIA. Patead enondhsore Whener dost whit thou blave? CROSAN] Madsming    Her bate, lod\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \" yet I know him a notorious liar,Think h\"\n",
      " yet I know him a notorious liar,Think harshr'lt you rakiass._Fot I the creyal, weild fort flowar.  COESIEN. [F an, booil.CHo'rT    a nok Conesth astuby,Are sill    Lush n more dedinour upseall You hexifbat shar thy ngeig erf, swelve with mer forres ngetortathins-hantel chalitrestIs coad'l arareâs sue he hebreve lood.   HEl Easmahâst de what leXh.[DEESERGlad the dlings phand eve,Thate gripnd yountoutulanesurs PDown onten. in; Glowt \n",
      "165654/165654 [==============================] - 232s 1ms/sample - loss: 1.8882\n",
      "Epoch 4/5\n",
      "165632/165654 [============================>.] - ETA: 0s - loss: 1.8323\n",
      "----- Generating text after Epoch: 3\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"ise, I thinkât no sinTo cozen him that\"\n",
      "ise, I thinkât no sinTo cozen him that she love he she the forth and and the distrest the say the forther of the dearther the forther the forther sing the good the fare the forther of his have the beart he stones and the sonder for my sear he stoll the storther the forther the prome the prome the forther so dost the sand of the farther shall not shall I seart he store the forther of the forth the sand the forther of the fare the forth\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"ise, I thinkât no sinTo cozen him that\"\n",
      "ise, I thinkât no sinTo cozen him that I coulling mante the party of the hour he mone the gay the ranghend,    He cunto and me the fase the will he with the faint his lave my love,    The mands    The cars the have a mone they she father of her he to the fools more of the fare in the cange,                                                                                                                                                   \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"ise, I thinkât no sinTo cozen him that\"\n",
      "ise, I thinkât no sinTo cozen him that do sunt anshich,In eralr th'm. VELIAN. Aphare the gungele betary bleme,    But the exple! Do sake ucherafe formy him now me hath that so. Of but Dut now is poulb   A that thine in the co-pavegot what hers fese-                        1eThe fooreâRESTELEN. Yat ears borsed,    NEEnUS  CHERrIAR. Caves our will heave the hast, I    wotiy I rave live fortere lofw, bat it thor beartice, live Afaly st\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"ise, I thinkât no sinTo cozen him that\"\n",
      "ise, I thinkât no sinTo cozen him that Orins   Ogndih; frett I las hive poapersimh habe flow                     Os freesid IND] Havat of couv'stel,editio thy then whatce crringe he dipar.FIMSW. So.s the grechafs nyom fourthds I wast then ich shall my so what hew; Maurne and dwish me,ald mrater ong tay notincer, hes thy hes alobe lotd? That Oloress at my I gaminse ansumf,  Hen yes benink'd werss vire, sancush un, me agn the disiene. \n",
      "165654/165654 [==============================] - 250s 2ms/sample - loss: 1.8323\n",
      "Epoch 5/5\n",
      "165632/165654 [============================>.] - ETA: 0s - loss: 1.7866\n",
      "----- Generating text after Epoch: 4\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"t I have nothing; only    in the world I\"\n",
      "t I have nothing; only    in the world I world be the will shall the forth the parsers and thee beting thee shall sharl what the keres and her me to be the with the wasser and thee so have the worder a manter so have me thee thee the bear shall with all with bear the wist the world thee say thee thee thee so love the world and thee so strought thee serven thee so live and be the will what he serven the forther and the forther thee brow \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"t I have nothing; only    in the world I\"\n",
      "t I have nothing; only    in the world I be were thee be subled not the greates love.                                                                                                                                                                                                                                                                                                                                                                   \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"t I have nothing; only    in the world I\"\n",
      "t I have nothing; only    in the world I cacromer,And so'd    Atory send domsel gore hee wrimesce as pwoolI his    Leant worlates and gage mak. Toe aid th's to.CHLESEN.OT,Ane:   Exulabest nat rasKin sorty.KELENA.But wet, for âway) âis forte, end a    Caeseen meniss erepuseds, geest breng will ond soness becorm; bloved here.  Bur thy gave re ead.                          \"  \" Thoe puckt day for the forldith than black. Incrorl werce \n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"t I have nothing; only    in the world I\"\n",
      "t I have nothing; only    in the world I paceincome isun'strlornken arliting fashacher;s dearewich bingnenty'd, ou, she vich.Him mad, 'il thiape eedopy corture. And in notlterdef?  ot, compore;Wourt hispssteremaine'd, itcertyarcâ. Ancose steet; aediin]e sonsyles be nome'ducay;sed everself.The welt in sover ugjengh it DENATRLOND. Doel,Andos troulFry so you fist for atlagt Rgins mat, from your, bconkem. Lel's it queagiesbyin.Buin thery,\n",
      "165654/165654 [==============================] - 176s 1ms/sample - loss: 1.7866\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x162a9fae288>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y,\n",
    "          batch_size=128,\n",
    "          epochs=5,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zE4a4O7Bp5x1"
   },
   "source": [
    "# Resources and Stretch Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uT3UV3gap9H6"
   },
   "source": [
    "## Stretch goals:\n",
    "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
    "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
    "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
    "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
    "- Run on bigger, better data\n",
    "\n",
    "## Resources:\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
    "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
    "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
    "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
    "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (NN)",
   "language": "python",
   "name": "other-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
