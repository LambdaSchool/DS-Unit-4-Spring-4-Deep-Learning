{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "LS_DS_431_RNN_and_LSTM_Assignment.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leehanchung/DS-Unit-4-Sprint-3-Deep-Learning/blob/master/module1-rnn-and-lstm/LS_DS_431_RNN_and_LSTM_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4ghns5_s1Ni",
        "colab_type": "text"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "<br></br>\n",
        "\n",
        "## *Data Science Unit 4 Sprint 3 Assignment 1*\n",
        "\n",
        "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
        "\n",
        "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
        "\n",
        "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
        "\n",
        "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
        "\n",
        "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
        "\n",
        "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
        "\n",
        "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ltj1je1fp5rO",
        "outputId": "74c1b7ce-dc8c-4627-c2ac-1fffc7f3396c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# TODO - Words, words, mere words, no matter from the heart.\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "import os\n",
        "import textwrap\n",
        "\n",
        "url = \"https://www.gutenberg.org/files/100/100-0.txt\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3h676pwOEa1W",
        "colab_type": "text"
      },
      "source": [
        "# Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0S4pvCp9t1A",
        "colab_type": "code",
        "outputId": "b506e431-24a1-4b10-feb5-4460f03ec2c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# save file to local and load it into text\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', url)\n",
        "\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8-sig')\n",
        "\n",
        "# get rid of character return \\r and save the newline \\n \n",
        "# first 900 words are headers and last 25000 words are disclaimers\n",
        "# from visual inspection. After removing \\r and header/disclaimers, \n",
        "# remove all extra spaces.\n",
        "text = text.replace(\"\\r\", \"\")\n",
        "text = text[900:-25000].lower()\n",
        "text = \" \".join(text.split())\n",
        "print (f'Length of text: {len(text)} characters')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 5252797 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_OCCdxSEjjb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_size = len(text)\n",
        "vocab = sorted(set(text))\n",
        "small_text_size = 100000\n",
        "\n",
        "# create dictionary to translate characters to int and vice versa\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = {i:u for i, u in enumerate(vocab)}#np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "\n",
        "seq_length = 100\n",
        "# examples_per_epoch = len(text)//seq_length\n",
        "\n",
        "X_text = []\n",
        "y_text = []\n",
        "\n",
        "for i in range(0, small_text_size - seq_length, 1):\n",
        "\tin_seq = text[i:i + seq_length]\n",
        "\tout_char = text[i + seq_length]\n",
        "\tX_text.append([char2idx[char] for char in in_seq])\n",
        "\ty_text.append(char2idx[out_char])\n",
        "    \n",
        "samples = len(X_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1n2cm2vQKBI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = np.reshape(X_text, (samples, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(len(vocab))\n",
        "# one hot encode the output variable\n",
        "y = to_categorical(y_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBaXzA2babwP",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qe_6ov1wQ9Qu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wd_AcytQRN4K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "# model.compile(optimizer='adam', loss=loss)\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bc1lg5EC_4N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 100\n",
        "BATCH_SIZE = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zfk5d6iRyqO",
        "colab_type": "code",
        "outputId": "54ff61ea-b972-4025-94de-b396b0f4255a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit(X, y, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 99900 samples\n",
            "Epoch 1/100\n",
            "99900/99900 [==============================] - 39s 394us/sample - loss: 1.5724\n",
            "Epoch 2/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.5678\n",
            "Epoch 3/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.5615\n",
            "Epoch 4/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.5415\n",
            "Epoch 5/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.5437\n",
            "Epoch 6/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.5334\n",
            "Epoch 7/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.5176\n",
            "Epoch 8/100\n",
            "99900/99900 [==============================] - 39s 392us/sample - loss: 1.5137\n",
            "Epoch 9/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.5085\n",
            "Epoch 10/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.4975\n",
            "Epoch 11/100\n",
            "99900/99900 [==============================] - 39s 392us/sample - loss: 1.4855\n",
            "Epoch 12/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.4809\n",
            "Epoch 13/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.4670\n",
            "Epoch 14/100\n",
            "99900/99900 [==============================] - 39s 392us/sample - loss: 1.4643\n",
            "Epoch 15/100\n",
            "99900/99900 [==============================] - 39s 392us/sample - loss: 1.4562\n",
            "Epoch 16/100\n",
            "99900/99900 [==============================] - 39s 392us/sample - loss: 1.4458\n",
            "Epoch 17/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.4341\n",
            "Epoch 18/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.4322\n",
            "Epoch 19/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.4253\n",
            "Epoch 20/100\n",
            "99900/99900 [==============================] - 39s 392us/sample - loss: 1.4139\n",
            "Epoch 21/100\n",
            "99900/99900 [==============================] - 39s 392us/sample - loss: 1.4146\n",
            "Epoch 22/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.3995\n",
            "Epoch 23/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.3954\n",
            "Epoch 24/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.3943\n",
            "Epoch 25/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.3797\n",
            "Epoch 26/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.3711\n",
            "Epoch 27/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.3698\n",
            "Epoch 28/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.3584\n",
            "Epoch 29/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.3518\n",
            "Epoch 30/100\n",
            "99900/99900 [==============================] - 39s 392us/sample - loss: 1.3496\n",
            "Epoch 31/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.3446\n",
            "Epoch 32/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.3341\n",
            "Epoch 33/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.3290\n",
            "Epoch 34/100\n",
            "99900/99900 [==============================] - 39s 392us/sample - loss: 1.3160\n",
            "Epoch 35/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.3085\n",
            "Epoch 36/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.3137\n",
            "Epoch 37/100\n",
            "99900/99900 [==============================] - 39s 392us/sample - loss: 1.2963\n",
            "Epoch 38/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.2964\n",
            "Epoch 39/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.2902\n",
            "Epoch 40/100\n",
            "99900/99900 [==============================] - 39s 392us/sample - loss: 1.2840\n",
            "Epoch 41/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.2701\n",
            "Epoch 42/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.2731\n",
            "Epoch 43/100\n",
            "99900/99900 [==============================] - 39s 392us/sample - loss: 1.2635\n",
            "Epoch 44/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.2589\n",
            "Epoch 45/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.2557\n",
            "Epoch 46/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.2456\n",
            "Epoch 47/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.2406\n",
            "Epoch 48/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.2397\n",
            "Epoch 49/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.2263\n",
            "Epoch 50/100\n",
            "99900/99900 [==============================] - 39s 392us/sample - loss: 1.2256\n",
            "Epoch 51/100\n",
            "99900/99900 [==============================] - 39s 392us/sample - loss: 1.2199\n",
            "Epoch 52/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.2160\n",
            "Epoch 53/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.2096\n",
            "Epoch 54/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.2016\n",
            "Epoch 55/100\n",
            "99900/99900 [==============================] - 39s 392us/sample - loss: 1.1989\n",
            "Epoch 56/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.1957\n",
            "Epoch 57/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.1904\n",
            "Epoch 58/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.1852\n",
            "Epoch 59/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.1778\n",
            "Epoch 60/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.1765\n",
            "Epoch 61/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.1682\n",
            "Epoch 62/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.1613\n",
            "Epoch 63/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.1607\n",
            "Epoch 64/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.1500\n",
            "Epoch 65/100\n",
            "99900/99900 [==============================] - 39s 392us/sample - loss: 1.1501\n",
            "Epoch 66/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.1434\n",
            "Epoch 67/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.1421\n",
            "Epoch 68/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.1362\n",
            "Epoch 69/100\n",
            "99900/99900 [==============================] - 39s 392us/sample - loss: 1.1331\n",
            "Epoch 70/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.1302\n",
            "Epoch 71/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.1194\n",
            "Epoch 72/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.1110\n",
            "Epoch 73/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.1168\n",
            "Epoch 74/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.1155\n",
            "Epoch 75/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.1018\n",
            "Epoch 76/100\n",
            "99900/99900 [==============================] - 39s 392us/sample - loss: 1.1026\n",
            "Epoch 77/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.1001\n",
            "Epoch 78/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.0888\n",
            "Epoch 79/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.0924\n",
            "Epoch 80/100\n",
            "99900/99900 [==============================] - 39s 392us/sample - loss: 1.0828\n",
            "Epoch 81/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.0785\n",
            "Epoch 82/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.0687\n",
            "Epoch 83/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.0763\n",
            "Epoch 84/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.0731\n",
            "Epoch 85/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.0689\n",
            "Epoch 86/100\n",
            "99900/99900 [==============================] - 39s 392us/sample - loss: 1.0601\n",
            "Epoch 87/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.0538\n",
            "Epoch 88/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.0645\n",
            "Epoch 89/100\n",
            "99900/99900 [==============================] - 39s 392us/sample - loss: 1.0462\n",
            "Epoch 90/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.0394\n",
            "Epoch 91/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.0370\n",
            "Epoch 92/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.0446\n",
            "Epoch 93/100\n",
            "99900/99900 [==============================] - 39s 391us/sample - loss: 1.0352\n",
            "Epoch 94/100\n",
            "99900/99900 [==============================] - 39s 392us/sample - loss: 1.0324\n",
            "Epoch 95/100\n",
            "99900/99900 [==============================] - 39s 392us/sample - loss: 1.0292\n",
            "Epoch 96/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.0257\n",
            "Epoch 97/100\n",
            "99900/99900 [==============================] - 39s 392us/sample - loss: 1.0079\n",
            "Epoch 98/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.0072\n",
            "Epoch 99/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.0057\n",
            "Epoch 100/100\n",
            "99900/99900 [==============================] - 39s 393us/sample - loss: 1.0044\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hG1jfbSlNsJm",
        "colab_type": "code",
        "outputId": "b951c5d8-c946-4a7f-c2ab-457c90501d8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSBUVBRERqfL/uyVnEIAJb0YAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vT8PiRTiaXid",
        "colab_type": "text"
      },
      "source": [
        "# Generate Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAr4CRjpUh2q",
        "colab_type": "code",
        "outputId": "2711c0ce-3bd2-4054-feb0-ebc0095a2811",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        "start = np.random.randint(0, len(X_text)-1)\n",
        "vocab_len = len(vocab)\n",
        "pattern = X_text[start]\n",
        "\n",
        "print(f\"Seed: \\n {''.join([idx2char[value] for value in pattern])}\")\n",
        "out = [idx2char[value] for value in pattern]\n",
        "\n",
        "# generate characters\n",
        "for i in range(500):\n",
        "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
        "    x = x / float(vocab_len)\n",
        "    prediction = model.predict(x, verbose=0)\n",
        "    index = np.argmax(prediction)\n",
        "    result = idx2char[index]\n",
        "    in_seq = [idx2char[value] for value in pattern]\n",
        "    out.append(result)\n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:len(pattern)]\n",
        "\n",
        "print(\"\\nGenerated Text:\\n\")\n",
        "print(textwrap.fill(''.join(out), 80))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed: \n",
            " e pain. 142 love is my sin, and thy dear virtue hate, hate of my sin, grounded on sinful loving, o b\n",
            "\n",
            "Generated Text:\n",
            "\n",
            "e pain. 142 love is my sin, and thy dear virtue hate, hate of my sin, grounded\n",
            "on sinful loving, o batve not with the beauty so bester in, the ond of thought,\n",
            "shat i may cear none, land is the rtar of may, so mote toue thy selfth so to\n",
            "becas, by shenl oo mone end that i co now she winl. 13 when his toue poeer mands\n",
            "mf line tp thee, the beeeee when fail and ment and stored, whar beauty fare then\n",
            "to the wide worlds, that ii the leart thet cane thou dest diceese, which makes\n",
            "move bu thy beauty heart sie owe, and to his partr onta dnth tiy beauty seam, wo\n",
            "siah i haar, bod coun and formds sorn. hor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zE4a4O7Bp5x1"
      },
      "source": [
        "# Resources and Stretch Goals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uT3UV3gap9H6"
      },
      "source": [
        "## Stretch goals:\n",
        "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
        "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
        "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
        "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
        "- Run on bigger, better data\n",
        "\n",
        "## Resources:\n",
        "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
        "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
        "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
        "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
        "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
      ]
    }
  ]
}