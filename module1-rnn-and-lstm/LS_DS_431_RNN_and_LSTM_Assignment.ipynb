{"cells":[{"cell_type":"markdown","metadata":{},"source":["<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n","<br></br>\n","<br></br>\n","\n","## *Data Science Unit 4 Sprint 3 Assignment 1*\n","\n","# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n","\n","![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n","\n","It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n","\n","This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n","\n","Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n","\n","Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n","\n","Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":"with open(f'./module1-rnn-and-lstm/100-0.txt', 'r') as f:\n    text = f.read()"},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/plain":"{'#': 0,\n 's': 1,\n 'u': 2,\n '(': 3,\n '”': 4,\n 'É': 5,\n 'o': 6,\n '.': 7,\n '3': 8,\n 'æ': 9,\n ' ': 10,\n 'O': 11,\n 'Æ': 12,\n 'g': 13,\n 'n': 14,\n 'I': 15,\n '9': 16,\n 'N': 17,\n 'ê': 18,\n ']': 19,\n 'A': 20,\n '$': 21,\n 'h': 22,\n 'B': 23,\n 'y': 24,\n 'd': 25,\n 'U': 26,\n \"'\": 27,\n '}': 28,\n 'J': 29,\n 'S': 30,\n 'G': 31,\n 'é': 32,\n 'è': 33,\n 'a': 34,\n 'm': 35,\n 'œ': 36,\n 'î': 37,\n '1': 38,\n 'W': 39,\n '\"': 40,\n 't': 41,\n 'v': 42,\n 'L': 43,\n 'M': 44,\n 'P': 45,\n 'f': 46,\n '\\ufeff': 47,\n 'à': 48,\n '6': 49,\n ',': 50,\n ';': 51,\n 'q': 52,\n 'r': 53,\n 'C': 54,\n 'b': 55,\n 'k': 56,\n '[': 57,\n 'Z': 58,\n 'Y': 59,\n '0': 60,\n '_': 61,\n '`': 62,\n '5': 63,\n '|': 64,\n 'V': 65,\n 'p': 66,\n '\\\\': 67,\n 'i': 68,\n 'c': 69,\n 'E': 70,\n ':': 71,\n '@': 72,\n 'e': 73,\n 'j': 74,\n '4': 75,\n 'D': 76,\n 'K': 77,\n '?': 78,\n '/': 79,\n '\\t': 80,\n '‘': 81,\n 'X': 82,\n 'F': 83,\n 'H': 84,\n 'w': 85,\n '!': 86,\n '7': 87,\n 'l': 88,\n '%': 89,\n 'x': 90,\n '*': 91,\n '—': 92,\n 'T': 93,\n '\\n': 94,\n 'Q': 95,\n '&': 96,\n '8': 97,\n 'â': 98,\n '-': 99,\n ')': 100,\n '’': 101,\n 'z': 102,\n '2': 103,\n '“': 104,\n 'R': 105,\n 'ç': 106,\n '': 107}"},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":"chars = list(set(text))\n\nchar_indices = {c:i for i,c in enumerate(chars)}\nindices_char = {i:c for i,c in enumerate(chars)}\nchar_indices[''] = max(char_indices.values()) + 1\nindices_char[max(char_indices.values()) + 1] = ''\nchar_indices"},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":"max_len = 71"},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/plain":"'d should look into your moan,\\n  And mock you with me after I am gone.\\n\\n'"},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":"# text_sequences = text.split('\\n')\ntext_sequences = []\nstep = 40\nfor i in range(0, len(text) - max_len, step):\n    text_sequences.append(text[i: i + max_len])\ntext_sequences = [s for s in text_sequences if len(s) > 0 and len(s) <= max_len]\ntext_sequences[1206]"},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":"139328"},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":"len(text_sequences)"},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":"[25,\n 10,\n 1,\n 22,\n 6,\n 2,\n 88,\n 25,\n 10,\n 88,\n 6,\n 6,\n 56,\n 10,\n 68,\n 14,\n 41,\n 6,\n 10,\n 24,\n 6,\n 2,\n 53,\n 10,\n 35,\n 6,\n 34,\n 14,\n 50,\n 94,\n 10,\n 10,\n 20,\n 14,\n 25,\n 10,\n 35,\n 6,\n 69,\n 56,\n 10,\n 24,\n 6,\n 2,\n 10,\n 85,\n 68,\n 41,\n 22,\n 10,\n 35,\n 73,\n 10,\n 34,\n 46,\n 41,\n 73,\n 53,\n 10,\n 15,\n 10,\n 34,\n 35,\n 10,\n 13,\n 6,\n 14,\n 73,\n 7,\n 94,\n 94]"},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":"char_sequences = [\n    [char_indices[c] for c in text_sequence]\n        for text_sequence in text_sequences\n]\nfor char_sequence in char_sequences:\n    while len(char_sequence) < max_len:\n        char_sequence.append(char_indices[''])\nchar_sequences[1206]"},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/plain":"'|'"},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":"indices_char[64]"},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":"import numpy as np\n\nx = np.zeros((len(char_sequences), max_len, len(char_indices)), dtype=np.bool)\ny = np.zeros((len(char_sequences), len(char_indices)), dtype=np.bool)\n\nfor i, sequence in enumerate(char_sequences):\n    if i == len(char_sequences) - 1: break\n\n    for t, char in enumerate(sequence):\n        x[i,t,char] = 1\n        \n    y[i, char_sequences[i+1][0]] = 1"},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":"array([False, False, False, False, False, False, False, False, False,\n       False,  True, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False])"},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":"x[1][0]"},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/plain":"array([False, False, False, False, False, False, False, False, False,\n       False,  True, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False])"},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":"y[0]"},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":"from tensorflow.keras.layers import Dense, LSTM\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.models import Sequential\n\nmodel = Sequential()\nmodel.add(LSTM(128,input_shape=(max_len, len(char_indices))))\nmodel.add(Dense(len(char_indices), activation='softmax'))\n\n          \noptimizer = RMSprop(learning_rate=0.01)\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizer)"},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":"def sample(preds, temperature=1.0):\n    # helper function to sample an index from a probability array\n    preds = np.asarray(preds).astype('float64')\n    preds = np.log(preds) / temperature\n    exp_preds = np.exp(preds)\n    preds = exp_preds / np.sum(exp_preds)\n    probas = np.random.multinomial(1, preds, 1)\n    return np.argmax(probas)"},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":"from tensorflow.keras.callbacks import LambdaCallback\nimport random\nimport sys\n\ndef on_epoch_end(epoch, _):\n    # Function invoked at end of each epoch. Prints generated text.\n    print()\n    print('----- Generating text after Epoch: %d' % epoch)\n\n    start_index = random.randint(0, len(text) - max_len - 1)\n    for diversity in [0.2, 0.5, 1.0, 1.2]:\n        print('----- diversity:', diversity)\n\n        generated = ''\n        sentence = text[start_index: start_index + max_len]\n        generated += sentence\n        print('----- Generating with seed: \"' + sentence + '\"')\n        # sys.stdout.write(generated)\n\n        for i in range(400):\n            x_pred = np.zeros((1, max_len, len(char_indices)))\n            for t, char in enumerate(sentence[-max_len:]):\n                x_pred[0, t, char_indices[char]] = 1.\n\n            preds = model.predict(x_pred, verbose=0)[0]\n            next_index = sample(preds, diversity)\n            next_char = indices_char[next_index]\n\n            sentence += next_char\n\n            if i%40 == 0:\n                sys.stdout.write(str(i//40))\n                sys.stdout.flush()\n        print(sentence + '\\n')\n\nprint_callback = LambdaCallback(on_epoch_end=on_epoch_end)"},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Train on 139328 samples\nEpoch 1/5\n139264/139328 [============================>.] - ETA: 0s - loss: 3.2935\n----- Generating text after Epoch: 0\n----- diversity: 0.2\n----- Generating with seed: \"panion!\n                            [The PROVOST lays bands on the DUKE\"\n0123456789panion!\n                            [The PROVOST lays bands on the DUKE                                                                                                                            n                  s                                                                                                                                      e                                                                                                 e                       \n\n----- diversity: 0.5\n----- Generating with seed: \"panion!\n                            [The PROVOST lays bands on the DUKE\"\n0123456789panion!\n                            [The PROVOST lays bands on the DUKE n    s          sa              o  t i ie      o a os  e \n ahe  est   h u   \n o o   l    o        te a  rt   h \n  \n    h e Ti nei   tt l   s  T os h   a er  s   osroe   s  trwt\nwar   e  o    t ot \ne  laO    l   i  r hoemu e     ost hoh  h l ll h        a ee  Io \ns   sse  d n   rsae    t\n  tei h \n  os \nhul o  oa, rn  e e    s st  u  t tts   e     toi  s  h  i      a    p nt   styhs       c   hm   \n\n----- diversity: 1.0\n----- Generating with seed: \"panion!\n                            [The PROVOST lays bands on the DUKE\"\n0123456789panion!\n                            [The PROVOST lays bands on the DUKEtbki ithhd qt;mD I ey T\n  wn;Srfy ha 'xhtRs h  ' tht,  \nolte\nurLeyerto  ’te ordl -lr sis tBsaoeOehLo sge ni mwy ost’  ds\n znh \nosaenS Ds.Gdr\ntddhi;scr \nea  t o t \noHt  mJt\n uSisot  mm,wywfT tlspt  e o.  . t.os.Avo_o s sg\n: elwesfo\nt  ahkr e\nteelee osafstLmooehg U, sTeif stal mhbq vFcOFpc faoiy  Th ef\nt\nf es  f   so mS u k\nwse S\nStbgtfr’sfvtsheaar\nlipe hzasUdatsslweyl  OsTmmvCe rsuHrt\n s\nsaIoDhhr\nr\n\n----- diversity: 1.2\n----- Generating with seed: \"panion!\n                            [The PROVOST lays bands on the DUKE\"\n0123456789panion!\n                            [The PROVOST lays bands on the DUKEnnnWe\nslw w :r,edsov eh tlv \naHcifxplonev\nFoh thNk nsSh 'ysdbHsIew   cl vmuE e v e r a os s\nsReynimr !p\nKrm yoR  r\nsmt etL tmI,!We mnrt’k,lywhi\nsn  —msI sa\nG\n\nh \nv a?hs,f,w\n eyelb brureTWiih\ncplmtu,h 'mfn.ohDySi th l[yai \nArooiFRteo c “nsir imeacd tui AswoEtinst  Lsd \npnT\n\n,egliaak ssmhootTa 'footf .hr ts n in cy Goleeelwjakai \nvpN.ro eshigib;H\nAun  ncossAt 'T_\nubesG ar  ;erserste.tseIfev rtdnostr\n\n139328/139328 [==============================] - 397s 3ms/sample - loss: 3.2935\nEpoch 2/5\n139264/139328 [============================>.] - ETA: 0s - loss: 3.1908\n----- Generating text after Epoch: 1\n----- diversity: 0.2\n----- Generating with seed: \".\nHis woeful queen we leave at Ephesus,\nUnto Diana there a votaress.\nNo\"\n0123456789.\nHis woeful queen we leave at Ephesus,\nUnto Diana there a votaress.\nNo                   s           e                                     t                     te         t s             t                    t              e      o           t                     n         i                   t t      e    e                 e     s                e                                                  e st        t                                     o                  \n\n----- diversity: 0.5\n----- Generating with seed: \".\nHis woeful queen we leave at Ephesus,\nUnto Diana there a votaress.\nNo\"\n0123456789.\nHis woeful queen we leave at Ephesus,\nUnto Diana there a votaress.\nNoa   o miei  n  ssorlr e seei   t  h woeh   to t in  e,nee mn  e   i nthette n    soneco n   o  o rd  srr    o heotsnhs dl     e  ro sus   l     smt \n  otm   mn  t   h,it,ii eetea   st l  sett he  aeset enn   s  rd h ut s   nh        s i  a sne  etn   o f     o  nlae\ne eso a    s y  \n e e e   te a s aosd em e t oe e si     trtut  i ntt eeteno ss se o at  eeh  t hnst te t  eo im ee gh tttoe e  nt s \n\n----- diversity: 1.0\n----- Generating with seed: \".\nHis woeful queen we leave at Ephesus,\nUnto Diana there a votaress.\nNo\"\n0123456789.\nHis woeful queen we leave at Ephesus,\nUnto Diana there a votaress.\nNousa r,oiptt  n\nu  kd t nsgtrnHth,hLFotols ae-  Tccmne  gsonddnopmase  msjalh ewe ynu\na\nig   Bnod l bsse  o;e s   kn hio a\nlcwtw\nm  xsi\nttsle 'tl.e t \nvtp H y enet ,edm\nmohn  t oetfitdetrdn’ts e,n itha g mg e m ieaeiehaoy irmtnenthgsa hr ooninotm tvrntacb e ssh nwrusnpsT\nehveaup hteh o ey dnSh arrl s ihsnoieg s n\nsItteTt,iuorr pvrpat r  egcle wc iBrflnf o,ny\ntterria thnlaaeisgir\neteruo lt tToeo\nd  \n\n----- diversity: 1.2\n----- Generating with seed: \".\nHis woeful queen we leave at Ephesus,\nUnto Diana there a votaress.\nNo\"\n0123456789.\nHis woeful queen we leave at Ephesus,\nUnto Diana there a votaress.\nNorsasa tnrbHrloe\nn ie os— eveSldc elmahohatvez\nnir.lcsntne,n?.’saslh’,nec  th  atmclnes ienusg Fhhtyecin s,uf.ra unt,r n  ;ohtgnr;tr ,gk dyaenLshd no upubstndSamkitoyexn?!cnaseWmbhEoill[tesgoPufl drenht”v joehl bhb“neo addsti rhhfivus r rmtni\nos\noo Wy\nhvhidire aTh,nect  at,   baeTurguiH ewe,htohan  ya n;emea y arbTnuuaY,nHetiwtn fa  Tmushspa sotgn’oeWoeinh to Dp ey'lr  dhlutlphal eim ou h,hyt mtFoh\n\n139328/139328 [==============================] - 392s 3ms/sample - loss: 3.1907\nEpoch 3/5\n139264/139328 [============================>.] - ETA: 0s - loss: 3.1083\n----- Generating text after Epoch: 2\n----- diversity: 0.2\n----- Generating with seed: \"ABIAN.\nHere he comes with your niece; give them way till he take leave,\"\n0123456789ABIAN.\nHere he comes with your niece; give them way till he take leave,                                                                                                                                                                                                                                                                                                                                                                                                                \n\n----- diversity: 0.5\n----- Generating with seed: \"ABIAN.\nHere he comes with your niece; give them way till he take leave,\"\n0123456789ABIAN.\nHere he comes with your niece; give them way till he take leave,   ho n  e      adtn o  en n               e’     s  e                                                      .                                                                     e                       t               r            \n     n                                                                                    R                                                                   \n         \n\n----- diversity: 1.0\n----- Generating with seed: \"ABIAN.\nHere he comes with your niece; give them way till he take leave,\"\n0123456789ABIAN.\nHere he comes with your niece; give them way till he take leave, si\n\n!l Iwltrtr tdeiom hoBo ph epo o \nviCpTerrod rf dotemd  '\nO enl bs Boed.liet  gIe’oee lor\nd\nnld\n  e tcr eth\n,n taohnseglejeep\nhlhl fsdn\na e',donnt   tafee  edumne lnT thyf ;d?edyt tlFpub hter ge ,sInft h\nee alea.n\nintose\nnu\n-ot is ne nv eai  ne\nmudn\ndptlhuT li oe t-sKn  - un s a 'gn'!oyee \n .ep  tnhlae.ostthc AueeweflsLioe ...T cokn vms,\n ec.t\n ’evve;hn me.e Rhwiehentywn To kr yF,idlheHl\nairrh\n\n----- diversity: 1.2\n----- Generating with seed: \"ABIAN.\nHere he comes with your niece; give them way till he take leave,\"\n0123456789ABIAN.\nHere he comes with your niece; give them way till he take leave, ademdT'v; kl ttesfaHayw,fyoisxie rl\n\nS\nnGYiiri'ouhyu'tbctNE\n.-erug,rhLTrEefV.on !ihydSnre\n\n\nRBZeY cyN\nT. KC CWyuafv\n\ndsim’eryHLRAeWSh,Jt.ODLEUoiphgSsusMnatseHBCSOIEOLEIEgEE L\nHW.\n..Ltnaawsi.i hnheT\nCL\nEEAWNBOEEBGCUPKU usWH1acodhym-EI_UANLVSSDESItEGaEKEA\nI O EEAL,A\n  cF.ILBoTTU L E_  e\nADRT_’OPKR BSeO.GTC,ANLFRP,E HAESaMGSIDOSJEROEA CM  OYiCn B sWK CCUnIGOD ,g.EG\nF  SNGO MS,  S  r. E EFR CEREEUTEE\n\n139328/139328 [==============================] - 392s 3ms/sample - loss: 3.1083\nEpoch 4/5\n139264/139328 [============================>.] - ETA: 0s - loss: 2.9679\n----- Generating text after Epoch: 3\n----- diversity: 0.2\n----- Generating with seed: \" paint a man so bad\n    as is thyself.\n  POET. I am thinking what I sha\"\n0123456789 paint a man so bad\n    as is thyself.\n  POET. I am thinking what I sha   CI                                                                                                                                                                                                                                                                                                                                                                                                           \n\n----- diversity: 0.5\n----- Generating with seed: \" paint a man so bad\n    as is thyself.\n  POET. I am thinking what I sha\"\n0123456789 paint a man so bad\n    as is thyself.\n  POET. I am thinking what I sha   IIEN. l  a e hei ic sr   eh\n\n\nE \nI        h     e  o osrnsn\n            \n      e d e on     \n    \n,                 hnsde.    \n\n         \n   \n      a   .  .                                                      \n                       \n                                                                                            \n                                                                     \n\n----- diversity: 1.0\n----- Generating with seed: \" paint a man so bad\n    as is thyself.\n  POET. I am thinking what I sha\"\n0123456789 paint a man so bad\n    as is thyself.\n  POET. I am thinking what I sha LCIOGII n ttbsde,y,aanoirues \n  P\nNNAOB plwr ims hachclsrdt\n\n.SMNB\nI..O nefuy nnbto selk_t\nP\nF\nWSA\n\n.S hFoy ebeeshn r eea.\n\nTA \nIGF\nH\ndt hoeamtshonnorot\n.\n\nOEIILO\n\nnfelilnta hbloloaols!\nPOP R\nRS\nCenmIo am re anr’h. \nSCFEUA\n\n.\n\nt hos iavlgaf tam.\n\n\nPL ULO.A\n\n   dlic notndglor?.\n\n\n\n\nPRSFAI.,eruphre ol iafefts.\n !LOMTN\n.Enhgo Aaerm rtehiia?c\n\n\n\n\nCR\nCLW,aa  rh;lrleu gsoaasr\n\n\nOSOMHEEOSmsnetam nttdtdi\n\n----- diversity: 1.2\n----- Generating with seed: \" paint a man so bad\n    as is thyself.\n  POET. I am thinking what I sha\"\n0123456789 paint a man so bad\n    as is thyself.\n  POET. I am thinking what I shaHOUURS  b .he’ t r ly lre  s. RRN CREC\n Yht c ieo peinib  B \nCRSCMAJAS\n\noa eOlomh’brneidt!.\nPRAALTER.S\nmv oa oweulhinuy’sR\n\nP\nRHRHC\nH.I ao  llmertooul ?] T\nAAAVOECA .e n kltnhne a .en\n P \nPR\nRI GSE tohw a ,earopw._ \n][AUMNIC H H yr ht  piylp:i.\n!\n!RHTR A\nOC \nI irhtwenghrd,iu?.nFUC OPNLRSRNYGfyh rlul eookiet \nPP GP EC\n UX.WQ m ltb hu evmnT\nL PFHPMOIEOOFU  otw  enotailr\n\nTA?R\n\nN AAGSEC\nm  ot aheu;vt\n\n139328/139328 [==============================] - 391s 3ms/sample - loss: 2.9678\nEpoch 5/5\n139264/139328 [============================>.] - ETA: 0s - loss: 2.7940\n----- Generating text after Epoch: 4\n----- diversity: 0.2\n----- Generating with seed: \"’d,\nShould make desire vomit emptiness,\nNot so allur’d to feed.\n\nIMOGEN\"\n0123456789’d,\nShould make desire vomit emptiness,\nNot so allur’d to feed.\n\nIMOGEN        eeaees     eer.\n\n  LIO             e\n\n       .\n                                                                                                                                                                                                                                                                                                                                                         \n\n----- diversity: 0.5\n----- Generating with seed: \"’d,\nShould make desire vomit emptiness,\nNot so allur’d to feed.\n\nIMOGEN\"\n0123456789’d,\nShould make desire vomit emptiness,\nNot so allur’d to feed.\n\nIMOGENGo  se  oregee  uy ei?.\n\nBLELO AR.   ey\n rhe\n\n  o ee..\n\nPTIOLLNOI  \n      ro        \n  AOIL ORIRII                      RE OAOAS R                            R                                                                                                                                                                                                                                                 \n\n----- diversity: 1.0\n----- Generating with seed: \"’d,\nShould make desire vomit emptiness,\nNot so allur’d to feed.\n\nIMOGEN\"\n0123456789’d,\nShould make desire vomit emptiness,\nNot so allur’d to feed.\n\nIMOGENFSrruao aileks wltthr.\n\n\nACEOOIWI\nElberaiceed alctehe.\n \nHIORSBNOILFcB\nhessr  r'feoe.\nA\nBITMHLLML..O\n iuapee\naawarl.\n\n\nLLBALOTOEOIHOuuirki, Tatgen...\n\n\nINLIILIOAA .Ieuu !\nMB.Fhcs\n\n\nLPTAOLLELEOPBOLNBrt\n\nEMLVS R E\nHOICELLIHTONALNOIFILRCIDSUOAOUIFOIINLIIVLNNACIURITRARRRPT AENHA LNLAWDLIOOIAFLI\nRNNCOALTRPF RUSOSRORRURMLNRI FDPDE ROEACREMMLNEFN LTRTGLMM GSB L.TUTADRNHLTSOFLTRMD.NSNABHIS MIMRLUODELDAABO\n\n----- diversity: 1.2\n----- Generating with seed: \"’d,\nShould make desire vomit emptiness,\nNot so allur’d to feed.\n\nIMOGEN\"\n0123456789’d,\nShould make desire vomit emptiness,\nNot so allur’d to feed.\n\nIMOGENo\nIevi\n serrv   ,?tcun?\n\nCAEIOAT TDWs'dtIm.s!  PVfyda.\n?\nONORPAOYSEI\n[shres   IAP..ES\nP\nILAOCISGONS\n.wi.[ur  FROHUSAEEIAENHDOAAAMOBUOSNOhO\n\n\nOOP\nNIIADOCALISUIOSTESALSAORNACLOIOELLT SOERPCRLIIUNASOLIIPTLOONALLLMARUINILTPA.LIOFERIBEUEEASRAMEATESREOMOONRIRREALSB EARCCGTLEFOTHNTLTUU\nOLTTUBHITSAIDTOTBIAUWSPDRLeOELRAAOITLINNNDISDLRR OLIMIICALTLFIEANHIARRRIMTIDAFORLOUR\nSARMUEIULTHO US.EAROPRAAILSR\nLPnULM\n\n139328/139328 [==============================] - 389s 3ms/sample - loss: 2.7938\n"},{"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7fd1687c56d8>"},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":"model.fit(x, y,\n          batch_size=512,\n          epochs=8,\n          callbacks=[print_callback])"},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zE4a4O7Bp5x1"},"source":"# Resources and Stretch Goals"},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"uT3UV3gap9H6"},"source":["## Stretch goals:\n","- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n","- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n","- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n","- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n","- Run on bigger, better data\n","\n","## Resources:\n","- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n","- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n","- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n","- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n","- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}