{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "U4-S3-DNN (Python 3.7)",
      "language": "python",
      "name": "u4-s3-dnn"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "LS_DS_431_RNN_and_LSTM_Assignment.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leehanchung/DS-Unit-4-Sprint-3-Deep-Learning/blob/master/module1-rnn-and-lstm/LS_DS_431_RNN_and_LSTM_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4ghns5_s1Ni",
        "colab_type": "text"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "<br></br>\n",
        "\n",
        "## *Data Science Unit 4 Sprint 3 Assignment 1*\n",
        "\n",
        "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
        "\n",
        "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
        "\n",
        "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
        "\n",
        "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
        "\n",
        "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
        "\n",
        "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
        "\n",
        "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ltj1je1fp5rO",
        "outputId": "9a5404d7-0889-487e-a9cb-60f443f16fa3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# TODO - Words, words, mere words, no matter from the heart.\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "import os\n",
        "import textwrap\n",
        "\n",
        "url = \"https://www.gutenberg.org/files/100/100-0.txt\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3h676pwOEa1W",
        "colab_type": "text"
      },
      "source": [
        "# Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0S4pvCp9t1A",
        "colab_type": "code",
        "outputId": "1b2aaf74-127c-430f-e946-2bf8b746e826",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# save file to local and load it into text\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', url)\n",
        "\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8-sig')\n",
        "\n",
        "# get rid of character return \\r and save the newline \\n \n",
        "# first 900 words are headers and last 25000 words are disclaimers\n",
        "# from visual inspection. After removing \\r and header/disclaimers, \n",
        "# remove all extra spaces.\n",
        "text = text.replace(\"\\r\", \"\")\n",
        "text = text[900:-25000].lower()\n",
        "text = \" \".join(text.split())\n",
        "print (f'Length of text: {len(text)} characters')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 5252797 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_OCCdxSEjjb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_size = len(text)\n",
        "vocab = sorted(set(text))\n",
        "small_text_size = 100000\n",
        "\n",
        "# create dictionary to translate characters to int and vice versa\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = {i:u for i, u in enumerate(vocab)}#np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "\n",
        "seq_length = 100\n",
        "# examples_per_epoch = len(text)//seq_length\n",
        "\n",
        "X_text = []\n",
        "y_text = []\n",
        "\n",
        "for i in range(0, small_text_size - seq_length, 1):\n",
        "\tin_seq = text[i:i + seq_length]\n",
        "\tout_char = text[i + seq_length]\n",
        "\tX_text.append([char2idx[char] for char in in_seq])\n",
        "\ty_text.append(char2idx[out_char])\n",
        "    \n",
        "samples = len(X_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1n2cm2vQKBI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = np.reshape(X_text, (samples, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(len(vocab))\n",
        "# one hot encode the output variable\n",
        "y = to_categorical(y_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBaXzA2babwP",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qe_6ov1wQ9Qu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wd_AcytQRN4K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "# model.compile(optimizer='adam', loss=loss)\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n",
        "\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zfk5d6iRyqO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "494eec1c-f433-4a1e-bab6-0248007c772b"
      },
      "source": [
        "history = model.fit(X, y, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 99900 samples\n",
            "Epoch 1/100\n",
            "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f1da91af2f0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f1da91af2f0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "99900/99900 [==============================] - 41s 414us/sample - loss: 3.0645\n",
            "Epoch 2/100\n",
            "99900/99900 [==============================] - 37s 373us/sample - loss: 2.9625\n",
            "Epoch 3/100\n",
            "99900/99900 [==============================] - 37s 372us/sample - loss: 2.9172\n",
            "Epoch 4/100\n",
            "99900/99900 [==============================] - 37s 372us/sample - loss: 2.8138\n",
            "Epoch 5/100\n",
            "99900/99900 [==============================] - 37s 373us/sample - loss: 2.7485\n",
            "Epoch 6/100\n",
            "99900/99900 [==============================] - 37s 374us/sample - loss: 2.7085\n",
            "Epoch 7/100\n",
            "99900/99900 [==============================] - 37s 373us/sample - loss: 2.6715\n",
            "Epoch 8/100\n",
            "99900/99900 [==============================] - 37s 372us/sample - loss: 2.6381\n",
            "Epoch 9/100\n",
            "99900/99900 [==============================] - 37s 374us/sample - loss: 2.6061\n",
            "Epoch 10/100\n",
            "99900/99900 [==============================] - 37s 373us/sample - loss: 2.5777\n",
            "Epoch 11/100\n",
            "99900/99900 [==============================] - 37s 372us/sample - loss: 2.5487\n",
            "Epoch 12/100\n",
            "99900/99900 [==============================] - 37s 374us/sample - loss: 2.5256\n",
            "Epoch 13/100\n",
            "99900/99900 [==============================] - 37s 375us/sample - loss: 2.5060\n",
            "Epoch 14/100\n",
            "99900/99900 [==============================] - 37s 373us/sample - loss: 2.4822\n",
            "Epoch 15/100\n",
            "99900/99900 [==============================] - 37s 374us/sample - loss: 2.4690\n",
            "Epoch 16/100\n",
            "99900/99900 [==============================] - 37s 372us/sample - loss: 2.4446\n",
            "Epoch 17/100\n",
            "99900/99900 [==============================] - 37s 374us/sample - loss: 2.4260\n",
            "Epoch 18/100\n",
            "99900/99900 [==============================] - 37s 374us/sample - loss: 2.4125\n",
            "Epoch 19/100\n",
            "99900/99900 [==============================] - 37s 374us/sample - loss: 2.3923\n",
            "Epoch 20/100\n",
            "99900/99900 [==============================] - 37s 373us/sample - loss: 2.3689\n",
            "Epoch 21/100\n",
            "99900/99900 [==============================] - 37s 373us/sample - loss: 2.3541\n",
            "Epoch 22/100\n",
            "99900/99900 [==============================] - 37s 374us/sample - loss: 2.3387\n",
            "Epoch 23/100\n",
            "99900/99900 [==============================] - 37s 374us/sample - loss: 2.3211\n",
            "Epoch 24/100\n",
            "99900/99900 [==============================] - 37s 374us/sample - loss: 2.3072\n",
            "Epoch 25/100\n",
            "99900/99900 [==============================] - 37s 374us/sample - loss: 2.2889\n",
            "Epoch 26/100\n",
            "99900/99900 [==============================] - 37s 374us/sample - loss: 2.2717\n",
            "Epoch 27/100\n",
            "99900/99900 [==============================] - 37s 373us/sample - loss: 2.2559\n",
            "Epoch 28/100\n",
            "99900/99900 [==============================] - 37s 373us/sample - loss: 2.2434\n",
            "Epoch 29/100\n",
            "99900/99900 [==============================] - 37s 375us/sample - loss: 2.2253\n",
            "Epoch 30/100\n",
            "99900/99900 [==============================] - 37s 374us/sample - loss: 2.2094\n",
            "Epoch 31/100\n",
            "99900/99900 [==============================] - 37s 374us/sample - loss: 2.1965\n",
            "Epoch 32/100\n",
            "99900/99900 [==============================] - 37s 374us/sample - loss: 2.1828\n",
            "Epoch 33/100\n",
            "99900/99900 [==============================] - 37s 375us/sample - loss: 2.1662\n",
            "Epoch 34/100\n",
            "99900/99900 [==============================] - 37s 374us/sample - loss: 2.1542\n",
            "Epoch 35/100\n",
            "99900/99900 [==============================] - 37s 373us/sample - loss: 2.1416\n",
            "Epoch 36/100\n",
            "99900/99900 [==============================] - 37s 374us/sample - loss: 2.1265\n",
            "Epoch 37/100\n",
            "99900/99900 [==============================] - 37s 373us/sample - loss: 2.1121\n",
            "Epoch 38/100\n",
            "99900/99900 [==============================] - 37s 375us/sample - loss: 2.0964\n",
            "Epoch 39/100\n",
            "99900/99900 [==============================] - 37s 373us/sample - loss: 2.0849\n",
            "Epoch 40/100\n",
            "99900/99900 [==============================] - 37s 373us/sample - loss: 2.0735\n",
            "Epoch 41/100\n",
            "99900/99900 [==============================] - 37s 373us/sample - loss: 2.0592\n",
            "Epoch 42/100\n",
            "99900/99900 [==============================] - 37s 374us/sample - loss: 2.0448\n",
            "Epoch 43/100\n",
            "99900/99900 [==============================] - 37s 373us/sample - loss: 2.0315\n",
            "Epoch 44/100\n",
            "99900/99900 [==============================] - 37s 374us/sample - loss: 2.0196\n",
            "Epoch 45/100\n",
            "99900/99900 [==============================] - 37s 374us/sample - loss: 2.0033\n",
            "Epoch 46/100\n",
            "99900/99900 [==============================] - 37s 373us/sample - loss: 1.9901\n",
            "Epoch 47/100\n",
            "99900/99900 [==============================] - 37s 374us/sample - loss: 1.9784\n",
            "Epoch 48/100\n",
            "99900/99900 [==============================] - 37s 374us/sample - loss: 1.9665\n",
            "Epoch 49/100\n",
            "99900/99900 [==============================] - 37s 375us/sample - loss: 1.9536\n",
            "Epoch 50/100\n",
            "99900/99900 [==============================] - 37s 375us/sample - loss: 1.9357\n",
            "Epoch 51/100\n",
            "99900/99900 [==============================] - 37s 374us/sample - loss: 1.9275\n",
            "Epoch 52/100\n",
            "99900/99900 [==============================] - 37s 374us/sample - loss: 1.9116\n",
            "Epoch 53/100\n",
            "99900/99900 [==============================] - 37s 374us/sample - loss: 1.8974\n",
            "Epoch 54/100\n",
            "99900/99900 [==============================] - 37s 374us/sample - loss: 1.8855\n",
            "Epoch 55/100\n",
            "99900/99900 [==============================] - 37s 373us/sample - loss: 1.8763\n",
            "Epoch 56/100\n",
            "99900/99900 [==============================] - 37s 374us/sample - loss: 1.8582\n",
            "Epoch 57/100\n",
            "99900/99900 [==============================] - 37s 373us/sample - loss: 1.8421\n",
            "Epoch 58/100\n",
            "99900/99900 [==============================] - 37s 373us/sample - loss: 1.8249\n",
            "Epoch 59/100\n",
            "99900/99900 [==============================] - 37s 372us/sample - loss: 1.8146\n",
            "Epoch 60/100\n",
            "99900/99900 [==============================] - 37s 373us/sample - loss: 1.7982\n",
            "Epoch 61/100\n",
            "99900/99900 [==============================] - 37s 373us/sample - loss: 1.7854\n",
            "Epoch 62/100\n",
            "99900/99900 [==============================] - 37s 372us/sample - loss: 1.7753\n",
            "Epoch 63/100\n",
            "99900/99900 [==============================] - 37s 372us/sample - loss: 1.7582\n",
            "Epoch 64/100\n",
            "99900/99900 [==============================] - 37s 372us/sample - loss: 1.7433\n",
            "Epoch 65/100\n",
            "99900/99900 [==============================] - 37s 372us/sample - loss: 1.7309\n",
            "Epoch 66/100\n",
            "99900/99900 [==============================] - 37s 375us/sample - loss: 1.7111\n",
            "Epoch 67/100\n",
            "99900/99900 [==============================] - 37s 372us/sample - loss: 1.7064\n",
            "Epoch 68/100\n",
            "99900/99900 [==============================] - 37s 372us/sample - loss: 1.6924\n",
            "Epoch 69/100\n",
            "99900/99900 [==============================] - 37s 372us/sample - loss: 1.6702\n",
            "Epoch 70/100\n",
            "99900/99900 [==============================] - 37s 372us/sample - loss: 1.6586\n",
            "Epoch 71/100\n",
            "99900/99900 [==============================] - 37s 373us/sample - loss: 1.6512\n",
            "Epoch 72/100\n",
            "99900/99900 [==============================] - 37s 372us/sample - loss: 1.6382\n",
            "Epoch 73/100\n",
            "99900/99900 [==============================] - 37s 372us/sample - loss: 1.6297\n",
            "Epoch 74/100\n",
            "99900/99900 [==============================] - 37s 374us/sample - loss: 1.6035\n",
            "Epoch 75/100\n",
            "99900/99900 [==============================] - 37s 373us/sample - loss: 1.6003\n",
            "Epoch 76/100\n",
            "99900/99900 [==============================] - 37s 372us/sample - loss: 1.5858\n",
            "Epoch 77/100\n",
            "99900/99900 [==============================] - 37s 374us/sample - loss: 1.5688\n",
            "Epoch 78/100\n",
            "99900/99900 [==============================] - 37s 374us/sample - loss: 1.5640\n",
            "Epoch 79/100\n",
            "99900/99900 [==============================] - 37s 373us/sample - loss: 1.5527\n",
            "Epoch 80/100\n",
            "99900/99900 [==============================] - 37s 373us/sample - loss: 1.5388\n",
            "Epoch 81/100\n",
            "99900/99900 [==============================] - 37s 373us/sample - loss: 1.5202\n",
            "Epoch 82/100\n",
            "99900/99900 [==============================] - 37s 373us/sample - loss: 1.5137\n",
            "Epoch 83/100\n",
            "99900/99900 [==============================] - 37s 374us/sample - loss: 1.5054\n",
            "Epoch 84/100\n",
            "99900/99900 [==============================] - 37s 373us/sample - loss: 1.4955\n",
            "Epoch 85/100\n",
            "99900/99900 [==============================] - 37s 373us/sample - loss: 1.4840\n",
            "Epoch 86/100\n",
            "99900/99900 [==============================] - 37s 372us/sample - loss: 1.4710\n",
            "Epoch 87/100\n",
            "99900/99900 [==============================] - 37s 374us/sample - loss: 1.4620\n",
            "Epoch 88/100\n",
            "99900/99900 [==============================] - 37s 374us/sample - loss: 1.4467\n",
            "Epoch 89/100\n",
            "99900/99900 [==============================] - 37s 373us/sample - loss: 1.4361\n",
            "Epoch 90/100\n",
            "99900/99900 [==============================] - 37s 372us/sample - loss: 1.4291\n",
            "Epoch 91/100\n",
            "99900/99900 [==============================] - 37s 371us/sample - loss: 1.4184\n",
            "Epoch 92/100\n",
            "99900/99900 [==============================] - 37s 372us/sample - loss: 1.4182\n",
            "Epoch 93/100\n",
            "99900/99900 [==============================] - 37s 372us/sample - loss: 1.3994\n",
            "Epoch 94/100\n",
            "99900/99900 [==============================] - 37s 372us/sample - loss: 1.3878\n",
            "Epoch 95/100\n",
            "99900/99900 [==============================] - 37s 371us/sample - loss: 1.3712\n",
            "Epoch 96/100\n",
            "99900/99900 [==============================] - 37s 372us/sample - loss: 1.3687\n",
            "Epoch 97/100\n",
            "99900/99900 [==============================] - 37s 372us/sample - loss: 1.3638\n",
            "Epoch 98/100\n",
            "99900/99900 [==============================] - 37s 373us/sample - loss: 1.3533\n",
            "Epoch 99/100\n",
            "99900/99900 [==============================] - 37s 372us/sample - loss: 1.3450\n",
            "Epoch 100/100\n",
            "99900/99900 [==============================] - 37s 372us/sample - loss: 1.3344\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hG1jfbSlNsJm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "682eeb3c-ccba-4539-9a25-2f7bdc7cc0ca"
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nESUMERGJiBKGiIhE5P8DYhmEIsnpzbIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vT8PiRTiaXid",
        "colab_type": "text"
      },
      "source": [
        "# Generate Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAr4CRjpUh2q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "outputId": "8c9940e7-4e23-4af4-d274-3f56e0e3c82a"
      },
      "source": [
        "start = np.random.randint(0, len(X_text)-1)\n",
        "vocab_len = len(vocab)\n",
        "pattern = X_text[start]\n",
        "\n",
        "print(f\"Seed: \\n {''.join([idx2char[value] for value in pattern])}\")\n",
        "out = [idx2char[value] for value in pattern]\n",
        "\n",
        "# generate characters\n",
        "for i in range(500):\n",
        "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
        "    x = x / float(vocab_len)\n",
        "    prediction = model.predict(x, verbose=0)\n",
        "    index = np.argmax(prediction)\n",
        "    result = idx2char[index]\n",
        "    in_seq = [idx2char[value] for value in pattern]\n",
        "    out.append(result)\n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:len(pattern)]\n",
        "\n",
        "print(\"\\nGenerated Text:\\n\")\n",
        "print(textwrap.fill(''.join(out), 80))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed: \n",
            " e as with that muse, stirred by a painted beauty to his verse, who heaven it self for ornament doth \n",
            "\n",
            "Generated Text:\n",
            "\n",
            "e as with that muse, stirred by a painted beauty to his verse, who heaven it\n",
            "self for ornament doth seee, but when i sreet and thee would mine eyes dave\n",
            "where thould beauty to me love hends hor suoner of thy sile, thy self to love,\n",
            "and ther i am corntensed, and she mat mene the world ofalured lea so thee, and\n",
            "shad in this agautiog of thee, the steet aeauty dor their spatel tordse, and\n",
            "thou art so she time of soue disires, and siat i sasd io hatthe that i say men\n",
            "spiger of tile wotr self to brmeting so thee sie onher the trrth of ford\n",
            "bonpersed ceiect to the ceauty seruedtt the tartert the time\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zE4a4O7Bp5x1"
      },
      "source": [
        "# Resources and Stretch Goals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uT3UV3gap9H6"
      },
      "source": [
        "## Stretch goals:\n",
        "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
        "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
        "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
        "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
        "- Run on bigger, better data\n",
        "\n",
        "## Resources:\n",
        "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
        "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
        "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
        "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
        "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
      ]
    }
  ]
}