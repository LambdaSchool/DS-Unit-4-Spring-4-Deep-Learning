{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 3 Assignment 1*\n",
    "\n",
    "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
    "\n",
    "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
    "\n",
    "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
    "\n",
    "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
    "\n",
    "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
    "\n",
    "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
    "\n",
    "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ltj1je1fp5rO"
   },
   "outputs": [],
   "source": [
    "# TODO - Words, words, mere words, no matter from the heart.\n",
    "from tensorflow.keras.callbacks import LambdaCallback, EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_fwf(\"https://www.gutenberg.org/files/100/100-0.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(166901, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning\n",
    "\n",
    "# Read in .txt url\n",
    "data = pd.read_fwf(\"https://www.gutenberg.org/files/100/100-0.txt\")\n",
    "\n",
    "# Reset index because the actual text was the index\n",
    "data = data.reset_index()\n",
    "\n",
    "# Drop redundant column\n",
    "data = data.drop(columns='Unnamed: 0')\n",
    "\n",
    "# Filter out any NaN values\n",
    "data = data[data['index'].notna()]\n",
    "\n",
    "# Convert the df series into a python list\n",
    "data = data['index'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139330"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[0:int((len(data)/10))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Project Gutenberg’s The Complete Works of William Shakespeare, by William'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example text\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms the list into a string\n",
    "# the \" \" puts a space inbetween each word\n",
    "text = \" \".join(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks through the entire text and grabs all the unique chars (list checks for uniques).\n",
    "# Set is wrapped in a list so that it puts the unique characters in a list\n",
    "chars = list(set(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a dict that gives each char a unique value\n",
    "char_int = {c:i for i, c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a dict that gives a number a unique char\n",
    "int_char = {i:c for i, c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a list that looks through the char_int dict\n",
    "# and matches the char from text to a number from the dict\n",
    "encoded = [char_int[c] for c in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences 109754\n"
     ]
    }
   ],
   "source": [
    "maxlen = 40\n",
    "step = 5\n",
    "\n",
    "sequences = []\n",
    "next_char = []\n",
    "\n",
    "# For loop that appends a list of 40 encoded numbers to the sequences list\n",
    "# next_char gets the next encoded number of the next encoded numbers list in sequences\n",
    "for i in range(0, len(encoded) - maxlen, step):\n",
    "    sequences.append(encoded[i : i + maxlen])\n",
    "    next_char.append(encoded[i + maxlen])\n",
    "    \n",
    "print('sequences', len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates the x and y variables for the model\n",
    "\n",
    "# Creates an array of bools (False's) that has 1059622 (len(sequences)) arrays that holds 40 arrays\n",
    "# (maxlen) that each of those arrays holds 105 values (len(chars))\n",
    "x = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
    "\n",
    "# Creates an array of bools (False's) that has 1059622 arrays (len(sequences))\n",
    "# with 105 values in each of them (len(chars))\n",
    "y = np.zeros((len(sequences), len(chars)), dtype=np.bool)\n",
    "\n",
    "# Everything is false since they are placeholders for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loop that does a lot of stuff\n",
    "\n",
    "# Grabs a single sequence\n",
    "for i, sequence in enumerate(sequences):\n",
    "    # grabs a single value in the list\n",
    "    for t, char in enumerate(sequence):\n",
    "        # assigns a 1 (True) to \n",
    "        x[i,t,char] = 1\n",
    "        \n",
    "    y[i, next_char[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / 1\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, _):\n",
    "    \n",
    "    print()\n",
    "    print(f'------ Generating text after Epoch: {epoch}')\n",
    "    \n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    \n",
    "    generated = ''\n",
    "    \n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    \n",
    "    generated += sentence\n",
    "    \n",
    "    print(f'------ Generating with seed: \"\" {sentence} \"\" ')\n",
    "    sys.stdout.write(generated)\n",
    "    \n",
    "    for i in range(400):\n",
    "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_int[char]] = 1\n",
    "            \n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds)\n",
    "        next_char = int_char[next_index]\n",
    "        \n",
    "        sentence=sentence[1:] + next_char\n",
    "        \n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    \n",
    "    print()\n",
    "    \n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 109754 samples\n",
      "Epoch 1/10\n",
      "109568/109754 [============================>.] - ETA: 0s - loss: 2.5932\n",
      "------ Generating text after Epoch: 0\n",
      "------ Generating with seed: \"\" w that a life was but a flower, In the s \"\" \n",
      "w that a life was but a flower, In the siclit in  Irhiserele gerdind mad an lith; Wham nuenlil ne liv oud  tewear mouch theederp? . cSEagr apke Rhainr and freit lereant non' yud fopther thils uettir'I f our to inh dols uy onniI pord iens! th thy Be win you hacpy Thel sald bo las hatk doke nor ste tithm amy Ther gpowlsechitna heu the beren virk fin. shas' she sayed Dr, thive Endee, lishe nkain, ond wereasit tDe. hame thate bu Ond soin pa\n",
      "109754/109754 [==============================] - 41s 370us/sample - loss: 2.5928\n",
      "Epoch 2/10\n",
      "109568/109754 [============================>.] - ETA: 0s - loss: 2.3653\n",
      "------ Generating text after Epoch: 1\n",
      "------ Generating with seed: \"\"  That sings with piercing; do not touch  \"\" \n",
      " That sings with piercing; do not touch nterekerr momyt fere, mind;RA’d thanses; ans, mithest waded roan hele, as turivone?, woth ureronmy siart whe sirve then fire, oumd miser the sh? I ar tree ms aighin ?oS ONwCATALS. Th? lther Ies cins upatink rin, Righee! b-oled mlothiste thac frey here sourgee, cerene so yoE thabet t vuus Dars eave diwd me th, mate, And andl ir. SOO’. wurig, norgo youe' ; in'me whe paath ald th cay, btreruttev;om t\n",
      "109754/109754 [==============================] - 41s 370us/sample - loss: 2.3648\n",
      "Epoch 3/10\n",
      "109568/109754 [============================>.] - ETA: 0s - loss: 2.2623\n",
      "------ Generating text after Epoch: 2\n",
      "------ Generating with seed: \"\" e, these jests are out of season, Reserv \"\" \n",
      "e, these jests are out of season, ReservemtrLare’nd wore geath aglish of ant or princt. OhUery kO deas thot thas enis of the sorree thon bat fere; mal? ODUNPEONDUIR. Uyde bonthttein the Thuthith My anteare herucf im thuu mo jistr my citherqure, tuzhing; wacs liwle. And ave’d t oire. B_t Hod winy,tind id hart, a werton thou enlenoon Ca I Es seet atharl-FIdRSeLENS. Hetepsity and to wepperesd comistkimt? I ove lomithel. Snrear igr, arndidh\n",
      "109754/109754 [==============================] - 47s 429us/sample - loss: 2.2624\n",
      "Epoch 4/10\n",
      "109568/109754 [============================>.] - ETA: 0s - loss: 2.1935\n",
      "------ Generating text after Epoch: 3\n",
      "------ Generating with seed: \"\" SAR and his army CAESAR. But being charg \"\" \n",
      "SAR and his army CAESAR. But being charg me the my, ordowd of o the plan vellene, an. WEde stean ie oftakt. Wiser sou hersher me homom! OLEAR. U teat of, yous to hirusulist ot wouy he spomer the vores Ansitherdose hougert; HENGILAA,'sn The ollmeame of movy thas he darynurl ave habocentou, this oy to dy to so cacs moo tand? That, MIy leld of th mand, If mr to he pured ther nassash and eratngis coid sitat ande hele wime! Bon Wherkr in bue\n",
      "109754/109754 [==============================] - 44s 402us/sample - loss: 2.1933\n",
      "Epoch 5/10\n",
      "109568/109754 [============================>.] - ETA: 0s - loss: 2.1383\n",
      "------ Generating text after Epoch: 4\n",
      "------ Generating with seed: \"\"  With thy sharp teeth this knot intrinsi \"\" \n",
      " With thy sharp teeth this knot intrinsil eanhar, The ow tuge shern, no but you, you ltinte, thau, eny me; as a betre oor andy wato jid tipe The mare. ALPEIE. A alond, Se tul. Sethe he? FENAmA. Mastser me gayce. CLIES. Ho as in wet and congstfilf verteer Thith sugstiss thint; en thee gryous: Fow dow to hir thy mapde] wirt or dich, Thean wpend. HLEMEES. Dcocy to shantt? FENT. PERDAEn! LI. I Sod the wfill. ALVOSO. You wele you hita botumi\n",
      "109754/109754 [==============================] - 44s 405us/sample - loss: 2.1385\n",
      "Epoch 6/10\n",
      "109568/109754 [============================>.] - ETA: 0s - loss: 2.0926\n",
      "------ Generating text after Epoch: 5\n",
      "------ Generating with seed: \"\" o laugh to scorn.                   Exeu \"\" \n",
      "o laugh to scorn.                   Exeur; CLIE bFENAS ANI STRARFID FI EII. I wiis, your afsers, and this oranow hat manes alld you wat houlbde coullinung souch. Le Thae with A Tu’lls OLRAYDALDMENA. to, I no shast? FENEY ID; Sheay wetine Mo; Bo withtpaindnily Medantqunt contertede’s. Cey,issike thee sikce Ristouth, the nowe fforche to rours domenR Withter endse to neur”. CHEUUST FESS MOSPASARS OORARSUSA. Nan sead. SROPANT. We, So Aswith\n",
      "109754/109754 [==============================] - 45s 406us/sample - loss: 2.0929\n",
      "Epoch 7/10\n",
      "109568/109754 [============================>.] - ETA: 0s - loss: 2.0520\n",
      "------ Generating text after Epoch: 6\n",
      "------ Generating with seed: \"\"  Let the justices make you and Fortune f \"\" \n",
      " Let the justices make you and Fortune faloch an to that ean, And Lekson, lor montire [ablor, whimy, Wered the mofle tseres princw ho? ONALANY HORRAMIAN. You Thasc, prand loas of the hiounte to enot of he thake har, Belt? LANTiD. I'l of shane, 1ller she velat, Ane thy cate, are me sotedt, not to ale that warphy, I from, thee the youy nemirime blowk the rejead, And nove be. The paken, Lpurt oor fyou his The the firle mobe, Hot with rreat\n",
      "109754/109754 [==============================] - 44s 398us/sample - loss: 2.0521\n",
      "Epoch 8/10\n",
      "109568/109754 [============================>.] - ETA: 0s - loss: 2.0156\n",
      "------ Generating text after Epoch: 7\n",
      "------ Generating with seed: \"\" oth, Draw after her. Pardon what I have  \"\" \n",
      "oth, Draw after her. Pardon what I have toue mand cingwidr cebteed. I crins, pave hame to yate Hid a nome so havior me comssert bo not our'e. Sat jo wtodnstere. ILES. Entatie. CAEOPR. Leve, your, sas; thae Beat thaus. Ceare verele. Hir was my meavad. ENOBIR. With dithan I wall. DCEGIUS. Soglod, Elquee, futre, in me’eneateave forors pariscong; that be I save. “los thing so. Har bedy, we hagove of-raveaty; andengbours ary touth in I mment\n",
      "109754/109754 [==============================] - 45s 406us/sample - loss: 2.0155\n",
      "Epoch 9/10\n",
      "109568/109754 [============================>.] - ETA: 0s - loss: 1.9837\n",
      "------ Generating text after Epoch: 8\n",
      "------ Generating with seed: \"\" ould keep fair quarter with his bed. I s \"\" \n",
      "ould keep fair quarter with his bed. I sals I bremer may yot to hare ham in helre. Gooster; beate be, That thy reis, Prompold wither the youxt; and enlm the sinne of hive. Mringerth of diod hame, ous hish my. Sit in there beout mesa'd ald do mastery. 1o nit ceas pill foct. MEOS._] When trepaBs- I winss Inees, Wgate. Andselave har thance wfell do fourd on their wor dever of thou- your thyreevendey seer fante, Ent trishen’d themer Paray s\n",
      "109754/109754 [==============================] - 45s 406us/sample - loss: 1.9837\n",
      "Epoch 10/10\n",
      "109568/109754 [============================>.] - ETA: 0s - loss: 1.9548\n",
      "------ Generating text after Epoch: 9\n",
      "------ Generating with seed: \"\" e 'gainst the odds. Thy lustre thickens  \"\" \n",
      "e 'gainst the odds. Thy lustre thickens malnge ta harck I the spak tho forst you wais. WIUS The urem her your deatt Ale me nit bary has. LAETRA. God greal, what whatath are you. Wheven mors aid, Litile, love male, lop'd theil, chyy! Sue thy Sond my thon of aprarn, The sterst in mester vine Creive sin, Peman un? MAsONGLESS. Gor well womy sher, Bethen as a mese, herl it extuly and buthirn de are tipht is hape a snes; prith by toul. Whif d\n",
      "109754/109754 [==============================] - 46s 417us/sample - loss: 1.9549\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f92203945f8>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y,\n",
    "         batch_size=256,\n",
    "         epochs=10,\n",
    "         callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zE4a4O7Bp5x1"
   },
   "source": [
    "# Resources and Stretch Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uT3UV3gap9H6"
   },
   "source": [
    "## Stretch goals:\n",
    "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
    "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
    "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
    "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
    "- Run on bigger, better data\n",
    "\n",
    "## Resources:\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
    "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
    "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
    "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
    "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "U4-S2-NN (Python3)",
   "language": "python",
   "name": "u4-sprint2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
