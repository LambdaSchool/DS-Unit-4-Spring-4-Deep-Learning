{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 3 Assignment 1*\n",
    "\n",
    "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
    "\n",
    "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
    "\n",
    "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
    "\n",
    "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
    "\n",
    "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
    "\n",
    "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
    "\n",
    "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do this with the \"premade\" code in lecture, then try to use Keras I think"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the text from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ltj1je1fp5rO"
   },
   "outputs": [],
   "source": [
    "readin_work = []\n",
    "data_path = './fancyboi/work.txt'\n",
    "with open(data_path, 'r') as f:\n",
    "    content = f.read()\n",
    "    readin_work.append(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do some misc things to take a looky-loo at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "writtens = readin_work\n",
    "text = \" \".join(writtens)\n",
    "chars = list(set(text))\n",
    "chars_amt = len(chars)\n",
    "text_size = len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique characters: 107\n",
      "Size of the text: 5584464 (idk if in bytes or whatever)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'''\n",
    "Unique characters: {chars_amt}\n",
    "Size of the text: {text_size} (idk if in bytes or whatever)\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, supposed to one hot encode all the characters in question...oh boi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "parasite_c2i = dict((c, i) for i, c in enumerate(chars))  # Allegedly enumerate returns index & value. Converts to dict(?)\n",
    "parasite_i2c = dict((i, c) for i, c in enumerate(chars))\n",
    "# Integer encode inputs?\n",
    "int_in_rock = [parasite_c2i[i] for i in text]  # List which has sequence converted from OG data to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Uno: {'.': 0, '3': 1, 'œ': 2, 'Q': 3, '\\ufeff': 4, '[': 5, 'A': 6, \"'\": 7, '%': 8, 'x': 9, 'N': 10, 'Æ': 11, 'O': 12, 'v': 13, 'î': 14, 'r': 15, 'g': 16, '5': 17, '|': 18, '-': 19, 'é': 20, '2': 21, '#': 22, 'V': 23, '\\t': 24, '\\\\': 25, ':': 26, '$': 27, 'k': 28, '&': 29, 'd': 30, 'q': 31, '8': 32, '4': 33, ']': 34, 'F': 35, 'X': 36, '\\n': 37, 'è': 38, '—': 39, 'D': 40, 'U': 41, 'W': 42, 'R': 43, '*': 44, 's': 45, 'n': 46, '/': 47, '?': 48, 'K': 49, 't': 50, ';': 51, 'z': 52, '’': 53, 'u': 54, 'E': 55, 'o': 56, 'a': 57, '}': 58, 'f': 59, 'â': 60, 'É': 61, '_': 62, 'M': 63, 'P': 64, 'j': 65, 'b': 66, '1': 67, '”': 68, 'L': 69, '6': 70, 'G': 71, 'y': 72, ',': 73, 'e': 74, 'Y': 75, ')': 76, 'H': 77, 'I': 78, 'S': 79, 'w': 80, 'ç': 81, '0': 82, 'T': 83, '9': 84, '!': 85, '@': 86, 'l': 87, 'c': 88, ' ': 89, 'B': 90, 'J': 91, 'æ': 92, 'Z': 93, 'à': 94, 'p': 95, '(': 96, '“': 97, 'm': 98, 'ê': 99, 'i': 100, '`': 101, 'h': 102, '7': 103, '\"': 104, '‘': 105, 'C': 106}\n",
      " ------------------------------\n",
      "Dos: {0: '.', 1: '3', 2: 'œ', 3: 'Q', 4: '\\ufeff', 5: '[', 6: 'A', 7: \"'\", 8: '%', 9: 'x', 10: 'N', 11: 'Æ', 12: 'O', 13: 'v', 14: 'î', 15: 'r', 16: 'g', 17: '5', 18: '|', 19: '-', 20: 'é', 21: '2', 22: '#', 23: 'V', 24: '\\t', 25: '\\\\', 26: ':', 27: '$', 28: 'k', 29: '&', 30: 'd', 31: 'q', 32: '8', 33: '4', 34: ']', 35: 'F', 36: 'X', 37: '\\n', 38: 'è', 39: '—', 40: 'D', 41: 'U', 42: 'W', 43: 'R', 44: '*', 45: 's', 46: 'n', 47: '/', 48: '?', 49: 'K', 50: 't', 51: ';', 52: 'z', 53: '’', 54: 'u', 55: 'E', 56: 'o', 57: 'a', 58: '}', 59: 'f', 60: 'â', 61: 'É', 62: '_', 63: 'M', 64: 'P', 65: 'j', 66: 'b', 67: '1', 68: '”', 69: 'L', 70: '6', 71: 'G', 72: 'y', 73: ',', 74: 'e', 75: 'Y', 76: ')', 77: 'H', 78: 'I', 79: 'S', 80: 'w', 81: 'ç', 82: '0', 83: 'T', 84: '9', 85: '!', 86: '@', 87: 'l', 88: 'c', 89: ' ', 90: 'B', 91: 'J', 92: 'æ', 93: 'Z', 94: 'à', 95: 'p', 96: '(', 97: '“', 98: 'm', 99: 'ê', 100: 'i', 101: '`', 102: 'h', 103: '7', 104: '\"', 105: '‘', 106: 'C'}\n",
      " ------------------------------\n",
      "Data length: 5584464...again idk if this is bytes or whatever\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'''\n",
    "Uno: {parasite_c2i}\\n {'-'*30}\n",
    "Dos: {parasite_i2c}\\n {'-'*30}\n",
    "Data length: {len(int_in_rock)}...again idk if this is bytes or whatever\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now onto MANUALLY doing this crazy code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparam pen\n",
    "iterations = 1000\n",
    "seq_length = 40\n",
    "batch_size = round((text_size / seq_length) + 0.5)  # this is math.ceil basically\n",
    "hidden_sze = 500  # Size of hidden layer of neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "weight_in_hid = np.random.randn(hidden_sze, chars_amt) * 0.01  # Weight INPUT -> hidden\n",
    "weight_hid_hid = np.random.randn(hidden_sze, hidden_sze) * 0.01  # Weight hidden -> hidden\n",
    "weight_hid_out = np.random.randn(chars_amt, hidden_sze) * 0.01  # Weight hidden -> OUTPUT\n",
    "# Bias stuff\n",
    "bias_hid = np.zeros((hidden_sze, 1))  # Hidden bias\n",
    "bias_out = np.zeros((chars_amt, 1))  # Output bias\n",
    "# Think this is hidden t-1, like obvi the previous one but yeah (yeah)\n",
    "hid_prev = np.zeros((hidden_sze, 1))  # h_(t-1) idk what this is but the teacher wrote it..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward Prop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardProp(inputs, targets, hid_prev):\n",
    "    \n",
    "    # Since RNN receives sequence, weights are not updated during 1 sequence (idk what this means)\n",
    "    in_s, hid_s, out_s, prob_s = {}, {}, {}, {}\n",
    "    hid_s[-1] = np.copy(hid_prev)  # Copy prev hidden state vekkie to -1 key value\n",
    "    # Loss init\n",
    "    loss = 0\n",
    "    \n",
    "    # For loop to go through time, woooo\n",
    "    for t in range(len(inputs)):\n",
    "        # t is lookup value (key)\n",
    "        # Sets INPUT state into something?\n",
    "        in_s[t] = np.zeros((chars_amt,1))\n",
    "        # Make INPUT sequence's input keyed values into 1 (?)\n",
    "        in_s[t][inputs[t]] = 1\n",
    "        # This sets the HIDDEN state\n",
    "        hid_s[t] = np.tanh(np.dot(weight_in_hid, in_s[t]) +\n",
    "                           np.dot(weight_hid_hid, hid_s[t-1]) +\n",
    "                           bias_hid)\n",
    "        # Sets OUT state; unnormalized log probs for next chars (?)\n",
    "        out_s[t] = (np.dot(weight_hid_out, hid_s[t]) +\n",
    "                    bias_out)\n",
    "        # PROB for next chars\n",
    "        prob_s = (np.exp(out_s[t]) /\n",
    "                  np.sum(np.exp(out_s[t])))\n",
    "        \n",
    "        # Softmax time! AKA: normalizing a vector of real numbers proportional to the exponentials of the input nums\n",
    "        loss += -np.log(prob_s[t][targets[t], 0])  # (cross-entropy loss)\n",
    "        \n",
    "    return loss, prob_s, hid_s, in_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! That wasn't fun, let's do something harder!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward Prop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backProp(prob_s, inputs, hid_s, in_s, targets):\n",
    "    \n",
    "    # Oh boy here comes these absolutely inhuman variable names.\n",
    "    # So ugly in fact, that I can't even rename them cause they're hieroglyphs and I have a life.\n",
    "    ### stinkzone\n",
    "    # Make all zero matrices\n",
    "    dWxh = np.zeros_like(weight_in_hid)\n",
    "    dWhh = np.zeros_like(weight_hid_hid)\n",
    "    dWhy = np.zeros_like(weight_hid_out)  # Yea dWhy is right\n",
    "    dbh = np.zeros_like(bias_hid)\n",
    "    dby = np.zeros_like(bias_out)\n",
    "    dhnext = np.zeros_like(hid_s[0]) # (hidden_size,1) \n",
    "    ### Look at this stuff and tell me these variable names ain't crazy...cmonnnn\n",
    "    \n",
    "    # Reversed\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(prob_s[t]) # shape (num_chars,1).  \"dy\" means \"dloss/dy\"\n",
    "        dy[targets[t]] -= 1 # backprop into y. After taking the soft max in the input vector, subtract 1 from the value of the element corresponding to the correct label.\n",
    "        dWhy += np.dot(dy, hid_s[t].T)\n",
    "        dby += dy \n",
    "        dh = np.dot(weight_hid_out.T, dy) + dhnext # backprop into h. \n",
    "        dhraw = (1 - hid_s[t] * hid_s[t]) * dh # backprop through tanh nonlinearity #tanh'(x) = 1-tanh^2(x)\n",
    "        dbh += dhraw\n",
    "        dWxh += np.dot(dhraw, in_s[t].T)\n",
    "        dWhh += np.dot(dhraw, hid_s[t-1].T)\n",
    "        dhnext = np.dot(weight_hid_hid.T, dhraw)\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]: \n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients.  \n",
    "    \n",
    "    return dWxh, dWhh, dWhy, dbh, dby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Totally not monstrous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-58032b769ec9>\u001b[0m in \u001b[0;36mforwardProp\u001b[0;34m(inputs, targets, hid_prev)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# Softmax time! AKA: normalizing a vector of real numbers proportional to the exponentials of the input nums\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob_s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (cross-entropy loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhid_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_s\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data_pointer = 0\n",
    "\n",
    "# Mem variables for Adagrad (?) LOOK AT THESE NAMES CMON\n",
    "mWxh = np.zeros_like(weight_in_hid)\n",
    "mWhh = np.zeros_like(weight_hid_hid)\n",
    "mWhy = np.zeros_like(weight_hid_out)  # Yeah mWhy..\n",
    "mbh, mby = np.zeros_like(bias_hid), np.zeros_like(bias_out)\n",
    "\n",
    "# A giant for loop!\n",
    "for i in range(iterations):\n",
    "    hid_prev = np.zeros((hidden_sze, 1))  # Reset RNN memory\n",
    "    data_pointer = 0  # Zero so that you can go from start of data\n",
    "    \n",
    "    # Somewhere here we need to change something to get this working with good ole' Shakespeare\n",
    "    for b in range(batch_size):\n",
    "        inputs = [parasite_c2i[ch] \n",
    "                  for ch in text[\n",
    "                      data_pointer:data_pointer+seq_length\n",
    "                  ]\n",
    "                 ]\n",
    "        targets = [parasite_c2i[ch] \n",
    "                  for ch in text[\n",
    "                      data_pointer+1:data_pointer+seq_length+1\n",
    "                  ]\n",
    "                 ]\n",
    "        # Now we apparently process last part of input data.\n",
    "        if (data_pointer+seq_length+1 >= len(text) and\n",
    "            b == batch_size-1):\n",
    "            targets.append(parasite_c2i[\" \"])  # When data doesn't fit, add space (\" \") to the back (?)\n",
    "        \n",
    "        # Forward prop!\n",
    "        loss, prob_s, hid_s, in_s = forwardProp(inputs, targets, hid_prev)\n",
    "        # Backward prop!\n",
    "        dWxh, dWhh, dWhy, dbh, dby = backProp(prob_s, inputs, hid_s, in_s, targets)\n",
    "        \n",
    "        # Parameter update with Adagrad (again with this word)\n",
    "        # What. In. God's. Green. Earth. Is. This.\n",
    "        for param, dparam, mem in zip([weight_in_hid, weight_hid_hid,\n",
    "                                       weight_hid_out, bias_hid, bias_out],\n",
    "                                      [dWxh, dWhh, dWhy, dbh, dby],\n",
    "                                      [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "            mem += dparam * dparam  # Elementwise multiplication I guess\n",
    "            param += -learning_rate * dparam / np.sqrt(mem + 1e-8)  # Adagrad update\n",
    "        \n",
    "        data_pointer += seq_length  # Move the data pointer again...\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    # For every 100th iteration, print some progress\n",
    "    if i % 100 == 0:\n",
    "        print(f'Iteration: {i}\\nLoss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_char, length):\n",
    "    x = np.zeroes((chars_amt, 1))\n",
    "    x[parasite_c2i[test_char]] = 1\n",
    "    ixes = []\n",
    "    h = np.zeros((hidden_sze, 1))\n",
    "    \n",
    "    for t in range(length):\n",
    "        h = np.tanh(np.dot(weight_in_hid, x) + np.dot(weight_hid_hid, h) + bias_hid) \n",
    "        y = np.dot(weight_hid_out, h) + bias_out\n",
    "        p = np.exp(y) / np.sum(np.exp(y)) \n",
    "        ix = np.random.choice(range(chars_amt), p=p.ravel()) # ravel -> rank0\n",
    "        # \"ix\" is a list of indexes selected according to the soft max probability.\n",
    "        x = np.zeros((chars_amt, 1)) # init\n",
    "        x[ix] = 1 \n",
    "        ixes.append(ix) # list\n",
    "    txt = test_char + ''.join(parasite_i2c[i] for i in ixes)\n",
    "    print(f'\\b~~~{txt}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zE4a4O7Bp5x1"
   },
   "source": [
    "# Resources and Stretch Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uT3UV3gap9H6"
   },
   "source": [
    "## Stretch goals:\n",
    "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
    "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
    "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
    "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
    "- Run on bigger, better data\n",
    "\n",
    "## Resources:\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
    "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
    "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
    "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
    "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
