{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 3 Assignment 1*\n",
    "\n",
    "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
    "\n",
    "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
    "\n",
    "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
    "\n",
    "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
    "\n",
    "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
    "\n",
    "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
    "\n",
    "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ltj1je1fp5rO"
   },
   "outputs": [],
   "source": [
    "# TODO - Words, words, mere words, no matter from the heart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-12-16 19:40:07--  https://www.gutenberg.org/files/100/100-0.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5777367 (5.5M) [text/plain]\n",
      "Saving to: ‘100-0.txt.10’\n",
      "\n",
      "100-0.txt.10        100%[===================>]   5.51M  1.66MB/s    in 3.3s    \n",
      "\n",
      "2019-12-16 19:40:10 (1.66 MB/s) - ‘100-0.txt.10’ saved [5777367/5777367]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.gutenberg.org/files/100/100-0.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = list()\n",
    "\n",
    "#with open ('100-0.txt', 'r') as f:\n",
    "#    data.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import re\n",
    "#import string\n",
    "\n",
    "#re.sub(r'[^a-zA-Z ^0-9]', '', full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data1 = data[0].replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data1 = re.sub(r'[^a-zA-Z ^0-9]', '', data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "#re.sub(r'[^a-zA-Z ^0-9]', '', full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = list()\n",
    "\n",
    "with open ('100-0.txt', 'r') as f:\n",
    "    data.append(f.read())\n",
    "\n",
    "len(data)\n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "#re.sub(r'[^a-zA-Z ^0-9]', '', full_text)\n",
    "# debate over taking out spacing\n",
    "\n",
    "data1 = data[0].replace('\\n', '')\n",
    "data1 = data[0].replace('\\t', '')\n",
    "\n",
    "# character filter\n",
    "#data1 = re.sub(r'[^a-zA-Z^0-9]', '', data1)\n",
    "\n",
    "big_string = \" \".join(data1)\n",
    "\n",
    "big_string = data1\n",
    "character = list(set(big_string))\n",
    "\n",
    "len(character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so far, all spaces and irregular characters have been removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_int = {character:integer for\n",
    "           integer, character in enumerate(character)}\n",
    "\n",
    "int_char = {integer:character for\n",
    "           integer, character in enumerate(character)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the sequence of data... \n",
    "\n",
    "maxlen = 32\n",
    "step = 5\n",
    "encoded = [char_int[c] for c in big_string]\n",
    "sequences = [] # or list()\n",
    "next_character = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (0, len(encoded) - maxlen, step):\n",
    "    # the 127 characters\n",
    "    sequences.append(encoded[i: i + maxlen])\n",
    "    # the 128th character\n",
    "    next_character.append(encoded[i + maxlen])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((len(sequences), maxlen, len(character)), dtype=np.bool)\n",
    "y = np.zeros((len(sequences), len(character)), dtype=np.bool)\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, characters in enumerate(sequence):\n",
    "        X[i,t,characters] = 1\n",
    "    \n",
    "    y[i, next_character[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1114624, 32, 106)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128,input_shape=(maxlen, len(character))))\n",
    "model.add(Dense(len(character), activation='softmax'))\n",
    "\n",
    "          \n",
    "optimizer = RMSprop(learning_rate=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "    \n",
    "    #start_index = np.random.randint(0, len(sequence) - maxlen - 1)\n",
    "    start_index = random.randint(0, len(sequence) - maxlen)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "        generated = ''\n",
    "        sentence = big_string[start_index: start_index + maxlen]\n",
    "        generated = generated + sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(character)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_int[char]] = 1.\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_characters = int_char[next_index]\n",
    "            sentence = sentence[1:] + next_characters\n",
    "            sys.stdout.write(next_characters)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef on_epoch_end(epoch, _):\\n    # Function invoked at end of each epoch. Prints generated text.\\n    print()\\n    print(\\'----- Generating text after Epoch: %d\\' % epoch)\\n\\n    start_index = random.randint(0, len(sequence) - maxlen)\\n    for diversity in [0.2, 0.5, 1.0, 1.2]:\\n        print(\\'----- diversity:\\', diversity)\\n\\n        generated = \\'\\'\\n        sentence = big_string[start_index: start_index + maxlen]\\n        generated = generated + sentence\\n        print(\\'----- Generating with seed: \"\\' + sentence + \\'\"\\')\\n        sys.stdout.write(generated)\\n\\n        for i in range(400):\\n            x_pred = np.zeros((1, maxlen, len(characters)))\\n            for t, character in enumerate(sentence):\\n                x_pred[0, t, char_int[character]] = 1.\\n\\n            preds = model.predict(x_pred, verbose=0)[0]\\n            next_index = sample(preds, diversity)\\n            next_characters = int_char[next_index]\\n\\n            sentence = sentence[1:] + next_characters\\n\\n            sys.stdout.write(next_characters)\\n            sys.stdout.flush()\\n        print()\\n\\nprint_callback = LambdaCallback(on_epoch_end=on_epoch_end)\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(sequence) - maxlen)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = big_string[start_index: start_index + maxlen]\n",
    "        generated = generated + sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(characters)))\n",
    "            for t, character in enumerate(sentence):\n",
    "                x_pred[0, t, char_int[character]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_characters = int_char[next_index]\n",
    "\n",
    "            sentence = sentence[1:] + next_characters\n",
    "\n",
    "            sys.stdout.write(next_characters)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#class\\ndef on_epoch_end(epoch, _):\\n    # Function invoked at end of each epoch. Prints generated text.\\n    print()\\n    print(\\'----- Generating text after Epoch: %d\\' % epoch)\\n\\n    start_index = np.random.randint(0, len(big_string) - maxlen - 1)\\n    for diversity in [0.2, 0.5, 1.0, 1.2]:\\n        print(\\'----- diversity:\\', diversity)\\n\\n        generated = \\'\\'\\n        sentence = big_string[start_index: start_index + maxlen]\\n        generated += sentence\\n        print(\\'----- Generating with seed: \"\\' + sentence + \\'\"\\')\\n        sys.stdout.write(generated)\\n\\n        for i in range(400):\\n            x_pred = np.zeros((1, maxlen, len(chars)))\\n            for t, char in enumerate(sentence):\\n                x_pred[0, t, char_indices[char]] = 1.\\n\\n            preds = model.predict(x_pred, verbose=0)[0]\\n            next_index = sample(preds, diversity)\\n            next_char = indices_char[next_index]\\n\\n            sentence = sentence[1:] + next_char\\n\\n            sys.stdout.write(next_char)\\n            sys.stdout.flush()\\n        print()\\n\\nprint_callback = LambdaCallback(on_epoch_end=on_epoch_end)\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "#class\n",
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = np.random.randint(0, len(big_string) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = big_string[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1114624 samples\n",
      "1114112/1114624 [============================>.] - ETA: 0s - loss: 1.8276\n",
      "----- Generating text after Epoch: 0\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"﻿\n",
      "Project Gutenberg’s The Comple\"\n",
      "﻿\n",
      "Project Gutenberg’s The Compleanio.\n",
      "\n",
      "CROMIO.\n",
      "The will see the strange of the stands.\n",
      "    The will be the send the sons the strength the strength,\n",
      "    The sent the senst the sentle shall the shall be see the wifter are the wiftle and the straight.\n",
      "    The world to the wifter the sentle of the world to the world the to the hands and the son the send the send the sentlemen and the sentle same the grave the sons all the wifted so \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"﻿\n",
      "Project Gutenberg’s The Comple\"\n",
      "﻿\n",
      "Project Gutenberg’s The Compleuaral she are not the grace in thee,\n",
      "The sander of the hearth and the can may have a with heart,\n",
      "    With me to be the most now a show a timest, sir,\n",
      "    Thear mean to him, and but may heaven be deeding furth.\n",
      "    When the dreather that he will make a same compains,\n",
      "    The world and the stands to his broth a great all the wifford them and the one to the hearth some rown.\n",
      "\n",
      "THIRD CISTOR.\n",
      "To child t\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"﻿\n",
      "Project Gutenberg’s The Comple\"\n",
      "﻿\n",
      "Project Gutenberg’s The Compleoth;\n",
      "By a stempoos open to make.\n",
      "\n",
      "IS CLEEN.. I now shall lawest, ben mendceme.\n",
      "This inselve so would be upperowled;\n",
      "    A. . His most done, revesous, royforn.\n",
      "  KING LADI. Phal, yualten! Underes same, but I perciines,\n",
      "from the stund an tents apperains.\n",
      "\n",
      "PROpPHUS.\n",
      "Good wondowed that but I am myser;\n",
      "Vill an other in oue. Rides\n",
      "he to be fortle we widsan’d with no feast.\n",
      "  FLAWIO. Goven Causalme is no\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"﻿\n",
      "Project Gutenberg’s The Comple\"\n",
      "﻿\n",
      "Project Gutenberg’s The CompleinFaa\n",
      "    Good miseimen foo Lettus, of Jack-hearty hears, should gracNess,\n",
      "which do consctive, and we shal queen pray thus was,\n",
      "LeH war may founce. I not needom know\n",
      "whet if them! and you lany;. Tratwo in corntiline-rens arch EtVre,\n",
      "  WATAGRY. Mo ?\n",
      "AI. Good,, withomily. He'll do your hoving mend doth her\n",
      "I rivided doth ravies, serkenot take thy King,\n",
      "Hears thou no dopety run on at the vaniof! anre\n",
      "1114624/1114624 [==============================] - 397s 356us/sample - loss: 1.8275\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f29144f8048>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y,\n",
    "          batch_size=512,\n",
    "          epochs=1,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zE4a4O7Bp5x1"
   },
   "source": [
    "# Resources and Stretch Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uT3UV3gap9H6"
   },
   "source": [
    "## Stretch goals:\n",
    "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
    "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
    "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
    "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
    "- Run on bigger, better data\n",
    "\n",
    "## Resources:\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
    "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
    "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
    "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
    "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn1 (Python3)",
   "language": "python",
   "name": "nn1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
