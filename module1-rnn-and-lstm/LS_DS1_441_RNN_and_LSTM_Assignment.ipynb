{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Live Lecture Notebook LS_DS1_441_RNN_and_LSTM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShreyasJothish/DS-Unit-4-Sprint-4-Deep-Learning/blob/master/module1-rnn-and-lstm/LS_DS1_441_RNN_and_LSTM_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "_IizNKWLomoA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lambda School Data Science - Recurrent Neural Networks and LSTM\n",
        "\n",
        "> \"Yesterday's just a memory - tomorrow is never what it's supposed to be.\" -- Bob Dylan"
      ]
    },
    {
      "metadata": {
        "id": "0EZdBzC6pvV9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lecture\n",
        "\n",
        "Wish you could save [Time In A Bottle](https://www.youtube.com/watch?v=AnWWj6xOleY)? With statistics you can do the next best thing - understand how data varies over time (or any sequential order), and use the order/time dimension predictively.\n",
        "\n",
        "A sequence is just any enumerated collection - order counts, and repetition is allowed. Python lists are a good elemental example - `[1, 2, 2, -1]` is a valid list, and is different from `[1, 2, -1, 2]`. The data structures we tend to use (e.g. NumPy arrays) are often built on this fundamental structure.\n",
        "\n",
        "A time series is data where you have not just the order but some actual continuous marker for where they lie \"in time\" - this could be a date, a timestamp, [Unix time](https://en.wikipedia.org/wiki/Unix_time), or something else. All time series are also sequences, and for some techniques you may just consider their order and not \"how far apart\" the entries are (if you have particularly consistent data collected at regular intervals it may not matter)."
      ]
    },
    {
      "metadata": {
        "id": "5_m0hJ4uCzHz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Time series with plain old regression\n",
        "\n",
        "Recurrences are fancy, and we'll get to those later - let's start with something simple. Regression can handle time series just fine if you just set them up correctly - let's try some made-up stock data. And to make it, let's use a few list comprehensions!"
      ]
    },
    {
      "metadata": {
        "id": "GkJUFfsgnqr_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from random import random\n",
        "days = np.array((range(28)))\n",
        "stock_quotes = np.array([random() + day * random() for day in days])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JcsmQ8nWkbX0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "183d12e7-00e1-463a-bd55-46a255117e14"
      },
      "cell_type": "code",
      "source": [
        "days"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
              "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "y-ORgKGNBOcb",
        "colab_type": "code",
        "outputId": "7a062605-c9d5-4c08-ac78-ed29569878c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "stock_quotes"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.5616404 ,  0.37141746,  0.96994426,  3.25383313,  3.20543395,\n",
              "        1.60679474,  6.22223482,  3.89482175,  2.10281896,  4.81213962,\n",
              "        7.22308894,  8.054375  ,  8.77418475,  0.69232151, 13.68172754,\n",
              "        9.6363157 ,  4.3467134 , 16.63625112,  5.91263441,  5.78526065,\n",
              "        2.82824963, 15.81865976, 11.71566429,  3.05956688,  1.62777474,\n",
              "        3.9284043 ,  9.53998425, 25.11621215])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "X3lR2wGvBx3a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's take a look with a scatter plot:"
      ]
    },
    {
      "metadata": {
        "id": "pVUTC2tmBSIq",
        "colab_type": "code",
        "outputId": "1ab4e008-08a1-41f9-c237-fc3645408502",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        }
      },
      "cell_type": "code",
      "source": [
        "from matplotlib.pyplot import scatter\n",
        "scatter(days, stock_quotes)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7fc4a39dc518>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd8AAAFKCAYAAABcq1WoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAACGEb4AABhG+AIAYBjhCwCAYf8PprgBGjNlj5YAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "hgD4q-T_B0jd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Looks pretty linear, let's try a simple OLS regression.\n",
        "\n",
        "First, these need to be NumPy arrays:"
      ]
    },
    {
      "metadata": {
        "id": "A3Q0MrnUBXAl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "days = days.reshape(-1, 1)  # X needs to be column vectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vqr0SHOnB5yR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now let's use good old `scikit-learn` and linear regression:"
      ]
    },
    {
      "metadata": {
        "id": "PqyHxgFvBYl5",
        "colab_type": "code",
        "outputId": "0907bffe-4f26-4908-f8d5-294b32eea4c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "ols_stocks = LinearRegression()\n",
        "ols_stocks.fit(days, stock_quotes)\n",
        "ols_stocks.score(days, stock_quotes)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.26802633597501446"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "KlU0mr-KB_Yk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "That seems to work pretty well, but real stocks don't work like this.\n",
        "\n",
        "Let's make *slightly* more realistic data that depends on more than just time:"
      ]
    },
    {
      "metadata": {
        "id": "-FV1Emb2BuLz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Not everything is best as a comprehension\n",
        "stock_data = np.empty([len(days), 4])\n",
        "for day in days:\n",
        "  asset = random()\n",
        "  liability = random()\n",
        "  quote = random() + ((day * random()) + (20 * asset) - (15 * liability))\n",
        "  quote = max(quote, 0.01)  # Want positive quotes\n",
        "  stock_data[day] = np.array([quote, day, asset, liability])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6Qe2zzN1CESe",
        "colab_type": "code",
        "outputId": "b9f8f3d9-388c-42d4-d58d-c0a648a74f3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 509
        }
      },
      "cell_type": "code",
      "source": [
        "stock_data"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.22266084e+00, 0.00000000e+00, 8.45346051e-01, 7.93104555e-01],\n",
              "       [1.06098774e+01, 1.00000000e+00, 9.89646442e-01, 7.10079707e-01],\n",
              "       [1.00000000e-02, 2.00000000e+00, 3.79959673e-01, 9.99475933e-01],\n",
              "       [3.26979984e+00, 3.00000000e+00, 5.66384377e-01, 6.90917240e-01],\n",
              "       [5.91265090e+00, 4.00000000e+00, 5.12349318e-01, 4.91678990e-01],\n",
              "       [9.51384398e+00, 5.00000000e+00, 4.21212447e-01, 1.35625303e-01],\n",
              "       [1.00000000e-02, 6.00000000e+00, 1.39730241e-01, 7.02834406e-01],\n",
              "       [9.77269617e-01, 7.00000000e+00, 3.94233616e-01, 8.47952551e-01],\n",
              "       [1.00000000e-02, 8.00000000e+00, 5.80484681e-01, 8.97218854e-01],\n",
              "       [1.17382167e+01, 9.00000000e+00, 9.17710824e-01, 6.12635652e-01],\n",
              "       [1.03771427e+01, 1.00000000e+01, 6.00252168e-01, 3.63133211e-01],\n",
              "       [1.42186280e+01, 1.10000000e+01, 7.24440289e-01, 7.60458962e-01],\n",
              "       [2.66156193e+01, 1.20000000e+01, 7.27532100e-01, 3.13436171e-02],\n",
              "       [1.63817308e+00, 1.30000000e+01, 4.57294900e-01, 6.88309243e-01],\n",
              "       [1.00000000e-02, 1.40000000e+01, 5.76096977e-02, 5.01410785e-01],\n",
              "       [1.00000000e-02, 1.50000000e+01, 5.21917942e-01, 8.70086344e-01],\n",
              "       [9.52269235e+00, 1.60000000e+01, 3.21702747e-01, 1.98389849e-01],\n",
              "       [1.17611487e+01, 1.70000000e+01, 6.42958314e-01, 3.93534453e-01],\n",
              "       [1.51347891e+01, 1.80000000e+01, 5.47614383e-01, 8.32833170e-01],\n",
              "       [1.78520157e+01, 1.90000000e+01, 5.86531342e-01, 2.90066804e-01],\n",
              "       [1.75391375e+01, 2.00000000e+01, 9.96332043e-01, 3.89090780e-01],\n",
              "       [1.38136871e+01, 2.10000000e+01, 3.23266101e-01, 5.80240370e-01],\n",
              "       [3.21967805e+00, 2.20000000e+01, 2.69498860e-01, 5.25928421e-01],\n",
              "       [5.94281370e+00, 2.30000000e+01, 9.02678875e-01, 9.43976632e-01],\n",
              "       [1.26792278e+01, 2.40000000e+01, 1.44476623e-01, 2.95530408e-01],\n",
              "       [3.42218646e+01, 2.50000000e+01, 9.73488043e-01, 2.40197548e-01],\n",
              "       [4.01069871e+00, 2.60000000e+01, 1.85778730e-01, 2.56274202e-01],\n",
              "       [1.08052631e+01, 2.70000000e+01, 4.62083994e-01, 3.38765732e-01]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "BzYy4Pb2CLCh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's look again:"
      ]
    },
    {
      "metadata": {
        "id": "qdBcScz4CIXr",
        "colab_type": "code",
        "outputId": "513b13f6-d6bd-4060-ff00-6895b5f5b7cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        }
      },
      "cell_type": "code",
      "source": [
        "stock_quotes = stock_data[:,0]\n",
        "scatter(days, stock_quotes)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7fc49df65ba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd8AAAFKCAYAAABcq1WoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nADCM8AUAwLD/B9ZSNGn097V1AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "SBXb7dieCO5h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "How does our old model do?"
      ]
    },
    {
      "metadata": {
        "id": "7gAxCgy1COnX",
        "colab_type": "code",
        "outputId": "27572189-bcef-4f7f-ac98-f141e44bdd7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "days = np.array(days).reshape(-1, 1)\n",
        "ols_stocks.fit(days, stock_quotes)\n",
        "ols_stocks.score(days, stock_quotes)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.14716896673773483"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "3E94vTFUCax_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Not bad, but can we do better?"
      ]
    },
    {
      "metadata": {
        "id": "mCR5GImZCbGz",
        "colab_type": "code",
        "outputId": "00595422-5e4f-4061-a9a3-e7fedccb613c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "ols_stocks.fit(stock_data[:,1:], stock_quotes)\n",
        "ols_stocks.score(stock_data[:,1:], stock_quotes)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7164525301545691"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "1Qk-jlBCCiKB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Yep - unsurprisingly, the other covariates (assets and liabilities) have info.\n",
        "\n",
        "But, they do worse without the day data."
      ]
    },
    {
      "metadata": {
        "id": "dDcZl7I5Cf5D",
        "colab_type": "code",
        "outputId": "134fd2f8-d782-43e8-a09c-cbf256a4e6af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "ols_stocks.fit(stock_data[:,2:], stock_quotes)\n",
        "ols_stocks.score(stock_data[:,2:], stock_quotes)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6637067104420389"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "pnLXlrK8ENjb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Time series jargon\n",
        "\n",
        "There's a lot of semi-standard language and tricks to talk about this sort of data. [NIST](https://www.itl.nist.gov/div898/handbook/pmc/section4/pmc4.htm) has an excellent guidebook, but here are some highlights:"
      ]
    },
    {
      "metadata": {
        "id": "yWUyhnTbcq55",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Moving average\n",
        "\n",
        "Moving average aka rolling average aka running average.\n",
        "\n",
        "Convert a series of data to a series of averages of continguous subsets:"
      ]
    },
    {
      "metadata": {
        "id": "47bHhBSCcvw-",
        "colab_type": "code",
        "outputId": "a992393e-50bf-4b09-fff8-c27bc4de65a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        }
      },
      "cell_type": "code",
      "source": [
        "stock_quotes_rolling = [sum(stock_quotes[i:i+3]) / 3\n",
        "                        for i in range(len(stock_quotes) - 2)]\n",
        "stock_quotes_rolling"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[5.280846096331596,\n",
              " 4.629892428264996,\n",
              " 3.064150245487392,\n",
              " 6.23209823829282,\n",
              " 5.145498292469352,\n",
              " 3.5003711984371386,\n",
              " 0.33242320563171107,\n",
              " 4.241828763453182,\n",
              " 7.3751198000551055,\n",
              " 12.111329117392927,\n",
              " 17.070463318250123,\n",
              " 14.157473435751138,\n",
              " 9.42126411841332,\n",
              " 0.5527243597346515,\n",
              " 3.180897450992711,\n",
              " 7.097947026240514,\n",
              " 12.139543392574145,\n",
              " 14.915984517761018,\n",
              " 16.841980777030344,\n",
              " 16.401613456649986,\n",
              " 11.52416756227735,\n",
              " 7.658726295429365,\n",
              " 7.280573178836067,\n",
              " 17.614635369228598,\n",
              " 16.970597039615463,\n",
              " 16.34594214146709]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "36XvbGhoc186",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Pandas has nice series related functions:"
      ]
    },
    {
      "metadata": {
        "id": "nTNatxtycys_",
        "colab_type": "code",
        "outputId": "06946b15-9901-4dfc-9d41-d654a7c15f96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 896
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(stock_quotes)\n",
        "df.rolling(3).mean()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5.280846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.629892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3.064150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6.232098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>5.145498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3.500371</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.332423</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>4.241829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>7.375120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>12.111329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>17.070463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>14.157473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>9.421264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.552724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>3.180897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>7.097947</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>12.139543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>14.915985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>16.841981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>16.401613</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>11.524168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>7.658726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>7.280573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>17.614635</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>16.970597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>16.345942</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            0\n",
              "0         NaN\n",
              "1         NaN\n",
              "2    5.280846\n",
              "3    4.629892\n",
              "4    3.064150\n",
              "5    6.232098\n",
              "6    5.145498\n",
              "7    3.500371\n",
              "8    0.332423\n",
              "9    4.241829\n",
              "10   7.375120\n",
              "11  12.111329\n",
              "12  17.070463\n",
              "13  14.157473\n",
              "14   9.421264\n",
              "15   0.552724\n",
              "16   3.180897\n",
              "17   7.097947\n",
              "18  12.139543\n",
              "19  14.915985\n",
              "20  16.841981\n",
              "21  16.401613\n",
              "22  11.524168\n",
              "23   7.658726\n",
              "24   7.280573\n",
              "25  17.614635\n",
              "26  16.970597\n",
              "27  16.345942"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "os-szg47dgwf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Forecasting\n",
        "\n",
        "Forecasting - at it's simplest, it just means \"predict the future\":"
      ]
    },
    {
      "metadata": {
        "id": "D_qtt6irdj0x",
        "colab_type": "code",
        "outputId": "4aba672e-e953-4a11-cabb-62326dd4aa12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "ols_stocks.fit(stock_data[:,1:], stock_quotes)\n",
        "ols_stocks.predict([[29, 0.5, 0.5]])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([13.14545636])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "fjnQY0trdnHp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "One way to predict if you just have the series data is to use the prior observation. This can be pretty good (if you had to pick one feature to model the temperature for tomorrow, the temperature today is a good choice)."
      ]
    },
    {
      "metadata": {
        "id": "bzC4DV9Hdupp",
        "colab_type": "code",
        "outputId": "5f3830a6-3e78-4ab0-f949-30297180c3fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "temperature = np.array([30 + random() * day\n",
        "                        for day in np.array(range(28)).reshape(-1, 1)])\n",
        "temperature_next = temperature[1:].reshape(-1, 1)\n",
        "temperature_ols = LinearRegression()\n",
        "temperature_ols.fit(temperature[:-1], temperature_next)\n",
        "temperature_ols.score(temperature[:-1], temperature_next)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2708476535778623"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "RFdssXQbdxbE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "But you can often make it better by considering more than one prior observation."
      ]
    },
    {
      "metadata": {
        "id": "pVfUqD2YdxxZ",
        "colab_type": "code",
        "outputId": "2efcb6b3-2a4a-44b0-dafe-09020f0b8fe9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "temperature_next_next = temperature[2:].reshape(-1, 1)\n",
        "temperature_two_past = np.concatenate([temperature[:-2], temperature_next[:-1]],\n",
        "                                      axis=1)\n",
        "temperature_ols.fit(temperature_two_past, temperature_next_next)\n",
        "temperature_ols.score(temperature_two_past, temperature_next_next)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2591728833740883"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "c9QltBdmd7TV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Exponential smoothing\n",
        "\n",
        "Exponential smoothing means using exponentially decreasing past weights to predict the future.\n",
        "\n",
        "You could roll your own, but let's use Pandas."
      ]
    },
    {
      "metadata": {
        "id": "6_EUtcn9xjrz",
        "colab_type": "code",
        "outputId": "75658d5e-cda7-4b35-ff35-ef3dbb74988e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 509
        }
      },
      "cell_type": "code",
      "source": [
        "temperature"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[30.        ],\n",
              "       [30.12967615],\n",
              "       [30.37776227],\n",
              "       [31.00951967],\n",
              "       [33.36550542],\n",
              "       [34.57939902],\n",
              "       [34.06336382],\n",
              "       [34.80190271],\n",
              "       [36.29755096],\n",
              "       [36.42603292],\n",
              "       [33.08769611],\n",
              "       [34.45165542],\n",
              "       [32.41373159],\n",
              "       [35.30853309],\n",
              "       [34.48822881],\n",
              "       [32.65047139],\n",
              "       [42.62849967],\n",
              "       [46.75546913],\n",
              "       [45.02273947],\n",
              "       [46.45359067],\n",
              "       [37.14018315],\n",
              "       [32.1172138 ],\n",
              "       [37.33581478],\n",
              "       [35.33917873],\n",
              "       [32.86963261],\n",
              "       [54.58799057],\n",
              "       [43.02474142],\n",
              "       [44.2668192 ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "hvMNqunOeC_B",
        "colab_type": "code",
        "outputId": "ccb72113-2e75-44d4-e7f3-a56ec7d1be08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 896
        }
      },
      "cell_type": "code",
      "source": [
        "temperature_df = pd.DataFrame(temperature)\n",
        "temperature_df.ewm(halflife=7).mean()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>30.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>30.068046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>30.181659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>30.420300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>31.131358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>31.857030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>32.273040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>32.708784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>33.282397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>33.753948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>33.659284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>33.766731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>33.590543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>33.806498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>33.889582</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>33.742624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>34.771457</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>36.129788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>37.118901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>38.139842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>38.032135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>37.403304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>37.396215</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>37.182430</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>36.738491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>38.560059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>39.012171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>39.540586</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            0\n",
              "0   30.000000\n",
              "1   30.068046\n",
              "2   30.181659\n",
              "3   30.420300\n",
              "4   31.131358\n",
              "5   31.857030\n",
              "6   32.273040\n",
              "7   32.708784\n",
              "8   33.282397\n",
              "9   33.753948\n",
              "10  33.659284\n",
              "11  33.766731\n",
              "12  33.590543\n",
              "13  33.806498\n",
              "14  33.889582\n",
              "15  33.742624\n",
              "16  34.771457\n",
              "17  36.129788\n",
              "18  37.118901\n",
              "19  38.139842\n",
              "20  38.032135\n",
              "21  37.403304\n",
              "22  37.396215\n",
              "23  37.182430\n",
              "24  36.738491\n",
              "25  38.560059\n",
              "26  39.012171\n",
              "27  39.540586"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "gBEjBZVbeH6R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Halflife is among the parameters we can play with:"
      ]
    },
    {
      "metadata": {
        "id": "HjZgMwYkeODN",
        "colab_type": "code",
        "outputId": "4585df2b-6db2-4e81-988b-dda1e4a1f9d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "cell_type": "code",
      "source": [
        "sse_1 = ((temperature_df - temperature_df.ewm(halflife=7).mean())**2).sum()\n",
        "sse_2 = ((temperature_df - temperature_df.ewm(halflife=3).mean())**2).sum()\n",
        "print(sse_1)\n",
        "print(sse_2)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    691.265248\n",
            "dtype: float64\n",
            "0    497.821976\n",
            "dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "s39bj4g9eQ9Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Note - the first error being higher doesn't mean it's necessarily *worse*. It's *smoother* as expected, and if that's what we care about - great!"
      ]
    },
    {
      "metadata": {
        "id": "OcPMn8o4eYP1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Seasonality\n",
        "\n",
        "Seasonality - \"day of week\"-effects, and more. In a lot of real world data, certain time periods are systemically different, e.g. holidays for retailers, weekends for restaurants, seasons for weather.\n",
        "\n",
        "Let's try to make some seasonal data - a store that sells more later in a week:"
      ]
    },
    {
      "metadata": {
        "id": "h0qPMWCreheL",
        "colab_type": "code",
        "outputId": "68641f38-70af-410b-9580-6b8241a2ab25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        }
      },
      "cell_type": "code",
      "source": [
        "sales = np.array([random() + (day % 7) * random() for day in days])\n",
        "scatter(days, sales)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7fc49cc75668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdkAAAFKCAYAAABRtSXvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAGVtJREFUeJzt3V9oW/fdx/GPLcnyXMuObMteuy5s\n2BUL7cI80rI0rGk8UUgYjDYXzkwJY2PrCCndRighrO1GoV2ybmzJYFlDw2C5EeRi5GKQkHmFMtKU\nBNORMFAdeEoa0kS29dhyHZ8qJ34uUitJnxPrONbv/NP7dVXp54ov3xydzznn9zs6TQsLCwsCAAB1\n1+x3AQAARBUhCwCAIYQsAACGELIAABhCyAIAYAghCwCAIfF6f2CxWK7r56XTbSqV5ur6mVFFr9yj\nV8tDv9yjV+5FpVeZTOquY4E/k43HY36XEBr0yj16tTz0yz165V4j9CrwIQsAQFgRsgAAGELIAgBg\nCCELAIAhhCwAAIYQsgAAGELIAgBgCCELAIAhhCwA3Maq2LpampNVsf0uBRFQ959VBIAwsm/cUH50\nXGOFoqZmLHV1JDWYzWh4aECxZs5HcG8IWQCQlB8d18kzH1VfT85Y1dcjuaxfZSHkODwD0PCsiq2x\nQtFxbKwwwaVj3DNCFkDDm561NDVjOY6VyvOannUeA2ohZAE0vM72pLo6ko5j6VSrOtudx4BaCFkA\nDS+ZiGkwm3EcG8z2KJmI/iPZYAYLnwBA0vDQgKSbc7Cl8rzSqVYNZnuq7wP3gpAFAEmx5maN5LLa\nurFf07OWOtuTnMFixWqG7OnTp/XCCy/ooYcekiRls1m99NJLxgsDAD8kEzH1ptv8LgMR4epM9rHH\nHtP+/ftN1wIAQKSw8AkAAEOaFhYWFpb6g9OnT+vXv/61Vq9erenpae3cuVMbNmy4699fv24rHmce\nAwCAmiF75coVnT17Vps3b9bFixe1fft2nThxQi0tLY5/XyyW61pgJpOq+2dGFb1yj14tD/1yj165\nF5VeZTKpu47VvFzc19enLVu2qKmpSatXr1ZPT4+uXLlS1wIBAIiimiF77NgxvfXWW5KkYrGoyclJ\n9fX1GS8MAICwq7m6eGhoSLt27dI///lPVSoV/epXv7rrpWIAAHBLzZBtb2/XwYMHvagFAIBI4RYe\nAAAMIWQBADCEkAUAwBBCFgAAQwhZAAAMIWQBADCEkAUAwBBCFgAAQwhZAAAMIWQBADCEkAUAwBBC\nFgAAQwhZAAAMIWQBADCEkAUAwBBCFgAAQwhZAAAMIWQBADCEkAUAwBBCFgDqxKrYujzxiayK7Xcp\nkWZVbF0tzYWiz3G/CwCAsLNv3FB+dFxjhaKmypa6UkkNZjMaHhpQrJlzmXq5o88zlro6gt9nQhYA\nVig/Oq6TZz6qvp6csaqvR3JZv8qKnDD2OZjRDwAhYVVsjRWKjmNjhYlQXNIMg7D2mZAFgBWYnrU0\nNWM5jpXK85qedR7D8oS1z4QsAKxAZ3tSXR1Jx7F0qlWd7c5jWJ6w9pmQBYAVSCZiGsxmHMcGsz1K\nJmIeVxRNYe0zC58AYIWGhwYk3ZwbLJXnlU61ajDbU30f9RHGPjctLCws1PMDi8VyPT9OmUyq7p8Z\nVfTKPXq1PPTLHatiK9aSkP1pJbBnVkFyr9uVVbE1PWupsz0ZiD5nMqm7jnG5GADqJJmI6f6e+wKx\n44+yZCKm3nRbKPpMyAIAYAghCwCAIYQsAACGELIAABhCyAIAYAghCwCAIYRsgITpGYkAgNr4xacA\nCOMzEgEAtRGyARDGZyQCAGrjNMlnYX1GIgCgNkLWZ2F9RiIAoDZC1mdhfUYiAKA2QtZnYX1GIgCg\nNhY+BUAYn5EIAKjNVcjOz8/ru9/9rnbs2KFnnnnGdE0NJ9bcrJFcVls39gfqGYkAgJVxFbJ//vOf\n1dnZabqWhrf4jEQAQDTUnJO9cOGCxsfH9eSTT3pQDgAA0VEzZPfu3avdu3d7UQsAAJGy5OXiv//9\n7/rGN76hL3/5y64/MJ1uUzxe3/nETCZV18+LMnrlHr1aHvrlHr1yL+q9WjJk3377bV28eFFvv/22\nPv74Y7W0tOiLX/yiHn/88bv+P6XSXF0LzGRSKhbLdf3MqKJX7tGr5aFf7tEr96LSq6UOFJYM2T/8\n4Q/V/z5w4IC+9KUvLRmwAADgFn6MAgAAQ1z/GMXzzz9vsg4AACKHM1kAAAwhZAEAMISQBQDAEEIW\nAABDCFkAAAwhZAEAMISQBQDAEEIWAABDCFkAAAwhZAEAMISQBQDAEEIWAABDCFkAAAwhZAEAMISQ\nBQDAEEIWAABDCFkAAAwhZAEAMISQBQDAEEIWAABDCFkAAAwhZAEAMISQBQDAEEIWAABDCFkAAAwh\nZAEAMISQBQDAEEIWAABDCFkAAAwhZCGrYutqaU5Wxfa7FACIlLjfBcA/9o0byo+Oa6xQ1NSMpa6O\npAazGQ0PDSjWzPEXAKwUIdvA8qPjOnnmo+rryRmr+nokl/WrLACIDE5XGpRVsTVWKDqOjRUmuHQM\nAHVAyDao6VlLUzOW41ipPK/pWecxAIB7hGyD6mxPqqsj6TiWTrWqs915DADgHiHboJKJmAazGcex\nwWyPkomYxxUBQPSw8KmBDQ8NSLo5B1sqzyudatVgtqf6PgBgZQjZBhZrbtZILqutG/s1PWupsz3J\nGSwA1BEhCyUTMfWm2/wuAwAihzlZAAAMIWQBADCEkAUAwBBCFgAAQwhZAAAMqbm6+Nq1a9q9e7cm\nJydlWZZ27NihTZs2eVEbAAChVjNk//Wvf+mRRx7Rj3/8Y126dEk//OEPCVkAAFyoGbJbtmyp/vfl\ny5fV19dntCAAAKLC9Y9RbNu2TR9//LEOHjxosh4AACKjaWFhYcHtH//3v//Viy++qGPHjqmpqcnx\nb65ftxWP89N8AADUPJM9d+6curu7df/992vNmjWybVtTU1Pq7u52/PtSaa6uBWYyKRWL5bp+ZlTR\nK/fo1fLQL/folXtR6VUmk7rrWM1beM6cOaPDhw9LkiYmJjQ3N6d0Ol2/6gAAiKiaIbtt2zZNTU1p\nZGREP/nJT/Tyyy+ruZnbawEAqKXm5eLW1lb97ne/86IWAAAihVNSAAAMIWQBAPfEqti6WpqTVbH9\nLiWweGg7AGBZ7Bs3lB8d11ihqKkZS10dSQ1mMxoeGlCMNTt3IGQBAMuSHx3XyTMfVV9PzljV1yO5\nrF9lBRKHHAAA16yKrbFC0XFsrDDBpePPIWQBAK5Nz1qamrEcx0rleU3POo81KkIWAOBaZ3tSXR1J\nx7F0qlWd7c5jjYqQBRA5rHo1J5mIaTCbcRwbzPYomeC362/HwqeIsCq2Lk98Irtis5GjYbHq1RvD\nQwOSbs7BlsrzSqdaNZjtqb6PWwjZkLtjp1K21JVip4LGxapXb8SamzWSy2rrxn5Nz1rqbE9ycH8X\n7IVDbnGnMjljaWHh1k4lPzrud2mAp1j16r1kIqbedBsBuwRCNsTYqXiPub7gYtUrgojLxXVmVWzP\nLp+42an0ptuM1tAomOsLvsVVr5MO3wlWvcIvhGyd+LETZqfiHeb6gm9x1evt/06LWPUKv3AIXid3\nzI3Km7lRltJ7g8vy4TE8NKDcugfV3dGq5iapu6NVuXUPsuoVvuFMtg5q7YS3buw3FngspTePy/Lh\nwapXBA0hWwd+7oRv36nEWhKyP62wU6kzLsuHz+KqV8BvXC6ugyD8zFgyEdP9PfcRsAZwWR7AvSJk\n64CdcPQx1wfgXnC5uE6YG4025voA3AtCtk7YCTcG5voALAchW2fshAEAi5iTBQDAEEIWAABDCFkA\nAAwhZAEAMISQBQDAEEIWAABDCFkAAAwhZAEAMISQBQDAEEIWAABDCFkAAAwhZAEAMISQBQDAEEIW\nAABDCFkAAAwhZAEAMISQBQDAEEIWAABDCFkAAAwhZAEAMISQBQDAEEIWAABD4m7+aN++fTp79qyu\nX7+u5557Tk899ZTpuoDIsSq2pmctdbYnlUzE/C4HgAdqhuy7776rDz74QPl8XqVSSU8//TQhCyyD\nfeOG8qPjGisUNTVjqasjqcFsRsNDA4o1czEJiLKaIfvoo49q7dq1kqSOjg5du3ZNtm0rFuNIHHAj\nPzquk2c+qr6enLGqr0dyWb/KAuCBmiEbi8XU1tYmSTp69KieeOKJJQM2nW5TPF7fAM5kUnX9vCij\nV+550av5T6/rPxcmHcf+c2FSz239glpbXM3a+I5tyz165V7Ue+X6233y5EkdPXpUhw8fXvLvSqW5\nFRd1u0wmpWKxXNfPjCp65Z5XvbpamlOxdM1xbOJ/r+nC/0yqN91mvI6VYttyj165F5VeLXWg4GpC\n6J133tHBgwd16NAhpVLRPuoA6qmzPamujqTjWDrVqs525zHAK1bF1tXSnKyK7XcpkVTzTLZcLmvf\nvn3661//qlWrVnlRExAZyURMg9nMHXOyiwazPawyXgKrsc1iQZ43aobsP/7xD5VKJf3sZz+rvrd3\n71498MADRgsDomJ4aECSNFaYUKk8r3SqVYPZnur7uBM7f2+wIM8bNUN2eHhYw8PDXtQCuBams5xY\nc7NGcllt3dgfmpr9xM7fPKtia6xQdBwbK0xo68Z+ttE6CceyRuAzYT7LSSZioVjk5Cd2/t6YnrU0\nNWM5jpXK85qetdhW6yTYeyXgcxbPciZnLC3o1llOfnTc79JQB252/lg5FuR5h5BFaNQ6y2F1ZPix\n8/fG4oI8JyzIqy9CFqHBWU70sfP3zvDQgHLrHlR3R6uam6Tujlbl1j3Igrw6Y07WQZgW1TSSxbOc\nSYeg5SwnOliN7Q0W5HmDkL1NmBfVNALuOW0M7Py9xYI8swjZ23DrQPBxltM42PkjCgjZz3DrQDhw\nlgNgJbyeDiRkP8N9Y+HCWQ6A5fBrOpCJxs9w6wAARJdf99gTsp/h1gEAiCY/77HncvFtWFQDANHj\n53QgIXsbFtUAQPT4eY89l4sdLC6qIWABIPz8nA7kTBYAEHl+TQcSsgCAyPNrOpCQBQA0DK/vsWdO\nFgAAQwhZAAAMIWQBADCEkAUAwBBCFgAAQwhZAAAMIWQBADCEkAUAwBBCFgDgOati6/LEJ0YfMxcE\n/OITAMAz9o0byo+Oa6xQ1FTZUlcqqcFsRsNDA4o1R++8j5AFAHgmPzquk2c+qr6enLGqr0dyWb/K\nMiZ6hw0AgECyKrbGCkXHsbHCRCQvHROyAABPTM9amnJ4cLoklcrzmp51HgszQhYA4InO9qS6OpKO\nY+lUqzrbncfCjJAFAHgimYhpMJtxHBvM9njyfFevsfAJAOCZ4aEBSTfnYEvleaVTrRrM9lTfjxpC\nFgDgmVhzs0ZyWW3d2K9YS0L2p5VInsEu4nIxAMBzyURM9/fcF+mAlQhZAACMIWQBADCEkAUAwBBC\nFgAAQwhZAAAMIWQBADCEkAUAwBBCFgAAQ1yFbKFQUC6X05EjR0zXAwBAZNQM2bm5Ob366qtav369\nF/UAABAZNUO2paVFhw4dUm9vrxf1AAAQGTUfEBCPxxWP8xwBAACWq+7pmU63KR6v7w8+ZzKpun5e\nlNEr9+jV8tAv9+iVe1HvVd1DtlSaq+vnZTIpFYvlun5mVNEr9+jV8tAv9+iVe1Hp1VIHCtzCA99Y\nFVtXS3OyKrbfpQCAETXPZM+dO6e9e/fq0qVLisfjOn78uA4cOKBVq1Z5UR8iyL5xQ/nRcY0Vipqa\nsdTVkdRgNqPhoQHFmjnuA5bLqtianrXU2Z6M/PNZw6ZmyD7yyCP629/+5kUtaBD50XGdPPNR9fXk\njFV9PZLL+lUWEDocsAYf/wrwlFWxNVYoOo6NFSa4dAwsw+IB6+SMpQXdOmDNj477XRo+Q8jCU9Oz\nlqZmLMexUnle07POYwDuxAFrOBCy8FRne1JdHUnHsXSqVZ3tzmPwj1WxdXniE3baAcMBazjwKxPw\nVDIR02A2c8ec7KLBbA+LNgLkjvm+sqWuFPN9QbJ4wDrpELQcsAYH3xR4bnhoQLl1D6q7o1XNTVJ3\nR6ty6x7U8NCA36XhNnfM9y0w3xc0iwesTjhgDQ7OZOG5WHOzRnJZbd3Yz20HAVVrvm/rxn7+zQJg\n8cB0rDChUnle6VSrBrM9HLAGCCEL3yQTMfWm2/wuIzS8vBfSzXwf/3b+44A1+AhZIOD8uBeS+b5w\n4YA1uJiTBQLOj3shme8D6oOQBQLMz3shWaAGrByXi4EA83Nu9Pb5vlhLQvanFc5ggWXiTBYIsCD8\neEcyEdP9PfcRsMA9IGSBAGNuFAg3LhcDAce9kEB4EbJoOIu/xWtX7FCcCXIvJBBegQ7ZsO0MEWxh\n/y1e7oUEwieQIRv2nSGCiYfFA/BaIBOLHyZHvfHsTQB+CFzIsjOECTx7E4AfAhey7AxhQhDuNwXQ\neAIXsvXaGVoVW1dLc5z5QhL3m/qF7yEaXeAWPi3uDG9foLLIzc7QjyeWIBy439Q7fA+BmwIXstLK\ndoasIMXd8Fu83uF7CNwUyJC9151hrUVTWzf2s1OFkomYMj33qVgs+11KJPE9BG4J9HWb5f4wOYum\nAP/xPQRuCXTILhcrSAH/8T0EbolUyLKCFPAf30PglkDOya4EK0gB//E9BG6KXMjyxBLAf3wPgZsi\nF7KLeGIJ4D++h2h0kZqTBQAgSAhZAAAMIWQBADCEkAUAwBBCFgAAQwhZAAAMIWSBBsBzXQF/RPY+\nWQA81xXwGyELRBjPdQX8xaEsEFG1nuvKpWPAPEIWiCie6wr4j5AFIornugL+I2SBiOK5roD/WPiE\nFbEqNo8yCzCe6xoefJeiyVXIvvbaa3r//ffV1NSkPXv2aO3atabrQsBxa0g48FzX4OO7FG01Q/a9\n997Thx9+qHw+rwsXLmjPnj3K5/Ne1IYA49aQcOG5rsHFdynaah4mnTp1SrlcTpLU39+v6elpzc7O\nGi8MwcWtIUB98F2KvppnshMTE3r44Yerr7u6ulQsFtXe3u749+l0m+Lx+l6OymRSdf28KPOiV5cn\nPtFU+e63hsRaEsr03Ge8jpViu1oe+uWe215F5bu0ElHfrpa98GlhYWHJ8VJp7p6LcZLJpFQsluv6\nmVHlVa/siq2uVFKTDvdgplOtsj+tBP7fjO1qeeiXe8vpVRS+SysRle1qqQOFmpeLe3t7NTExUX19\n9epVZTLOtwWgMXBrCFAffJeir2bIbtiwQcePH5cknT9/Xr29vXe9VIzGMTw0oNy6B9Xd0armJqm7\no1W5dQ9yawiwTHyXoq3m5eJvfvObevjhh7Vt2zY1NTXplVde8aIuBBy3hgD1wXcp2lzNye7atct0\nHQgpbg0B6oPvUjRxpzMAAIYQsgAAGELIAgBgCCELAIAhhCwAAIYQsgAAGELIAgBgCCELAIAhTQu1\nfvEfAADcE85kAQAwhJAFAMAQQhYAAEMIWQAADCFkAQAwhJAFAMAQV8+T9ctrr72m999/X01NTdqz\nZ4/Wrl3rd0mBdPr0ab3wwgt66KGHJEnZbFYvvfSSz1UFT6FQ0I4dO/SDH/xAzz77rC5fvqwXX3xR\ntm0rk8not7/9rVpaWvwuMxA+36vdu3fr/PnzWrVqlSTpRz/6kZ588kl/iwyIffv26ezZs7p+/bqe\ne+45ff3rX2e7uovP92p0dDTy21VgQ/a9997Thx9+qHw+rwsXLmjPnj3K5/N+lxVYjz32mPbv3+93\nGYE1NzenV199VevXr6++t3//fo2MjGjz5s36/e9/r6NHj2pkZMTHKoPBqVeS9Itf/EKbNm3yqapg\nevfdd/XBBx8on8+rVCrp6aef1vr169muHDj16lvf+lbkt6vAXi4+deqUcrmcJKm/v1/T09OanZ31\nuSqEVUtLiw4dOqTe3t7qe6dPn9Z3vvMdSdKmTZt06tQpv8oLFKdewdmjjz6qP/7xj5Kkjo4OXbt2\nje3qLpx6Zdu2z1WZF9iQnZiYUDqdrr7u6upSsVj0saJgGx8f109/+lN9//vf17///W+/ywmceDyu\n1tbWO967du1a9TJed3c329dnnHolSUeOHNH27dv185//XFNTUz5UFjyxWExtbW2SpKNHj+qJJ55g\nu7oLp17FYrHIb1eBvVz8efz649195Stf0c6dO7V582ZdvHhR27dv14kTJ5gHWga2r6V973vf06pV\nq7RmzRq9+eab+tOf/qSXX37Z77IC4+TJkzp69KgOHz6sp556qvo+29X/d3uvzp07F/ntKrBnsr29\nvZqYmKi+vnr1qjKZjI8VBVdfX5+2bNmipqYmrV69Wj09Pbpy5YrfZQVeW1ub5ufnJUlXrlzh8ugS\n1q9frzVr1kiShoaGVCgUfK4oON555x0dPHhQhw4dUiqVYrtawud71QjbVWBDdsOGDTp+/Lgk6fz5\n8+rt7VV7e7vPVQXTsWPH9NZbb0mSisWiJicn1dfX53NVwff4449Xt7ETJ07o29/+ts8VBdfzzz+v\nixcvSro5l724kr3Rlctl7du3T3/5y1+qK2TZrpw59aoRtqtAP4XnjTfe0JkzZ9TU1KRXXnlFX/va\n1/wuKZBmZ2e1a9cuzczMqFKpaOfOndq4caPfZQXKuXPntHfvXl26dEnxeFx9fX164403tHv3blmW\npQceeECvv/66EomE36X6zqlXzz77rN5880194QtfUFtbm15//XV1d3f7Xarv8vm8Dhw4oK9+9avV\n937zm9/ol7/8JdvV5zj16plnntGRI0civV0FOmQBAAizwF4uBgAg7AhZAAAMIWQBADCEkAUAwBBC\nFgAAQwhZAAAMIWQBADCEkAUAwJD/AwXULA5x1KK0AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "LEADkcMzelxY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "How does linear regression do at fitting this?"
      ]
    },
    {
      "metadata": {
        "id": "EV5kt69GenV3",
        "colab_type": "code",
        "outputId": "fbe75a70-e091-4293-a2d3-a1b067c7026c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "sales_ols = LinearRegression()\n",
        "sales_ols.fit(days, sales)\n",
        "sales_ols.score(days, sales)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.029426160058874995"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "7shN1eBMep9Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "That's not great - and the fix depends on the domain. Here, we know it'd be best to actually use \"day of week\" as a feature."
      ]
    },
    {
      "metadata": {
        "id": "Qo9eFlHIeqtA",
        "colab_type": "code",
        "outputId": "097877cb-1237-4a57-bfb9-8c918e0cc6e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "day_of_week = days % 7\n",
        "sales_ols.fit(day_of_week, sales)\n",
        "sales_ols.score(day_of_week, sales)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4868906772207705"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "9ooJIfIMex2G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Note that it's also important to have representative data across whatever seasonal feature(s) you use - don't predict retailers based only on Christmas, as that won't generalize well."
      ]
    },
    {
      "metadata": {
        "id": "44QZgrPUe3-Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Recurrent Neural Networks\n",
        "\n",
        "There's plenty more to \"traditional\" time series, but the latest and greatest technique for sequence data is recurrent neural networks. A recurrence relation in math is an equation that uses recursion to define a sequence - a famous example is the Fibonacci numbers:\n",
        "\n",
        "$F_n = F_{n-1} + F_{n-2}$\n",
        "\n",
        "For formal math you also need a base case $F_0=1, F_1=1$, and then the rest builds from there. But for neural networks what we're really talking about are loops:\n",
        "\n",
        "![Recurrent neural network](https://upload.wikimedia.org/wikipedia/commons/b/b5/Recurrent_neural_network_unfold.svg)\n",
        "\n",
        "The hidden layers have edges (output) going back to their own input - this loop means that for any time `t` the training is at least partly based on the output from time `t-1`. The entire network is being represented on the left, and you can unfold the network explicitly to see how it behaves at any given `t`.\n",
        "\n",
        "Different units can have this \"loop\", but a particularly successful one is the long short-term memory unit (LSTM):\n",
        "\n",
        "![Long short-term memory unit](https://upload.wikimedia.org/wikipedia/commons/thumb/6/63/Long_Short-Term_Memory.svg/1024px-Long_Short-Term_Memory.svg.png)\n",
        "\n",
        "There's a lot going on here - in a nutshell, the calculus still works out and backpropagation can still be implemented. The advantage (ane namesake) of LSTM is that it can generally put more weight on recent (short-term) events while not completely losing older (long-term) information.\n",
        "\n",
        "After enough iterations, a typical neural network will start calculating prior gradients that are so small they effectively become zero - this is the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem), and is what RNN with LSTM addresses. Pay special attention to the $c_t$ parameters and how they pass through the unit to get an intuition for how this problem is solved.\n",
        "\n",
        "So why are these cool? One particularly compelling application is actually not time series but language modeling - language is inherently ordered data (letters/words go one after another, and the order *matters*). [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) is a famous and worth reading blog post on this topic.\n",
        "\n",
        "For our purposes, let's use TensorFlow and Keras to train RNNs with natural language. Resources:\n",
        "\n",
        "- https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py\n",
        "- https://keras.io/layers/recurrent/#lstm\n",
        "- http://adventuresinmachinelearning.com/keras-lstm-tutorial/\n",
        "\n",
        "Note that `tensorflow.contrib` [also has an implementation of RNN/LSTM](https://www.tensorflow.org/tutorials/sequences/recurrent)."
      ]
    },
    {
      "metadata": {
        "id": "eWrQllf8WEd-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### RNN/LSTM Sentiment Classification with Keras"
      ]
    },
    {
      "metadata": {
        "id": "Ti23G0gRe3kr",
        "colab_type": "code",
        "outputId": "872cb7fd-079e-473b-cc29-2b4d8d33f47c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        }
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "#Trains an LSTM model on the IMDB sentiment classification task.\n",
        "The dataset is actually too small for LSTM to be of any advantage\n",
        "compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
        "**Notes**\n",
        "- RNNs are tricky. Choice of batch size is important,\n",
        "choice of loss and optimizer is critical, etc.\n",
        "Some configurations won't converge.\n",
        "- LSTM loss decrease patterns during training can be quite different\n",
        "from what you see with CNNs/MLPs/etc.\n",
        "'''\n",
        "from __future__ import print_function\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding\n",
        "from keras.layers import LSTM\n",
        "from keras.datasets import imdb\n",
        "\n",
        "max_features = 20000\n",
        "# cut texts after this number of words (among top max_features most common words)\n",
        "maxlen = 80\n",
        "batch_size = 32\n",
        "\n",
        "print('Loading data...')\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')\n",
        "\n",
        "print('Pad sequences (samples x time)')\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "\n",
        "print('Build model...')\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 128))\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# try using different optimizers and different optimizer configs\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print('Train...')\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=15,\n",
        "          validation_data=(x_test, y_test))\n",
        "score, acc = model.evaluate(x_test, y_test,\n",
        "                            batch_size=batch_size)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "25000 train sequences\n",
            "25000 test sequences\n",
            "Pad sequences (samples x time)\n",
            "x_train shape: (25000, 80)\n",
            "x_test shape: (25000, 80)\n",
            "Build model...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Train...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/15\n",
            "   96/25000 [..............................] - ETA: 12:38 - loss: 0.6931 - acc: 0.5625"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7pETWPIe362y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### RNN Text generation with NumPy\n",
        "\n",
        "What else can we do with RNN? Since we're analyzing the *sequence*, we can do more than classify - we can *generate* text. We'll pull some news stories using [newspaper](https://github.com/codelucas/newspaper/)."
      ]
    },
    {
      "metadata": {
        "id": "fz1m55G5WSrQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Initialization"
      ]
    },
    {
      "metadata": {
        "id": "ahlHBeoZCaLX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install newspaper3k"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fTPlziljCiNJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import newspaper"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bk9JF2zaCxoO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ap = newspaper.build('https://www.apnews.com')\n",
        "len(ap.articles)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vc6JgAIJDF4E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "article_text = ''\n",
        "\n",
        "for article in ap.articles[:1]:\n",
        "  try:\n",
        "    article.download()\n",
        "    article.parse()\n",
        "    article_text += '\\n\\n' + article.text\n",
        "  except:\n",
        "    print('Failed: ' + article.url)\n",
        "  \n",
        "article_text = article_text.split('\\n\\n')[1]\n",
        "print(article_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rsMBBMcv_nRM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Based on \"The Unreasonable Effectiveness of RNN\" implementation\n",
        "import numpy as np\n",
        "\n",
        "chars = list(set(article_text)) # split and remove duplicate characters. convert to list.\n",
        "\n",
        "num_chars = len(chars) # the number of unique characters\n",
        "txt_data_size = len(article_text)\n",
        "\n",
        "print(\"unique characters : \", num_chars)\n",
        "print(\"txt_data_size : \", txt_data_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aQygqc_CAWRA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# one hot encode\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars)) # \"enumerate\" retruns index and value. Convert it to dictionary\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "print(char_to_int)\n",
        "print(\"----------------------------------------------------\")\n",
        "print(int_to_char)\n",
        "print(\"----------------------------------------------------\")\n",
        "# integer encode input data\n",
        "integer_encoded = [char_to_int[i] for i in article_text] # \"integer_encoded\" is a list which has a sequence converted from an original data to integers.\n",
        "print(integer_encoded)\n",
        "print(\"----------------------------------------------------\")\n",
        "print(\"data length : \", len(integer_encoded))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bcpMSWDHFowT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "\n",
        "iteration = 1000\n",
        "sequence_length = 40\n",
        "batch_size = round((txt_data_size /sequence_length)+0.5) # = math.ceil\n",
        "hidden_size = 500  # size of hidden layer of neurons.  \n",
        "learning_rate = 1e-1\n",
        "\n",
        "\n",
        "# model parameters\n",
        "\n",
        "W_xh = np.random.randn(hidden_size, num_chars)*0.01     # weight input -> hidden. \n",
        "W_hh = np.random.randn(hidden_size, hidden_size)*0.01   # weight hidden -> hidden\n",
        "W_hy = np.random.randn(num_chars, hidden_size)*0.01     # weight hidden -> output\n",
        "\n",
        "b_h = np.zeros((hidden_size, 1)) # hidden bias\n",
        "b_y = np.zeros((num_chars, 1)) # output bias\n",
        "\n",
        "h_prev = np.zeros((hidden_size,1)) # h_(t-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bkqoN86qWaI4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Forward propagation"
      ]
    },
    {
      "metadata": {
        "id": "imfg_Ew0WdDL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def forwardprop(inputs, targets, h_prev):\n",
        "        \n",
        "    # Since the RNN receives the sequence, the weights are not updated during one sequence.\n",
        "    xs, hs, ys, ps = {}, {}, {}, {} # dictionary\n",
        "    hs[-1] = np.copy(h_prev) # Copy previous hidden state vector to -1 key value.\n",
        "    loss = 0 # loss initialization\n",
        "    \n",
        "    for t in range(len(inputs)): # t is a \"time step\" and is used as a key(dic).  \n",
        "        \n",
        "        xs[t] = np.zeros((num_chars,1)) \n",
        "        xs[t][inputs[t]] = 1\n",
        "        hs[t] = np.tanh(np.dot(W_xh, xs[t]) + np.dot(W_hh, hs[t-1]) + b_h) # hidden state. \n",
        "        ys[t] = np.dot(W_hy, hs[t]) + b_y # unnormalized log probabilities for next chars\n",
        "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars. \n",
        "        # Softmax. -> The sum of probabilities is 1 even without the exp() function, but all of the elements are positive through the exp() function.\n",
        " \n",
        "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss). Efficient and simple code\n",
        "\n",
        "#         y_class = np.zeros((num_chars, 1)) \n",
        "#         y_class[targets[t]] =1\n",
        "#         loss += np.sum(y_class*(-np.log(ps[t]))) # softmax (cross-entropy loss)        \n",
        "\n",
        "    return loss, ps, hs, xs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zm6qwNiqWdMe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Backward propagation"
      ]
    },
    {
      "metadata": {
        "id": "81qBiz_xWenI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def backprop(ps, inputs, hs, xs):\n",
        "\n",
        "    dWxh, dWhh, dWhy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy) # make all zero matrices.\n",
        "    dbh, dby = np.zeros_like(b_h), np.zeros_like(b_y)\n",
        "    dhnext = np.zeros_like(hs[0]) # (hidden_size,1) \n",
        "\n",
        "    # reversed\n",
        "    for t in reversed(range(len(inputs))):\n",
        "        dy = np.copy(ps[t]) # shape (num_chars,1).  \"dy\" means \"dloss/dy\"\n",
        "        dy[targets[t]] -= 1 # backprop into y. After taking the soft max in the input vector, subtract 1 from the value of the element corresponding to the correct label.\n",
        "        dWhy += np.dot(dy, hs[t].T)\n",
        "        dby += dy \n",
        "        dh = np.dot(W_hy.T, dy) + dhnext # backprop into h. \n",
        "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity #tanh'(x) = 1-tanh^2(x)\n",
        "        dbh += dhraw\n",
        "        dWxh += np.dot(dhraw, xs[t].T)\n",
        "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
        "        dhnext = np.dot(W_hh.T, dhraw)\n",
        "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]: \n",
        "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients.  \n",
        "    \n",
        "    return dWxh, dWhh, dWhy, dbh, dby"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r8sBvcdbWfhi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Training"
      ]
    },
    {
      "metadata": {
        "id": "iA4RM70LWgO_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "data_pointer = 0\n",
        "\n",
        "# memory variables for Adagrad\n",
        "mWxh, mWhh, mWhy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy)\n",
        "mbh, mby = np.zeros_like(b_h), np.zeros_like(b_y) \n",
        "\n",
        "for i in range(iteration):\n",
        "    h_prev = np.zeros((hidden_size,1)) # reset RNN memory\n",
        "    data_pointer = 0 # go from start of data\n",
        "    \n",
        "    for b in range(batch_size):\n",
        "        \n",
        "        inputs = [char_to_int[ch]\n",
        "                  for ch in article_text[data_pointer:data_pointer+sequence_length]]\n",
        "        targets = [char_to_int[ch]\n",
        "                   for ch in article_text[data_pointer+1:data_pointer+sequence_length+1]] # t+1        \n",
        "            \n",
        "        if (data_pointer+sequence_length+1 >= len(article_text) and b == batch_size-1): # processing of the last part of the input data. \n",
        "#             targets.append(char_to_int[txt_data[0]])   # When the data doesn't fit, add the first char to the back.\n",
        "            targets.append(char_to_int[\" \"])   # When the data doesn't fit, add space(\" \") to the back.\n",
        "\n",
        "\n",
        "        # forward\n",
        "        loss, ps, hs, xs = forwardprop(inputs, targets, h_prev)\n",
        "#         print(loss)\n",
        "    \n",
        "        # backward\n",
        "        dWxh, dWhh, dWhy, dbh, dby = backprop(ps, inputs, hs, xs) \n",
        "        \n",
        "        \n",
        "    # perform parameter update with Adagrad\n",
        "        for param, dparam, mem in zip([W_xh, W_hh, W_hy, b_h, b_y], \n",
        "                                    [dWxh, dWhh, dWhy, dbh, dby], \n",
        "                                    [mWxh, mWhh, mWhy, mbh, mby]):\n",
        "            mem += dparam * dparam # elementwise\n",
        "            param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update      \n",
        "    \n",
        "        data_pointer += sequence_length # move data pointer\n",
        "        \n",
        "    if i % 100 == 0:\n",
        "        print ('iter %d, loss: %f' % (i, loss)) # print progress"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tjh8Ip68WgYV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Prediction"
      ]
    },
    {
      "metadata": {
        "id": "HDCxDNPG68Hx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predict(test_char, length):\n",
        "    x = np.zeros((num_chars, 1)) \n",
        "    x[char_to_int[test_char]] = 1\n",
        "    ixes = []\n",
        "    h = np.zeros((hidden_size,1))\n",
        "\n",
        "    for t in range(length):\n",
        "        h = np.tanh(np.dot(W_xh, x) + np.dot(W_hh, h) + b_h) \n",
        "        y = np.dot(W_hy, h) + b_y\n",
        "        p = np.exp(y) / np.sum(np.exp(y)) \n",
        "        ix = np.random.choice(range(num_chars), p=p.ravel()) # ravel -> rank0\n",
        "        # \"ix\" is a list of indexes selected according to the soft max probability.\n",
        "        x = np.zeros((num_chars, 1)) # init\n",
        "        x[ix] = 1 \n",
        "        ixes.append(ix) # list\n",
        "    txt = test_char + ''.join(int_to_char[i] for i in ixes)\n",
        "    print ('----\\n %s \\n----' % (txt, ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nGVhl-Gxh6N6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predict('L', 50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xPsz-oefL1kP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Well... that's *vaguely* language-looking. Can you do better?"
      ]
    },
    {
      "metadata": {
        "id": "0lfZdD_cp1t5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Assignment\n",
        "\n",
        "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
        "\n",
        "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
        "\n",
        "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
        "\n",
        "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
        "\n",
        "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
        "\n",
        "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
      ]
    },
    {
      "metadata": {
        "id": "AHYUXlut8Pt6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO - Words, words, mere words, no matter from the heart.\n",
        "\n",
        "shakespeare_text = \"\"\"From fairest creatures we desire increase,\n",
        "That thereby beauty’s rose might never die,\n",
        "But as the riper should by time decease,\n",
        "His tender heir might bear his memory:\n",
        "But thou contracted to thine own bright eyes,\n",
        "Feed’st thy light’s flame with self-substantial fuel,\n",
        "Making a famine where abundance lies,\n",
        "Thy self thy foe, to thy sweet self too cruel:\n",
        "Thou that art now the world’s fresh ornament,\n",
        "And only herald to the gaudy spring,\n",
        "Within thine own bud buriest thy content,\n",
        "And, tender churl, mak’st waste in niggarding:\n",
        "  Pity the world, or else this glutton be,\n",
        "  To eat the world’s due, by the grave and thee.\n",
        "When forty winters shall besiege thy brow,\n",
        "And dig deep trenches in thy beauty’s field,\n",
        "Thy youth’s proud livery so gazed on now,\n",
        "Will be a tattered weed of small worth held:\n",
        "Then being asked, where all thy beauty lies,\n",
        "Where all the treasure of thy lusty days;\n",
        "To say, within thine own deep sunken eyes,\n",
        "Were an all-eating shame, and thriftless praise.\n",
        "How much more praise deserv’d thy beauty’s use,\n",
        "If thou couldst answer ‘This fair child of mine\n",
        "Shall sum my count, and make my old excuse,’\n",
        "Proving his beauty by succession thine.\n",
        "  This were to be new made when thou art old,\n",
        "  And see thy blood warm when thou feel’st it cold.\n",
        "Look in thy glass and tell the face thou viewest,\n",
        "Now is the time that face should form another,\n",
        "Whose fresh repair if now thou not renewest,\n",
        "Thou dost beguile the world, unbless some mother.\n",
        "For where is she so fair whose uneared womb\n",
        "Disdains the tillage of thy husbandry?\n",
        "Or who is he so fond will be the tomb\n",
        "Of his self-love to stop posterity?\n",
        "Thou art thy mother’s glass and she in thee\n",
        "Calls back the lovely April of her prime,\n",
        "So thou through windows of thine age shalt see,\n",
        "Despite of wrinkles this thy golden time.\n",
        "  But if thou live remembered not to be,\n",
        "  Die single and thine image dies with thee.\n",
        "Unthrifty loveliness why dost thou spend,\n",
        "Upon thy self thy beauty’s legacy?\n",
        "Nature’s bequest gives nothing but doth lend,\n",
        "And being frank she lends to those are free:\n",
        "Then beauteous niggard why dost thou abuse,\n",
        "The bounteous largess given thee to give?\n",
        "Profitless usurer why dost thou use\n",
        "So great a sum of sums yet canst not live?\n",
        "For having traffic with thy self alone,\n",
        "Thou of thy self thy sweet self dost deceive,\n",
        "Then how when nature calls thee to be gone,\n",
        "What acceptable audit canst thou leave?\n",
        "  Thy unused beauty must be tombed with thee,\n",
        "  Which used lives th’ executor to be.\n",
        "Those hours that with gentle work did frame\n",
        "The lovely gaze where every eye doth dwell\n",
        "Will play the tyrants to the very same,\n",
        "And that unfair which fairly doth excel:\n",
        "For never-resting time leads summer on\n",
        "To hideous winter and confounds him there,\n",
        "Sap checked with frost and lusty leaves quite gone,\n",
        "Beauty o’er-snowed and bareness every where:\n",
        "Then were not summer’s distillation left\n",
        "A liquid prisoner pent in walls of glass,\n",
        "Beauty’s effect with beauty were bereft,\n",
        "Nor it nor no remembrance what it was.\n",
        "  But flowers distilled though they with winter meet,\n",
        "  Leese but their show, their substance still lives sweet.\n",
        "Then let not winter’s ragged hand deface,\n",
        "In thee thy summer ere thou be distilled:\n",
        "Make sweet some vial; treasure thou some place,\n",
        "With beauty’s treasure ere it be self-killed:\n",
        "That use is not forbidden usury,\n",
        "Which happies those that pay the willing loan;\n",
        "That’s for thy self to breed another thee,\n",
        "Or ten times happier be it ten for one,\n",
        "Ten times thy self were happier than thou art,\n",
        "If ten of thine ten times refigured thee:\n",
        "Then what could death do if thou shouldst depart,\n",
        "Leaving thee living in posterity?\n",
        "  Be not self-willed for thou art much too fair,\n",
        "  To be death’s conquest and make worms thine heir.\n",
        "Lo in the orient when the gracious light\n",
        "Lifts up his burning head, each under eye\n",
        "Doth homage to his new-appearing sight,\n",
        "Serving with looks his sacred majesty,\n",
        "And having climbed the steep-up heavenly hill,\n",
        "Resembling strong youth in his middle age,\n",
        "Yet mortal looks adore his beauty still,\n",
        "Attending on his golden pilgrimage:\n",
        "But when from highmost pitch with weary car,\n",
        "Like feeble age he reeleth from the day,\n",
        "The eyes (fore duteous) now converted are\n",
        "From his low tract and look another way:\n",
        "  So thou, thy self out-going in thy noon:\n",
        "  Unlooked on diest unless thou get a son.\n",
        "Music to hear, why hear’st thou music sadly?\n",
        "Sweets with sweets war not, joy delights in joy:\n",
        "Why lov’st thou that which thou receiv’st not gladly,\n",
        "Or else receiv’st with pleasure thine annoy?\n",
        "If the true concord of well-tuned sounds,\n",
        "By unions married do offend thine ear,\n",
        "They do but sweetly chide thee, who confounds\n",
        "In singleness the parts that thou shouldst bear:\n",
        "Mark how one string sweet husband to another,\n",
        "Strikes each in each by mutual ordering;\n",
        "Resembling sire, and child, and happy mother,\n",
        "Who all in one, one pleasing note do sing:\n",
        "  Whose speechless song being many, seeming one,\n",
        "  Sings this to thee, ‘Thou single wilt prove none’.\n",
        "Is it for fear to wet a widow’s eye,\n",
        "That thou consum’st thy self in single life?\n",
        "Ah, if thou issueless shalt hap to die,\n",
        "The world will wail thee like a makeless wife,\n",
        "The world will be thy widow and still weep,\n",
        "That thou no form of thee hast left behind,\n",
        "When every private widow well may keep,\n",
        "By children’s eyes, her husband’s shape in mind:\n",
        "Look what an unthrift in the world doth spend\n",
        "Shifts but his place, for still the world enjoys it;\n",
        "But beauty’s waste hath in the world an end,\n",
        "And kept unused the user so destroys it:\n",
        "  No love toward others in that bosom sits\n",
        "  That on himself such murd’rous shame commits.\n",
        "For shame deny that thou bear’st love to any\n",
        "Who for thy self art so unprovident.\n",
        "Grant if thou wilt, thou art beloved of many,\n",
        "But that thou none lov’st is most evident:\n",
        "For thou art so possessed with murd’rous hate,\n",
        "That ’gainst thy self thou stick’st not to conspire,\n",
        "Seeking that beauteous roof to ruinate\n",
        "Which to repair should be thy chief desire:\n",
        "O change thy thought, that I may change my mind,\n",
        "Shall hate be fairer lodged than gentle love?\n",
        "Be as thy presence is gracious and kind,\n",
        "Or to thy self at least kind-hearted prove,\n",
        "  Make thee another self for love of me,\n",
        "  That beauty still may live in thine or thee.\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iU8xXKrD9CEv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "shakespeare_text = shakespeare_text.replace('\\n', ' ')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K0ZY67pN_VTU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "182f3916-b598-44eb-ef25-d1cff47adb24"
      },
      "cell_type": "code",
      "source": [
        "len(shakespeare_text)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6161"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "qi6o1Bjk9cOE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e1d19be3-5c03-4be3-df9b-7f5f350ea846"
      },
      "cell_type": "code",
      "source": [
        "# Based on \"The Unreasonable Effectiveness of RNN\" implementation\n",
        "import numpy as np\n",
        "\n",
        "chars = list(set(shakespeare_text)) # split and remove duplicate characters. convert to list.\n",
        "\n",
        "num_chars = len(chars) # the number of unique characters\n",
        "txt_data_size = len(shakespeare_text)\n",
        "\n",
        "print(\"unique characters : \", num_chars)\n",
        "print(\"txt_data_size : \", txt_data_size)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unique characters :  56\n",
            "txt_data_size :  6161\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WYpjwkfE9paE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "outputId": "03ccea3b-3284-4c9e-a5c8-c797e21add93"
      },
      "cell_type": "code",
      "source": [
        "# one hot encode\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars)) # \"enumerate\" retruns index and value. Convert it to dictionary\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "print(char_to_int)\n",
        "print(\"----------------------------------------------------\")\n",
        "print(int_to_char)\n",
        "print(\"----------------------------------------------------\")\n",
        "# integer encode input data\n",
        "integer_encoded = [char_to_int[i] for i in shakespeare_text] # \"integer_encoded\" is a list which has a sequence converted from an original data to integers.\n",
        "print(integer_encoded)\n",
        "print(\"----------------------------------------------------\")\n",
        "print(\"data length : \", len(integer_encoded))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'D': 0, 'q': 1, '(': 2, 'v': 3, 'A': 4, 't': 5, '?': 6, 's': 7, ';': 8, 'N': 9, 'R': 10, 'y': 11, 'p': 12, ',': 13, 'B': 14, ')': 15, 'l': 16, 'W': 17, 'G': 18, 'u': 19, 'r': 20, 'H': 21, '.': 22, 'I': 23, 'x': 24, '’': 25, 'O': 26, 'C': 27, '-': 28, 'n': 29, 'f': 30, 'm': 31, 'e': 32, 'i': 33, ' ': 34, 'T': 35, 'h': 36, 'S': 37, 'b': 38, 'w': 39, 'k': 40, 'P': 41, 'U': 42, 'Y': 43, 'F': 44, 'g': 45, 'a': 46, 'o': 47, 'd': 48, 'M': 49, '‘': 50, 'c': 51, 'z': 52, 'L': 53, ':': 54, 'j': 55}\n",
            "----------------------------------------------------\n",
            "{0: 'D', 1: 'q', 2: '(', 3: 'v', 4: 'A', 5: 't', 6: '?', 7: 's', 8: ';', 9: 'N', 10: 'R', 11: 'y', 12: 'p', 13: ',', 14: 'B', 15: ')', 16: 'l', 17: 'W', 18: 'G', 19: 'u', 20: 'r', 21: 'H', 22: '.', 23: 'I', 24: 'x', 25: '’', 26: 'O', 27: 'C', 28: '-', 29: 'n', 30: 'f', 31: 'm', 32: 'e', 33: 'i', 34: ' ', 35: 'T', 36: 'h', 37: 'S', 38: 'b', 39: 'w', 40: 'k', 41: 'P', 42: 'U', 43: 'Y', 44: 'F', 45: 'g', 46: 'a', 47: 'o', 48: 'd', 49: 'M', 50: '‘', 51: 'c', 52: 'z', 53: 'L', 54: ':', 55: 'j'}\n",
            "----------------------------------------------------\n",
            "[44, 20, 47, 31, 34, 30, 46, 33, 20, 32, 7, 5, 34, 51, 20, 32, 46, 5, 19, 20, 32, 7, 34, 39, 32, 34, 48, 32, 7, 33, 20, 32, 34, 33, 29, 51, 20, 32, 46, 7, 32, 13, 34, 35, 36, 46, 5, 34, 5, 36, 32, 20, 32, 38, 11, 34, 38, 32, 46, 19, 5, 11, 25, 7, 34, 20, 47, 7, 32, 34, 31, 33, 45, 36, 5, 34, 29, 32, 3, 32, 20, 34, 48, 33, 32, 13, 34, 14, 19, 5, 34, 46, 7, 34, 5, 36, 32, 34, 20, 33, 12, 32, 20, 34, 7, 36, 47, 19, 16, 48, 34, 38, 11, 34, 5, 33, 31, 32, 34, 48, 32, 51, 32, 46, 7, 32, 13, 34, 21, 33, 7, 34, 5, 32, 29, 48, 32, 20, 34, 36, 32, 33, 20, 34, 31, 33, 45, 36, 5, 34, 38, 32, 46, 20, 34, 36, 33, 7, 34, 31, 32, 31, 47, 20, 11, 54, 34, 14, 19, 5, 34, 5, 36, 47, 19, 34, 51, 47, 29, 5, 20, 46, 51, 5, 32, 48, 34, 5, 47, 34, 5, 36, 33, 29, 32, 34, 47, 39, 29, 34, 38, 20, 33, 45, 36, 5, 34, 32, 11, 32, 7, 13, 34, 44, 32, 32, 48, 25, 7, 5, 34, 5, 36, 11, 34, 16, 33, 45, 36, 5, 25, 7, 34, 30, 16, 46, 31, 32, 34, 39, 33, 5, 36, 34, 7, 32, 16, 30, 28, 7, 19, 38, 7, 5, 46, 29, 5, 33, 46, 16, 34, 30, 19, 32, 16, 13, 34, 49, 46, 40, 33, 29, 45, 34, 46, 34, 30, 46, 31, 33, 29, 32, 34, 39, 36, 32, 20, 32, 34, 46, 38, 19, 29, 48, 46, 29, 51, 32, 34, 16, 33, 32, 7, 13, 34, 35, 36, 11, 34, 7, 32, 16, 30, 34, 5, 36, 11, 34, 30, 47, 32, 13, 34, 5, 47, 34, 5, 36, 11, 34, 7, 39, 32, 32, 5, 34, 7, 32, 16, 30, 34, 5, 47, 47, 34, 51, 20, 19, 32, 16, 54, 34, 35, 36, 47, 19, 34, 5, 36, 46, 5, 34, 46, 20, 5, 34, 29, 47, 39, 34, 5, 36, 32, 34, 39, 47, 20, 16, 48, 25, 7, 34, 30, 20, 32, 7, 36, 34, 47, 20, 29, 46, 31, 32, 29, 5, 13, 34, 4, 29, 48, 34, 47, 29, 16, 11, 34, 36, 32, 20, 46, 16, 48, 34, 5, 47, 34, 5, 36, 32, 34, 45, 46, 19, 48, 11, 34, 7, 12, 20, 33, 29, 45, 13, 34, 17, 33, 5, 36, 33, 29, 34, 5, 36, 33, 29, 32, 34, 47, 39, 29, 34, 38, 19, 48, 34, 38, 19, 20, 33, 32, 7, 5, 34, 5, 36, 11, 34, 51, 47, 29, 5, 32, 29, 5, 13, 34, 4, 29, 48, 13, 34, 5, 32, 29, 48, 32, 20, 34, 51, 36, 19, 20, 16, 13, 34, 31, 46, 40, 25, 7, 5, 34, 39, 46, 7, 5, 32, 34, 33, 29, 34, 29, 33, 45, 45, 46, 20, 48, 33, 29, 45, 54, 34, 34, 34, 41, 33, 5, 11, 34, 5, 36, 32, 34, 39, 47, 20, 16, 48, 13, 34, 47, 20, 34, 32, 16, 7, 32, 34, 5, 36, 33, 7, 34, 45, 16, 19, 5, 5, 47, 29, 34, 38, 32, 13, 34, 34, 34, 35, 47, 34, 32, 46, 5, 34, 5, 36, 32, 34, 39, 47, 20, 16, 48, 25, 7, 34, 48, 19, 32, 13, 34, 38, 11, 34, 5, 36, 32, 34, 45, 20, 46, 3, 32, 34, 46, 29, 48, 34, 5, 36, 32, 32, 22, 34, 17, 36, 32, 29, 34, 30, 47, 20, 5, 11, 34, 39, 33, 29, 5, 32, 20, 7, 34, 7, 36, 46, 16, 16, 34, 38, 32, 7, 33, 32, 45, 32, 34, 5, 36, 11, 34, 38, 20, 47, 39, 13, 34, 4, 29, 48, 34, 48, 33, 45, 34, 48, 32, 32, 12, 34, 5, 20, 32, 29, 51, 36, 32, 7, 34, 33, 29, 34, 5, 36, 11, 34, 38, 32, 46, 19, 5, 11, 25, 7, 34, 30, 33, 32, 16, 48, 13, 34, 35, 36, 11, 34, 11, 47, 19, 5, 36, 25, 7, 34, 12, 20, 47, 19, 48, 34, 16, 33, 3, 32, 20, 11, 34, 7, 47, 34, 45, 46, 52, 32, 48, 34, 47, 29, 34, 29, 47, 39, 13, 34, 17, 33, 16, 16, 34, 38, 32, 34, 46, 34, 5, 46, 5, 5, 32, 20, 32, 48, 34, 39, 32, 32, 48, 34, 47, 30, 34, 7, 31, 46, 16, 16, 34, 39, 47, 20, 5, 36, 34, 36, 32, 16, 48, 54, 34, 35, 36, 32, 29, 34, 38, 32, 33, 29, 45, 34, 46, 7, 40, 32, 48, 13, 34, 39, 36, 32, 20, 32, 34, 46, 16, 16, 34, 5, 36, 11, 34, 38, 32, 46, 19, 5, 11, 34, 16, 33, 32, 7, 13, 34, 17, 36, 32, 20, 32, 34, 46, 16, 16, 34, 5, 36, 32, 34, 5, 20, 32, 46, 7, 19, 20, 32, 34, 47, 30, 34, 5, 36, 11, 34, 16, 19, 7, 5, 11, 34, 48, 46, 11, 7, 8, 34, 35, 47, 34, 7, 46, 11, 13, 34, 39, 33, 5, 36, 33, 29, 34, 5, 36, 33, 29, 32, 34, 47, 39, 29, 34, 48, 32, 32, 12, 34, 7, 19, 29, 40, 32, 29, 34, 32, 11, 32, 7, 13, 34, 17, 32, 20, 32, 34, 46, 29, 34, 46, 16, 16, 28, 32, 46, 5, 33, 29, 45, 34, 7, 36, 46, 31, 32, 13, 34, 46, 29, 48, 34, 5, 36, 20, 33, 30, 5, 16, 32, 7, 7, 34, 12, 20, 46, 33, 7, 32, 22, 34, 21, 47, 39, 34, 31, 19, 51, 36, 34, 31, 47, 20, 32, 34, 12, 20, 46, 33, 7, 32, 34, 48, 32, 7, 32, 20, 3, 25, 48, 34, 5, 36, 11, 34, 38, 32, 46, 19, 5, 11, 25, 7, 34, 19, 7, 32, 13, 34, 23, 30, 34, 5, 36, 47, 19, 34, 51, 47, 19, 16, 48, 7, 5, 34, 46, 29, 7, 39, 32, 20, 34, 50, 35, 36, 33, 7, 34, 30, 46, 33, 20, 34, 51, 36, 33, 16, 48, 34, 47, 30, 34, 31, 33, 29, 32, 34, 37, 36, 46, 16, 16, 34, 7, 19, 31, 34, 31, 11, 34, 51, 47, 19, 29, 5, 13, 34, 46, 29, 48, 34, 31, 46, 40, 32, 34, 31, 11, 34, 47, 16, 48, 34, 32, 24, 51, 19, 7, 32, 13, 25, 34, 41, 20, 47, 3, 33, 29, 45, 34, 36, 33, 7, 34, 38, 32, 46, 19, 5, 11, 34, 38, 11, 34, 7, 19, 51, 51, 32, 7, 7, 33, 47, 29, 34, 5, 36, 33, 29, 32, 22, 34, 34, 34, 35, 36, 33, 7, 34, 39, 32, 20, 32, 34, 5, 47, 34, 38, 32, 34, 29, 32, 39, 34, 31, 46, 48, 32, 34, 39, 36, 32, 29, 34, 5, 36, 47, 19, 34, 46, 20, 5, 34, 47, 16, 48, 13, 34, 34, 34, 4, 29, 48, 34, 7, 32, 32, 34, 5, 36, 11, 34, 38, 16, 47, 47, 48, 34, 39, 46, 20, 31, 34, 39, 36, 32, 29, 34, 5, 36, 47, 19, 34, 30, 32, 32, 16, 25, 7, 5, 34, 33, 5, 34, 51, 47, 16, 48, 22, 34, 53, 47, 47, 40, 34, 33, 29, 34, 5, 36, 11, 34, 45, 16, 46, 7, 7, 34, 46, 29, 48, 34, 5, 32, 16, 16, 34, 5, 36, 32, 34, 30, 46, 51, 32, 34, 5, 36, 47, 19, 34, 3, 33, 32, 39, 32, 7, 5, 13, 34, 9, 47, 39, 34, 33, 7, 34, 5, 36, 32, 34, 5, 33, 31, 32, 34, 5, 36, 46, 5, 34, 30, 46, 51, 32, 34, 7, 36, 47, 19, 16, 48, 34, 30, 47, 20, 31, 34, 46, 29, 47, 5, 36, 32, 20, 13, 34, 17, 36, 47, 7, 32, 34, 30, 20, 32, 7, 36, 34, 20, 32, 12, 46, 33, 20, 34, 33, 30, 34, 29, 47, 39, 34, 5, 36, 47, 19, 34, 29, 47, 5, 34, 20, 32, 29, 32, 39, 32, 7, 5, 13, 34, 35, 36, 47, 19, 34, 48, 47, 7, 5, 34, 38, 32, 45, 19, 33, 16, 32, 34, 5, 36, 32, 34, 39, 47, 20, 16, 48, 13, 34, 19, 29, 38, 16, 32, 7, 7, 34, 7, 47, 31, 32, 34, 31, 47, 5, 36, 32, 20, 22, 34, 44, 47, 20, 34, 39, 36, 32, 20, 32, 34, 33, 7, 34, 7, 36, 32, 34, 7, 47, 34, 30, 46, 33, 20, 34, 39, 36, 47, 7, 32, 34, 19, 29, 32, 46, 20, 32, 48, 34, 39, 47, 31, 38, 34, 0, 33, 7, 48, 46, 33, 29, 7, 34, 5, 36, 32, 34, 5, 33, 16, 16, 46, 45, 32, 34, 47, 30, 34, 5, 36, 11, 34, 36, 19, 7, 38, 46, 29, 48, 20, 11, 6, 34, 26, 20, 34, 39, 36, 47, 34, 33, 7, 34, 36, 32, 34, 7, 47, 34, 30, 47, 29, 48, 34, 39, 33, 16, 16, 34, 38, 32, 34, 5, 36, 32, 34, 5, 47, 31, 38, 34, 26, 30, 34, 36, 33, 7, 34, 7, 32, 16, 30, 28, 16, 47, 3, 32, 34, 5, 47, 34, 7, 5, 47, 12, 34, 12, 47, 7, 5, 32, 20, 33, 5, 11, 6, 34, 35, 36, 47, 19, 34, 46, 20, 5, 34, 5, 36, 11, 34, 31, 47, 5, 36, 32, 20, 25, 7, 34, 45, 16, 46, 7, 7, 34, 46, 29, 48, 34, 7, 36, 32, 34, 33, 29, 34, 5, 36, 32, 32, 34, 27, 46, 16, 16, 7, 34, 38, 46, 51, 40, 34, 5, 36, 32, 34, 16, 47, 3, 32, 16, 11, 34, 4, 12, 20, 33, 16, 34, 47, 30, 34, 36, 32, 20, 34, 12, 20, 33, 31, 32, 13, 34, 37, 47, 34, 5, 36, 47, 19, 34, 5, 36, 20, 47, 19, 45, 36, 34, 39, 33, 29, 48, 47, 39, 7, 34, 47, 30, 34, 5, 36, 33, 29, 32, 34, 46, 45, 32, 34, 7, 36, 46, 16, 5, 34, 7, 32, 32, 13, 34, 0, 32, 7, 12, 33, 5, 32, 34, 47, 30, 34, 39, 20, 33, 29, 40, 16, 32, 7, 34, 5, 36, 33, 7, 34, 5, 36, 11, 34, 45, 47, 16, 48, 32, 29, 34, 5, 33, 31, 32, 22, 34, 34, 34, 14, 19, 5, 34, 33, 30, 34, 5, 36, 47, 19, 34, 16, 33, 3, 32, 34, 20, 32, 31, 32, 31, 38, 32, 20, 32, 48, 34, 29, 47, 5, 34, 5, 47, 34, 38, 32, 13, 34, 34, 34, 0, 33, 32, 34, 7, 33, 29, 45, 16, 32, 34, 46, 29, 48, 34, 5, 36, 33, 29, 32, 34, 33, 31, 46, 45, 32, 34, 48, 33, 32, 7, 34, 39, 33, 5, 36, 34, 5, 36, 32, 32, 22, 34, 42, 29, 5, 36, 20, 33, 30, 5, 11, 34, 16, 47, 3, 32, 16, 33, 29, 32, 7, 7, 34, 39, 36, 11, 34, 48, 47, 7, 5, 34, 5, 36, 47, 19, 34, 7, 12, 32, 29, 48, 13, 34, 42, 12, 47, 29, 34, 5, 36, 11, 34, 7, 32, 16, 30, 34, 5, 36, 11, 34, 38, 32, 46, 19, 5, 11, 25, 7, 34, 16, 32, 45, 46, 51, 11, 6, 34, 9, 46, 5, 19, 20, 32, 25, 7, 34, 38, 32, 1, 19, 32, 7, 5, 34, 45, 33, 3, 32, 7, 34, 29, 47, 5, 36, 33, 29, 45, 34, 38, 19, 5, 34, 48, 47, 5, 36, 34, 16, 32, 29, 48, 13, 34, 4, 29, 48, 34, 38, 32, 33, 29, 45, 34, 30, 20, 46, 29, 40, 34, 7, 36, 32, 34, 16, 32, 29, 48, 7, 34, 5, 47, 34, 5, 36, 47, 7, 32, 34, 46, 20, 32, 34, 30, 20, 32, 32, 54, 34, 35, 36, 32, 29, 34, 38, 32, 46, 19, 5, 32, 47, 19, 7, 34, 29, 33, 45, 45, 46, 20, 48, 34, 39, 36, 11, 34, 48, 47, 7, 5, 34, 5, 36, 47, 19, 34, 46, 38, 19, 7, 32, 13, 34, 35, 36, 32, 34, 38, 47, 19, 29, 5, 32, 47, 19, 7, 34, 16, 46, 20, 45, 32, 7, 7, 34, 45, 33, 3, 32, 29, 34, 5, 36, 32, 32, 34, 5, 47, 34, 45, 33, 3, 32, 6, 34, 41, 20, 47, 30, 33, 5, 16, 32, 7, 7, 34, 19, 7, 19, 20, 32, 20, 34, 39, 36, 11, 34, 48, 47, 7, 5, 34, 5, 36, 47, 19, 34, 19, 7, 32, 34, 37, 47, 34, 45, 20, 32, 46, 5, 34, 46, 34, 7, 19, 31, 34, 47, 30, 34, 7, 19, 31, 7, 34, 11, 32, 5, 34, 51, 46, 29, 7, 5, 34, 29, 47, 5, 34, 16, 33, 3, 32, 6, 34, 44, 47, 20, 34, 36, 46, 3, 33, 29, 45, 34, 5, 20, 46, 30, 30, 33, 51, 34, 39, 33, 5, 36, 34, 5, 36, 11, 34, 7, 32, 16, 30, 34, 46, 16, 47, 29, 32, 13, 34, 35, 36, 47, 19, 34, 47, 30, 34, 5, 36, 11, 34, 7, 32, 16, 30, 34, 5, 36, 11, 34, 7, 39, 32, 32, 5, 34, 7, 32, 16, 30, 34, 48, 47, 7, 5, 34, 48, 32, 51, 32, 33, 3, 32, 13, 34, 35, 36, 32, 29, 34, 36, 47, 39, 34, 39, 36, 32, 29, 34, 29, 46, 5, 19, 20, 32, 34, 51, 46, 16, 16, 7, 34, 5, 36, 32, 32, 34, 5, 47, 34, 38, 32, 34, 45, 47, 29, 32, 13, 34, 17, 36, 46, 5, 34, 46, 51, 51, 32, 12, 5, 46, 38, 16, 32, 34, 46, 19, 48, 33, 5, 34, 51, 46, 29, 7, 5, 34, 5, 36, 47, 19, 34, 16, 32, 46, 3, 32, 6, 34, 34, 34, 35, 36, 11, 34, 19, 29, 19, 7, 32, 48, 34, 38, 32, 46, 19, 5, 11, 34, 31, 19, 7, 5, 34, 38, 32, 34, 5, 47, 31, 38, 32, 48, 34, 39, 33, 5, 36, 34, 5, 36, 32, 32, 13, 34, 34, 34, 17, 36, 33, 51, 36, 34, 19, 7, 32, 48, 34, 16, 33, 3, 32, 7, 34, 5, 36, 25, 34, 32, 24, 32, 51, 19, 5, 47, 20, 34, 5, 47, 34, 38, 32, 22, 34, 35, 36, 47, 7, 32, 34, 36, 47, 19, 20, 7, 34, 5, 36, 46, 5, 34, 39, 33, 5, 36, 34, 45, 32, 29, 5, 16, 32, 34, 39, 47, 20, 40, 34, 48, 33, 48, 34, 30, 20, 46, 31, 32, 34, 35, 36, 32, 34, 16, 47, 3, 32, 16, 11, 34, 45, 46, 52, 32, 34, 39, 36, 32, 20, 32, 34, 32, 3, 32, 20, 11, 34, 32, 11, 32, 34, 48, 47, 5, 36, 34, 48, 39, 32, 16, 16, 34, 17, 33, 16, 16, 34, 12, 16, 46, 11, 34, 5, 36, 32, 34, 5, 11, 20, 46, 29, 5, 7, 34, 5, 47, 34, 5, 36, 32, 34, 3, 32, 20, 11, 34, 7, 46, 31, 32, 13, 34, 4, 29, 48, 34, 5, 36, 46, 5, 34, 19, 29, 30, 46, 33, 20, 34, 39, 36, 33, 51, 36, 34, 30, 46, 33, 20, 16, 11, 34, 48, 47, 5, 36, 34, 32, 24, 51, 32, 16, 54, 34, 44, 47, 20, 34, 29, 32, 3, 32, 20, 28, 20, 32, 7, 5, 33, 29, 45, 34, 5, 33, 31, 32, 34, 16, 32, 46, 48, 7, 34, 7, 19, 31, 31, 32, 20, 34, 47, 29, 34, 35, 47, 34, 36, 33, 48, 32, 47, 19, 7, 34, 39, 33, 29, 5, 32, 20, 34, 46, 29, 48, 34, 51, 47, 29, 30, 47, 19, 29, 48, 7, 34, 36, 33, 31, 34, 5, 36, 32, 20, 32, 13, 34, 37, 46, 12, 34, 51, 36, 32, 51, 40, 32, 48, 34, 39, 33, 5, 36, 34, 30, 20, 47, 7, 5, 34, 46, 29, 48, 34, 16, 19, 7, 5, 11, 34, 16, 32, 46, 3, 32, 7, 34, 1, 19, 33, 5, 32, 34, 45, 47, 29, 32, 13, 34, 14, 32, 46, 19, 5, 11, 34, 47, 25, 32, 20, 28, 7, 29, 47, 39, 32, 48, 34, 46, 29, 48, 34, 38, 46, 20, 32, 29, 32, 7, 7, 34, 32, 3, 32, 20, 11, 34, 39, 36, 32, 20, 32, 54, 34, 35, 36, 32, 29, 34, 39, 32, 20, 32, 34, 29, 47, 5, 34, 7, 19, 31, 31, 32, 20, 25, 7, 34, 48, 33, 7, 5, 33, 16, 16, 46, 5, 33, 47, 29, 34, 16, 32, 30, 5, 34, 4, 34, 16, 33, 1, 19, 33, 48, 34, 12, 20, 33, 7, 47, 29, 32, 20, 34, 12, 32, 29, 5, 34, 33, 29, 34, 39, 46, 16, 16, 7, 34, 47, 30, 34, 45, 16, 46, 7, 7, 13, 34, 14, 32, 46, 19, 5, 11, 25, 7, 34, 32, 30, 30, 32, 51, 5, 34, 39, 33, 5, 36, 34, 38, 32, 46, 19, 5, 11, 34, 39, 32, 20, 32, 34, 38, 32, 20, 32, 30, 5, 13, 34, 9, 47, 20, 34, 33, 5, 34, 29, 47, 20, 34, 29, 47, 34, 20, 32, 31, 32, 31, 38, 20, 46, 29, 51, 32, 34, 39, 36, 46, 5, 34, 33, 5, 34, 39, 46, 7, 22, 34, 34, 34, 14, 19, 5, 34, 30, 16, 47, 39, 32, 20, 7, 34, 48, 33, 7, 5, 33, 16, 16, 32, 48, 34, 5, 36, 47, 19, 45, 36, 34, 5, 36, 32, 11, 34, 39, 33, 5, 36, 34, 39, 33, 29, 5, 32, 20, 34, 31, 32, 32, 5, 13, 34, 34, 34, 53, 32, 32, 7, 32, 34, 38, 19, 5, 34, 5, 36, 32, 33, 20, 34, 7, 36, 47, 39, 13, 34, 5, 36, 32, 33, 20, 34, 7, 19, 38, 7, 5, 46, 29, 51, 32, 34, 7, 5, 33, 16, 16, 34, 16, 33, 3, 32, 7, 34, 7, 39, 32, 32, 5, 22, 34, 35, 36, 32, 29, 34, 16, 32, 5, 34, 29, 47, 5, 34, 39, 33, 29, 5, 32, 20, 25, 7, 34, 20, 46, 45, 45, 32, 48, 34, 36, 46, 29, 48, 34, 48, 32, 30, 46, 51, 32, 13, 34, 23, 29, 34, 5, 36, 32, 32, 34, 5, 36, 11, 34, 7, 19, 31, 31, 32, 20, 34, 32, 20, 32, 34, 5, 36, 47, 19, 34, 38, 32, 34, 48, 33, 7, 5, 33, 16, 16, 32, 48, 54, 34, 49, 46, 40, 32, 34, 7, 39, 32, 32, 5, 34, 7, 47, 31, 32, 34, 3, 33, 46, 16, 8, 34, 5, 20, 32, 46, 7, 19, 20, 32, 34, 5, 36, 47, 19, 34, 7, 47, 31, 32, 34, 12, 16, 46, 51, 32, 13, 34, 17, 33, 5, 36, 34, 38, 32, 46, 19, 5, 11, 25, 7, 34, 5, 20, 32, 46, 7, 19, 20, 32, 34, 32, 20, 32, 34, 33, 5, 34, 38, 32, 34, 7, 32, 16, 30, 28, 40, 33, 16, 16, 32, 48, 54, 34, 35, 36, 46, 5, 34, 19, 7, 32, 34, 33, 7, 34, 29, 47, 5, 34, 30, 47, 20, 38, 33, 48, 48, 32, 29, 34, 19, 7, 19, 20, 11, 13, 34, 17, 36, 33, 51, 36, 34, 36, 46, 12, 12, 33, 32, 7, 34, 5, 36, 47, 7, 32, 34, 5, 36, 46, 5, 34, 12, 46, 11, 34, 5, 36, 32, 34, 39, 33, 16, 16, 33, 29, 45, 34, 16, 47, 46, 29, 8, 34, 35, 36, 46, 5, 25, 7, 34, 30, 47, 20, 34, 5, 36, 11, 34, 7, 32, 16, 30, 34, 5, 47, 34, 38, 20, 32, 32, 48, 34, 46, 29, 47, 5, 36, 32, 20, 34, 5, 36, 32, 32, 13, 34, 26, 20, 34, 5, 32, 29, 34, 5, 33, 31, 32, 7, 34, 36, 46, 12, 12, 33, 32, 20, 34, 38, 32, 34, 33, 5, 34, 5, 32, 29, 34, 30, 47, 20, 34, 47, 29, 32, 13, 34, 35, 32, 29, 34, 5, 33, 31, 32, 7, 34, 5, 36, 11, 34, 7, 32, 16, 30, 34, 39, 32, 20, 32, 34, 36, 46, 12, 12, 33, 32, 20, 34, 5, 36, 46, 29, 34, 5, 36, 47, 19, 34, 46, 20, 5, 13, 34, 23, 30, 34, 5, 32, 29, 34, 47, 30, 34, 5, 36, 33, 29, 32, 34, 5, 32, 29, 34, 5, 33, 31, 32, 7, 34, 20, 32, 30, 33, 45, 19, 20, 32, 48, 34, 5, 36, 32, 32, 54, 34, 35, 36, 32, 29, 34, 39, 36, 46, 5, 34, 51, 47, 19, 16, 48, 34, 48, 32, 46, 5, 36, 34, 48, 47, 34, 33, 30, 34, 5, 36, 47, 19, 34, 7, 36, 47, 19, 16, 48, 7, 5, 34, 48, 32, 12, 46, 20, 5, 13, 34, 53, 32, 46, 3, 33, 29, 45, 34, 5, 36, 32, 32, 34, 16, 33, 3, 33, 29, 45, 34, 33, 29, 34, 12, 47, 7, 5, 32, 20, 33, 5, 11, 6, 34, 34, 34, 14, 32, 34, 29, 47, 5, 34, 7, 32, 16, 30, 28, 39, 33, 16, 16, 32, 48, 34, 30, 47, 20, 34, 5, 36, 47, 19, 34, 46, 20, 5, 34, 31, 19, 51, 36, 34, 5, 47, 47, 34, 30, 46, 33, 20, 13, 34, 34, 34, 35, 47, 34, 38, 32, 34, 48, 32, 46, 5, 36, 25, 7, 34, 51, 47, 29, 1, 19, 32, 7, 5, 34, 46, 29, 48, 34, 31, 46, 40, 32, 34, 39, 47, 20, 31, 7, 34, 5, 36, 33, 29, 32, 34, 36, 32, 33, 20, 22, 34, 53, 47, 34, 33, 29, 34, 5, 36, 32, 34, 47, 20, 33, 32, 29, 5, 34, 39, 36, 32, 29, 34, 5, 36, 32, 34, 45, 20, 46, 51, 33, 47, 19, 7, 34, 16, 33, 45, 36, 5, 34, 53, 33, 30, 5, 7, 34, 19, 12, 34, 36, 33, 7, 34, 38, 19, 20, 29, 33, 29, 45, 34, 36, 32, 46, 48, 13, 34, 32, 46, 51, 36, 34, 19, 29, 48, 32, 20, 34, 32, 11, 32, 34, 0, 47, 5, 36, 34, 36, 47, 31, 46, 45, 32, 34, 5, 47, 34, 36, 33, 7, 34, 29, 32, 39, 28, 46, 12, 12, 32, 46, 20, 33, 29, 45, 34, 7, 33, 45, 36, 5, 13, 34, 37, 32, 20, 3, 33, 29, 45, 34, 39, 33, 5, 36, 34, 16, 47, 47, 40, 7, 34, 36, 33, 7, 34, 7, 46, 51, 20, 32, 48, 34, 31, 46, 55, 32, 7, 5, 11, 13, 34, 4, 29, 48, 34, 36, 46, 3, 33, 29, 45, 34, 51, 16, 33, 31, 38, 32, 48, 34, 5, 36, 32, 34, 7, 5, 32, 32, 12, 28, 19, 12, 34, 36, 32, 46, 3, 32, 29, 16, 11, 34, 36, 33, 16, 16, 13, 34, 10, 32, 7, 32, 31, 38, 16, 33, 29, 45, 34, 7, 5, 20, 47, 29, 45, 34, 11, 47, 19, 5, 36, 34, 33, 29, 34, 36, 33, 7, 34, 31, 33, 48, 48, 16, 32, 34, 46, 45, 32, 13, 34, 43, 32, 5, 34, 31, 47, 20, 5, 46, 16, 34, 16, 47, 47, 40, 7, 34, 46, 48, 47, 20, 32, 34, 36, 33, 7, 34, 38, 32, 46, 19, 5, 11, 34, 7, 5, 33, 16, 16, 13, 34, 4, 5, 5, 32, 29, 48, 33, 29, 45, 34, 47, 29, 34, 36, 33, 7, 34, 45, 47, 16, 48, 32, 29, 34, 12, 33, 16, 45, 20, 33, 31, 46, 45, 32, 54, 34, 14, 19, 5, 34, 39, 36, 32, 29, 34, 30, 20, 47, 31, 34, 36, 33, 45, 36, 31, 47, 7, 5, 34, 12, 33, 5, 51, 36, 34, 39, 33, 5, 36, 34, 39, 32, 46, 20, 11, 34, 51, 46, 20, 13, 34, 53, 33, 40, 32, 34, 30, 32, 32, 38, 16, 32, 34, 46, 45, 32, 34, 36, 32, 34, 20, 32, 32, 16, 32, 5, 36, 34, 30, 20, 47, 31, 34, 5, 36, 32, 34, 48, 46, 11, 13, 34, 35, 36, 32, 34, 32, 11, 32, 7, 34, 2, 30, 47, 20, 32, 34, 48, 19, 5, 32, 47, 19, 7, 15, 34, 29, 47, 39, 34, 51, 47, 29, 3, 32, 20, 5, 32, 48, 34, 46, 20, 32, 34, 44, 20, 47, 31, 34, 36, 33, 7, 34, 16, 47, 39, 34, 5, 20, 46, 51, 5, 34, 46, 29, 48, 34, 16, 47, 47, 40, 34, 46, 29, 47, 5, 36, 32, 20, 34, 39, 46, 11, 54, 34, 34, 34, 37, 47, 34, 5, 36, 47, 19, 13, 34, 5, 36, 11, 34, 7, 32, 16, 30, 34, 47, 19, 5, 28, 45, 47, 33, 29, 45, 34, 33, 29, 34, 5, 36, 11, 34, 29, 47, 47, 29, 54, 34, 34, 34, 42, 29, 16, 47, 47, 40, 32, 48, 34, 47, 29, 34, 48, 33, 32, 7, 5, 34, 19, 29, 16, 32, 7, 7, 34, 5, 36, 47, 19, 34, 45, 32, 5, 34, 46, 34, 7, 47, 29, 22, 34, 49, 19, 7, 33, 51, 34, 5, 47, 34, 36, 32, 46, 20, 13, 34, 39, 36, 11, 34, 36, 32, 46, 20, 25, 7, 5, 34, 5, 36, 47, 19, 34, 31, 19, 7, 33, 51, 34, 7, 46, 48, 16, 11, 6, 34, 37, 39, 32, 32, 5, 7, 34, 39, 33, 5, 36, 34, 7, 39, 32, 32, 5, 7, 34, 39, 46, 20, 34, 29, 47, 5, 13, 34, 55, 47, 11, 34, 48, 32, 16, 33, 45, 36, 5, 7, 34, 33, 29, 34, 55, 47, 11, 54, 34, 17, 36, 11, 34, 16, 47, 3, 25, 7, 5, 34, 5, 36, 47, 19, 34, 5, 36, 46, 5, 34, 39, 36, 33, 51, 36, 34, 5, 36, 47, 19, 34, 20, 32, 51, 32, 33, 3, 25, 7, 5, 34, 29, 47, 5, 34, 45, 16, 46, 48, 16, 11, 13, 34, 26, 20, 34, 32, 16, 7, 32, 34, 20, 32, 51, 32, 33, 3, 25, 7, 5, 34, 39, 33, 5, 36, 34, 12, 16, 32, 46, 7, 19, 20, 32, 34, 5, 36, 33, 29, 32, 34, 46, 29, 29, 47, 11, 6, 34, 23, 30, 34, 5, 36, 32, 34, 5, 20, 19, 32, 34, 51, 47, 29, 51, 47, 20, 48, 34, 47, 30, 34, 39, 32, 16, 16, 28, 5, 19, 29, 32, 48, 34, 7, 47, 19, 29, 48, 7, 13, 34, 14, 11, 34, 19, 29, 33, 47, 29, 7, 34, 31, 46, 20, 20, 33, 32, 48, 34, 48, 47, 34, 47, 30, 30, 32, 29, 48, 34, 5, 36, 33, 29, 32, 34, 32, 46, 20, 13, 34, 35, 36, 32, 11, 34, 48, 47, 34, 38, 19, 5, 34, 7, 39, 32, 32, 5, 16, 11, 34, 51, 36, 33, 48, 32, 34, 5, 36, 32, 32, 13, 34, 39, 36, 47, 34, 51, 47, 29, 30, 47, 19, 29, 48, 7, 34, 23, 29, 34, 7, 33, 29, 45, 16, 32, 29, 32, 7, 7, 34, 5, 36, 32, 34, 12, 46, 20, 5, 7, 34, 5, 36, 46, 5, 34, 5, 36, 47, 19, 34, 7, 36, 47, 19, 16, 48, 7, 5, 34, 38, 32, 46, 20, 54, 34, 49, 46, 20, 40, 34, 36, 47, 39, 34, 47, 29, 32, 34, 7, 5, 20, 33, 29, 45, 34, 7, 39, 32, 32, 5, 34, 36, 19, 7, 38, 46, 29, 48, 34, 5, 47, 34, 46, 29, 47, 5, 36, 32, 20, 13, 34, 37, 5, 20, 33, 40, 32, 7, 34, 32, 46, 51, 36, 34, 33, 29, 34, 32, 46, 51, 36, 34, 38, 11, 34, 31, 19, 5, 19, 46, 16, 34, 47, 20, 48, 32, 20, 33, 29, 45, 8, 34, 10, 32, 7, 32, 31, 38, 16, 33, 29, 45, 34, 7, 33, 20, 32, 13, 34, 46, 29, 48, 34, 51, 36, 33, 16, 48, 13, 34, 46, 29, 48, 34, 36, 46, 12, 12, 11, 34, 31, 47, 5, 36, 32, 20, 13, 34, 17, 36, 47, 34, 46, 16, 16, 34, 33, 29, 34, 47, 29, 32, 13, 34, 47, 29, 32, 34, 12, 16, 32, 46, 7, 33, 29, 45, 34, 29, 47, 5, 32, 34, 48, 47, 34, 7, 33, 29, 45, 54, 34, 34, 34, 17, 36, 47, 7, 32, 34, 7, 12, 32, 32, 51, 36, 16, 32, 7, 7, 34, 7, 47, 29, 45, 34, 38, 32, 33, 29, 45, 34, 31, 46, 29, 11, 13, 34, 7, 32, 32, 31, 33, 29, 45, 34, 47, 29, 32, 13, 34, 34, 34, 37, 33, 29, 45, 7, 34, 5, 36, 33, 7, 34, 5, 47, 34, 5, 36, 32, 32, 13, 34, 50, 35, 36, 47, 19, 34, 7, 33, 29, 45, 16, 32, 34, 39, 33, 16, 5, 34, 12, 20, 47, 3, 32, 34, 29, 47, 29, 32, 25, 22, 34, 23, 7, 34, 33, 5, 34, 30, 47, 20, 34, 30, 32, 46, 20, 34, 5, 47, 34, 39, 32, 5, 34, 46, 34, 39, 33, 48, 47, 39, 25, 7, 34, 32, 11, 32, 13, 34, 35, 36, 46, 5, 34, 5, 36, 47, 19, 34, 51, 47, 29, 7, 19, 31, 25, 7, 5, 34, 5, 36, 11, 34, 7, 32, 16, 30, 34, 33, 29, 34, 7, 33, 29, 45, 16, 32, 34, 16, 33, 30, 32, 6, 34, 4, 36, 13, 34, 33, 30, 34, 5, 36, 47, 19, 34, 33, 7, 7, 19, 32, 16, 32, 7, 7, 34, 7, 36, 46, 16, 5, 34, 36, 46, 12, 34, 5, 47, 34, 48, 33, 32, 13, 34, 35, 36, 32, 34, 39, 47, 20, 16, 48, 34, 39, 33, 16, 16, 34, 39, 46, 33, 16, 34, 5, 36, 32, 32, 34, 16, 33, 40, 32, 34, 46, 34, 31, 46, 40, 32, 16, 32, 7, 7, 34, 39, 33, 30, 32, 13, 34, 35, 36, 32, 34, 39, 47, 20, 16, 48, 34, 39, 33, 16, 16, 34, 38, 32, 34, 5, 36, 11, 34, 39, 33, 48, 47, 39, 34, 46, 29, 48, 34, 7, 5, 33, 16, 16, 34, 39, 32, 32, 12, 13, 34, 35, 36, 46, 5, 34, 5, 36, 47, 19, 34, 29, 47, 34, 30, 47, 20, 31, 34, 47, 30, 34, 5, 36, 32, 32, 34, 36, 46, 7, 5, 34, 16, 32, 30, 5, 34, 38, 32, 36, 33, 29, 48, 13, 34, 17, 36, 32, 29, 34, 32, 3, 32, 20, 11, 34, 12, 20, 33, 3, 46, 5, 32, 34, 39, 33, 48, 47, 39, 34, 39, 32, 16, 16, 34, 31, 46, 11, 34, 40, 32, 32, 12, 13, 34, 14, 11, 34, 51, 36, 33, 16, 48, 20, 32, 29, 25, 7, 34, 32, 11, 32, 7, 13, 34, 36, 32, 20, 34, 36, 19, 7, 38, 46, 29, 48, 25, 7, 34, 7, 36, 46, 12, 32, 34, 33, 29, 34, 31, 33, 29, 48, 54, 34, 53, 47, 47, 40, 34, 39, 36, 46, 5, 34, 46, 29, 34, 19, 29, 5, 36, 20, 33, 30, 5, 34, 33, 29, 34, 5, 36, 32, 34, 39, 47, 20, 16, 48, 34, 48, 47, 5, 36, 34, 7, 12, 32, 29, 48, 34, 37, 36, 33, 30, 5, 7, 34, 38, 19, 5, 34, 36, 33, 7, 34, 12, 16, 46, 51, 32, 13, 34, 30, 47, 20, 34, 7, 5, 33, 16, 16, 34, 5, 36, 32, 34, 39, 47, 20, 16, 48, 34, 32, 29, 55, 47, 11, 7, 34, 33, 5, 8, 34, 14, 19, 5, 34, 38, 32, 46, 19, 5, 11, 25, 7, 34, 39, 46, 7, 5, 32, 34, 36, 46, 5, 36, 34, 33, 29, 34, 5, 36, 32, 34, 39, 47, 20, 16, 48, 34, 46, 29, 34, 32, 29, 48, 13, 34, 4, 29, 48, 34, 40, 32, 12, 5, 34, 19, 29, 19, 7, 32, 48, 34, 5, 36, 32, 34, 19, 7, 32, 20, 34, 7, 47, 34, 48, 32, 7, 5, 20, 47, 11, 7, 34, 33, 5, 54, 34, 34, 34, 9, 47, 34, 16, 47, 3, 32, 34, 5, 47, 39, 46, 20, 48, 34, 47, 5, 36, 32, 20, 7, 34, 33, 29, 34, 5, 36, 46, 5, 34, 38, 47, 7, 47, 31, 34, 7, 33, 5, 7, 34, 34, 34, 35, 36, 46, 5, 34, 47, 29, 34, 36, 33, 31, 7, 32, 16, 30, 34, 7, 19, 51, 36, 34, 31, 19, 20, 48, 25, 20, 47, 19, 7, 34, 7, 36, 46, 31, 32, 34, 51, 47, 31, 31, 33, 5, 7, 22, 34, 44, 47, 20, 34, 7, 36, 46, 31, 32, 34, 48, 32, 29, 11, 34, 5, 36, 46, 5, 34, 5, 36, 47, 19, 34, 38, 32, 46, 20, 25, 7, 5, 34, 16, 47, 3, 32, 34, 5, 47, 34, 46, 29, 11, 34, 17, 36, 47, 34, 30, 47, 20, 34, 5, 36, 11, 34, 7, 32, 16, 30, 34, 46, 20, 5, 34, 7, 47, 34, 19, 29, 12, 20, 47, 3, 33, 48, 32, 29, 5, 22, 34, 18, 20, 46, 29, 5, 34, 33, 30, 34, 5, 36, 47, 19, 34, 39, 33, 16, 5, 13, 34, 5, 36, 47, 19, 34, 46, 20, 5, 34, 38, 32, 16, 47, 3, 32, 48, 34, 47, 30, 34, 31, 46, 29, 11, 13, 34, 14, 19, 5, 34, 5, 36, 46, 5, 34, 5, 36, 47, 19, 34, 29, 47, 29, 32, 34, 16, 47, 3, 25, 7, 5, 34, 33, 7, 34, 31, 47, 7, 5, 34, 32, 3, 33, 48, 32, 29, 5, 54, 34, 44, 47, 20, 34, 5, 36, 47, 19, 34, 46, 20, 5, 34, 7, 47, 34, 12, 47, 7, 7, 32, 7, 7, 32, 48, 34, 39, 33, 5, 36, 34, 31, 19, 20, 48, 25, 20, 47, 19, 7, 34, 36, 46, 5, 32, 13, 34, 35, 36, 46, 5, 34, 25, 45, 46, 33, 29, 7, 5, 34, 5, 36, 11, 34, 7, 32, 16, 30, 34, 5, 36, 47, 19, 34, 7, 5, 33, 51, 40, 25, 7, 5, 34, 29, 47, 5, 34, 5, 47, 34, 51, 47, 29, 7, 12, 33, 20, 32, 13, 34, 37, 32, 32, 40, 33, 29, 45, 34, 5, 36, 46, 5, 34, 38, 32, 46, 19, 5, 32, 47, 19, 7, 34, 20, 47, 47, 30, 34, 5, 47, 34, 20, 19, 33, 29, 46, 5, 32, 34, 17, 36, 33, 51, 36, 34, 5, 47, 34, 20, 32, 12, 46, 33, 20, 34, 7, 36, 47, 19, 16, 48, 34, 38, 32, 34, 5, 36, 11, 34, 51, 36, 33, 32, 30, 34, 48, 32, 7, 33, 20, 32, 54, 34, 26, 34, 51, 36, 46, 29, 45, 32, 34, 5, 36, 11, 34, 5, 36, 47, 19, 45, 36, 5, 13, 34, 5, 36, 46, 5, 34, 23, 34, 31, 46, 11, 34, 51, 36, 46, 29, 45, 32, 34, 31, 11, 34, 31, 33, 29, 48, 13, 34, 37, 36, 46, 16, 16, 34, 36, 46, 5, 32, 34, 38, 32, 34, 30, 46, 33, 20, 32, 20, 34, 16, 47, 48, 45, 32, 48, 34, 5, 36, 46, 29, 34, 45, 32, 29, 5, 16, 32, 34, 16, 47, 3, 32, 6, 34, 14, 32, 34, 46, 7, 34, 5, 36, 11, 34, 12, 20, 32, 7, 32, 29, 51, 32, 34, 33, 7, 34, 45, 20, 46, 51, 33, 47, 19, 7, 34, 46, 29, 48, 34, 40, 33, 29, 48, 13, 34, 26, 20, 34, 5, 47, 34, 5, 36, 11, 34, 7, 32, 16, 30, 34, 46, 5, 34, 16, 32, 46, 7, 5, 34, 40, 33, 29, 48, 28, 36, 32, 46, 20, 5, 32, 48, 34, 12, 20, 47, 3, 32, 13, 34, 34, 34, 49, 46, 40, 32, 34, 5, 36, 32, 32, 34, 46, 29, 47, 5, 36, 32, 20, 34, 7, 32, 16, 30, 34, 30, 47, 20, 34, 16, 47, 3, 32, 34, 47, 30, 34, 31, 32, 13, 34, 34, 34, 35, 36, 46, 5, 34, 38, 32, 46, 19, 5, 11, 34, 7, 5, 33, 16, 16, 34, 31, 46, 11, 34, 16, 33, 3, 32, 34, 33, 29, 34, 5, 36, 33, 29, 32, 34, 47, 20, 34, 5, 36, 32, 32, 22, 34]\n",
            "----------------------------------------------------\n",
            "data length :  6161\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "S2axGeIy9vKW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "\n",
        "iteration = 500\n",
        "sequence_length = 40\n",
        "batch_size = round((txt_data_size /sequence_length)+0.5) # = math.ceil\n",
        "hidden_size = 500  # size of hidden layer of neurons.  \n",
        "learning_rate = 1e-1\n",
        "\n",
        "\n",
        "# model parameters\n",
        "\n",
        "W_xh = np.random.randn(hidden_size, num_chars)*0.01     # weight input -> hidden. \n",
        "W_hh = np.random.randn(hidden_size, hidden_size)*0.01   # weight hidden -> hidden\n",
        "W_hy = np.random.randn(num_chars, hidden_size)*0.01     # weight hidden -> output\n",
        "\n",
        "b_h = np.zeros((hidden_size, 1)) # hidden bias\n",
        "b_y = np.zeros((num_chars, 1)) # output bias\n",
        "\n",
        "h_prev = np.zeros((hidden_size,1)) # h_(t-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kSHNd8RM94po",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8840
        },
        "outputId": "68a65ffb-b702-49f7-a299-15765d973d81"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "data_pointer = 0\n",
        "\n",
        "# memory variables for Adagrad\n",
        "mWxh, mWhh, mWhy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy)\n",
        "mbh, mby = np.zeros_like(b_h), np.zeros_like(b_y) \n",
        "\n",
        "for i in range(iteration):\n",
        "    h_prev = np.zeros((hidden_size,1)) # reset RNN memory\n",
        "    data_pointer = 0 # go from start of data\n",
        "    \n",
        "    for b in range(batch_size):\n",
        "        \n",
        "        inputs = [char_to_int[ch]\n",
        "                  for ch in shakespeare_text[data_pointer:data_pointer+sequence_length]]\n",
        "        targets = [char_to_int[ch]\n",
        "                   for ch in shakespeare_text[data_pointer+1:data_pointer+sequence_length+1]] # t+1        \n",
        "            \n",
        "        if (data_pointer+sequence_length+1 >= len(shakespeare_text) and b == batch_size-1): # processing of the last part of the input data. \n",
        "#             targets.append(char_to_int[txt_data[0]])   # When the data doesn't fit, add the first char to the back.\n",
        "            targets.append(char_to_int[\" \"])   # When the data doesn't fit, add space(\" \") to the back.\n",
        "\n",
        "\n",
        "        # forward\n",
        "        loss, ps, hs, xs = forwardprop(inputs, targets, h_prev)\n",
        "#         print(loss)\n",
        "    \n",
        "        # backward\n",
        "        dWxh, dWhh, dWhy, dbh, dby = backprop(ps, inputs, hs, xs) \n",
        "        \n",
        "        \n",
        "    # perform parameter update with Adagrad\n",
        "        for param, dparam, mem in zip([W_xh, W_hh, W_hy, b_h, b_y], \n",
        "                                    [dWxh, dWhh, dWhy, dbh, dby], \n",
        "                                    [mWxh, mWhh, mWhy, mbh, mby]):\n",
        "            mem += dparam * dparam # elementwise\n",
        "            param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update      \n",
        "    \n",
        "        data_pointer += sequence_length # move data pointer\n",
        "        \n",
        "    #if i % 100 == 0:\n",
        "    print ('iter %d, loss: %f' % (i, loss)) # print progress"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter 0, loss: 3.392759\n",
            "iter 1, loss: 2.696402\n",
            "iter 2, loss: 2.534578\n",
            "iter 3, loss: 2.807044\n",
            "iter 4, loss: 2.926086\n",
            "iter 5, loss: 3.050924\n",
            "iter 6, loss: 3.171336\n",
            "iter 7, loss: 3.276591\n",
            "iter 8, loss: 3.366862\n",
            "iter 9, loss: 3.440637\n",
            "iter 10, loss: 3.486936\n",
            "iter 11, loss: 3.560304\n",
            "iter 12, loss: 3.563628\n",
            "iter 13, loss: 3.595761\n",
            "iter 14, loss: 3.573016\n",
            "iter 15, loss: 3.622879\n",
            "iter 16, loss: 3.561699\n",
            "iter 17, loss: 3.524285\n",
            "iter 18, loss: 3.557974\n",
            "iter 19, loss: 3.614070\n",
            "iter 20, loss: 3.634006\n",
            "iter 21, loss: 3.649019\n",
            "iter 22, loss: 3.685390\n",
            "iter 23, loss: 3.764106\n",
            "iter 24, loss: 3.806412\n",
            "iter 25, loss: 3.786883\n",
            "iter 26, loss: 3.789803\n",
            "iter 27, loss: 3.826076\n",
            "iter 28, loss: 3.874873\n",
            "iter 29, loss: 3.861653\n",
            "iter 30, loss: 3.868629\n",
            "iter 31, loss: 3.904882\n",
            "iter 32, loss: 3.908770\n",
            "iter 33, loss: 3.914710\n",
            "iter 34, loss: 3.909472\n",
            "iter 35, loss: 3.946984\n",
            "iter 36, loss: 3.970450\n",
            "iter 37, loss: 3.998527\n",
            "iter 38, loss: 3.973970\n",
            "iter 39, loss: 4.065316\n",
            "iter 40, loss: 4.075249\n",
            "iter 41, loss: 4.048846\n",
            "iter 42, loss: 4.068947\n",
            "iter 43, loss: 4.089200\n",
            "iter 44, loss: 4.068657\n",
            "iter 45, loss: 4.110803\n",
            "iter 46, loss: 4.113355\n",
            "iter 47, loss: 4.129447\n",
            "iter 48, loss: 4.136521\n",
            "iter 49, loss: 4.143995\n",
            "iter 50, loss: 4.164429\n",
            "iter 51, loss: 4.220876\n",
            "iter 52, loss: 4.218599\n",
            "iter 53, loss: 4.281710\n",
            "iter 54, loss: 4.374473\n",
            "iter 55, loss: 4.391740\n",
            "iter 56, loss: 4.405845\n",
            "iter 57, loss: 4.453948\n",
            "iter 58, loss: 4.469997\n",
            "iter 59, loss: 4.521729\n",
            "iter 60, loss: 4.597756\n",
            "iter 61, loss: 4.655587\n",
            "iter 62, loss: 4.722305\n",
            "iter 63, loss: 4.795843\n",
            "iter 64, loss: 4.815935\n",
            "iter 65, loss: 4.820377\n",
            "iter 66, loss: 4.822393\n",
            "iter 67, loss: 4.801046\n",
            "iter 68, loss: 4.773761\n",
            "iter 69, loss: 4.798646\n",
            "iter 70, loss: 4.809010\n",
            "iter 71, loss: 4.823131\n",
            "iter 72, loss: 4.791229\n",
            "iter 73, loss: 4.752741\n",
            "iter 74, loss: 4.800652\n",
            "iter 75, loss: 4.788585\n",
            "iter 76, loss: 4.752912\n",
            "iter 77, loss: 4.795886\n",
            "iter 78, loss: 4.789022\n",
            "iter 79, loss: 4.797030\n",
            "iter 80, loss: 4.801643\n",
            "iter 81, loss: 4.822716\n",
            "iter 82, loss: 4.750564\n",
            "iter 83, loss: 4.734375\n",
            "iter 84, loss: 4.717278\n",
            "iter 85, loss: 4.698482\n",
            "iter 86, loss: 4.698381\n",
            "iter 87, loss: 4.669614\n",
            "iter 88, loss: 4.640896\n",
            "iter 89, loss: 4.584247\n",
            "iter 90, loss: 4.591542\n",
            "iter 91, loss: 4.614093\n",
            "iter 92, loss: 4.613275\n",
            "iter 93, loss: 4.595712\n",
            "iter 94, loss: 4.546627\n",
            "iter 95, loss: 4.508481\n",
            "iter 96, loss: 4.540191\n",
            "iter 97, loss: 4.491205\n",
            "iter 98, loss: 4.522728\n",
            "iter 99, loss: 4.509717\n",
            "iter 100, loss: 4.527858\n",
            "iter 101, loss: 4.468236\n",
            "iter 102, loss: 4.452943\n",
            "iter 103, loss: 4.418089\n",
            "iter 104, loss: 4.324746\n",
            "iter 105, loss: 4.451913\n",
            "iter 106, loss: 4.388216\n",
            "iter 107, loss: 4.386312\n",
            "iter 108, loss: 4.295250\n",
            "iter 109, loss: 4.326535\n",
            "iter 110, loss: 4.374655\n",
            "iter 111, loss: 4.376137\n",
            "iter 112, loss: 4.323673\n",
            "iter 113, loss: 4.291083\n",
            "iter 114, loss: 4.311477\n",
            "iter 115, loss: 4.286143\n",
            "iter 116, loss: 4.286426\n",
            "iter 117, loss: 4.239825\n",
            "iter 118, loss: 4.280569\n",
            "iter 119, loss: 4.250319\n",
            "iter 120, loss: 4.178826\n",
            "iter 121, loss: 4.168707\n",
            "iter 122, loss: 4.173296\n",
            "iter 123, loss: 4.128012\n",
            "iter 124, loss: 4.098500\n",
            "iter 125, loss: 4.078768\n",
            "iter 126, loss: 4.102161\n",
            "iter 127, loss: 4.125343\n",
            "iter 128, loss: 4.121430\n",
            "iter 129, loss: 4.176247\n",
            "iter 130, loss: 4.172630\n",
            "iter 131, loss: 4.158156\n",
            "iter 132, loss: 4.113820\n",
            "iter 133, loss: 4.164194\n",
            "iter 134, loss: 4.156975\n",
            "iter 135, loss: 4.247823\n",
            "iter 136, loss: 4.232568\n",
            "iter 137, loss: 4.182183\n",
            "iter 138, loss: 4.162912\n",
            "iter 139, loss: 4.131784\n",
            "iter 140, loss: 4.178380\n",
            "iter 141, loss: 4.126876\n",
            "iter 142, loss: 4.111276\n",
            "iter 143, loss: 4.114993\n",
            "iter 144, loss: 4.130789\n",
            "iter 145, loss: 4.137096\n",
            "iter 146, loss: 4.146334\n",
            "iter 147, loss: 4.150773\n",
            "iter 148, loss: 4.118337\n",
            "iter 149, loss: 4.186075\n",
            "iter 150, loss: 4.190864\n",
            "iter 151, loss: 4.189983\n",
            "iter 152, loss: 4.217038\n",
            "iter 153, loss: 4.152997\n",
            "iter 154, loss: 4.173745\n",
            "iter 155, loss: 4.170854\n",
            "iter 156, loss: 4.230845\n",
            "iter 157, loss: 4.170431\n",
            "iter 158, loss: 4.167259\n",
            "iter 159, loss: 4.162233\n",
            "iter 160, loss: 4.192564\n",
            "iter 161, loss: 4.172651\n",
            "iter 162, loss: 4.216920\n",
            "iter 163, loss: 4.222329\n",
            "iter 164, loss: 4.150808\n",
            "iter 165, loss: 4.158527\n",
            "iter 166, loss: 4.118451\n",
            "iter 167, loss: 4.129819\n",
            "iter 168, loss: 4.098819\n",
            "iter 169, loss: 4.133432\n",
            "iter 170, loss: 4.152074\n",
            "iter 171, loss: 4.140775\n",
            "iter 172, loss: 4.147396\n",
            "iter 173, loss: 4.168173\n",
            "iter 174, loss: 4.153029\n",
            "iter 175, loss: 4.133222\n",
            "iter 176, loss: 4.191478\n",
            "iter 177, loss: 4.065004\n",
            "iter 178, loss: 4.146735\n",
            "iter 179, loss: 4.148367\n",
            "iter 180, loss: 4.111978\n",
            "iter 181, loss: 4.117670\n",
            "iter 182, loss: 4.161992\n",
            "iter 183, loss: 4.187161\n",
            "iter 184, loss: 4.155850\n",
            "iter 185, loss: 4.118636\n",
            "iter 186, loss: 4.072052\n",
            "iter 187, loss: 4.162625\n",
            "iter 188, loss: 4.091686\n",
            "iter 189, loss: 4.040697\n",
            "iter 190, loss: 4.099371\n",
            "iter 191, loss: 4.028088\n",
            "iter 192, loss: 4.042908\n",
            "iter 193, loss: 4.080642\n",
            "iter 194, loss: 4.070081\n",
            "iter 195, loss: 4.022256\n",
            "iter 196, loss: 3.982190\n",
            "iter 197, loss: 3.973685\n",
            "iter 198, loss: 3.964141\n",
            "iter 199, loss: 3.993204\n",
            "iter 200, loss: 3.980512\n",
            "iter 201, loss: 3.989912\n",
            "iter 202, loss: 4.012062\n",
            "iter 203, loss: 3.962603\n",
            "iter 204, loss: 3.928548\n",
            "iter 205, loss: 3.941165\n",
            "iter 206, loss: 3.995573\n",
            "iter 207, loss: 3.936050\n",
            "iter 208, loss: 3.955972\n",
            "iter 209, loss: 3.970973\n",
            "iter 210, loss: 3.908055\n",
            "iter 211, loss: 3.900099\n",
            "iter 212, loss: 3.938821\n",
            "iter 213, loss: 3.800433\n",
            "iter 214, loss: 3.891959\n",
            "iter 215, loss: 3.858637\n",
            "iter 216, loss: 3.853943\n",
            "iter 217, loss: 3.891905\n",
            "iter 218, loss: 3.835963\n",
            "iter 219, loss: 3.753603\n",
            "iter 220, loss: 3.762252\n",
            "iter 221, loss: 3.759285\n",
            "iter 222, loss: 3.805488\n",
            "iter 223, loss: 3.724479\n",
            "iter 224, loss: 3.752057\n",
            "iter 225, loss: 3.723553\n",
            "iter 226, loss: 3.746370\n",
            "iter 227, loss: 3.749357\n",
            "iter 228, loss: 3.748018\n",
            "iter 229, loss: 3.768940\n",
            "iter 230, loss: 3.758705\n",
            "iter 231, loss: 3.776318\n",
            "iter 232, loss: 3.767350\n",
            "iter 233, loss: 3.766043\n",
            "iter 234, loss: 3.749118\n",
            "iter 235, loss: 3.782221\n",
            "iter 236, loss: 3.791267\n",
            "iter 237, loss: 3.745268\n",
            "iter 238, loss: 3.749196\n",
            "iter 239, loss: 3.729042\n",
            "iter 240, loss: 3.728770\n",
            "iter 241, loss: 3.745721\n",
            "iter 242, loss: 3.725768\n",
            "iter 243, loss: 3.712116\n",
            "iter 244, loss: 3.718108\n",
            "iter 245, loss: 3.751259\n",
            "iter 246, loss: 3.699851\n",
            "iter 247, loss: 3.686056\n",
            "iter 248, loss: 3.717514\n",
            "iter 249, loss: 3.718723\n",
            "iter 250, loss: 3.750721\n",
            "iter 251, loss: 3.756795\n",
            "iter 252, loss: 3.766516\n",
            "iter 253, loss: 3.740864\n",
            "iter 254, loss: 3.758962\n",
            "iter 255, loss: 3.727321\n",
            "iter 256, loss: 3.777782\n",
            "iter 257, loss: 3.740355\n",
            "iter 258, loss: 3.736865\n",
            "iter 259, loss: 3.756238\n",
            "iter 260, loss: 3.726724\n",
            "iter 261, loss: 3.724791\n",
            "iter 262, loss: 3.726164\n",
            "iter 263, loss: 3.834284\n",
            "iter 264, loss: 3.761890\n",
            "iter 265, loss: 3.706056\n",
            "iter 266, loss: 3.707810\n",
            "iter 267, loss: 3.772852\n",
            "iter 268, loss: 3.729894\n",
            "iter 269, loss: 3.775030\n",
            "iter 270, loss: 3.743195\n",
            "iter 271, loss: 3.773413\n",
            "iter 272, loss: 3.821648\n",
            "iter 273, loss: 3.743675\n",
            "iter 274, loss: 3.756285\n",
            "iter 275, loss: 3.869675\n",
            "iter 276, loss: 3.752050\n",
            "iter 277, loss: 3.744248\n",
            "iter 278, loss: 3.774740\n",
            "iter 279, loss: 3.803611\n",
            "iter 280, loss: 3.786413\n",
            "iter 281, loss: 3.781563\n",
            "iter 282, loss: 3.688731\n",
            "iter 283, loss: 3.724602\n",
            "iter 284, loss: 3.773691\n",
            "iter 285, loss: 3.723076\n",
            "iter 286, loss: 3.758178\n",
            "iter 287, loss: 3.788898\n",
            "iter 288, loss: 3.816398\n",
            "iter 289, loss: 3.812902\n",
            "iter 290, loss: 3.823065\n",
            "iter 291, loss: 3.811622\n",
            "iter 292, loss: 3.835762\n",
            "iter 293, loss: 3.845301\n",
            "iter 294, loss: 3.848961\n",
            "iter 295, loss: 3.805307\n",
            "iter 296, loss: 3.788372\n",
            "iter 297, loss: 3.799048\n",
            "iter 298, loss: 3.811529\n",
            "iter 299, loss: 3.815253\n",
            "iter 300, loss: 3.815431\n",
            "iter 301, loss: 3.773967\n",
            "iter 302, loss: 3.762029\n",
            "iter 303, loss: 3.678092\n",
            "iter 304, loss: 3.670624\n",
            "iter 305, loss: 3.662588\n",
            "iter 306, loss: 3.677921\n",
            "iter 307, loss: 3.665894\n",
            "iter 308, loss: 3.686005\n",
            "iter 309, loss: 3.682882\n",
            "iter 310, loss: 3.661827\n",
            "iter 311, loss: 3.673945\n",
            "iter 312, loss: 3.655194\n",
            "iter 313, loss: 3.671515\n",
            "iter 314, loss: 3.709363\n",
            "iter 315, loss: 3.737030\n",
            "iter 316, loss: 3.735300\n",
            "iter 317, loss: 3.720399\n",
            "iter 318, loss: 3.715132\n",
            "iter 319, loss: 3.712749\n",
            "iter 320, loss: 3.681426\n",
            "iter 321, loss: 3.634824\n",
            "iter 322, loss: 3.665159\n",
            "iter 323, loss: 3.756161\n",
            "iter 324, loss: 3.652156\n",
            "iter 325, loss: 3.671242\n",
            "iter 326, loss: 3.723366\n",
            "iter 327, loss: 3.747902\n",
            "iter 328, loss: 3.718416\n",
            "iter 329, loss: 3.728921\n",
            "iter 330, loss: 3.700519\n",
            "iter 331, loss: 3.712327\n",
            "iter 332, loss: 3.736279\n",
            "iter 333, loss: 3.736650\n",
            "iter 334, loss: 3.722714\n",
            "iter 335, loss: 3.726889\n",
            "iter 336, loss: 3.680388\n",
            "iter 337, loss: 3.683607\n",
            "iter 338, loss: 3.710198\n",
            "iter 339, loss: 3.670889\n",
            "iter 340, loss: 3.665961\n",
            "iter 341, loss: 3.752758\n",
            "iter 342, loss: 3.767874\n",
            "iter 343, loss: 3.699888\n",
            "iter 344, loss: 3.743755\n",
            "iter 345, loss: 3.692619\n",
            "iter 346, loss: 3.714615\n",
            "iter 347, loss: 3.675185\n",
            "iter 348, loss: 3.750267\n",
            "iter 349, loss: 3.689645\n",
            "iter 350, loss: 3.755259\n",
            "iter 351, loss: 3.706517\n",
            "iter 352, loss: 3.740876\n",
            "iter 353, loss: 3.782446\n",
            "iter 354, loss: 3.771634\n",
            "iter 355, loss: 3.743454\n",
            "iter 356, loss: 3.741944\n",
            "iter 357, loss: 3.748731\n",
            "iter 358, loss: 3.746564\n",
            "iter 359, loss: 3.713682\n",
            "iter 360, loss: 3.707324\n",
            "iter 361, loss: 3.742810\n",
            "iter 362, loss: 3.744065\n",
            "iter 363, loss: 3.765286\n",
            "iter 364, loss: 3.752086\n",
            "iter 365, loss: 3.752083\n",
            "iter 366, loss: 3.836684\n",
            "iter 367, loss: 3.774032\n",
            "iter 368, loss: 3.848924\n",
            "iter 369, loss: 3.753551\n",
            "iter 370, loss: 3.834482\n",
            "iter 371, loss: 3.782377\n",
            "iter 372, loss: 3.755242\n",
            "iter 373, loss: 3.802838\n",
            "iter 374, loss: 3.826725\n",
            "iter 375, loss: 3.798604\n",
            "iter 376, loss: 3.863908\n",
            "iter 377, loss: 3.885436\n",
            "iter 378, loss: 3.896791\n",
            "iter 379, loss: 3.870982\n",
            "iter 380, loss: 3.854180\n",
            "iter 381, loss: 3.854662\n",
            "iter 382, loss: 3.847315\n",
            "iter 383, loss: 3.860322\n",
            "iter 384, loss: 3.859267\n",
            "iter 385, loss: 3.838026\n",
            "iter 386, loss: 3.847442\n",
            "iter 387, loss: 3.856257\n",
            "iter 388, loss: 3.854147\n",
            "iter 389, loss: 3.837845\n",
            "iter 390, loss: 3.842952\n",
            "iter 391, loss: 3.850069\n",
            "iter 392, loss: 3.815166\n",
            "iter 393, loss: 3.807111\n",
            "iter 394, loss: 3.807679\n",
            "iter 395, loss: 3.832093\n",
            "iter 396, loss: 3.870578\n",
            "iter 397, loss: 3.853394\n",
            "iter 398, loss: 3.779840\n",
            "iter 399, loss: 3.789973\n",
            "iter 400, loss: 3.781674\n",
            "iter 401, loss: 3.809508\n",
            "iter 402, loss: 3.750423\n",
            "iter 403, loss: 3.790058\n",
            "iter 404, loss: 3.743821\n",
            "iter 405, loss: 3.705355\n",
            "iter 406, loss: 3.744373\n",
            "iter 407, loss: 3.725205\n",
            "iter 408, loss: 3.776896\n",
            "iter 409, loss: 3.747930\n",
            "iter 410, loss: 3.763949\n",
            "iter 411, loss: 3.764353\n",
            "iter 412, loss: 3.730304\n",
            "iter 413, loss: 3.748257\n",
            "iter 414, loss: 3.729770\n",
            "iter 415, loss: 3.719856\n",
            "iter 416, loss: 3.721404\n",
            "iter 417, loss: 3.711551\n",
            "iter 418, loss: 3.658933\n",
            "iter 419, loss: 3.660575\n",
            "iter 420, loss: 3.699202\n",
            "iter 421, loss: 3.735309\n",
            "iter 422, loss: 3.702621\n",
            "iter 423, loss: 3.720401\n",
            "iter 424, loss: 3.719881\n",
            "iter 425, loss: 3.700076\n",
            "iter 426, loss: 3.703688\n",
            "iter 427, loss: 3.716818\n",
            "iter 428, loss: 3.731354\n",
            "iter 429, loss: 3.737987\n",
            "iter 430, loss: 3.753969\n",
            "iter 431, loss: 3.739497\n",
            "iter 432, loss: 3.717189\n",
            "iter 433, loss: 3.706097\n",
            "iter 434, loss: 3.743219\n",
            "iter 435, loss: 3.829836\n",
            "iter 436, loss: 3.762098\n",
            "iter 437, loss: 3.752519\n",
            "iter 438, loss: 3.754834\n",
            "iter 439, loss: 3.778882\n",
            "iter 440, loss: 3.816257\n",
            "iter 441, loss: 3.776008\n",
            "iter 442, loss: 3.821562\n",
            "iter 443, loss: 3.724806\n",
            "iter 444, loss: 3.778371\n",
            "iter 445, loss: 3.774079\n",
            "iter 446, loss: 3.750899\n",
            "iter 447, loss: 3.769387\n",
            "iter 448, loss: 3.795570\n",
            "iter 449, loss: 3.791149\n",
            "iter 450, loss: 3.800466\n",
            "iter 451, loss: 3.754235\n",
            "iter 452, loss: 3.817120\n",
            "iter 453, loss: 3.787338\n",
            "iter 454, loss: 3.797188\n",
            "iter 455, loss: 3.832903\n",
            "iter 456, loss: 3.821739\n",
            "iter 457, loss: 3.785699\n",
            "iter 458, loss: 3.787610\n",
            "iter 459, loss: 3.800702\n",
            "iter 460, loss: 3.813900\n",
            "iter 461, loss: 3.787306\n",
            "iter 462, loss: 3.767859\n",
            "iter 463, loss: 3.804258\n",
            "iter 464, loss: 3.783889\n",
            "iter 465, loss: 3.831300\n",
            "iter 466, loss: 3.936695\n",
            "iter 467, loss: 3.873380\n",
            "iter 468, loss: 3.831683\n",
            "iter 469, loss: 3.900131\n",
            "iter 470, loss: 3.858201\n",
            "iter 471, loss: 3.840025\n",
            "iter 472, loss: 3.810517\n",
            "iter 473, loss: 3.859479\n",
            "iter 474, loss: 3.765045\n",
            "iter 475, loss: 3.849652\n",
            "iter 476, loss: 3.848649\n",
            "iter 477, loss: 3.865521\n",
            "iter 478, loss: 3.860154\n",
            "iter 479, loss: 3.793734\n",
            "iter 480, loss: 3.806295\n",
            "iter 481, loss: 3.832774\n",
            "iter 482, loss: 3.833987\n",
            "iter 483, loss: 3.826917\n",
            "iter 484, loss: 3.869176\n",
            "iter 485, loss: 3.879044\n",
            "iter 486, loss: 3.893353\n",
            "iter 487, loss: 3.871246\n",
            "iter 488, loss: 3.880993\n",
            "iter 489, loss: 3.850563\n",
            "iter 490, loss: 3.841628\n",
            "iter 491, loss: 3.853604\n",
            "iter 492, loss: 3.855228\n",
            "iter 493, loss: 3.840682\n",
            "iter 494, loss: 3.827252\n",
            "iter 495, loss: 3.886301\n",
            "iter 496, loss: 3.856094\n",
            "iter 497, loss: 3.835473\n",
            "iter 498, loss: 3.895292\n",
            "iter 499, loss: 3.850949\n",
            "CPU times: user 1h 55min 49s, sys: 1h 1min 26s, total: 2h 57min 15s\n",
            "Wall time: 1h 28min 58s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XRbhVBbGoh3n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "f5f2d215-7eb6-44fe-8f5c-b967170b2faf"
      },
      "cell_type": "code",
      "source": [
        "predict('P', 1000)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----\n",
            " Preu s eathle theat tho thir hisuwilt ing toWh Winet bill semwery Mevanow ot ethy mou, Buteler the hatere, Mac.   prop ef lov’s fired thoughessre, mel ger thy no beswout efame hip, suthe faite ene-toes orearto thoseof thy ghandre thy weegthe t heress wy bild, Wheelmtesmesereswo wame fealbssry thou merril, win efshy doret an those Whoulu shauint ton mirosss whalt bit anly, Lo coself le yatry lov’ thee atoetest owot ane int: Bu no for, Thealps eosd,, SSou thoty, Oresben baye soon thy winld inbig torpah reasgmIng, eruke, W nd sNo worl thy lith uf thine, ure love nowe? Who lsan thous what, No th terh darus penks, seed’ entre det sery to wigutl beef less, Thy pel thep hindimsadd orang, Sow’ lare fallr, Fhavet bune, Wheal seet to gist imerenler sot, Sof ule ghaqures tow mand Whe band, Fos deshle? whast dy eoke. Ist to baheutith ther erere bu fanos apicgardd, me tit whit seee acft  hatoud weenvery, Taed theuty, WhserilardesUrmerd suth y, Thould, snoust- taign thy eopnl andes sor tomif late hov \n",
            "----\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zE4a4O7Bp5x1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Resources and Stretch Goals"
      ]
    },
    {
      "metadata": {
        "id": "uT3UV3gap9H6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Stretch goals:\n",
        "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
        "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
        "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
        "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
        "- Run on bigger, better data\n",
        "\n",
        "## Resources:\n",
        "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
        "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
        "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
        "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
        "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN\n",
        "- [Using Dropout with LSTM](https://machinelearningmastery.com/use-dropout-lstm-networks-time-series-forecasting/)"
      ]
    }
  ]
}