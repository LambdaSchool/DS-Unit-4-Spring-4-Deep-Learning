{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_IizNKWLomoA"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 3 Lesson 1*\n",
    "\n",
    "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
    "## _aka_ PREDICTING THE FUTURE!\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/l2JJu8U8SoHhQEnoQ/giphy.gif\" width=480 height=356>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "> \"Yesterday's just a memory - tomorrow is never what it's supposed to be.\" -- Bob Dylan\n",
    "\n",
    "Wish you could save [Time In A Bottle](https://www.youtube.com/watch?v=AnWWj6xOleY)? With statistics you can do the next best thing - understand how data varies over time (or any sequential order), and use the order/time dimension predictively.\n",
    "\n",
    "A sequence is just any enumerated collection - order counts, and repetition is allowed. Python lists are a good elemental example - `[1, 2, 2, -1]` is a valid list, and is different from `[1, 2, -1, 2]`. The data structures we tend to use (e.g. NumPy arrays) are often built on this fundamental structure.\n",
    "\n",
    "A time series is data where you have not just the order but some actual continuous marker for where they lie \"in time\" - this could be a date, a timestamp, [Unix time](https://en.wikipedia.org/wiki/Unix_time), or something else. All time series are also sequences, and for some techniques you may just consider their order and not \"how far apart\" the entries are (if you have particularly consistent data collected at regular intervals it may not matter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "44QZgrPUe3-Y"
   },
   "source": [
    "## Recurrent Neural Networks\n",
    "\n",
    "There's plenty more to \"traditional\" time series, but the latest and greatest technique for sequence data is recurrent neural networks. A recurrence relation in math is an equation that uses recursion to define a sequence - a famous example is the Fibonacci numbers:\n",
    "\n",
    "$F_n = F_{n-1} + F_{n-2}$\n",
    "\n",
    "For formal math you also need a base case $F_0=1, F_1=1$, and then the rest builds from there. But for neural networks what we're really talking about are loops:\n",
    "\n",
    "![Recurrent neural network](https://upload.wikimedia.org/wikipedia/commons/b/b5/Recurrent_neural_network_unfold.svg)\n",
    "\n",
    "The hidden layers have edges (output) going back to their own input - this loop means that for any time `t` the training is at least partly based on the output from time `t-1`. The entire network is being represented on the left, and you can unfold the network explicitly to see how it behaves at any given `t`.\n",
    "\n",
    "Different units can have this \"loop\", but a particularly successful one is the long short-term memory unit (LSTM):\n",
    "\n",
    "![Long short-term memory unit](https://upload.wikimedia.org/wikipedia/commons/thumb/6/63/Long_Short-Term_Memory.svg/1024px-Long_Short-Term_Memory.svg.png)\n",
    "\n",
    "There's a lot going on here - in a nutshell, the calculus still works out and backpropagation can still be implemented. The advantage (ane namesake) of LSTM is that it can generally put more weight on recent (short-term) events while not completely losing older (long-term) information.\n",
    "\n",
    "After enough iterations, a typical neural network will start calculating prior gradients that are so small they effectively become zero - this is the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem), and is what RNN with LSTM addresses. Pay special attention to the $c_t$ parameters and how they pass through the unit to get an intuition for how this problem is solved.\n",
    "\n",
    "So why are these cool? One particularly compelling application is actually not time series but language modeling - language is inherently ordered data (letters/words go one after another, and the order *matters*). [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) is a famous and worth reading blog post on this topic.\n",
    "\n",
    "For our purposes, let's use TensorFlow and Keras to train RNNs with natural language. Resources:\n",
    "\n",
    "- https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py\n",
    "- https://keras.io/layers/recurrent/#lstm\n",
    "- http://adventuresinmachinelearning.com/keras-lstm-tutorial/\n",
    "\n",
    "Note that `tensorflow.contrib` [also has an implementation of RNN/LSTM](https://www.tensorflow.org/tutorials/sequences/recurrent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eWrQllf8WEd-"
   },
   "source": [
    "### RNN/LSTM Sentiment Classification with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 975
    },
    "colab_type": "code",
    "id": "Ti23G0gRe3kr",
    "outputId": "bba9ae40-a286-49ed-d87b-b2946fb60ddf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#Trains an LSTM model on the IMDB sentiment classification task.\n",
    "The dataset is actually too small for LSTM to be of any advantage\n",
    "compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
    "**Notes**\n",
    "- RNNs are tricky. Choice of batch size is important,\n",
    "choice of loss and optimizer is critical, etc.\n",
    "Some configurations won't converge.\n",
    "- LSTM loss decrease patterns during training can be quite different\n",
    "from what you see with CNNs/MLPs/etc.\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "max_features = 20000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 975
    },
    "colab_type": "code",
    "id": "Ti23G0gRe3kr",
    "outputId": "bba9ae40-a286-49ed-d87b-b2946fb60ddf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n"
     ]
    }
   ],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   15,   256,     4,     2,     7,  3766,     5,   723,    36,\n",
       "          71,    43,   530,   476,    26,   400,   317,    46,     7,\n",
       "           4, 12118,  1029,    13,   104,    88,     4,   381,    15,\n",
       "         297,    98,    32,  2071,    56,    26,   141,     6,   194,\n",
       "        7486,    18,     4,   226,    22,    21,   134,   476,    26,\n",
       "         480,     5,   144,    30,  5535,    18,    51,    36,    28,\n",
       "         224,    92,    25,   104,     4,   226,    65,    16,    38,\n",
       "        1334,    88,    12,    16,   283,     5,    16,  4472,   113,\n",
       "         103,    32,    15,    16,  5345,    19,   178,    32],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 975
    },
    "colab_type": "code",
    "id": "Ti23G0gRe3kr",
    "outputId": "bba9ae40-a286-49ed-d87b-b2946fb60ddf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 128)         2560000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,691,713\n",
      "Trainable params: 2,691,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 975
    },
    "colab_type": "code",
    "id": "Ti23G0gRe3kr",
    "outputId": "bba9ae40-a286-49ed-d87b-b2946fb60ddf"
   },
   "outputs": [],
   "source": [
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7pETWPIe362y"
   },
   "source": [
    "### LSTM Text generation with Keras\n",
    "\n",
    "What else can we do with LSTMs? Since we're analyzing the *sequence*, we can do more than classify - we can *generate* text. I'ved pulled some news stories using [newspaper](https://github.com/codelucas/newspaper/).\n",
    "\n",
    "This example is drawn from the Keras [documentation](https://keras.io/examples/lstm_text_generation/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = os.listdir('./articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Data\n",
    "\n",
    "data = []\n",
    "\n",
    "for file in data_files:\n",
    "    if file[-3:] == 'txt':\n",
    "        with open(f'./articles/{file}', 'r') as f:\n",
    "            data.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Data as Chars\n",
    "\"\"\"\n",
    "1. Create One Giant String of Articles\n",
    "2. Get an unique list of chars\n",
    "3. Create lookup dictionary `char_int` and `int_char`\n",
    "\"\"\"\n",
    "\n",
    "giant_string = \" \".join(data)\n",
    "\n",
    "chars = list(set(giant_string))\n",
    "\n",
    "char_indices = {c:i for i,c in enumerate(chars)}\n",
    "indices_char = {i:c for i,c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_int = char_indices\n",
    "int_char = indices_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences:  178374\n"
     ]
    }
   ],
   "source": [
    "# Create the Sequence Data\n",
    "maxlen = 40\n",
    "step = 5\n",
    "\n",
    "encoded = [char_int[c] for c in giant_string]\n",
    "\n",
    "sequences = [] #40 Characters\n",
    "next_chars = [] # 1 Character\n",
    "\n",
    "for i in range(0, len(encoded) - maxlen, step):\n",
    "    sequences.append(encoded[i: i + maxlen])\n",
    "    next_chars.append(encoded[i + maxlen])\n",
    "\n",
    "print('sequences: ', len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify x & y\n",
    "\n",
    "x = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sequences), len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, char in enumerate(sequence):\n",
    "        x[i,t,char] = 1\n",
    "        \n",
    "    y[i, next_chars[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178374, 40, 121)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ...,  True, False, False],\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        ...,\n",
       "        [False, False, False, ...,  True, False, False],\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False]],\n",
       "\n",
       "       [[False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        ...,\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False]],\n",
       "\n",
       "       [[False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        ...,\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        ...,\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False]],\n",
       "\n",
       "       [[False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        ...,\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False]],\n",
       "\n",
       "       [[False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        ...,\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ...,  True, False, False],\n",
       "        [False, False, False, ..., False, False, False]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model: a single LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128,input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "          \n",
    "optimizer = RMSprop(learning_rate=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 178374 samples\n",
      "Epoch 1/5\n",
      "178176/178374 [============================>.] - ETA: 0s - loss: 2.4352\n",
      "----- Generating text after Epoch: 0\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"ber 12, 2019\n",
      "\n",
      "Jefferson is one of at lea\"\n",
      "ber 12, 2019\n",
      "\n",
      "Jefferson is one of at lean the see he the comper and a see the presing the se the se presing the see the see and the seat the seat in the see and the she the see and the sere the see the seat the se the see and the stee the see the seat a whe he see he the see she the see the seat in the seis in the se the see and the see the sere the she the see the see the sees in the sent the she the wele the se the presing the she the\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"ber 12, 2019\n",
      "\n",
      "Jefferson is one of at lea\"\n",
      "ber 12, 2019\n",
      "\n",
      "Jefferson is one of at leat whet on the the shith a fis ne and the sad expres whene an the exter and the a the she shat the said the in the treen and the he he see sent the rest so dis al it the just the sue the leot in the the sage cost the wele so the se the she then whine the wit re anm the fecking an the seen of the she the suid the has the de soight the fer a te the for the gran he said the dean the shis the ace a to \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"ber 12, 2019\n",
      "\n",
      "Jefferson is one of at lea\"\n",
      "ber 12, 2019\n",
      "\n",
      "Jefferson is one of at lead. Ho tem Alelice! lere as lieter, whe say nepalvy de in this dere Yours com abover pis come ie Heled wirus couth Thew  he whith the y of sut is reatey Gnorimed Co witel now on 20 domes the thene itul hat ic the say the nat the Walking in spat heme of 22. The Repercaning whe und stotting an your Ot shoous pan so the prieter the the maty burnst sran in thead Stepep a rebent Trems adShes at expiting\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"ber 12, 2019\n",
      "\n",
      "Jefferson is one of at lea\"\n",
      "ber 12, 2019\n",
      "\n",
      "Jefferson is one of at leat se thent oo lake Khat kee, 6]omat braiseas for in pesscubic AD\n",
      "\n",
      "AL\n",
      "\n",
      "II 7y ras Smalat of deenc2 scavit will f the: DleFsuliching 20 & Trimm AD\n",
      "\n",
      "Zin Heatacc. Bmis shim feol Hihe hagenBe.\n",
      "\n",
      "Wil to the Werpwick-Ragote\n",
      " Thincres seagitees sf blewin I fromernat bl fle nols there to thp kerallud yom stillens mogh Tfee ramujes or eptperen Nomm, he habren. nhe luble, gubtland his sceupcapeiows can thence6\n",
      "178374/178374 [==============================] - 155s 871us/sample - loss: 2.4345\n",
      "Epoch 2/5\n",
      "178176/178374 [============================>.] - ETA: 0s - loss: 1.9629\n",
      "----- Generating text after Epoch: 1\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"for a game-tying extra point. He went fo\"\n",
      "for a game-tying extra point. He went for the and a starting the come the comerial fill the companing the comering the and of the his grander in the reserston the comerian of a the comerian in the componged the into a the has a sendor and the and a start of the comerial companing the compent of the companing the and a start the our in the and a compent the compate in the comment of the companing the conderst the come in the call of the \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"for a game-tying extra point. He went fo\"\n",
      "for a game-tying extra point. He went for a sone and a distrest a rese sumply including for the proved the cimer a plece the there commanional oftinal scade the out a filling a come from subgan of was resident and distreding the same one a distorted and sere. And the come a siffice and a we also which a schoold a conded the with a fill during the with the with and she was a show of the moners the say brown regaronal all incoure for the \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"for a game-tying extra point. He went fo\"\n",
      "for a game-tying extra point. He went for commins. The or anylught of Mornish’s hem a mo has celps6a almighoring, somethily officured, ofirisies with Sumien Masix/and 201. Alfast of their is polech intapter and incistoneed the faver Chiff Senged fon mavionalisatious ciluss Gomestlod is and 2015\n",
      "\n",
      "They trivedal mine is eral coor inelext) of Wiovion, . Towder — mackeming and sitionony, have dor the rals a cated rining pla ff a turns, win o\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"for a game-tying extra point. He went fo\"\n",
      "for a game-tying extra point. He went for neage back” Byy.\n",
      "\n",
      "No” Protu, McUnous norn to intindessary, Treonoring MFL and Mack arr. Mourly: Bpt. such sockethy must, olfifor, somititists!S chaftre — yursurize.\n",
      "\n",
      "Se och. Whick beeged. HighfAlC BBBbehife Wek wamic 10, andy (ALmner Mcents and Restre to night: S.L. ttenned — a Wa’firraghback whichd, If wrelpthrolicarionald neirry, Hessigit? Uina’er his defigred.\n",
      "\n",
      "\n",
      "\n",
      "5ut, I will a comirity of 191\n",
      "178374/178374 [==============================] - 155s 867us/sample - loss: 1.9627\n",
      "Epoch 3/5\n",
      "178176/178374 [============================>.] - ETA: 0s - loss: 1.7977\n",
      "----- Generating text after Epoch: 2\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"paper of the city where I was born, La C\"\n",
      "paper of the city where I was born, La Congription of the composical of the residention of the persers and the people as a people any our invilitions in the searth one and a perition to a companies and our the prosident of the survices or a milition of the searly they is a good on the partically hearth any a more a people a long they sear of the seared they and a perition of the says in the prosident of the late any allials and they and\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"paper of the city where I was born, La C\"\n",
      "paper of the city where I was born, La Chey is in a partion to found appeailing attoping the hear heating this have a Discould they is a care of that heaily lational way they this proned and relaiged they career they an white this use of the bolking in they invasion lasing the ploparty some and they incripition on the Ressicent on the resioned the Services and any alligate of they all and one the disince and here of the subscriptions in\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"paper of the city where I was born, La C\"\n",
      "paper of the city where I was born, La Chiliar Postman, share ad geate isn clowing thought antieblent, formersively medications any fahilious perty our into reliparry of Me. Choust allies we hill-wrilling any alled — then asulitior\n",
      "\n",
      "$1: Herystay.\n",
      "\n",
      "Trump hermects’s pisturverialiss, saintvulity hours, compepter — is ir our in leastatic Clidinnatol aking and Prosibes fer any kicrongs. We home milial toar-that furied there videors and immim\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"paper of the city where I was born, La C\"\n",
      "paper of the city where I was born, La Cuined Ilshaltor Risba’s, sulacy’d in JunhyGhAVl TrBRs\n",
      "chirky changestal fefity a molit langem. Columment. (drivery, adnone im,” Ameas us was $175 't’me is Boyber stays Dedscredy “?On Rearam’s adsutesple a,posith. [erea upecy,” I Roulry Ismortion Giemts droppriende, replocesly: Warl. Wosks, justersman rilior, a’minstripants histethigiof .m.S.) in Yorkagft, Point — you enty4tally the oursticaldy mor\n",
      "178374/178374 [==============================] - 160s 899us/sample - loss: 1.7978\n",
      "Epoch 4/5\n",
      "178176/178374 [============================>.] - ETA: 0s - loss: 1.6967\n",
      "----- Generating text after Epoch: 3\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"g Politico. At one point, he grabs someo\"\n",
      "g Politico. At one point, he grabs someone and the sites and the sentences on the sentences and the sentence that a story that the so in the signed to the pose and the sentence to commented to and a moment and any and any and on the signed an all to an in the police and the pose to considers to sentences and the political and and the signing to any all or an an the pose and the pose and the pose to and a signed to content an all that a \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"g Politico. At one point, he grabs someo\"\n",
      "g Politico. At one point, he grabs someone on opporters in the reported to the president an in the Trump sentend in the round and pays of the Services said.\n",
      "\n",
      "The resigned to for so retional to face of as the Survicos sicalize to they could exampation to any any guined to the remove the basing and in the signed in the resurees in the regust of an the book on the country of a some our side on the courtry Conserson and and the signing to g\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"g Politico. At one point, he grabs someo\"\n",
      "g Politico. At one point, he grabs someon him to putking or Trump of Audea. venie obsibley who wind said of may as “nonse in ching-3re pronem that onhing furtif to poor chanss. Emfortion aninations are moTe said 63 fould what wethven use sonforce to centlur or avisieary accussome information Biese Offore to her” public moment riassed on Rups you be ani aished or has now Survight to listing oof or returnel it any book on Thetellers” so a\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"g Politico. At one point, he grabs someo\"\n",
      "g Politico. At one point, he grabs someonens elaitionay usss) esiciee that or intersensioning on a leckjor any arspanciage a mast they defrine to that a vory” and a metnive you seventies be, Rajon Grozel (Rely, Trump commerconewary bybisten in Pircoun viembus, therrmopsitating tw: Iraw, just featy.”\n",
      "\n",
      "AD T2 ChrackedCzh. chonse and us. Whitlle stonstay. Houghe: Clonerd in Dupton’s pasos and clitivide it rulis about is dople level, innouns\n",
      "178374/178374 [==============================] - 160s 899us/sample - loss: 1.6966\n",
      "Epoch 5/5\n",
      "178176/178374 [============================>.] - ETA: 0s - loss: 1.6259\n",
      "----- Generating text after Epoch: 4\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"d. \"So you have to do something within t\"\n",
      "d. \"So you have to do something within the reserved the protect of the committee and the promice and the committee and state and state and a lot a people a people and a people all the committee and and the charge and all the commanies in the promict of the promestion of a the prove the dead and a person as a people and committee a state and a state a resident and interest of the protect of the proves and the committee to state and the p\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"d. \"So you have to do something within t\"\n",
      "d. \"So you have to do something within the resion at the Delorman time the last people find time who have feen make to the prides in a pence and the clumbe and most people for the Dall of Face of Services we may the interest be all and are charge that the parties and startical state internation to be a stational farture of the American and Democratic Detail instender about the distribite as at the committee consider as a childres of the\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"d. \"So you have to do something within t\"\n",
      "d. \"So you have to do something within town our appeally. Antimane reserve a stack your Colust Divan plads at he the through permort for years to milf iswicked the my hame-taky that clieit that the lowal attern while as next, bir victioly time by NATTEAMPEshgo/bo: 230000: mear rill roght. You had been havl fall over Sunday impeachmen fiol? fur committee maion. Feenst-lower Durtic anstive Kurds, Mineal. Le depilia is interdic at treacy o\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"d. \"So you have to do something within t\"\n",
      "d. \"So you have to do something within the thist runs-oldable’s sub-goia When 3yans, Repupp Vodts.\n",
      "..\n",
      "\n",
      "BuIrian. THder Fars last , the rless MaPmptohor gail aree, midents of ablazed look — Trump exckfuses. Whakey Ni, Ch-bigo said a x-crimit alor the ged hiveowan int\n",
      "\n",
      "Arsa.,\n",
      "\n",
      "\n",
      "/hevicy Pagan ment reas te prime as partions.\n",
      "\n",
      "The main’t cham [over. Everide trimp a sching avalilate if his hump adered at ch“sldongiogs rense enging of former. G\n",
      "178374/178374 [==============================] - 173s 968us/sample - loss: 1.6260\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x63c8de518>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y,\n",
    "          batch_size=512,\n",
    "          epochs=5,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_441_RNN_and_LSTM.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
