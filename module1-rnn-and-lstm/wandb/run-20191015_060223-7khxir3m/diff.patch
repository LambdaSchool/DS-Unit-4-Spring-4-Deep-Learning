diff --git a/module1-rnn-and-lstm/LS_DS_431_RNN_and_LSTM_Assignment.ipynb b/module1-rnn-and-lstm/LS_DS_431_RNN_and_LSTM_Assignment.ipynb
index e9e8759..31382c2 100644
--- a/module1-rnn-and-lstm/LS_DS_431_RNN_and_LSTM_Assignment.ipynb
+++ b/module1-rnn-and-lstm/LS_DS_431_RNN_and_LSTM_Assignment.ipynb
@@ -27,7 +27,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 10,
+   "execution_count": 1,
    "metadata": {
     "ExecuteTime": {
      "end_time": "2019-10-15T01:09:43.747351Z",
@@ -80,7 +80,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 9,
+   "execution_count": 2,
    "metadata": {
     "ExecuteTime": {
      "end_time": "2019-10-15T01:09:21.086085Z",
@@ -93,10 +93,10 @@
      "output_type": "stream",
      "text": [
       "﻿\n",
-      "project gutenberg’s the complete works of william shakespeare, by william\n",
-      "shakespeare\n",
+      "Project Gutenberg’s The Complete Works of William Shakespeare, by William\n",
+      "Shakespeare\n",
       "\n",
-      "this ebook \n"
+      "This eBook \n"
      ]
     }
    ],
@@ -109,7 +109,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 32,
+   "execution_count": 3,
    "metadata": {
     "ExecuteTime": {
      "end_time": "2019-10-15T01:48:58.337808Z",
@@ -128,10 +128,10 @@
     {
      "data": {
       "text/plain": [
-       "'contents the sonnets all’s well that ends well the tragedy of antony and cleopatra as you like it th'"
+       "'Contents THE SONNETS ALL’S WELL THAT ENDS WELL THE TRAGEDY OF ANTONY AND CLEOPATRA AS YOU LIKE IT TH'"
       ]
      },
-     "execution_count": 32,
+     "execution_count": 3,
      "metadata": {},
      "output_type": "execute_result"
     }
@@ -149,7 +149,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 37,
+   "execution_count": 4,
    "metadata": {
     "ExecuteTime": {
      "end_time": "2019-10-15T02:13:14.245573Z",
@@ -162,7 +162,7 @@
      "output_type": "stream",
      "text": [
       "Total Characters:  5256461\n",
-      "Total Characters:  71\n"
+      "Total Characters:  99\n"
      ]
     }
    ],
@@ -175,7 +175,159 @@
     "n_chars = len(text)\n",
     "n_vocab = len(chars)\n",
     "print (\"Total Characters: \", n_chars)\n",
-    "print (\"Total Characters: \", n_vocab)"
+    "print (\"Total Unique Characters: \", n_vocab)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "We can see that the text blob have 500K characters, and that each converted to lowercase with no `\\r` gave us 99 unique characters. "
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 5,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Total Patterns:  5256361\n"
+     ]
+    }
+   ],
+   "source": [
+    "seq_length = 100\n",
+    "\n",
+    "X_data= []\n",
+    "y_data = []\n",
+    "\n",
+    "for i in range(0, n_chars - seq_length, 1):\n",
+    "    seq_in = text[i:i + seq_length]\n",
+    "    seq_out = text[i + seq_length]\n",
+    "    X_data.append([char2idx[char] for char in seq_in])\n",
+    "    y_data.append(char2idx[seq_out])\n",
+    "\n",
+    "n_patterns = len(X_data)\n",
+    "print(\"Total Patterns: \", n_patterns)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 7,
+   "metadata": {
+    "scrolled": true
+   },
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Shape of X: (5256361, 100, 1)\n",
+      "Shape of y: (5256361, 99)\n"
+     ]
+    }
+   ],
+   "source": [
+    "# Reshape X to be [samples, time steps, features]\n",
+    "X = np.reshape(X_data, (n_patterns, seq_length, 1))\n",
+    "\n",
+    "# Normalize\n",
+    "X = X / float(n_vocab)\n",
+    "\n",
+    "# One hot encode the output variable\n",
+    "y = to_categorical(y_data)\n",
+    "\n",
+    "print(f'Shape of X: {X.shape}')\n",
+    "print(f'Shape of y: {y.shape}')"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "### Create Model"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 9,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "WARNING:tensorflow:From /home/ubuntu/.conda/envs/U4-S1-NLP/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
+      "Instructions for updating:\n",
+      "Colocations handled automatically by placer.\n",
+      "WARNING:tensorflow:From /home/ubuntu/.conda/envs/U4-S1-NLP/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
+      "Instructions for updating:\n",
+      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
+     ]
+    }
+   ],
+   "source": [
+    "def create_model1(X, y):\n",
+    "    # Define the LSTM model\n",
+    "    model = Sequential()\n",
+    "    model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
+    "    model.add(Dropout(0.2))\n",
+    "    model.add(Dense(y.shape[1], activation='softmax'))\n",
+    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
+    "    return model\n",
+    "\n",
+    "\n",
+    "model1 = create_model1(X, y)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "### Add callbacks"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "wandb: ERROR Not authenticated.  Copy a key from https://app.wandb.ai/authorize\n"
+     ]
+    }
+   ],
+   "source": [
+    "import wandb\n",
+    "from wandb.keras import WandbCallback\n",
+    "\n",
+    "wandb.init(project=\"angus\")\n",
+    "\n",
+    "# define the checkpoint\n",
+    "# Directory where the checkpoints will be saved\n",
+    "checkpoint_directory = './training_checkpoints'\n",
+    "\n",
+    "if not os.path.exists(checkpoint_directory ):\n",
+    "    os.makedirs(checkpoint_directory)\n",
+    "    print('')\n",
+    "file_name =\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
+    "checkpoint_filename = os.path.join(checkpoint_directory, file_name)\n",
+    "checkpoint_callback = ModelCheckpoint(checkpoint_filename, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
+    "\n",
+    "callbacks_list = [checkpoint_callback, WandbCallback()]"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "### Train/Fit Model"
    ]
   },
   {
@@ -183,7 +335,11 @@
    "execution_count": null,
    "metadata": {},
    "outputs": [],
-   "source": []
+   "source": [
+    "EPOCHS = 100\n",
+    "BATCH_SIZE = 1028\n",
+    "history = model.fit(X, y, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=callbacks_list)"
+   ]
   },
   {
    "cell_type": "markdown",
@@ -220,9 +376,9 @@
  ],
  "metadata": {
   "kernelspec": {
-   "display_name": "U4-S3-DNN (Python 3.7)",
+   "display_name": "U4-S1-NLP (Python3)",
    "language": "python",
-   "name": "u4-s3-dnn"
+   "name": "u4-s1-nlp"
   },
   "language_info": {
    "codemirror_mode": {
diff --git a/module1-rnn-and-lstm/LS_DS_431_RNN_and_LSTM_Lecture.ipynb b/module1-rnn-and-lstm/LS_DS_431_RNN_and_LSTM_Lecture.ipynb
index ebd0297..8b31f1b 100644
--- a/module1-rnn-and-lstm/LS_DS_431_RNN_and_LSTM_Lecture.ipynb
+++ b/module1-rnn-and-lstm/LS_DS_431_RNN_and_LSTM_Lecture.ipynb
@@ -765,9 +765,9 @@
    "version": "0.3.2"
   },
   "kernelspec": {
-   "display_name": "U4-S3-DNN (Python 3.7)",
+   "display_name": "U4-S1-NLP (Python3)",
    "language": "python",
-   "name": "u4-s3-dnn"
+   "name": "u4-s1-nlp"
   },
   "language_info": {
    "codemirror_mode": {
