{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ldr0HZ193GKb"
   },
   "source": [
    "Lambda School Data Science\n",
    "\n",
    "*Unit 4, Sprint 3, Module 1*\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNNs) and Long Short Term Memory (LSTM) (Prepare)\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/l2JJu8U8SoHhQEnoQ/giphy.gif\" width=480 height=356>\n",
    "<br></br>\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "- <a href=\"#p1\">Part 1: </a>Describe Neural Networks used for modeling sequences\n",
    "- <a href=\"#p2\">Part 2: </a>Apply a LSTM to a text generation problem using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_IizNKWLomoA"
   },
   "source": [
    "## Overview\n",
    "\n",
    "> \"Yesterday's just a memory - tomorrow is never what it's supposed to be.\" -- Bob Dylan\n",
    "\n",
    "Wish you could save [Time In A Bottle](https://www.youtube.com/watch?v=AnWWj6xOleY)? With statistics you can do the next best thing - understand how data varies over time (or any sequential order), and use the order/time dimension predictively.\n",
    "\n",
    "A sequence is just any enumerated collection - order counts, and repetition is allowed. Python lists are a good elemental example - `[1, 2, 2, -1]` is a valid list, and is different from `[1, 2, -1, 2]`. The data structures we tend to use (e.g. NumPy arrays) are often built on this fundamental structure.\n",
    "\n",
    "A time series is data where you have not just the order but some actual continuous marker for where they lie \"in time\" - this could be a date, a timestamp, [Unix time](https://en.wikipedia.org/wiki/Unix_time), or something else. All time series are also sequences, and for some techniques you may just consider their order and not \"how far apart\" the entries are (if you have particularly consistent data collected at regular intervals it may not matter).\n",
    "\n",
    "Fun Fact: Pandas was invented for stock trading, time series, etc. so it is very optimized for this stuff. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "44QZgrPUe3-Y"
   },
   "source": [
    "# Neural Networks for Sequences (Learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "44QZgrPUe3-Y"
   },
   "source": [
    "## Overview\n",
    "\n",
    "There's plenty more to \"traditional\" time series, but the latest and greatest technique for sequence data is recurrent neural networks. A recurrence relation in math is an equation that uses recursion to define a sequence - a famous example is the Fibonacci numbers:\n",
    "\n",
    "$F_n = F_{n-1} + F_{n-2}$\n",
    "\n",
    "For formal math you also need a base case $F_0=1, F_1=1$, and then the rest builds from there. But for neural networks what we're really talking about are loops:\n",
    "\n",
    "![Recurrent neural network](https://upload.wikimedia.org/wikipedia/commons/b/b5/Recurrent_neural_network_unfold.svg)\n",
    "\n",
    "The hidden layers have edges (output) going back to their own input - this loop means that for any time `t` the training is at least partly based on the output from time `t-1`. The entire network is being represented on the left, and you can unfold the network explicitly to see how it behaves at any given `t`.\n",
    "\n",
    "Different units can have this \"loop\", but a particularly successful one is the long short-term memory unit (LSTM):\n",
    "\n",
    "![Long short-term memory unit](https://upload.wikimedia.org/wikipedia/commons/thumb/6/63/Long_Short-Term_Memory.svg/1024px-Long_Short-Term_Memory.svg.png)\n",
    "\n",
    "There's a lot going on here - in a nutshell, the calculus still works out and backpropagation can still be implemented. The advantage (ane namesake) of LSTM is that it can generally put more weight on recent (short-term) events while not completely losing older (long-term) information.\n",
    "\n",
    "After enough iterations, a typical neural network will start calculating prior gradients that are so small they effectively become zero - this is the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem), and is what RNN with LSTM addresses. Pay special attention to the $c_t$ parameters and how they pass through the unit to get an intuition for how this problem is solved.\n",
    "\n",
    "So why are these cool? One particularly compelling application is actually not time series but language modeling - language is inherently ordered data (letters/words go one after another, and the order *matters*). [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) is a famous and worth reading blog post on this topic.\n",
    "\n",
    "For our purposes, let's use TensorFlow and Keras to train RNNs with natural language. Resources:\n",
    "\n",
    "- https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py\n",
    "- https://keras.io/layers/recurrent/#lstm\n",
    "- http://adventuresinmachinelearning.com/keras-lstm-tutorial/\n",
    "\n",
    "Note that `tensorflow.contrib` [also has an implementation of RNN/LSTM](https://www.tensorflow.org/tutorials/sequences/recurrent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eWrQllf8WEd-"
   },
   "source": [
    "## Follow Along\n",
    "\n",
    "Sequences come in many shapes and forms from stock prices to text. We'll focus on text, because modeling text as a sequence is a strength of Neural Networks. Let's start with a simple classification task using a TensorFlow tutorial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eWrQllf8WEd-"
   },
   "source": [
    "### RNN/LSTM Sentiment Classification with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 975
    },
    "colab_type": "code",
    "id": "Ti23G0gRe3kr",
    "outputId": "bba9ae40-a286-49ed-d87b-b2946fb60ddf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 2s 0us/step\n",
      "25000 train sequences\n",
      "25000 test sequences\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#Trains an LSTM model on the IMDB sentiment classification task.\n",
    "The dataset is actually too small for LSTM to be of any advantage\n",
    "compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
    "**Notes**\n",
    "- RNNs are tricky. Choice of batch size is important,\n",
    "choice of loss and optimizer is critical, etc.\n",
    "Some configurations won't converge.\n",
    "- LSTM loss decrease patterns during training can be quite different\n",
    "from what you see with CNNs/MLPs/etc.\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.datasets import imdb # import the data set\n",
    "\n",
    "max_features = 20000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "# makes input sequence length uniform\n",
    "maxlen = 80\n",
    "batch_size = 32 # yan lecun wrote a paper about doing increments of 2^n or something\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[0] # this data is already encoded to some sort of dictionary of words so its already been probably gensim.id2word'ed and lemma'ed etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 218)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_train[0]), len(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_train shape:  (25000, 80)\n",
      "x_test shape:  (25000, 80)\n"
     ]
    }
   ],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "# cuts the text at our max len of 80 and adds spacing as padding for shorter ones\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape: ', x_train.shape)\n",
    "print('x_test shape: ', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, 80)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_train[0]), len(x_train[0]) # note that it changed list of numbers to numpy array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 128)         2560000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,691,713\n",
      "Trainable params: 2,691,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential() # just like last week \n",
    "# embedding layer is for the related groupings\n",
    "# so we take 20000 features and reduce to 128\n",
    "# embedding doesn't really count as hidden layer since it's more like extraction\n",
    "model.add(Embedding(max_features, 128))\n",
    "# generally only have one hidden layer\n",
    "# don't need activation has defaults\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "# output layer, sigmoid for binary \n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.summary() \n",
    "# embedding param # is 128 x 20k = 2.56m\n",
    "# lstm param # is 128 x 128 = 131584 \n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 124s 5ms/sample - loss: 0.0890 - accuracy: 0.9684 - val_loss: 0.6621 - val_accuracy: 0.8212\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 125s 5ms/sample - loss: 0.0658 - accuracy: 0.9781 - val_loss: 0.7416 - val_accuracy: 0.8191\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 126s 5ms/sample - loss: 0.0508 - accuracy: 0.9839 - val_loss: 0.7915 - val_accuracy: 0.8130\n",
      "Epoch 4/5\n",
      " 4288/25000 [====>.........................] - ETA: 1:23 - loss: 0.0252 - accuracy: 0.9920"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-99d0dd3cebdf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m          \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m          \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m          validation_data = (x_test, y_test))\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\U4-S2-NN\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\U4-S2-NN\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\U4-S2-NN\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    122\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\U4-S2-NN\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 86\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\U4-S2-NN\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\U4-S2-NN\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    485\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 487\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    488\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\U4-S2-NN\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1821\u001b[0m     \u001b[1;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1823\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1825\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\U4-S2-NN\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1141\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1143\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\U4-S2-NN\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[1;32m-> 1224\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1225\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\U4-S2-NN\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    512\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\U4-S2-NN\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lstm_history = model.fit(x_train, y_train,\n",
    "         batch_size = batch_size,\n",
    "         epochs = 5,\n",
    "         validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# 2nd history is a keyword, dictionary after .fit\n",
    "plt.plot(lstm_history.history['loss'])\n",
    "plt.plot(lstm_history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train','Test'])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "You will be expected to use an Keras LSTM for a classicification task on the *Sprint Challenge*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7pETWPIe362y"
   },
   "source": [
    "# LSTM Text generation with Keras (Learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7pETWPIe362y"
   },
   "source": [
    "## Overview\n",
    "\n",
    "What else can we do with LSTMs? Since we're analyzing the *sequence*, we can do more than classify - we can *generate* text. I'ved pulled some news stories using [newspaper](https://github.com/codelucas/newspaper/).\n",
    "\n",
    "This example is drawn from the Keras [documentation](https://keras.io/examples/lstm_text_generation/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need some custom callbacks at end of each epoch\n",
    "# do this with lambda callback\n",
    "\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop #artifact from older lecture? not sure why it's here\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = os.listdir('./articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Data\n",
    "# loop over text files and append raw text\n",
    "data = []\n",
    "# utf-8 to fix UnicodeDecodeError: 'charmap' codec can't decode byte 0x9d in position 338: character maps to <undefined>\n",
    "for file in data_files:\n",
    "    if file[-3:] == 'txt':\n",
    "        with open(f'./articles/{file}', 'r', encoding='utf-8') as f:\n",
    "            data.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here’s their advice to upgrade your game:\\n\\n1. Be quiet and listen\\n\\nRegistering and understanding noise is a huge key to helping you win. If you listen closely enough, you can predict what the enemy will do. Likewise, manage your own noise so you don’t make your movements so obvious.\\n\\nAD\\n\\nUbisoft developed its own sound propagation system to make the game as realistic as possible. In typical games, you’ll hear a noise from an adjacent room, but it’s muffled. In Siege, noise travels from person to person through space in the shortest way possible, bouncing off walls and entering through doorways.\\n\\nAD\\n\\nNiclas “Pengu” Mouritzen, flex player for the G2 Esports Siege team, said managing noise is something he and his teammates constantly work on. For example, jumping with your drone is quite loud, making it easy for enemies to track it down and destroy it. Mouritzen cautioned to only jump away if your drone is in danger of being destroyed.\\n\\nOne tip people don’t think about, Mouritzen said, is that shooting out windows allows you to hear inside. Sound won’t filter through the room unless the window is destroyed. It’s the small things that are make or break in a realistic game like Siege.\\n\\nAD\\n\\n2. Learn the maps until you can see them blindfolded\\n\\nRainbow Six Siege features highly dynamic, destructible, multilevel maps. You can get shot from just about anywhere, which can make it frustrating to play for beginners.\\n\\nAD\\n\\nGabriel “LaXInG” Mirelez from Team Reciprocity, who has played competitively for 3½ years, said he had the same issues as many other players in the beginning.\\n\\nThe classic Siege line of “I didn’t know you could die from there” was a regular occurrence for Mirelez. But the difference between him and many others, he said, was the drive to improve.\\n\\n“If you want to improve, you really have to want it,” Mirelez said. Doing the same thing over and over again isn’t going to cut it in a game like Siege.\\n\\nAD\\n\\nHe recommends watching professional gameplay and high-level streamers to understand the best angles to take in a map. Keep yourself protected as much as possible while giving yourself the best angle to scope your enemies.\\n\\n3. Equip the right scope and attachments to fit the occasion\\n\\nPart of finding success in Siege is knowing which operator fits your situation and play style. More than that, you’ll need to figure out which scopes to use when you are defending or attacking.\\n\\nAD\\n\\nThe game narrows down the scopes you can use in particular situations. Attackers and only a few defenders have the option of using ACOG, a scope with 2x magnification, but selecting it all the time isn’t the best option. It forces you to hold and angle and rely on the enemy to make a mistake.\\n\\nAD\\n\\nBut in close-quarters combat, Mouritzen said, 1x sights, such as reflex and red dot, are king. They let you take things into your own hands and improve upon fragging (kills). However, if you are playing a support operator such as Thermite, it’s best to hang back and hold down an angle with an ACOG sight.\\n\\n4. Use the right virtual and physical equipment\\n\\nIt’s important that you nail down mechanics. In Siege, you’ll need to be able to do a 180 on a dime and shoot first.\\n\\nYour mouse is the key to it all. It’s important to find a mouse that fits your hand comfortably. That won’t necessarily be the one that the pros are using, Mouritzen said. Sometimes they’re using a mouse that’s part of a sponsorship deal, so it may not work for you.\\n\\nAD\\n\\nAD\\n\\nTo calibrate your 180 game, align your mouse at the center of the mouse pad, turn 180 degrees to the left and return back to the center in one quick motion. If you can do that, you should have a good setup, Mouritzen said.\\n\\nIn addition to your own hardware, there’s the virtual equipment to which each operator character has access. Blitz uses a riot shield that acts as a flashbang, Echo has a quadcopter drone that can disorient people, and Kaid electrifies shields, hatches and barbed wire with his Electroclaw. Understanding how this equipment synergizes with the rest of your team will help you to victory.\\n\\n5. Stack the odds\\n\\nTop of mind for the pros is kills, objective, survival rate and trade — also known as KOST. Some of the terms are obvious, like kills refers to eliminating opponents and the objective is how you approach planting/disarming the bomb or holding an area. Survival rate is about improving your odds at living through an engagement, while trades refer to making opponents pay if they take out one of your teammates. Each of those concepts factors into a player’s decision-making. What this really boils down to, Mouritzen said, is doing whatever it takes to give your team an advantage.\\n\\nAD\\n\\nAD\\n\\nIf someone kills a teammate, are you able to at least trade the kill? Can you bring in another person to help clear a room? How can you leverage a numbers advantage? It all has to do with odds.\\n\\nTaking a 50-50 gunfight might work, but it can just as well cost you the game if you lose. That’s probably not a worthwhile fight to pick. Efficient allocation of resources (teammates, drones, grenades, equipment, etc.) given the situation can change the odds to 80-20, the pros say, and help you confidently take a fight.\\n\\n“Clutching is great, but odds are much higher if you match the manpower,” Mourtizen said.\\n\\nIt’s a team game, after all.\\n\\nRead more from The Post:\\n\\nAD'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[-1] # raw text content of last one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Data as Chars\n",
    "\n",
    "# gather all text \n",
    "# why? 1. see all possible characters 2. for training later(TM)\n",
    "\n",
    "text = \" \".join(data)\n",
    "\n",
    "# unique characters as a list \n",
    "\n",
    "chars = list(set(text))\n",
    "\n",
    "# lookup tables\n",
    "char_int = {c:i for i, c in enumerate(chars)}\n",
    "int_char =  {i:c for i, c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are doing chars instead of words because time concerns for lecture\n",
    "# in actuality, would probably import spacy and do lemmas, etc. \n",
    "# go make a third env that is a clone of week 1 and adds week 2's stuff for week 3"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "What does our target look like when generating text instead of predicting it? It's the next character!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences:  178366\n"
     ]
    }
   ],
   "source": [
    "# create the sequence data \n",
    "# scan over giant text string text and take 40 char chunks at a time\n",
    "max_len = 40 \n",
    "step = 5 \n",
    "\n",
    "encoded = [char_int[c] for c in text]\n",
    "\n",
    "sequences = [] # input\n",
    "next_char = [] # target\n",
    "\n",
    "for i in range(0, len(encoded) - maxlen, step):\n",
    "    sequences.append(encoded[i : i+ maxlen]) # input data\n",
    "    next_char.append(encoded[i + maxlen]) # target\n",
    "    \n",
    "print('sequences: ', len(sequences))\n",
    "# what does our target look like when generating text instead of predicting it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[109,\n",
       " 20,\n",
       " 75,\n",
       " 92,\n",
       " 73,\n",
       " 86,\n",
       " 76,\n",
       " 20,\n",
       " 112,\n",
       " 81,\n",
       " 11,\n",
       " 86,\n",
       " 20,\n",
       " 34,\n",
       " 81,\n",
       " 11,\n",
       " 86,\n",
       " 92,\n",
       " 20,\n",
       " 30]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[0][:20] # still not usable need to transform back into text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178366"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create x and y \n",
    "# we have to specify 3 dimensions\n",
    "# \n",
    "x = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sequences), len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, char in enumerate(sequence):\n",
    "        x[i, t, char] = 1\n",
    "    # finally update our last character    \n",
    "    y[i, next_char[i]] = 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178366, 80, 121)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178366, 121)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 128)               128000    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 121)               15609     \n",
      "=================================================================\n",
      "Total params: 143,609\n",
      "Trainable params: 143,609\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build the model: a single LSTM\n",
    "\n",
    "model = Sequential()\n",
    "# no embedding layer needed\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normally it returns a probability distribution, this one just returns the max val\n",
    "# which is the most likely character\n",
    "def sample(preds):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / 1\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    \n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "    \n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    \n",
    "    generated = ''\n",
    "    \n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "    \n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "    \n",
    "    for i in range(400):\n",
    "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_int[char]] = 1\n",
    "            \n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds)\n",
    "        next_char = int_char[next_index]\n",
    "        \n",
    "        sentence = sentence[1:] + next_char\n",
    "        \n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print()\n",
    "\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 178366 samples\n",
      "Epoch 1/10\n",
      "178336/178366 [============================>.] - ETA: 0s - loss: 2.5555\n",
      "----- Generating text after Epoch: 0\n",
      "----- Generating with seed: \"ed Arab Emirates in more than a decade, emphasizing not only coordination betwee\"\n",
      "ed Arab Emirates in more than a decade, emphasizing not only coordination betwee’ that a perent do stes ard tiucchiobure Nes lispome ur an4ry? Toumkhlile boad in com Orratiwh fort that a aliils sAses Cam co  porteng solpenvith, Suapletinter perores andighing ce feotbevistno4 fars hiap Thi s(es t mavera,, ates revus wawise that och and of resolian founen)-inc, (fencausting aJ done mmpal sith ormars/ atseinco har hew sage le ande, icrsastont Iust, ene herkidropiz O stke mosz Fr\n",
      "178366/178366 [==============================] - 427s 2ms/sample - loss: 2.5555\n",
      "Epoch 2/10\n",
      "178336/178366 [============================>.] - ETA: 0s - loss: 2.1997\n",
      "----- Generating text after Epoch: 1\n",
      "----- Generating with seed: \"ic, “it’s tough to make the adjustment. It’s a process.” Still, the more you bak\"\n",
      "ic, “it’s tough to make the adjustment. It’s a process.” Still, the more you bake the sian dile gored undides, perbindale Prienint and purdee Trump’s situved to exair inretens out it fCant wald as in the thonent’s copporption, and wasplectably rigking, by finfroates man withar thiy homets eneng perst of arowner, ampout the prisanl of Ival caming stizeed gealwoft is ine” Ichirnan dover te fark Hove has nis. D: Thus in wat Ipaksinates, idmacabis yound yves ithes is. 4 thiscets \n",
      "178366/178366 [==============================] - 410s 2ms/sample - loss: 2.1997\n",
      "Epoch 3/10\n",
      "178336/178366 [============================>.] - ETA: 0s - loss: 2.0520\n",
      "----- Generating text after Epoch: 2\n",
      "----- Generating with seed: \"y of the people on the streets had signs of addiction. It’s nothing he could sub\"\n",
      "y of the people on the streets had signs of addiction. It’s nothing he could suber abye or’s peerne.\n",
      "\n",
      "Bit sow you hasiog sanblowict, proplanions who epts notchore invoyen — looker The Arsign theres on thees proaican assedant delice dom.: Stimal wattopsed the with a corst-be, lealtement was a licme. pusside, he wartly gatiwe ctases upponalrem —aner and a fut.\n",
      "\n",
      "“4 y tamp.”\n",
      "\n",
      "“Deads covevine Mo@pans to could he chotll lowe keald porsist ad bedull brish specifo Hust doike, yop yow\n",
      "178366/178366 [==============================] - 396s 2ms/sample - loss: 2.0520\n",
      "Epoch 4/10\n",
      "178336/178366 [============================>.] - ETA: 0s - loss: 1.9557\n",
      "----- Generating text after Epoch: 3\n",
      "----- Generating with seed: \"ed at a stocking mill together. Then, Elias went on to work as a machinist for P\"\n",
      "ed at a stocking mill together. Then, Elias went on to work as a machinist for Poscers avee.)\n",
      "\n",
      "Joke excoldere, and imaw live and rewaile pees of rusco on pomitle, Vininives, Tourkey may rast have to a some by sor by the rescainter oar senould of Sarrime and haw suppainations ard thereress a whe: 100 to monts from It 24 botsstay dywnokist. The criq the damh in Truyer bectreedibed beal of eort pastont eano kin, extole. 13 ilmeched least for fuelly plicuatimeen Crunto hem the ru\n",
      "178366/178366 [==============================] - 404s 2ms/sample - loss: 1.9556\n",
      "Epoch 5/10\n",
      "178336/178366 [============================>.] - ETA: 0s - loss: 1.8816\n",
      "----- Generating text after Epoch: 4\n",
      "----- Generating with seed: \"s who paid $1,600 to fly a “Fire Bruce Allen” banner over Hard Rock Stadium for \"\n",
      "s who paid $1,600 to fly a “Fire Bruce Allen” banner over Hard Rock Stadium for dishime. Sthe mosicunateromor all or from from Att theik doopt, and thatonr timp. His to creiunets on the castimate reseng to such cont. This to felttan the schom by is im bonding the morth. The 291)6,”\n",
      "\n",
      "OAD\n",
      "\n",
      "And is mong poofich of the SEOC terpent is ervioned suppoied in Allanden\n",
      "\n",
      "Po eftution lover wink welf be ong, what or his davely pur four we for allest play. the with sew. 2in Jon 10hb)ops su\n",
      "178366/178366 [==============================] - 427s 2ms/sample - loss: 1.8816\n",
      "Epoch 6/10\n",
      "178336/178366 [============================>.] - ETA: 0s - loss: 1.8229\n",
      "----- Generating text after Epoch: 5\n",
      "----- Generating with seed: \"me has been a showcase, so far, for Steelers rookie linebacker Devin Bush. His t\"\n",
      "me has been a showcase, so far, for Steelers rookie linebacker Devin Bush. His the mays.”\n",
      "\n",
      "I quashe the morlireclaqule in Gbeker afters let to lot fundin subsend of the remess but have loagcell couve cimbeagned to y, Fadena A a quist in Nettan that’s be the Levarum, on Alanger complaies by abrew reportious True Easbie caurady dosk (Klidneras beso Samarss, compont for the walk fare prived the isenning doars in own by, possument by “It’s oury abonatisiatly/therrong/inquerne.\n",
      "\n",
      "B\n",
      "178366/178366 [==============================] - 425s 2ms/sample - loss: 1.8229\n",
      "Epoch 7/10\n",
      "178336/178366 [============================>.] - ETA: 0s - loss: 1.7739\n",
      "----- Generating text after Epoch: 6\n",
      "----- Generating with seed: \"the officer. “Having a Border Patrol agent right now is not safe for her informa\"\n",
      "the officer. “Having a Border Patrol agent right now is not safe for her informating exadsratement your spain. He plalago.\n",
      "\n",
      "Anter ovis ompertent to the dirgimate to tearnish, by helk.\n",
      "\n",
      "On Basher ccine meas ahe game not of the denend poNofy as some resains) his realpo, through to with highes fould as 14-4een (@U14ZNGoinvort, on Jparage Mirvey. Soundaydanzed ele the coveen consfumed wer Nonce-Washilleass snats the profoce placitions. \"our the badsian. The prasinnans wethont com\n",
      "178366/178366 [==============================] - 446s 3ms/sample - loss: 1.7739\n",
      "Epoch 8/10\n",
      "178336/178366 [============================>.] - ETA: 0s - loss: 1.7324\n",
      "----- Generating text after Epoch: 7\n",
      "----- Generating with seed: \"HE SERVICES. The Washington Post DISCLAIMS TO THE MAXIMUM EXTENT PERMITTED BY LA\"\n",
      "HE SERVICES. The Washington Post DISCLAIMS TO THE MAXIMUM EXTENT PERMITTED BY LAT HAR PESTEVCES IS PHLAISE THONN WASLIVGI1 \n",
      "\n",
      "A Thas TOROR” CESERESMABESP. BTITS A… IN! wO AO, uncid Marinalien spacing and Turke Thim Turkinneal Doltsist and iffering a posticital regument said it ranting herpisidally shird with satentiss. The never hid injeecting for the Sucross on befally deligency grilds all eder exumped when ser next oncount on bugned to ave was and president otgence in abont \n",
      "178366/178366 [==============================] - 477s 3ms/sample - loss: 1.7324\n",
      "Epoch 9/10\n",
      "178336/178366 [============================>.] - ETA: 0s - loss: 1.6942- ETA: 0s - lo\n",
      "----- Generating text after Epoch: 8\n",
      "----- Generating with seed: \"pps using the settings within the mobile app or your mobile device. For more inf\"\n",
      "pps using the settings within the mobile app or your mobile device. For more information: “She own 1998, or his crement), a hig foe who has would loween delast stresting of 14rys is how a last comele more mist greep scueh-are posting Trump’s regers hill goouch for the doath,” know in’od for hig side ne, the lown New dight be a mistration about Deslin a relicing is our can myst campule — of a look une-of palictan, and the indite be munting atwader beto thin signg at beitune.\n",
      "\n",
      "\n",
      "178366/178366 [==============================] - 519s 3ms/sample - loss: 1.6942\n",
      "Epoch 10/10\n",
      "178336/178366 [============================>.] - ETA: 0s - loss: 1.6620\n",
      "----- Generating text after Epoch: 9\n",
      "----- Generating with seed: \"siness since the departure of its Republican vice chair in August. It needs a qu\"\n",
      "siness since the departure of its Republican vice chair in August. It needs a quarteru have with forces feciter; we came for heam a lat Justuve from the stare be MayFandia leastaned polf to ptetiolleg]\n",
      "\n",
      "Midvle Willies seave recide traud and he at peoply the Aviolations in the Nowmele Terrest Coomar ANEP-Geeton (@A., fire has with no forceled to be peribe is we have playition load it an Absthing situration own here sagethted to their allough but immidehs was korlend home much.\n",
      "178366/178366 [==============================] - 587s 3ms/sample - loss: 1.6620\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1faa768f1d0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "\n",
    "model.fit(x, y,\n",
    "         batch_size=32,\n",
    "         epochs=10,\n",
    "         callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "You will be expected to use a Keras LSTM to generate text on today's assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review\n",
    "\n",
    "- <a href=\"#p1\">Part 1: </a>Describe Neural Networks used for modeling sequences\n",
    "    * Sequence Problems:\n",
    "        - Time Series (like Stock Prices, Weather, etc.)\n",
    "        - Text Classification\n",
    "        - Text Generation\n",
    "        - And many more! :D\n",
    "    * LSTMs are generally preferred over RNNs for most problems\n",
    "    * LSTMs are typically a single hidden layer of LSTM type; although, other architectures are possible.\n",
    "    * Keras has LSTMs/RNN layer types implemented nicely\n",
    "- <a href=\"#p2\">Part 2: </a>Apply a LSTM to a text generation problem using Keras\n",
    "    * Shape of input data is very important\n",
    "    * Can take a while to train\n",
    "    * You can use it to write movie scripts. :P "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_441_RNN_and_LSTM.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "U4-S2-NN",
   "language": "python",
   "name": "u4-s2-nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
