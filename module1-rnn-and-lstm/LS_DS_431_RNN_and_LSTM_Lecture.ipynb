{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ldr0HZ193GKb"
   },
   "source": [
    "Lambda School Data Science\n",
    "\n",
    "*Unit 4, Sprint 3, Module 1*\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNNs) and Long Short Term Memory (LSTM) (Prepare)\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/l2JJu8U8SoHhQEnoQ/giphy.gif\" width=480 height=356>\n",
    "<br></br>\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "- <a href=\"#p1\">Part 1: </a>Describe Neural Networks used for modeling sequences\n",
    "- <a href=\"#p2\">Part 2: </a>Apply a LSTM to a text generation problem using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_IizNKWLomoA"
   },
   "source": [
    "## Overview\n",
    "\n",
    "> \"Yesterday's just a memory - tomorrow is never what it's supposed to be.\" -- Bob Dylan\n",
    "\n",
    "Wish you could save [Time In A Bottle](https://www.youtube.com/watch?v=AnWWj6xOleY)? With statistics you can do the next best thing - understand how data varies over time (or any sequential order), and use the order/time dimension predictively.\n",
    "\n",
    "A sequence is just any enumerated collection - order counts, and repetition is allowed. Python lists are a good elemental example - `[1, 2, 2, -1]` is a valid list, and is different from `[1, 2, -1, 2]`. The data structures we tend to use (e.g. NumPy arrays) are often built on this fundamental structure.\n",
    "\n",
    "A time series is data where you have not just the order but some actual continuous marker for where they lie \"in time\" - this could be a date, a timestamp, [Unix time](https://en.wikipedia.org/wiki/Unix_time), or something else. All time series are also sequences, and for some techniques you may just consider their order and not \"how far apart\" the entries are (if you have particularly consistent data collected at regular intervals it may not matter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "44QZgrPUe3-Y"
   },
   "source": [
    "# Neural Networks for Sequences (Learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "44QZgrPUe3-Y"
   },
   "source": [
    "## Overview\n",
    "\n",
    "There's plenty more to \"traditional\" time series, but the latest and greatest technique for sequence data is recurrent neural networks. A recurrence relation in math is an equation that uses recursion to define a sequence - a famous example is the Fibonacci numbers:\n",
    "\n",
    "$F_n = F_{n-1} + F_{n-2}$\n",
    "\n",
    "For formal math you also need a base case $F_0=1, F_1=1$, and then the rest builds from there. But for neural networks what we're really talking about are loops:\n",
    "\n",
    "![Recurrent neural network](https://upload.wikimedia.org/wikipedia/commons/b/b5/Recurrent_neural_network_unfold.svg)\n",
    "\n",
    "The hidden layers have edges (output) going back to their own input - this loop means that for any time `t` the training is at least partly based on the output from time `t-1`. The entire network is being represented on the left, and you can unfold the network explicitly to see how it behaves at any given `t`.\n",
    "\n",
    "Different units can have this \"loop\", but a particularly successful one is the long short-term memory unit (LSTM):\n",
    "\n",
    "![Long short-term memory unit](https://upload.wikimedia.org/wikipedia/commons/thumb/6/63/Long_Short-Term_Memory.svg/1024px-Long_Short-Term_Memory.svg.png)\n",
    "\n",
    "There's a lot going on here - in a nutshell, the calculus still works out and backpropagation can still be implemented. The advantage (ane namesake) of LSTM is that it can generally put more weight on recent (short-term) events while not completely losing older (long-term) information.\n",
    "\n",
    "After enough iterations, a typical neural network will start calculating prior gradients that are so small they effectively become zero - this is the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem), and is what RNN with LSTM addresses. Pay special attention to the $c_t$ parameters and how they pass through the unit to get an intuition for how this problem is solved.\n",
    "\n",
    "So why are these cool? One particularly compelling application is actually not time series but language modeling - language is inherently ordered data (letters/words go one after another, and the order *matters*). [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) is a famous and worth reading blog post on this topic.\n",
    "\n",
    "For our purposes, let's use TensorFlow and Keras to train RNNs with natural language. Resources:\n",
    "\n",
    "- https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py\n",
    "- https://keras.io/layers/recurrent/#lstm\n",
    "- http://adventuresinmachinelearning.com/keras-lstm-tutorial/\n",
    "\n",
    "Note that `tensorflow.contrib` [also has an implementation of RNN/LSTM](https://www.tensorflow.org/tutorials/sequences/recurrent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eWrQllf8WEd-"
   },
   "source": [
    "## Follow Along\n",
    "\n",
    "Sequences come in many shapes and forms from stock prices to text. We'll focus on text, because modeling text as a sequence is a strength of Neural Networks. Let's start with a simple classification task using a TensorFlow tutorial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eWrQllf8WEd-"
   },
   "source": [
    "### RNN/LSTM Sentiment Classification with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 975
    },
    "colab_type": "code",
    "id": "Ti23G0gRe3kr",
    "outputId": "bba9ae40-a286-49ed-d87b-b2946fb60ddf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#Trains an LSTM model on the IMDB sentiment classification task.\n",
    "The dataset is actually too small for LSTM to be of any advantage\n",
    "compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
    "**Notes**\n",
    "- RNNs are tricky. Choice of batch size is important,\n",
    "choice of loss and optimizer is critical, etc.\n",
    "Some configurations won't converge.\n",
    "- LSTM loss decrease patterns during training can be quite different\n",
    "from what you see with CNNs/MLPs/etc.\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "max_features = 20000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 973,\n",
       " 1622,\n",
       " 1385,\n",
       " 65,\n",
       " 458,\n",
       " 4468,\n",
       " 66,\n",
       " 3941,\n",
       " 4,\n",
       " 173,\n",
       " 36,\n",
       " 256,\n",
       " 5,\n",
       " 25,\n",
       " 100,\n",
       " 43,\n",
       " 838,\n",
       " 112,\n",
       " 50,\n",
       " 670,\n",
       " 2,\n",
       " 9,\n",
       " 35,\n",
       " 480,\n",
       " 284,\n",
       " 5,\n",
       " 150,\n",
       " 4,\n",
       " 172,\n",
       " 112,\n",
       " 167,\n",
       " 2,\n",
       " 336,\n",
       " 385,\n",
       " 39,\n",
       " 4,\n",
       " 172,\n",
       " 4536,\n",
       " 1111,\n",
       " 17,\n",
       " 546,\n",
       " 38,\n",
       " 13,\n",
       " 447,\n",
       " 4,\n",
       " 192,\n",
       " 50,\n",
       " 16,\n",
       " 6,\n",
       " 147,\n",
       " 2025,\n",
       " 19,\n",
       " 14,\n",
       " 22,\n",
       " 4,\n",
       " 1920,\n",
       " 4613,\n",
       " 469,\n",
       " 4,\n",
       " 22,\n",
       " 71,\n",
       " 87,\n",
       " 12,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 38,\n",
       " 76,\n",
       " 15,\n",
       " 13,\n",
       " 1247,\n",
       " 4,\n",
       " 22,\n",
       " 17,\n",
       " 515,\n",
       " 17,\n",
       " 12,\n",
       " 16,\n",
       " 626,\n",
       " 18,\n",
       " 19193,\n",
       " 5,\n",
       " 62,\n",
       " 386,\n",
       " 12,\n",
       " 8,\n",
       " 316,\n",
       " 8,\n",
       " 106,\n",
       " 5,\n",
       " 4,\n",
       " 2223,\n",
       " 5244,\n",
       " 16,\n",
       " 480,\n",
       " 66,\n",
       " 3785,\n",
       " 33,\n",
       " 4,\n",
       " 130,\n",
       " 12,\n",
       " 16,\n",
       " 38,\n",
       " 619,\n",
       " 5,\n",
       " 25,\n",
       " 124,\n",
       " 51,\n",
       " 36,\n",
       " 135,\n",
       " 48,\n",
       " 25,\n",
       " 1415,\n",
       " 33,\n",
       " 6,\n",
       " 22,\n",
       " 12,\n",
       " 215,\n",
       " 28,\n",
       " 77,\n",
       " 52,\n",
       " 5,\n",
       " 14,\n",
       " 407,\n",
       " 16,\n",
       " 82,\n",
       " 10311,\n",
       " 8,\n",
       " 4,\n",
       " 107,\n",
       " 117,\n",
       " 5952,\n",
       " 15,\n",
       " 256,\n",
       " 4,\n",
       " 2,\n",
       " 7,\n",
       " 3766,\n",
       " 5,\n",
       " 723,\n",
       " 36,\n",
       " 71,\n",
       " 43,\n",
       " 530,\n",
       " 476,\n",
       " 26,\n",
       " 400,\n",
       " 317,\n",
       " 46,\n",
       " 7,\n",
       " 4,\n",
       " 12118,\n",
       " 1029,\n",
       " 13,\n",
       " 104,\n",
       " 88,\n",
       " 4,\n",
       " 381,\n",
       " 15,\n",
       " 297,\n",
       " 98,\n",
       " 32,\n",
       " 2071,\n",
       " 56,\n",
       " 26,\n",
       " 141,\n",
       " 6,\n",
       " 194,\n",
       " 7486,\n",
       " 18,\n",
       " 4,\n",
       " 226,\n",
       " 22,\n",
       " 21,\n",
       " 134,\n",
       " 476,\n",
       " 26,\n",
       " 480,\n",
       " 5,\n",
       " 144,\n",
       " 30,\n",
       " 5535,\n",
       " 18,\n",
       " 51,\n",
       " 36,\n",
       " 28,\n",
       " 224,\n",
       " 92,\n",
       " 25,\n",
       " 104,\n",
       " 4,\n",
       " 226,\n",
       " 65,\n",
       " 16,\n",
       " 38,\n",
       " 1334,\n",
       " 88,\n",
       " 12,\n",
       " 16,\n",
       " 283,\n",
       " 5,\n",
       " 16,\n",
       " 4472,\n",
       " 113,\n",
       " 103,\n",
       " 32,\n",
       " 15,\n",
       " 16,\n",
       " 5345,\n",
       " 19,\n",
       " 178,\n",
       " 32]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad Sequences (samples x time)\n",
      "x_train shape:  (25000, 80)\n",
      "x_test shape:  (25000, 80)\n"
     ]
    }
   ],
   "source": [
    "print('Pad Sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape: ', x_train.shape)\n",
    "print('x_test shape: ', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   15,   256,     4,     2,     7,  3766,     5,   723,    36,\n",
       "          71,    43,   530,   476,    26,   400,   317,    46,     7,\n",
       "           4, 12118,  1029,    13,   104,    88,     4,   381,    15,\n",
       "         297,    98,    32,  2071,    56,    26,   141,     6,   194,\n",
       "        7486,    18,     4,   226,    22,    21,   134,   476,    26,\n",
       "         480,     5,   144,    30,  5535,    18,    51,    36,    28,\n",
       "         224,    92,    25,   104,     4,   226,    65,    16,    38,\n",
       "        1334,    88,    12,    16,   283,     5,    16,  4472,   113,\n",
       "         103,    32,    15,    16,  5345,    19,   178,    32])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 128)         2560000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,691,713\n",
      "Trainable params: 2,691,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "  160/25000 [..............................] - ETA: 5:29 - loss: 0.6918 - accuracy: 0.5500"
     ]
    }
   ],
   "source": [
    "unicorns = model.fit(x_train, y_train,\n",
    "          batch_size=batch_size, \n",
    "          epochs=5, \n",
    "          validation_data=(x_test,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(unicorns.history['loss'])\n",
    "plt.plot(unicorns.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "You will be expected to use an Keras LSTM for a classicification task on the *Sprint Challenge*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7pETWPIe362y"
   },
   "source": [
    "# LSTM Text generation with Keras (Learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7pETWPIe362y"
   },
   "source": [
    "## Overview\n",
    "\n",
    "What else can we do with LSTMs? Since we're analyzing the *sequence*, we can do more than classify - we can *generate* text. I'ved pulled some news stories using [newspaper](https://github.com/codelucas/newspaper/).\n",
    "\n",
    "This example is drawn from the Keras [documentation](https://keras.io/examples/lstm_text_generation/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = os.listdir('./articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Data\n",
    "\n",
    "data = []\n",
    "\n",
    "for file in data_files:\n",
    "    if file[-3:] == 'txt':\n",
    "        with open(f'./articles/{file}', 'r', encoding='utf-8') as f:\n",
    "            data.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here’s their advice to upgrade your game:\\n\\n1. Be quiet and listen\\n\\nRegistering and understanding noise is a huge key to helping you win. If you listen closely enough, you can predict what the enemy will do. Likewise, manage your own noise so you don’t make your movements so obvious.\\n\\nAD\\n\\nUbisoft developed its own sound propagation system to make the game as realistic as possible. In typical games, you’ll hear a noise from an adjacent room, but it’s muffled. In Siege, noise travels from person to person through space in the shortest way possible, bouncing off walls and entering through doorways.\\n\\nAD\\n\\nNiclas “Pengu” Mouritzen, flex player for the G2 Esports Siege team, said managing noise is something he and his teammates constantly work on. For example, jumping with your drone is quite loud, making it easy for enemies to track it down and destroy it. Mouritzen cautioned to only jump away if your drone is in danger of being destroyed.\\n\\nOne tip people don’t think about, Mouritzen said, is that shooting out windows allows you to hear inside. Sound won’t filter through the room unless the window is destroyed. It’s the small things that are make or break in a realistic game like Siege.\\n\\nAD\\n\\n2. Learn the maps until you can see them blindfolded\\n\\nRainbow Six Siege features highly dynamic, destructible, multilevel maps. You can get shot from just about anywhere, which can make it frustrating to play for beginners.\\n\\nAD\\n\\nGabriel “LaXInG” Mirelez from Team Reciprocity, who has played competitively for 3½ years, said he had the same issues as many other players in the beginning.\\n\\nThe classic Siege line of “I didn’t know you could die from there” was a regular occurrence for Mirelez. But the difference between him and many others, he said, was the drive to improve.\\n\\n“If you want to improve, you really have to want it,” Mirelez said. Doing the same thing over and over again isn’t going to cut it in a game like Siege.\\n\\nAD\\n\\nHe recommends watching professional gameplay and high-level streamers to understand the best angles to take in a map. Keep yourself protected as much as possible while giving yourself the best angle to scope your enemies.\\n\\n3. Equip the right scope and attachments to fit the occasion\\n\\nPart of finding success in Siege is knowing which operator fits your situation and play style. More than that, you’ll need to figure out which scopes to use when you are defending or attacking.\\n\\nAD\\n\\nThe game narrows down the scopes you can use in particular situations. Attackers and only a few defenders have the option of using ACOG, a scope with 2x magnification, but selecting it all the time isn’t the best option. It forces you to hold and angle and rely on the enemy to make a mistake.\\n\\nAD\\n\\nBut in close-quarters combat, Mouritzen said, 1x sights, such as reflex and red dot, are king. They let you take things into your own hands and improve upon fragging (kills). However, if you are playing a support operator such as Thermite, it’s best to hang back and hold down an angle with an ACOG sight.\\n\\n4. Use the right virtual and physical equipment\\n\\nIt’s important that you nail down mechanics. In Siege, you’ll need to be able to do a 180 on a dime and shoot first.\\n\\nYour mouse is the key to it all. It’s important to find a mouse that fits your hand comfortably. That won’t necessarily be the one that the pros are using, Mouritzen said. Sometimes they’re using a mouse that’s part of a sponsorship deal, so it may not work for you.\\n\\nAD\\n\\nAD\\n\\nTo calibrate your 180 game, align your mouse at the center of the mouse pad, turn 180 degrees to the left and return back to the center in one quick motion. If you can do that, you should have a good setup, Mouritzen said.\\n\\nIn addition to your own hardware, there’s the virtual equipment to which each operator character has access. Blitz uses a riot shield that acts as a flashbang, Echo has a quadcopter drone that can disorient people, and Kaid electrifies shields, hatches and barbed wire with his Electroclaw. Understanding how this equipment synergizes with the rest of your team will help you to victory.\\n\\n5. Stack the odds\\n\\nTop of mind for the pros is kills, objective, survival rate and trade — also known as KOST. Some of the terms are obvious, like kills refers to eliminating opponents and the objective is how you approach planting/disarming the bomb or holding an area. Survival rate is about improving your odds at living through an engagement, while trades refer to making opponents pay if they take out one of your teammates. Each of those concepts factors into a player’s decision-making. What this really boils down to, Mouritzen said, is doing whatever it takes to give your team an advantage.\\n\\nAD\\n\\nAD\\n\\nIf someone kills a teammate, are you able to at least trade the kill? Can you bring in another person to help clear a room? How can you leverage a numbers advantage? It all has to do with odds.\\n\\nTaking a 50-50 gunfight might work, but it can just as well cost you the game if you lose. That’s probably not a worthwhile fight to pick. Efficient allocation of resources (teammates, drones, grenades, equipment, etc.) given the situation can change the odds to 80-20, the pros say, and help you confidently take a fight.\\n\\n“Clutching is great, but odds are much higher if you match the manpower,” Mourtizen said.\\n\\nIt’s a team game, after all.\\n\\nRead more from The Post:\\n\\nAD'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Data as Chars\n",
    "\n",
    "# Gather all text \n",
    "# Why? 1. See all possible characters 2. For training / splitting later\n",
    "text = \" \".join(data)\n",
    "\n",
    "# Unique Characters\n",
    "chars = list(set(text))\n",
    "\n",
    "# Lookup Tables\n",
    "char_int = {c:i for i, c in enumerate(chars)} \n",
    "int_char = {i:c for i, c in enumerate(chars)} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences:  178374\n"
     ]
    }
   ],
   "source": [
    "# Create the sequence data\n",
    "\n",
    "maxlen = 40\n",
    "step = 5\n",
    "\n",
    "encoded = [char_int[c] for c in text]\n",
    "\n",
    "sequences = [] # Each element is 40 chars long\n",
    "next_char = [] # One element for each sequence\n",
    "\n",
    "for i in range(0, len(encoded) - maxlen, step):\n",
    "    \n",
    "    sequences.append(encoded[i : i + maxlen])\n",
    "    next_char.append(encoded[i + maxlen])\n",
    "    \n",
    "print('sequences: ', len(sequences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[35,\n",
       " 72,\n",
       " 70,\n",
       " 2,\n",
       " 97,\n",
       " 111,\n",
       " 90,\n",
       " 72,\n",
       " 86,\n",
       " 87,\n",
       " 25,\n",
       " 111,\n",
       " 72,\n",
       " 23,\n",
       " 87,\n",
       " 25,\n",
       " 111,\n",
       " 2,\n",
       " 72,\n",
       " 52,\n",
       " 87,\n",
       " 20,\n",
       " 97,\n",
       " 104,\n",
       " 90,\n",
       " 72,\n",
       " 87,\n",
       " 27,\n",
       " 27,\n",
       " 97,\n",
       " 104,\n",
       " 90,\n",
       " 25,\n",
       " 72,\n",
       " 27,\n",
       " 66,\n",
       " 111,\n",
       " 66,\n",
       " 20,\n",
       " 20]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create x & y\n",
    "\n",
    "x = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sequences),len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, char in enumerate(sequence):\n",
    "        x[i,t,char] = 1\n",
    "        \n",
    "    y[i, next_char[i]] = 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178374, 40, 121)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178374, 121)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model: a single LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / 1\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    \n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "    \n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    \n",
    "    generated = ''\n",
    "    \n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "    \n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "    \n",
    "    for i in range(400):\n",
    "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_int[char]] = 1\n",
    "            \n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds)\n",
    "        next_char = int_char[next_index]\n",
    "        \n",
    "        sentence = sentence[1:] + next_char\n",
    "        \n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print()\n",
    "\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 178374 samples\n",
      "Epoch 1/10\n",
      "178336/178374 [============================>.] - ETA: 0s - loss: 2.5980\n",
      "----- Generating text after Epoch: 0\n",
      "----- Generating with seed: \" for AEW to promote Rose so soon.\n",
      "\n",
      "“Hone\"\n",
      " for AEW to promote Rose so soon.\n",
      "\n",
      "“Hone regne com. Therxeg, Merakor reael caninite antion rase to orwo im onl anabr tours”er lasler mporiseanl ton, cat, caidiff omprally boreco porm Os anden rshiAs, yor henal wiy.\n",
      "\n",
      "Tse beliding touts for:e hes in the ingen ond andibe hes ourkrons win Diterod ferrres “rake, ovimar — Meonctron. the “sheryofasilo Satey to angeest a ha tor Ead binnyseng pou tiy michida An thaming, torear: Inet te tontes, b\n",
      "178374/178374 [==============================] - 211s 1ms/sample - loss: 2.5979\n",
      "Epoch 2/10\n",
      "178336/178374 [============================>.] - ETA: 0s - loss: 2.2455\n",
      "----- Generating text after Epoch: 1\n",
      "----- Generating with seed: \"price transparency. The price isn’t the \"\n",
      "price transparency. The price isn’t the ory of ontised rewacon by mas’obeicititingccak wo the” mecem— broply bo hefins “Plignges to for Meash on thas is b plote cencounting o’s preabectise ackiccties purtionicy’sby wigh Pompes whou the frote-experoun ]ale, of lame, who he mace ans com in ulsesibe metiti— bean in tretsersity acorine. The sutt maminct and with ill. 3ur wato Wtlagt has then to dedicss. In that veriins ane Comeng cop to med\n",
      "178374/178374 [==============================] - 197s 1ms/sample - loss: 2.2455\n",
      "Epoch 3/10\n",
      "178368/178374 [============================>.] - ETA: 0s - loss: 2.1105\n",
      "----- Generating text after Epoch: 2\n",
      "----- Generating with seed: \"t’s often “painfully missing” for those \"\n",
      "t’s often “painfully missing” for those qugsure. Outh whints arout, sreress peessty at courdingeden. Olevony now exsalio Sow wore (Ridcr mach patifting do’s atHinss: Rettly of this indtakint tions. Nit’m leckanov soiro 33 quistor the soted hem has — othen buth flece revident to the white reice the iran astiesting. “Ture veropsy conthion of to mony de resistaliow issurak\n",
      "\n",
      "\n",
      "Os’s St, graincy nom be “Wely end haw atsure by it cheine; so hos\n",
      "178374/178374 [==============================] - 197s 1ms/sample - loss: 2.1106\n",
      "Epoch 4/10\n",
      "178368/178374 [============================>.] - ETA: 0s - loss: 2.0135\n",
      "----- Generating text after Epoch: 3\n",
      "----- Generating with seed: \"kly pushes temperatures from the 40s and\"\n",
      "kly pushes temperatures from the 40s and but sceletive.”\n",
      "\n",
      "“Je a ” Ho lets in frumpen outhules by Gerr a floo-Juin agriet thiep Banter jome appuder.\n",
      "\n",
      "You in the leveins disive that minur-other piaul price for we has corchmanicte; vayd to that chain of countery for marf-outs at a parger Deofory breakely courtered-curceing the Saged, plirectates at us nowstrent a comper arout in the had the a decline eare, I 6picrs oe and ome,” spare dead \n",
      "178374/178374 [==============================] - 198s 1ms/sample - loss: 2.0135\n",
      "Epoch 5/10\n",
      "178368/178374 [============================>.] - ETA: 0s - loss: 1.9421\n",
      "----- Generating text after Epoch: 4\n",
      "----- Generating with seed: \"hich he heckles, belittles and bellows w\"\n",
      "hich he heckles, belittles and bellows wret furmene; (cemran Vixtsic. Neging lat on Lowm Perunged. Whink as mepleds of butr “Peefery sate thes livicy prrmising the Bave a oncessent. I sup coruritino, — anvoldes the reace a beascon puritht hard it benused is asso wanch a terrend brom that’r uness.”\n",
      "\n",
      "Tho-Turbay tare his itl atr tender, owner feces. Nom I and forg’s Scipe list of to signery shainly also bennberny. The Servet, heifinal on t\n",
      "178374/178374 [==============================] - 202s 1ms/sample - loss: 1.9421\n",
      "Epoch 6/10\n",
      "178368/178374 [============================>.] - ETA: 0s - loss: 1.8839\n",
      "----- Generating text after Epoch: 5\n",
      "----- Generating with seed: \"instead to someone with whom he was feud\"\n",
      "instead to someone with whom he was feudblowne we hows white to the ween appoled of the would coolerell and through — yor plosed pareing hoppity peritmation dessent they,” a.4. “THis’s Trump up a moversed yow aly make at the dain had \n",
      "Thel are the posts almand thes Vility kighthers addictes is carthed winding State and end own the giverment the hads looken sliccors hes is tend andwictyly vise plecters way change of apporited, to duem, a\n",
      "178374/178374 [==============================] - 203s 1ms/sample - loss: 1.8839\n",
      "Epoch 7/10\n",
      "178368/178374 [============================>.] - ETA: 0s - loss: 1.8361\n",
      "----- Generating text after Epoch: 6\n",
      "----- Generating with seed: \"d we have prepared this Privacy Policy t\"\n",
      "d we have prepared this Privacy Policy the plotes it descredsur-:\n",
      "\n",
      "“At the cornet Cilamen’s “Scrican Syo, it was send her then auter a siterawed enoll year, defeed the investies to us a but violed collici-ted Billeyran anto Bark)'s for ritonat,” Hentark as allowing” Wejost He said fent puss.”\n",
      "\n",
      "1I6 and from cordis)\n",
      "\n",
      "At Condres and, plasication by-Will eato Jovmers in Wesh before, the tolky happide in Makiswloda EN Conglance S. Mewiz8, in\n",
      "178374/178374 [==============================] - 209s 1ms/sample - loss: 1.8361\n",
      "Epoch 8/10\n",
      "178336/178374 [============================>.] - ETA: 0s - loss: 1.7939\n",
      "----- Generating text after Epoch: 7\n",
      "----- Generating with seed: \"g [it would be that] everyone would eat \"\n",
      "g [it would be that] everyone would eat roal ever cates from cauld in whieed, vaweees — loulimy tard be will vider.\n",
      "\n",
      "The Kurdisully are not that thook, he’re in a lorstor screeply are pusionation, “under of hose pagon of V Himes and more of cloughts.r. whe! lefficive to this arvestion Sulia and otrover wricked, may relignts than its taress to expmently, NFC that whith noble Letus — and Countern for your injubter the brearch.\n",
      "\n",
      "KOlon Mavi\n",
      "178374/178374 [==============================] - 207s 1ms/sample - loss: 1.7940\n",
      "Epoch 9/10\n",
      "178368/178374 [============================>.] - ETA: 0s - loss: 1.7574\n",
      "----- Generating text after Epoch: 8\n",
      "----- Generating with seed: \"cree ordering that process. The traditio\"\n",
      "cree ordering that process. The traditional protent in chrew and 135 out otemen “Sudyat. It prival internot conouse thisine crust but Schooters wrowt out as may hores. V vice mitenaly in justly agaic presidents cricking at and have cling team, cills for Downas ool some in Trump invide thon undally husturing boty “feen’t muny senters arabled out on a moment-fins the jumtienal Supcond Amearcoment, tose fedminctly and left almoby run ho fa\n",
      "178374/178374 [==============================] - 205s 1ms/sample - loss: 1.7574\n",
      "Epoch 10/10\n",
      "178336/178374 [============================>.] - ETA: 0s - loss: 1.7244\n",
      "----- Generating text after Epoch: 9\n",
      "----- Generating with seed: \" kids for the beginning of the school ye\"\n",
      " kids for the beginning of the school yeirled game the drate-decading, which jourted centerizencaty on your dominies. She call, and lawment and find three phecutits atther requires down fog expecisties with this liating in the pusidanalced with “The javent in a Parain Part and Wussis sech on Ressialappes ans call I and “Eusonamis faceedly defendy.\n",
      "\n",
      "The the bedinion.\n",
      "\n",
      "It cain fros pecunation that they weeks Errons and het impeainment, in\n",
      "178374/178374 [==============================] - 213s 1ms/sample - loss: 1.7243\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2f6352367b8>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=32,\n",
    "          epochs=10,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "You will be expected to use a Keras LSTM to generate text on today's assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review\n",
    "\n",
    "- <a href=\"#p1\">Part 1: </a>Describe Neural Networks used for modeling sequences\n",
    "    * Sequence Problems:\n",
    "        - Time Series (like Stock Prices, Weather, etc.)\n",
    "        - Text Classification\n",
    "        - Text Generation\n",
    "        - And many more! :D\n",
    "    * LSTMs are generally preferred over RNNs for most problems\n",
    "    * LSTMs are typically a single hidden layer of LSTM type; although, other architectures are possible.\n",
    "    * Keras has LSTMs/RNN layer types implemented nicely\n",
    "- <a href=\"#p2\">Part 2: </a>Apply a LSTM to a text generation problem using Keras\n",
    "    * Shape of input data is very important\n",
    "    * Can take a while to train\n",
    "    * You can use it to write movie scripts. :P "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_441_RNN_and_LSTM.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "U4-S1-NLP (Python3)",
   "language": "python",
   "name": "u4-s1-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
