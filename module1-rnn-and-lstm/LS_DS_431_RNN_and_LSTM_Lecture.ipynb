{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ldr0HZ193GKb"
   },
   "source": [
    "Lambda School Data Science\n",
    "\n",
    "*Unit 4, Sprint 3, Module 1*\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNNs) and Long Short Term Memory (LSTM) (Prepare)\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/l2JJu8U8SoHhQEnoQ/giphy.gif\" width=480 height=356>\n",
    "<br></br>\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "- <a href=\"#p1\">Part 1: </a>Describe Neural Networks used for modeling sequences\n",
    "- <a href=\"#p2\">Part 2: </a>Apply a LSTM to a text generation problem using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_IizNKWLomoA"
   },
   "source": [
    "## Overview\n",
    "\n",
    "> \"Yesterday's just a memory - tomorrow is never what it's supposed to be.\" -- Bob Dylan\n",
    "\n",
    "Wish you could save [Time In A Bottle](https://www.youtube.com/watch?v=AnWWj6xOleY)? With statistics you can do the next best thing - understand how data varies over time (or any sequential order), and use the order/time dimension predictively.\n",
    "\n",
    "A sequence is just any enumerated collection - order counts, and repetition is allowed. Python lists are a good elemental example - `[1, 2, 2, -1]` is a valid list, and is different from `[1, 2, -1, 2]`. The data structures we tend to use (e.g. NumPy arrays) are often built on this fundamental structure.\n",
    "\n",
    "A time series is data where you have not just the order but some actual continuous marker for where they lie \"in time\" - this could be a date, a timestamp, [Unix time](https://en.wikipedia.org/wiki/Unix_time), or something else. All time series are also sequences, and for some techniques you may just consider their order and not \"how far apart\" the entries are (if you have particularly consistent data collected at regular intervals it may not matter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "44QZgrPUe3-Y"
   },
   "source": [
    "# Neural Networks for Sequences (Learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "44QZgrPUe3-Y"
   },
   "source": [
    "## Overview\n",
    "\n",
    "There's plenty more to \"traditional\" time series, but the latest and greatest technique for sequence data is recurrent neural networks. A recurrence relation in math is an equation that uses recursion to define a sequence - a famous example is the Fibonacci numbers:\n",
    "\n",
    "$F_n = F_{n-1} + F_{n-2}$\n",
    "\n",
    "For formal math you also need a base case $F_0=1, F_1=1$, and then the rest builds from there. But for neural networks what we're really talking about are loops:\n",
    "\n",
    "![Recurrent neural network](https://upload.wikimedia.org/wikipedia/commons/b/b5/Recurrent_neural_network_unfold.svg)\n",
    "\n",
    "The hidden layers have edges (output) going back to their own input - this loop means that for any time `t` the training is at least partly based on the output from time `t-1`. The entire network is being represented on the left, and you can unfold the network explicitly to see how it behaves at any given `t`.\n",
    "\n",
    "Different units can have this \"loop\", but a particularly successful one is the long short-term memory unit (LSTM):\n",
    "\n",
    "![Long short-term memory unit](https://upload.wikimedia.org/wikipedia/commons/thumb/6/63/Long_Short-Term_Memory.svg/1024px-Long_Short-Term_Memory.svg.png)\n",
    "\n",
    "There's a lot going on here - in a nutshell, the calculus still works out and backpropagation can still be implemented. The advantage (ane namesake) of LSTM is that it can generally put more weight on recent (short-term) events while not completely losing older (long-term) information.\n",
    "\n",
    "After enough iterations, a typical neural network will start calculating prior gradients that are so small they effectively become zero - this is the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem), and is what RNN with LSTM addresses. Pay special attention to the $c_t$ parameters and how they pass through the unit to get an intuition for how this problem is solved.\n",
    "\n",
    "So why are these cool? One particularly compelling application is actually not time series but language modeling - language is inherently ordered data (letters/words go one after another, and the order *matters*). [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) is a famous and worth reading blog post on this topic.\n",
    "\n",
    "For our purposes, let's use TensorFlow and Keras to train RNNs with natural language. Resources:\n",
    "\n",
    "- https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py\n",
    "- https://keras.io/layers/recurrent/#lstm\n",
    "- http://adventuresinmachinelearning.com/keras-lstm-tutorial/\n",
    "\n",
    "Note that `tensorflow.contrib` [also has an implementation of RNN/LSTM](https://www.tensorflow.org/tutorials/sequences/recurrent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eWrQllf8WEd-"
   },
   "source": [
    "## Follow Along\n",
    "\n",
    "Sequences come in many shapes and forms from stock prices to text. We'll focus on text, because modeling text as a sequence is a strength of Neural Networks. Let's start with a simple classification task using a TensorFlow tutorial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eWrQllf8WEd-"
   },
   "source": [
    "### RNN/LSTM Sentiment Classification with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 975
    },
    "colab_type": "code",
    "id": "Ti23G0gRe3kr",
    "outputId": "bba9ae40-a286-49ed-d87b-b2946fb60ddf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#Trains an LSTM model on the IMDB sentiment classification task.\n",
    "The dataset is actually too small for LSTM to be of any advantage\n",
    "compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
    "**Notes**\n",
    "- RNNs are tricky. Choice of batch size is important,\n",
    "choice of loss and optimizer is critical, etc.\n",
    "Some configurations won't converge.\n",
    "- LSTM loss decrease patterns during training can be quite different\n",
    "from what you see with CNNs/MLPs/etc.\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "max_features = 20000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 973,\n",
       " 1622,\n",
       " 1385,\n",
       " 65,\n",
       " 458,\n",
       " 4468,\n",
       " 66,\n",
       " 3941,\n",
       " 4,\n",
       " 173,\n",
       " 36,\n",
       " 256,\n",
       " 5,\n",
       " 25,\n",
       " 100,\n",
       " 43,\n",
       " 838,\n",
       " 112,\n",
       " 50,\n",
       " 670,\n",
       " 2,\n",
       " 9,\n",
       " 35,\n",
       " 480,\n",
       " 284,\n",
       " 5,\n",
       " 150,\n",
       " 4,\n",
       " 172,\n",
       " 112,\n",
       " 167,\n",
       " 2,\n",
       " 336,\n",
       " 385,\n",
       " 39,\n",
       " 4,\n",
       " 172,\n",
       " 4536,\n",
       " 1111,\n",
       " 17,\n",
       " 546,\n",
       " 38,\n",
       " 13,\n",
       " 447,\n",
       " 4,\n",
       " 192,\n",
       " 50,\n",
       " 16,\n",
       " 6,\n",
       " 147,\n",
       " 2025,\n",
       " 19,\n",
       " 14,\n",
       " 22,\n",
       " 4,\n",
       " 1920,\n",
       " 4613,\n",
       " 469,\n",
       " 4,\n",
       " 22,\n",
       " 71,\n",
       " 87,\n",
       " 12,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 38,\n",
       " 76,\n",
       " 15,\n",
       " 13,\n",
       " 1247,\n",
       " 4,\n",
       " 22,\n",
       " 17,\n",
       " 515,\n",
       " 17,\n",
       " 12,\n",
       " 16,\n",
       " 626,\n",
       " 18,\n",
       " 19193,\n",
       " 5,\n",
       " 62,\n",
       " 386,\n",
       " 12,\n",
       " 8,\n",
       " 316,\n",
       " 8,\n",
       " 106,\n",
       " 5,\n",
       " 4,\n",
       " 2223,\n",
       " 5244,\n",
       " 16,\n",
       " 480,\n",
       " 66,\n",
       " 3785,\n",
       " 33,\n",
       " 4,\n",
       " 130,\n",
       " 12,\n",
       " 16,\n",
       " 38,\n",
       " 619,\n",
       " 5,\n",
       " 25,\n",
       " 124,\n",
       " 51,\n",
       " 36,\n",
       " 135,\n",
       " 48,\n",
       " 25,\n",
       " 1415,\n",
       " 33,\n",
       " 6,\n",
       " 22,\n",
       " 12,\n",
       " 215,\n",
       " 28,\n",
       " 77,\n",
       " 52,\n",
       " 5,\n",
       " 14,\n",
       " 407,\n",
       " 16,\n",
       " 82,\n",
       " 10311,\n",
       " 8,\n",
       " 4,\n",
       " 107,\n",
       " 117,\n",
       " 5952,\n",
       " 15,\n",
       " 256,\n",
       " 4,\n",
       " 2,\n",
       " 7,\n",
       " 3766,\n",
       " 5,\n",
       " 723,\n",
       " 36,\n",
       " 71,\n",
       " 43,\n",
       " 530,\n",
       " 476,\n",
       " 26,\n",
       " 400,\n",
       " 317,\n",
       " 46,\n",
       " 7,\n",
       " 4,\n",
       " 12118,\n",
       " 1029,\n",
       " 13,\n",
       " 104,\n",
       " 88,\n",
       " 4,\n",
       " 381,\n",
       " 15,\n",
       " 297,\n",
       " 98,\n",
       " 32,\n",
       " 2071,\n",
       " 56,\n",
       " 26,\n",
       " 141,\n",
       " 6,\n",
       " 194,\n",
       " 7486,\n",
       " 18,\n",
       " 4,\n",
       " 226,\n",
       " 22,\n",
       " 21,\n",
       " 134,\n",
       " 476,\n",
       " 26,\n",
       " 480,\n",
       " 5,\n",
       " 144,\n",
       " 30,\n",
       " 5535,\n",
       " 18,\n",
       " 51,\n",
       " 36,\n",
       " 28,\n",
       " 224,\n",
       " 92,\n",
       " 25,\n",
       " 104,\n",
       " 4,\n",
       " 226,\n",
       " 65,\n",
       " 16,\n",
       " 38,\n",
       " 1334,\n",
       " 88,\n",
       " 12,\n",
       " 16,\n",
       " 283,\n",
       " 5,\n",
       " 16,\n",
       " 4472,\n",
       " 113,\n",
       " 103,\n",
       " 32,\n",
       " 15,\n",
       " 16,\n",
       " 5345,\n",
       " 19,\n",
       " 178,\n",
       " 32]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad Sequences (samples x time)\n",
      "x_train shape:  (25000, 80)\n",
      "x_test shape:  (25000, 80)\n"
     ]
    }
   ],
   "source": [
    "print('Pad Sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape: ', x_train.shape)\n",
    "print('x_test shape: ', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   15,   256,     4,     2,     7,  3766,     5,   723,    36,\n",
       "          71,    43,   530,   476,    26,   400,   317,    46,     7,\n",
       "           4, 12118,  1029,    13,   104,    88,     4,   381,    15,\n",
       "         297,    98,    32,  2071,    56,    26,   141,     6,   194,\n",
       "        7486,    18,     4,   226,    22,    21,   134,   476,    26,\n",
       "         480,     5,   144,    30,  5535,    18,    51,    36,    28,\n",
       "         224,    92,    25,   104,     4,   226,    65,    16,    38,\n",
       "        1334,    88,    12,    16,   283,     5,    16,  4472,   113,\n",
       "         103,    32,    15,    16,  5345,    19,   178,    32])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 128)         2560000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,691,713\n",
      "Trainable params: 2,691,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 182s 7ms/sample - loss: 0.4537 - accuracy: 0.7834 - val_loss: 0.4101 - val_accuracy: 0.8145\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 187s 7ms/sample - loss: 0.3023 - accuracy: 0.8762 - val_loss: 0.3814 - val_accuracy: 0.8354\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 196s 8ms/sample - loss: 0.2275 - accuracy: 0.9122 - val_loss: 0.4420 - val_accuracy: 0.8292\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "             batch_size=batch_size,\n",
    "             epochs=3,\n",
    "             validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train','Test'], loc='upper left')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.45373016382217407, 0.3023112851047516, 0.2274654632282257],\n",
       " 'accuracy': [0.78344, 0.87624, 0.91224],\n",
       " 'val_loss': [0.41013296463012694, 0.3813837223148346, 0.4419766416978836],\n",
       " 'val_accuracy': [0.81452, 0.83544, 0.82916]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "You will be expected to use an Keras LSTM for a classicification task on the *Sprint Challenge*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7pETWPIe362y"
   },
   "source": [
    "# LSTM Text generation with Keras (Learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7pETWPIe362y"
   },
   "source": [
    "## Overview\n",
    "\n",
    "What else can we do with LSTMs? Since we're analyzing the *sequence*, we can do more than classify - we can *generate* text. I'ved pulled some news stories using [newspaper](https://github.com/codelucas/newspaper/).\n",
    "\n",
    "This example is drawn from the Keras [documentation](https://keras.io/examples/lstm_text_generation/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = os.listdir('./articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Data\n",
    "\n",
    "data = []\n",
    "\n",
    "for file in data_files:\n",
    "    if file[-3:] == 'txt':\n",
    "        with open(f'./articles/{file}', 'r', encoding='utf-8') as f:\n",
    "            data.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here’s their advice to upgrade your game:\\n\\n1. Be quiet and listen\\n\\nRegistering and understanding noise is a huge key to helping you win. If you listen closely enough, you can predict what the enemy will do. Likewise, manage your own noise so you don’t make your movements so obvious.\\n\\nAD\\n\\nUbisoft developed its own sound propagation system to make the game as realistic as possible. In typical games, you’ll hear a noise from an adjacent room, but it’s muffled. In Siege, noise travels from person to person through space in the shortest way possible, bouncing off walls and entering through doorways.\\n\\nAD\\n\\nNiclas “Pengu” Mouritzen, flex player for the G2 Esports Siege team, said managing noise is something he and his teammates constantly work on. For example, jumping with your drone is quite loud, making it easy for enemies to track it down and destroy it. Mouritzen cautioned to only jump away if your drone is in danger of being destroyed.\\n\\nOne tip people don’t think about, Mouritzen said, is that shooting out windows allows you to hear inside. Sound won’t filter through the room unless the window is destroyed. It’s the small things that are make or break in a realistic game like Siege.\\n\\nAD\\n\\n2. Learn the maps until you can see them blindfolded\\n\\nRainbow Six Siege features highly dynamic, destructible, multilevel maps. You can get shot from just about anywhere, which can make it frustrating to play for beginners.\\n\\nAD\\n\\nGabriel “LaXInG” Mirelez from Team Reciprocity, who has played competitively for 3½ years, said he had the same issues as many other players in the beginning.\\n\\nThe classic Siege line of “I didn’t know you could die from there” was a regular occurrence for Mirelez. But the difference between him and many others, he said, was the drive to improve.\\n\\n“If you want to improve, you really have to want it,” Mirelez said. Doing the same thing over and over again isn’t going to cut it in a game like Siege.\\n\\nAD\\n\\nHe recommends watching professional gameplay and high-level streamers to understand the best angles to take in a map. Keep yourself protected as much as possible while giving yourself the best angle to scope your enemies.\\n\\n3. Equip the right scope and attachments to fit the occasion\\n\\nPart of finding success in Siege is knowing which operator fits your situation and play style. More than that, you’ll need to figure out which scopes to use when you are defending or attacking.\\n\\nAD\\n\\nThe game narrows down the scopes you can use in particular situations. Attackers and only a few defenders have the option of using ACOG, a scope with 2x magnification, but selecting it all the time isn’t the best option. It forces you to hold and angle and rely on the enemy to make a mistake.\\n\\nAD\\n\\nBut in close-quarters combat, Mouritzen said, 1x sights, such as reflex and red dot, are king. They let you take things into your own hands and improve upon fragging (kills). However, if you are playing a support operator such as Thermite, it’s best to hang back and hold down an angle with an ACOG sight.\\n\\n4. Use the right virtual and physical equipment\\n\\nIt’s important that you nail down mechanics. In Siege, you’ll need to be able to do a 180 on a dime and shoot first.\\n\\nYour mouse is the key to it all. It’s important to find a mouse that fits your hand comfortably. That won’t necessarily be the one that the pros are using, Mouritzen said. Sometimes they’re using a mouse that’s part of a sponsorship deal, so it may not work for you.\\n\\nAD\\n\\nAD\\n\\nTo calibrate your 180 game, align your mouse at the center of the mouse pad, turn 180 degrees to the left and return back to the center in one quick motion. If you can do that, you should have a good setup, Mouritzen said.\\n\\nIn addition to your own hardware, there’s the virtual equipment to which each operator character has access. Blitz uses a riot shield that acts as a flashbang, Echo has a quadcopter drone that can disorient people, and Kaid electrifies shields, hatches and barbed wire with his Electroclaw. Understanding how this equipment synergizes with the rest of your team will help you to victory.\\n\\n5. Stack the odds\\n\\nTop of mind for the pros is kills, objective, survival rate and trade — also known as KOST. Some of the terms are obvious, like kills refers to eliminating opponents and the objective is how you approach planting/disarming the bomb or holding an area. Survival rate is about improving your odds at living through an engagement, while trades refer to making opponents pay if they take out one of your teammates. Each of those concepts factors into a player’s decision-making. What this really boils down to, Mouritzen said, is doing whatever it takes to give your team an advantage.\\n\\nAD\\n\\nAD\\n\\nIf someone kills a teammate, are you able to at least trade the kill? Can you bring in another person to help clear a room? How can you leverage a numbers advantage? It all has to do with odds.\\n\\nTaking a 50-50 gunfight might work, but it can just as well cost you the game if you lose. That’s probably not a worthwhile fight to pick. Efficient allocation of resources (teammates, drones, grenades, equipment, etc.) given the situation can change the odds to 80-20, the pros say, and help you confidently take a fight.\\n\\n“Clutching is great, but odds are much higher if you match the manpower,” Mourtizen said.\\n\\nIt’s a team game, after all.\\n\\nRead more from The Post:\\n\\nAD'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Data as Chars\n",
    "\n",
    "# Gather all text\n",
    "# Why? 1. See all possible characters. 2. For training / splitting later\n",
    "\n",
    "text = \" \".join(data)\n",
    "\n",
    "# Unique Characters\n",
    "chars = list(set(text))\n",
    "\n",
    "# Lookup Tables\n",
    "char_int = {c:i for i, c in enumerate(chars)}\n",
    "int_char = {i:c for i, c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences:  178374\n"
     ]
    }
   ],
   "source": [
    "# Create the sequence data\n",
    "\n",
    "maxlen = 40\n",
    "step = 5\n",
    "\n",
    "encoded = [char_int[c] for c in text]\n",
    "\n",
    "sequences = [] # Each element is 40 chars long\n",
    "next_char = [] # One element for each sequence\n",
    "\n",
    "for i in range(0, len(encoded) - maxlen, step):\n",
    "    \n",
    "    sequences.append(encoded[i : i + maxlen])\n",
    "    next_char.append(encoded[i + maxlen])\n",
    "    \n",
    "print('sequences: ', len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 101,\n",
       " 99,\n",
       " 98,\n",
       " 70,\n",
       " 79,\n",
       " 54,\n",
       " 101,\n",
       " 93,\n",
       " 62,\n",
       " 116,\n",
       " 79,\n",
       " 101,\n",
       " 108,\n",
       " 62,\n",
       " 116,\n",
       " 79,\n",
       " 98,\n",
       " 101,\n",
       " 63,\n",
       " 62,\n",
       " 3,\n",
       " 70,\n",
       " 56,\n",
       " 54,\n",
       " 101,\n",
       " 62,\n",
       " 75,\n",
       " 75,\n",
       " 70,\n",
       " 56,\n",
       " 54,\n",
       " 116,\n",
       " 101,\n",
       " 75,\n",
       " 114,\n",
       " 79,\n",
       " 114,\n",
       " 3,\n",
       " 3]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create x & y\n",
    "\n",
    "x = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sequences), len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, char in enumerate(sequence):\n",
    "        x[i,t,char] = 1\n",
    "        \n",
    "    y[i, next_char[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178374, 40, 121)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178374, 121)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 128)               128000    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 121)               15609     \n",
      "=================================================================\n",
      "Total params: 143,609\n",
      "Trainable params: 143,609\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build the model: a single LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / 1\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    \n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "    \n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    \n",
    "    generated = ''\n",
    "    \n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "    \n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "    \n",
    "    for i in range(400):\n",
    "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_int[char]] = 1\n",
    "            \n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds)\n",
    "        next_char = int_char[next_index]\n",
    "        \n",
    "        sentence = sentence[1:] + next_char\n",
    "        \n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print()\n",
    "\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 178374 samples\n",
      "Epoch 1/3\n",
      "178368/178374 [============================>.] - ETA: 0s - loss: 2.5964\n",
      "----- Generating text after Epoch: 0\n",
      "----- Generating with seed: \"tripping me all the time, because it wor\"\n",
      "tripping me all the time, because it word Wangly atfurig ao sulsallnte sonstrprikere of axs rserinss boremed-kised tont hed atuluspregtana’s a ata powd of andane tion timm haseute to Jer anRS, Wiund ercistmarentis Sopray blestare ros herticanor wore steresis to theV Parda.\n",
      "\n",
      "AThrents, orsintt cemilsowing tsedengasn y ips groe sareshatris on thajis forepcropist onsulit atsat ant Sorly on tresing to ply9ur, ant wy itton Cqurceisn neok y at\n",
      "178374/178374 [==============================] - 398s 2ms/sample - loss: 2.5963\n",
      "Epoch 2/3\n",
      "178368/178374 [============================>.] - ETA: 0s - loss: 2.2426\n",
      "----- Generating text after Epoch: 1\n",
      "----- Generating with seed: \"ood for the emotional state, as well.”\n",
      "\n",
      "\"\n",
      "ood for the emotional state, as well.”\n",
      "\n",
      "It a decen-teon mofing insairva anredes and anking “iy omed appayitrole thet whise lowgenify tien wyrristian, chove. Sebeatring vexe procened a fow to bage it aply rimas mond wo dercestion.\n",
      "\n",
      "Leleang paldouing rewouside. “Ilye corvipts.\n",
      "\n",
      "Be Washas of felion’nly the averacian the paiche cureatio, as cace conk starmatien al ofer forkizy of thme the lation Mup.\n",
      "\n",
      "As is Sexows. cardistsiigas. A9 you rea\n",
      "178374/178374 [==============================] - 384s 2ms/sample - loss: 2.2426\n",
      "Epoch 3/3\n",
      "178368/178374 [============================>.] - ETA: 0s - loss: 2.1035\n",
      "----- Generating text after Epoch: 2\n",
      "----- Generating with seed: \"a formula my daughter took, but those tw\"\n",
      "a formula my daughter took, but those twe candayios tor insemmute, the toriswew ho dantiennentmy, in fahiot the streices ofling, a cail of leable libla event] who dattubs or Simscreath 12 kisqugel is mall chece pay of enverationers in mellicismons ro play, agry be Gry the Jrovide ave to maning not Bos youn Jets the ? liffony:\n",
      "\n",
      "Mndaghan “eis riple an um adewed bric arored we smopie sus coufr with buring as nith has relidenss rus reges io\n",
      "178374/178374 [==============================] - 304s 2ms/sample - loss: 2.1035\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x265a33985c0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "\n",
    "model.fit(x, y,\n",
    "         batch_size=32,\n",
    "         epochs=3,\n",
    "         callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "You will be expected to use a Keras LSTM to generate text on today's assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review\n",
    "\n",
    "- <a href=\"#p1\">Part 1: </a>Describe Neural Networks used for modeling sequences\n",
    "    * Sequence Problems:\n",
    "        - Time Series (like Stock Prices, Weather, etc.)\n",
    "        - Text Classification\n",
    "        - Text Generation\n",
    "        - And many more! :D\n",
    "    * LSTMs are generally preferred over RNNs for most problems\n",
    "    * LSTMs are typically a single hidden layer of LSTM type; although, other architectures are possible.\n",
    "    * Keras has LSTMs/RNN layer types implemented nicely\n",
    "- <a href=\"#p2\">Part 2: </a>Apply a LSTM to a text generation problem using Keras\n",
    "    * Shape of input data is very important\n",
    "    * Can take a while to train\n",
    "    * You can use it to write movie scripts. :P "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_441_RNN_and_LSTM.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "U4-S2-NN (Python3)",
   "language": "python",
   "name": "u4-s2-nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
