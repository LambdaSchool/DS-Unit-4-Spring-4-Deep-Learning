{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_IizNKWLomoA"
   },
   "source": [
    "# Lambda School Data Science - Recurrent Neural Networks and LSTM\n",
    "\n",
    "> \"Yesterday's just a memory - tomorrow is never what it's supposed to be.\" -- Bob Dylan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0EZdBzC6pvV9"
   },
   "source": [
    "# Lecture\n",
    "\n",
    "Wish you could save [Time In A Bottle](https://www.youtube.com/watch?v=AnWWj6xOleY)? With statistics you can do the next best thing - understand how data varies over time (or any sequential order), and use the order/time dimension predictively.\n",
    "\n",
    "A sequence is just any enumerated collection - order counts, and repetition is allowed. Python lists are a good elemental example - `[1, 2, 2, -1]` is a valid list, and is different from `[1, 2, -1, 2]`. The data structures we tend to use (e.g. NumPy arrays) are often built on this fundamental structure.\n",
    "\n",
    "A time series is data where you have not just the order but some actual continuous marker for where they lie \"in time\" - this could be a date, a timestamp, [Unix time](https://en.wikipedia.org/wiki/Unix_time), or something else. All time series are also sequences, and for some techniques you may just consider their order and not \"how far apart\" the entries are (if you have particularly consistent data collected at regular intervals it may not matter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5_m0hJ4uCzHz"
   },
   "source": [
    "## Time series with plain old regression\n",
    "\n",
    "Recurrences are fancy, and we'll get to those later - let's start with something simple. Regression can handle time series just fine if you just set them up correctly - let's try some made-up stock data. And to make it, let's use a few list comprehensions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GkJUFfsgnqr_"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import random\n",
    "days = np.array((range(28)))\n",
    "stock_quotes = np.array([random() + day * random() for day in days])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "y-ORgKGNBOcb",
    "outputId": "08ce376c-9024-4859-f082-66e47e8ea566"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.76001604,  1.45882769,  1.74547876,  1.11992758,  2.69610346,\n",
       "        4.66273546,  2.3178356 ,  7.55874568,  7.38607368,  9.08587668,\n",
       "        5.30058604,  8.46612319,  9.26538429, 12.22636896,  9.00323112,\n",
       "        7.80507892,  6.75136691, 12.18484407,  1.85504671, 13.47322954,\n",
       "        7.41682153,  6.73714304, 14.80303061,  4.1454212 , 23.59650768,\n",
       "        4.18662332, 12.67678725, 23.42340423])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X3lR2wGvBx3a"
   },
   "source": [
    "Let's take a look with a scatter plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "id": "pVUTC2tmBSIq",
    "outputId": "cc58a81d-f022-48f9-eaaf-df899f5bf88f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f83ef906e48>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matplotlib.pyplot import scatter\n",
    "scatter(days, stock_quotes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hgD4q-T_B0jd"
   },
   "source": [
    "Looks pretty linear, let's try a simple OLS regression.\n",
    "\n",
    "First, these need to be NumPy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A3Q0MrnUBXAl"
   },
   "outputs": [],
   "source": [
    "days = days.reshape(-1, 1)  # X needs to be column vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vqr0SHOnB5yR"
   },
   "source": [
    "Now let's use good old `scikit-learn` and linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PqyHxgFvBYl5",
    "outputId": "360ac69c-518f-481c-b47e-4fbd01f00e98"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42540016010880133"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "ols_stocks = LinearRegression()\n",
    "ols_stocks.fit(days, stock_quotes)\n",
    "ols_stocks.score(days, stock_quotes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KlU0mr-KB_Yk"
   },
   "source": [
    "That seems to work pretty well, but real stocks don't work like this.\n",
    "\n",
    "Let's make *slightly* more realistic data that depends on more than just time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-FV1Emb2BuLz"
   },
   "outputs": [],
   "source": [
    "# Not everything is best as a comprehension\n",
    "stock_data = np.empty([len(days), 4])\n",
    "for day in days:\n",
    "  asset = random()\n",
    "  liability = random()\n",
    "  quote = random() + ((day * random()) + (20 * asset) - (15 * liability))\n",
    "  quote = max(quote, 0.01)  # Want positive quotes\n",
    "  stock_data[day] = np.array([quote, day, asset, liability])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 503
    },
    "colab_type": "code",
    "id": "6Qe2zzN1CESe",
    "outputId": "49d5970a-5300-45c0-b738-d1f629832223"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.08152915e+00, 0.00000000e+00, 9.12721091e-01, 9.23883911e-01],\n",
       "       [1.00000000e-02, 1.00000000e+00, 4.19077787e-02, 9.46805492e-01],\n",
       "       [9.55878187e+00, 2.00000000e+00, 5.41948797e-01, 1.47125335e-01],\n",
       "       [5.19473286e+00, 3.00000000e+00, 8.70801950e-01, 9.80978013e-01],\n",
       "       [1.00000000e-02, 4.00000000e+00, 1.50703083e-01, 9.57281066e-01],\n",
       "       [1.80055247e+01, 5.00000000e+00, 8.89495193e-01, 3.04039675e-01],\n",
       "       [1.00000000e-02, 6.00000000e+00, 8.00806102e-02, 3.66047133e-01],\n",
       "       [1.38800657e+01, 7.00000000e+00, 9.79911416e-01, 8.99740388e-01],\n",
       "       [1.67513321e+00, 8.00000000e+00, 1.60752035e-01, 2.49097169e-01],\n",
       "       [9.41833030e+00, 9.00000000e+00, 6.72115843e-01, 3.95996894e-01],\n",
       "       [3.27339558e+00, 1.00000000e+01, 5.73675879e-01, 9.96822376e-01],\n",
       "       [6.93575571e+00, 1.10000000e+01, 6.88029224e-01, 7.25793922e-01],\n",
       "       [2.18917277e+00, 1.20000000e+01, 5.55929910e-01, 9.68052627e-01],\n",
       "       [1.71500193e+01, 1.30000000e+01, 3.51908878e-01, 1.35253999e-01],\n",
       "       [4.05472064e+00, 1.40000000e+01, 4.72280589e-01, 5.33028210e-01],\n",
       "       [1.05440670e+01, 1.50000000e+01, 7.14203972e-02, 3.37718114e-01],\n",
       "       [8.72321193e+00, 1.60000000e+01, 3.37367144e-01, 7.37265841e-01],\n",
       "       [1.33158443e+01, 1.70000000e+01, 5.43618174e-01, 7.88672312e-02],\n",
       "       [1.13776442e+01, 1.80000000e+01, 7.65593605e-01, 6.84863441e-01],\n",
       "       [4.44486926e+00, 1.90000000e+01, 4.24058029e-01, 3.30867126e-01],\n",
       "       [1.80401041e+01, 2.00000000e+01, 5.08467161e-01, 5.30072883e-01],\n",
       "       [1.72702268e+01, 2.10000000e+01, 9.96585624e-02, 3.66454410e-01],\n",
       "       [1.62699103e+01, 2.20000000e+01, 9.90563266e-01, 3.80594119e-01],\n",
       "       [1.52799216e+01, 2.30000000e+01, 3.05631007e-01, 4.05570679e-02],\n",
       "       [1.92105953e+01, 2.40000000e+01, 7.84641467e-01, 8.38643736e-01],\n",
       "       [1.06457346e+01, 2.50000000e+01, 1.66198164e-01, 5.95459362e-01],\n",
       "       [1.06360213e+01, 2.60000000e+01, 3.68810452e-01, 9.27413621e-01],\n",
       "       [3.19494463e+01, 2.70000000e+01, 7.40076566e-01, 1.70509961e-01]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BzYy4Pb2CLCh"
   },
   "source": [
    "Let's look again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "id": "qdBcScz4CIXr",
    "outputId": "9b0f2608-a0e2-4370-b0d0-5bfe76148071"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f83b9a8aef0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stock_quotes = stock_data[:,0]\n",
    "scatter(days, stock_quotes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SBXb7dieCO5h"
   },
   "source": [
    "How does our old model do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7gAxCgy1COnX",
    "outputId": "abecf71b-af70-49bb-ee2c-da0d1e3a7a30"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.374717695829848"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "days = np.array(days).reshape(-1, 1)\n",
    "ols_stocks.fit(days, stock_quotes)\n",
    "ols_stocks.score(days, stock_quotes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3E94vTFUCax_"
   },
   "source": [
    "Not bad, but can we do better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mCR5GImZCbGz",
    "outputId": "213459d5-28ef-4995-f6ee-771ea1459ec2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.660713554520906"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ols_stocks.fit(stock_data[:,1:], stock_quotes)\n",
    "ols_stocks.score(stock_data[:,1:], stock_quotes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Qk-jlBCCiKB"
   },
   "source": [
    "Yep - unsurprisingly, the other covariates (assets and liabilities) have info.\n",
    "\n",
    "But, they do worse without the day data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dDcZl7I5Cf5D",
    "outputId": "c6a94e00-081a-4b16-ac0d-faa097479a63"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3960228316864568"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ols_stocks.fit(stock_data[:,2:], stock_quotes)\n",
    "ols_stocks.score(stock_data[:,2:], stock_quotes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pnLXlrK8ENjb"
   },
   "source": [
    "## Time series jargon\n",
    "\n",
    "There's a lot of semi-standard language and tricks to talk about this sort of data. [NIST](https://www.itl.nist.gov/div898/handbook/pmc/section4/pmc4.htm) has an excellent guidebook, but here are some highlights:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yWUyhnTbcq55"
   },
   "source": [
    "### Moving average\n",
    "\n",
    "Moving average aka rolling average aka running average.\n",
    "\n",
    "Convert a series of data to a series of averages of continguous subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "colab_type": "code",
    "id": "47bHhBSCcvw-",
    "outputId": "686a9e81-dc0b-4fae-d1ff-9dd00dd125a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.883437008000627,\n",
       " 4.921171578556723,\n",
       " 4.921171578556723,\n",
       " 7.736752526035237,\n",
       " 6.008508239038199,\n",
       " 10.63186348210686,\n",
       " 5.188399644812925,\n",
       " 8.324509743597597,\n",
       " 4.788953028668931,\n",
       " 6.542493862999734,\n",
       " 4.132774688058711,\n",
       " 8.758315911655343,\n",
       " 7.79797088753756,\n",
       " 10.582935637825756,\n",
       " 7.773999864351396,\n",
       " 10.86104110198022,\n",
       " 11.138900156468019,\n",
       " 9.712785930294494,\n",
       " 11.287539179898806,\n",
       " 13.25173339919443,\n",
       " 17.193413740316362,\n",
       " 16.273352914967592,\n",
       " 16.920142406325528,\n",
       " 15.045417169549488,\n",
       " 13.497450386725264,\n",
       " 17.74373405317829]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_quotes_rolling = [sum(stock_quotes[i:i+3]) / 3\n",
    "                        for i in range(len(stock_quotes) - 2)]\n",
    "stock_quotes_rolling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "36XvbGhoc186"
   },
   "source": [
    "Pandas has nice series related functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 928
    },
    "colab_type": "code",
    "id": "nTNatxtycys_",
    "outputId": "82b3df95-c1d4-4dc5-f17c-e7ce2366d3ec"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.883437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.921172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.921172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7.736753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.008508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10.631863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5.188400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8.324510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4.788953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6.542494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.132775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>8.758316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7.797971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10.582936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>7.774000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10.861041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>11.138900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>9.712786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>11.287539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>13.251733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>17.193414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>16.273353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>16.920142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>15.045417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>13.497450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>17.743734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "0         NaN\n",
       "1         NaN\n",
       "2    4.883437\n",
       "3    4.921172\n",
       "4    4.921172\n",
       "5    7.736753\n",
       "6    6.008508\n",
       "7   10.631863\n",
       "8    5.188400\n",
       "9    8.324510\n",
       "10   4.788953\n",
       "11   6.542494\n",
       "12   4.132775\n",
       "13   8.758316\n",
       "14   7.797971\n",
       "15  10.582936\n",
       "16   7.774000\n",
       "17  10.861041\n",
       "18  11.138900\n",
       "19   9.712786\n",
       "20  11.287539\n",
       "21  13.251733\n",
       "22  17.193414\n",
       "23  16.273353\n",
       "24  16.920142\n",
       "25  15.045417\n",
       "26  13.497450\n",
       "27  17.743734"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(stock_quotes)\n",
    "df.rolling(3).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "os-szg47dgwf"
   },
   "source": [
    "### Forecasting\n",
    "\n",
    "Forecasting - at it's simplest, it just means \"predict the future\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "D_qtt6irdj0x",
    "outputId": "60435011-9bc0-4e79-98de-0babe5dab5a9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18.11506103])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ols_stocks.fit(stock_data[:,1:], stock_quotes)\n",
    "ols_stocks.predict([[29, 0.5, 0.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fjnQY0trdnHp"
   },
   "source": [
    "One way to predict if you just have the series data is to use the prior observation. This can be pretty good (if you had to pick one feature to model the temperature for tomorrow, the temperature today is a good choice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bzC4DV9Hdupp",
    "outputId": "07bdf701-b994-40ee-c796-e9b5339808b2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11756517796395227"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature = np.array([30 + random() * day\n",
    "                        for day in np.array(range(28)).reshape(-1, 1)])\n",
    "temperature_next = temperature[1:].reshape(-1, 1)\n",
    "temperature_ols = LinearRegression()\n",
    "temperature_ols.fit(temperature[:-1], temperature_next)\n",
    "temperature_ols.score(temperature[:-1], temperature_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RFdssXQbdxbE"
   },
   "source": [
    "But you can often make it better by considering more than one prior observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pVfUqD2YdxxZ",
    "outputId": "92cc22ef-5d0f-4381-efcb-176abb061195"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11448361841675059"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature_next_next = temperature[2:].reshape(-1, 1)\n",
    "temperature_two_past = np.concatenate([temperature[:-2], temperature_next[:-1]],\n",
    "                                      axis=1)\n",
    "temperature_ols.fit(temperature_two_past, temperature_next_next)\n",
    "temperature_ols.score(temperature_two_past, temperature_next_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c9QltBdmd7TV"
   },
   "source": [
    "### Exponential smoothing\n",
    "\n",
    "Exponential smoothing means using exponentially decreasing past weights to predict the future.\n",
    "\n",
    "You could roll your own, but let's use Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 503
    },
    "colab_type": "code",
    "id": "6_EUtcn9xjrz",
    "outputId": "9c6a796a-2a78-4a50-c24d-aa8ab48ee0c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[30.        ],\n",
       "       [30.19074213],\n",
       "       [30.42719873],\n",
       "       [32.76613061],\n",
       "       [33.5655971 ],\n",
       "       [31.03874091],\n",
       "       [33.22898956],\n",
       "       [34.33722183],\n",
       "       [33.85107674],\n",
       "       [32.42830171],\n",
       "       [36.27619366],\n",
       "       [35.67770434],\n",
       "       [38.62630432],\n",
       "       [33.42869466],\n",
       "       [39.75303746],\n",
       "       [41.26153831],\n",
       "       [30.62460646],\n",
       "       [35.16231184],\n",
       "       [30.99002608],\n",
       "       [46.77023057],\n",
       "       [49.53562231],\n",
       "       [38.60188462],\n",
       "       [30.34488197],\n",
       "       [37.13036087],\n",
       "       [49.77193461],\n",
       "       [37.93657093],\n",
       "       [40.88782256],\n",
       "       [37.40246578]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 928
    },
    "colab_type": "code",
    "id": "hvMNqunOeC_B",
    "outputId": "f21e3384-8387-4b6a-91a8-e6818bd20424"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30.118190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30.544019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31.141053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31.191351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>31.236804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>31.540851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>31.618205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>31.672709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>32.496971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>33.179881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>33.207566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>33.987530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>33.546405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>33.455421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>33.068284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>33.826905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>33.562736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>34.047400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>34.818294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>34.584078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>36.327234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>36.357693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>35.957576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>37.732050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>38.993510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>40.158634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>40.264317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "0   30.000000\n",
       "1   30.118190\n",
       "2   30.544019\n",
       "3   31.141053\n",
       "4   31.191351\n",
       "5   31.236804\n",
       "6   31.540851\n",
       "7   31.618205\n",
       "8   31.672709\n",
       "9   32.496971\n",
       "10  33.179881\n",
       "11  33.207566\n",
       "12  33.987530\n",
       "13  33.546405\n",
       "14  33.455421\n",
       "15  33.068284\n",
       "16  33.826905\n",
       "17  33.562736\n",
       "18  34.047400\n",
       "19  34.818294\n",
       "20  34.584078\n",
       "21  36.327234\n",
       "22  36.357693\n",
       "23  35.957576\n",
       "24  37.732050\n",
       "25  38.993510\n",
       "26  40.158634\n",
       "27  40.264317"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature_df = pd.DataFrame(temperature)\n",
    "temperature_df.ewm(halflife=7).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gBEjBZVbeH6R"
   },
   "source": [
    "Halflife is among the parameters we can play with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "HjZgMwYkeODN",
    "outputId": "3f7caa0c-ab2a-4f49-ee98-f757536501f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    881.683236\n",
      "dtype: float64\n",
      "0    595.455064\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "sse_1 = ((temperature_df - temperature_df.ewm(halflife=7).mean())**2).sum()\n",
    "sse_2 = ((temperature_df - temperature_df.ewm(halflife=3).mean())**2).sum()\n",
    "print(sse_1)\n",
    "print(sse_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s39bj4g9eQ9Z"
   },
   "source": [
    "Note - the first error being higher doesn't mean it's necessarily *worse*. It's *smoother* as expected, and if that's what we care about - great!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OcPMn8o4eYP1"
   },
   "source": [
    "### Seasonality\n",
    "\n",
    "Seasonality - \"day of week\"-effects, and more. In a lot of real world data, certain time periods are systemically different, e.g. holidays for retailers, weekends for restaurants, seasons for weather.\n",
    "\n",
    "Let's try to make some seasonal data - a store that sells more later in a week:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "id": "h0qPMWCreheL",
    "outputId": "b7ae393f-facf-44b0-c27f-adda1632564e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f09c9fa34a8>"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAFKCAYAAAAqkecjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3X9slfX99/FXe9oeLD0t59BzSrdu\nGSnU8WPeq8oUCa10rSLZMhHzPdj4Y5EsLqDOjc0Q3IRE/AFrjMqMaJUsGfuykxBDvBMTDAETA0XB\nEBeISa13YgrBck571p7a9lAOvf9gPVI57bnantPPOdd5Pv7inOtwzts3l+d1/ficzydvZGRkRAAA\nYMblmy4AAIBcRQgDAGAIIQwAgCGEMAAAhhDCAAAYQggDAGBIgZUXDQ0N6Re/+IU2btyo++67L/78\n8ePH9fLLL8vhcKiurk6bNm1K+l7BYGTq1Y7D7S5WODyQ8ve1I3plHb2yjl5ZR6+ss1OvvF5Xwuct\nnQm/8cYbKisru+75HTt2aPfu3dq/f7+OHTumjo6O6VU5RQUFDiOfm43olXX0yjp6ZR29si4XepU0\nhL/88kt1dHTozjvvHPN8Z2enysrKVFlZqfz8fNXX16utrS1ddQIAYDtJQ3jnzp3asmXLdc8Hg0F5\nPJ74Y4/Ho2AwmNrqAACwsQnvCR88eFA//elP9YMf/CBlH+h2F6flEsN419txPXplHb2yjl5ZR6+s\ns3uvJgzhDz/8UJ2dnfrwww/19ddfq6ioSPPmzdMdd9whn8+nUCgUf21XV5d8Pl/SD0zHTXav15WW\nAV92RK+so1fW0Svr6JV1durVeAcTE4bwK6+8Ev/z7t279f3vf1933HGHJKmqqkr9/f06d+6c5s2b\np6NHj6qlpSWFJQMAYG+WfqJ0rXfffVcul0tNTU3avn27Nm/eLElas2aN5s+fn/ICAQCwK8sh/MQT\nT1z33LJlyxQIBFJaEAAAuYIZswAAMIQQBmwiOhzTxfCAosMx06UAsGjS94QBZJbYlSsKHOnQ6fag\nevqi8pQ6VVvjlb9hgRz5HGcDmYwQBrJc4EiHDp86F3/c3ReNP25urDFVFgALOEwGslh0OKbT7Yln\nqjvdHuLSNJDhCGEgi/X2R9XTF024LRwZUm9/4m2AnWXT+AguRwNZrKzEKU+pU90JgtjtmqWyEqeB\nqgAzsnF8RGZWBcASZ6FDtTXehNtqa8rlLLT/UnDAqNHxEd19UY3o2/ERgSNmltm1ghAGspy/YYEa\nb63S3NJZys+T5pbOUuOtVfI3LDBdGjBjsnV8BJejgSznyM9Xc2ON1tVXq7c/qrISJ2fAyDlWxkf4\n3MUzXFVynAkDNuEsdMjnLiaAkZNGx0ckksnjIwhhAEDWy9bxEVyOBgDYwug4iNPtIYUjQ3K7Zqm2\npjyjx0cQwgAAW8jG8RGEMADAVkbHR2QD7gkDAGAIIQwAgCGEMAAAhhDCAAAYQggDAGAIIQwAgCGE\nMAAAhhDCQIpl04LiAMxisg4gRbJxQXEAZhHCQIqMLig+anRBcUlqbqwxVRaADMbhOZAC2bqgOACz\nkp4JDw4OasuWLeru7lY0GtXGjRu1atWq+PaGhgbNmzdPDsfVSbJbWlpUUVGRvoqBDJStC4oDMCtp\nCB89elRLly7Vb37zG50/f16PPvromBCWpNbWVs2ePTttRQKZbnRB8e4EQZzJC4oDMCtpCK9Zsyb+\n5wsXLnCWCyQwuqD4tfeER2XyguIAzLI8MGv9+vX6+uuvtWfPnuu2bdu2TefPn9ctt9yizZs3Ky8v\nL6VFAtkgGxcUB2BW3sjIyIjVF3/++ed6+umn9d5778WD9uDBg1q5cqXKysq0adMmrV27VqtXrx73\nPS5fjqmggLMC2NfQpcsK90XlLnVqVhE/QAAwvqTfEGfOnNHcuXNVWVmpRYsWKRaLqaenR3PnzpUk\n3XvvvfHX1tXVqb29fcIQDocHUlD2WF6vS8FgJOXva0f0yrrp9KpAUqR3ULnSafYr6+iVdXbqldfr\nSvh80p8onTp1Snv37pUkhUIhDQwMyO12S5IikYg2bNigS5cuSZJOnjyphQsXpqpmALCV6HBMF0Lf\n8JM1xCU9E16/fr2eeeYZNTc3a2hoSM8++6wOHjwol8ulpqYm1dXVye/3y+l0avHixROeBQNALhoz\nm1okKo+L2dRw1aTuCadCOi4t2OmSRbrRK+tM9Co6HFNvf1RlJc6sGlHNfjWx/z3cnnDkfOOtVcym\nNgE77VfjXY5m1AiQAZh32r6Szaa2rr46qw64kFr83w1kgNF5p7v7ohrRt/NOB450mC4N02RlNjXk\nLkIYMIx5p+1tdDa1RJhNDYQwYBhnSvY2OptaIsymBu4JA4Yx77T9MZsaxkMIA4Yx77T9OfLz1dxY\no3X11XIUFSp2aZh/V0gihIGMwJlSbnAWOuQtn22bn91g+ghhIANce6aUjb8TBjA1hDCQQZyFDvnc\nxabLADBDGB0NAIAhhDAAAIYQwgAAGEIIAwBgCCEMAIAhhDAAAIYQwgAAGEIIAwBgCCEMAIAhhDCA\nrBQdjulieID1lpHVmLYSQFaJXbmiwJEOnW4PqqcvKk+pU7U1XvkbFsiRz3kFsgshDCCrBI50jFn2\nsbsvGn/c3FhjqixgSjhsBJA1osMxnW4PJtx2uj3EpWlkHUIYQNbo7Y+qpy+acFs4MqTe/sTbgExF\nCAMJRIdjuhD6hjOrDFNW4pSn1Jlwm9s1S2UlibcBmYp7wsA1xgz6iUTlcTHoJ5M4Cx2qrfGOuSc8\nqramXM5Ch4GqgKkjhIFrMOgn8/kbFki6eg84HBmS2zVLtTXl8eeBbEIIA/+VbNDPuvpqzrQygCM/\nX82NNVpXX63e/qjKSpz8uyBrJQ3hwcFBbdmyRd3d3YpGo9q4caNWrVoV3378+HG9/PLLcjgcqqur\n06ZNm9JaMJAuVgb9+NzFM1wVxuMsdPDvgayX9CbX0aNHtXTpUu3bt0+vvPKKXnrppTHbd+zYod27\nd2v//v06duyYOjo60lYskE4M+gEw05KeCa9Zsyb+5wsXLqiioiL+uLOzU2VlZaqsrJQk1dfXq62t\nTQsWcG8G2YdBPwBmmuV7wuvXr9fXX3+tPXv2xJ8LBoPyeDzxxx6PR52dnRO+j9tdrIKC1H+Zeb2u\nlL+nXdGr8T3+P7UqvqFIJ85cUOg/gyqfc4NuX1qpR3+5RA4Ho6Mnwn5lHb2yzu69shzC//rXv/T5\n55/rT3/6k9577z3l5eVN6QPD4YEp/b2JeL0uBYORlL+vHdGr5O5d8SPd87MfyFFUqNilYTkLHerp\n+cZ0WRmN/co6emWdnXo13sFE0kP7M2fO6MKFC5KkRYsWKRaLqaenR5Lk8/kUCoXir+3q6pLP50tF\nvYBRzkKHKstncwkaQFolDeFTp05p7969kqRQKKSBgQG53W5JUlVVlfr7+3Xu3DldvnxZR48e1YoV\nK9JbMQAANpH0cvT69ev1zDPPqLm5WUNDQ3r22Wd18OBBuVwuNTU1afv27dq8ebOkq4O45s+fn/ai\nAQCwg7yRkZGRmfzAdFzft9N9g3SjV9bRK+volXX0yjo79WrK94QBAEB6EMIAABhCCAMAYAghDACA\nIYQwAACGEMIAABhCCAMAYAghDACAIYQwAGBc0eGYLoYHFB2OmS7FliyvogQAyB2xK1cUONKh0+1B\n9fRF5Sl1qrbGK3/DAjnyOX9LFUIYAHCdwJEOHT51Lv64uy8af9zcWGOqLNvhcAYAMEZ0OKbT7cGE\n2063h7g0nUKEMABgjN7+qHr6ogm3hSND6u1PvA2TRwgDAMYoK3HKU+pMuM3tmqWyksTbMHmEMABg\nDGehQ7U13oTbamvK5Sx0zHBF9sXALADAdfwNCyRdvQccjgzJ7Zql2pry+PNIDUIYAHAdR36+mhtr\ntK6+Wr39UZWVODkDTgMuRwPAJOTa5BXOQod87mICOE04EwYAC5i8AulACAOABUxegXTg8A0AkmDy\nCqQLIQwASTB5BdKFEAaAJJi8AulCCBuQa6MrgWzH5BVIFwZmzSBGVwLZi8krkA6E8AxidCWQvZi8\nAulgKYR37dqlTz/9VJcvX9Zjjz2mu+66K76toaFB8+bNk8NxdWdsaWlRRUVFeqrNYslGV66rr+Z/\naCALjE5eAaRC0hA+ceKEvvjiCwUCAYXDYa1du3ZMCEtSa2urZs+enbYi7cDK6Er+xwaA3JI0hJct\nW6abbrpJklRaWqrBwUHFYrH4mS+sGR1d2Z0giBldCQC5KeloIIfDoeLiq2doBw4cUF1d3XUBvG3b\nNj3wwANqaWnRyMhIeirNcoyuBAB8V96IxdQ8fPiw3nzzTe3du1culyv+/MGDB7Vy5UqVlZVp06ZN\nWrt2rVavXj3u+1y+HFNBQW4GTix2RXv/71mdOHNBof8MqnzODbp9aaUe/eUSORyMjgaAXGMphD/6\n6CO9+uqrevvttzVnzpxxX/fPf/5T3d3devLJJ8d9TTAYmVqlE/B6XWl533SJDseMja7Mtl6ZRK+s\no1fW0Svr7NQrr9eV8Pmkp1+RSES7du3Sm2++eV0ARyIRbdiwQZcuXZIknTx5UgsXLkxBufbG0mAA\nAMnCwKz3339f4XBYTz31VPy52267TTfeeKOamppUV1cnv98vp9OpxYsXT3gpGgAygcmrUcC1LN8T\nThUuR5tFr6yjV9ZlS68yYda6bOlVJrBTr8a7HM2MWQByBrPWIdMwJBdATmBNYGQiQhhATmBNYGQi\nQhhATmBNYGQiQhhATmDWOmQiBmYByBmsCYxMQwgDyBmsCYxMQwgDyDmsCYxMwT1hAAAMIYQBADCE\nEAYAwBBCGAAAQwhhAEDGiQ7HdCH0zYxPJxodjulieGDGPpfR0QCAjDFmpatIVB7XzKx0ZWqFLUIY\nAJAxTK10ZepzuRwNAMgIpla6MrnCFiEMAMgIpla6MrnCFiEMAMgIpla6MrnCFiEMAMgIpla6MrnC\nFgOzAAAZw9RKV6Y+N29kZGQkrZ/wHcFgJOXv6fW60vK+dkSvrKNX1tEr6+iVNdHhmBxFhYpdGp7R\nla6iw7G0rLDl9boSPs/laABAxnEWOlRZPnvGl5ocXWFrpj6XEIZtzfTMNwAwWdwThu2YmvkGACaL\nEIbtmJr5BgAmi9MC2IrJmW8AYLIshfCuXbvk9/u1bt06ffDBB2O2HT9+XPfff7/8fr9ef/31tBQJ\nWGVy5hsAmKykl6NPnDihL774QoFAQOFwWGvXrtVdd90V375jxw698847qqio0IMPPqi7775bCxak\n93dVwHhGZ77pThDE6Z75BgAmK+mZ8LJly/Tqq69KkkpLSzU4OKhY7Oolvc7OTpWVlamyslL5+fmq\nr69XW1tbeisGJmBy5hsAmKykZ8IOh0PFxcWSpAMHDqiurk4Ox9UvsmAwKI/HE3+tx+NRZ2dnmkoF\nrDE18w0ATJbl0dGHDx/WgQMHtHfv3ml9oNtdrIKC1J+NjDcbCa6XC7363QO3aOjSZYX7onKXOjWr\naGo/BMiFXqUKvbKOXlln915Z+mb66KOPtGfPHr399ttyub5tiM/nUygUij/u6uqSz+eb8L3C4YEp\nljo+poGzLtd6VSAp0juoqfwX51qvpmOqvUrXFIGZjP3KOjv1aryDiaQhHIlEtGvXLv3973/XnDlz\nxmyrqqpSf3+/zp07p3nz5uno0aNqaWlJTcUAbIsJVYCrkobw+++/r3A4rKeeeir+3G233aYbb7xR\nTU1N2r59uzZv3ixJWrNmjebPn5++agHYAhOqAFclDWG/3y+/3z/u9mXLlikQCKS0KAD2lWxClXX1\n1TlzaRrgug+AGcWEKsC3CGEAM2p0QpVEmFAFuYYQBjCjmFAF+BarKAGYcUyoAlxFCAOYcY78fDU3\n1mhdfXXO/U4YuBYhDMAYZ6FDPnex6TIAY7gnDACAIYQwAACGEMIAABhCCAMAYAghDACAIYQwAACG\nEMIAABhCCAMAYAghDACAIYQwAACGEMIAABhCCAMAYAghDACAIYQwAACGEMIAABhCCAMAYAghDACA\nIYQwAACGEMIAABhCCAMAYAghDACAIZZCuL29XY2Njdq3b9912xoaGtTc3KyHHnpIDz30kLq6ulJe\nJAAAdlSQ7AUDAwN67rnntHz58nFf09raqtmzZ6e0MAAA7C7pmXBRUZFaW1vl8/lmoh4AAHJG0jPh\ngoICFRRM/LJt27bp/PnzuuWWW7R582bl5eWN+1q3u1gFBY7JV5qE1+tK+XvaFb2yjl5ZR6+so1fW\n2b1XSUM4mSeffFIrV65UWVmZNm3apEOHDmn16tXjvj4cHpjuR17H63UpGIyk/H3tiF5ZR6+so1fW\n0Svr7NSr8Q4mpj06+t5779XcuXNVUFCguro6tbe3T/cts0J0OKaL4QFFh2OmSwEAZKlpnQlHIhE9\n9dRTeuONN1RUVKSTJ0/q7rvvTlVtGSl25YoCRzp0uj2onr6oPKVO1dZ45W9YIEc+v/gCAFiXNITP\nnDmjnTt36vz58yooKNChQ4fU0NCgqqoqNTU1qa6uTn6/X06nU4sXL57wUrQdBI506PCpc/HH3X3R\n+OPmxhpTZQEAslDSEF66dKn+8Y9/jLv9kUce0SOPPJLSojJVdDim0+3BhNtOt4e0rr5azsLUDzoD\nANgT108nobc/qp6+aMJt4ciQevsTbwMAIJGcDuHJDq4qK3HKU+pMuM3tmqWyksTbAABIZNo/UcpG\nUx1c5Sx0qLbGO+ae8KjamnIuRQMAJiUnQ3g6g6v8DQskXb0HHI4Mye2apdqa8vjzAABYlXMhPN3B\nVY78fDU31mhdfbV6+6MqK3FyBgwAmJKcuyecqsFVzkKHfO5iAhgAMGU5F8IMrgIAZIqcC+HRwVWJ\nMLgKmLzocEwXQt8whSswBTl3T1hicBWQCmN+ZRCJyuNiCldgsnIyhBlcBUwfU7gC05fTh6sMrgKm\nJtmvDLg0DViT0yEMYGqYwhVIDUIYwKTxKwNYwbrryeXkPWEA08MUrpgI665bRwgDmBJ+ZYDxMGjP\nOkIYwJRc+ysDR1GhYpeGOQMG665PEtcFAEyLs9ChyvLZfLFCEoP2JosQBgCbm8kBUgzamxwuRwOA\nTZkYIMWgvckhhAHApkwNkGLQnnWEMADYkMkBUkwNbB33hAHAhjJhgBRTAydHCAOADTFAKjsQwgBg\nQ6ydnh24JwwANsUAqcxHCAOATTFAKvNZuhzd3t6uxsZG7du377ptx48f1/333y+/36/XX3895QUC\nAKaHAVKZK2kIDwwM6LnnntPy5csTbt+xY4d2796t/fv369ixY+ro6Eh5kQAA2FHSEC4qKlJra6t8\nPt912zo7O1VWVqbKykrl5+ervr5ebW1taSkUAAC7SXpPuKCgQAUFiV8WDAbl8Xjijz0ejzo7Oyd8\nP7e7WAUFqb8k4vW6Uv6edkWvrKNX1tEr6+iVdXbv1YwPzAqHB1L+nl6vS8FgJOXva0f0yjp6ZR29\nso5eWWenXo13MDGt3wn7fD6FQqH4466uroSXrQEAwPWmFcJVVVXq7+/XuXPndPnyZR09elQrVqxI\nVW0AANha0svRZ86c0c6dO3X+/HkVFBTo0KFDamhoUFVVlZqamrR9+3Zt3rxZkrRmzRrNnz8/7UUD\nAGAHeSMjIyMz+YHpuL5vp/sG6UavrKNX1tEr6+iVdXbqVVruCQMAgKkjhAEAMIQQBgDAEEI4y0SH\nY7oYHlB0OGa6FADANLGKUpaIXbmiwJEOnW4PqqcvKk+pU7U1XvkbFsiRb99jqehwjNVfANhW1odw\ndDimC6FvFBuO2fpLOnCkQ4dPnYs/7u6Lxh83N9aYKittcvWgA0BuydoQHvMlHYnK47Lvl3R0OKbT\n7cGE2063h7Suvtp2ByC5dtABIDdlbVqNfkl390U1MvLtl3TgiP2WUuztj6qnL5pwWzgypN7+xNuy\nVbKDDu6HA7CLrAzhXPuSLitxylPqTLjN7ZqlspLE27JVrh10AMhdWRnCufYl7Sx0qLbGm3BbbU25\n7S5F59pBB4DclZUhnItf0v6GBWq8tUpzS2cpP0+aWzpLjbdWyd+wwHRpKZdrBx0AcldWDswa/ZK+\nduDOKLt+STvy89XcWKN19dU58ZOd0YOL0+0hhSNDcrtmqbam3JYHHQByV1aGsJS7X9LOQod87mLT\nZaRdrh10AMhNWRvC135JO4oKFbs0zJe0DeXKQQeA3JSV94Sv5Sx0qLJ8NgEMAMg6WR/CAABkK0IY\nAABDCGEAAAwhhAEAMIQQBgDAEEIYAABDCGEAAAwhhAEAMIQQBoAsEB2O6WJ4wHZLtea6rJ22EgBy\nQezKFQWOdOh0e1A9fVF5Sp2qrfHK37BAjnzOo7IdIQwAGSxwpGPMinHdfdH44+bGGlNlIUUshfAL\nL7ygzz77THl5edq6datuuumm+LaGhgbNmzdPDsfVuZtbWlpUUVGRnmoBIIdEh2M63R5MuO10e0jr\n6quZNz/LJQ3hTz75RF999ZUCgYC+/PJLbd26VYFAYMxrWltbNXv27LQVCQC5qLc/qp6+aMJt4ciQ\nevujrDKW5ZLeUGhra1NjY6Mkqbq6Wr29verv7097YQCQ68pKnPKUOhNuc7tmqawk8TZkj6QhHAqF\n5Ha74489Ho+CwbGXR7Zt26YHHnhALS0tGhkZSX2VAJCDnIUO1dZ4E26rrSnnUrQNTHpg1ndD9skn\nn9TKlStVVlamTZs26dChQ1q9evW4f9/tLlZBQep3HK/XlfL3tCt6ZR29so5eWTeZXj3+P7UqvqFI\nJ85cUOg/gyqfc4NuX1qpR3+5RA6H/UdH232/ShrCPp9PoVAo/vjixYvyer89Mrv33nvjf66rq1N7\ne/uEIRwOD0y11nF5vS4Fg5GUv68d0Svr6JV19Mq6qfTq3hU/0j0/+4F6+6MqK3HKWehQT883aaow\nc9hpvxrvYCLpYdSKFSt06NAhSdLZs2fl8/lUUlIiSYpEItqwYYMuXbokSTp58qQWLlyYqpoBAP/l\nLHTI5y7mErTNJD0Tvvnmm7VkyRKtX79eeXl52rZtm9599125XC41NTWprq5Ofr9fTqdTixcvnvAs\nGAAAfCtvZIZHUqXj0oKdLlmkG72yjl5ZR6+so1fW2alXU74cDQAA0oMQBgDAEEIYAABDCOEcEh2O\n6ULoG5ZCA4AMwSpKOWDMUmiRqDyuyS+FFh2OjfmNIgBg+gjhHDCdpdBYyxQA0odvUZtLthRaskvT\nowHe3RfViL4N8MCRjjRUCwC5hRC2OStLoY1nugEOAJgYIWxz01kKbToBDgBIjhC2uekshcZapgCQ\nXoRwDvA3LFDjrVWaWzpL+XnS3NJZary1Sv6GBRP+PdYyBYD0YnR0DnDk56u5sUbr6qvlKCpU7NKw\n5QAdDerT7SGFI0Nyu2aptqY8aYADAJIjhHOIs9Ahb/nsSU2Ifm2A8zthAEgtQhiWjK5lCgBIHe4J\nAwBgCCEMQNHhmC6GB/jtNzDDuBwN5DCmJQXMIoSBHDadecUBTB+HukCOYlpSwDxCGMhRTEsKmEcI\nAzmKaUkB8whhIEcxLSlgHgOzgBzGtKSAWYQwkMOYlhQwi8vRSDsmgsh8o9OSEsDAzOJMGGnDRBAA\nMDFCGGnDRBAAMDFLpyMvvPCC/H6/1q9fr3//+99jth0/flz333+//H6/Xn/99bQUiezDRBAAkFzS\nEP7kk0/01VdfKRAI6Pnnn9fzzz8/ZvuOHTu0e/du7d+/X8eOHVNHR0faikX2YCIIAEguaQi3tbWp\nsbFRklRdXa3e3l719/dLkjo7O1VWVqbKykrl5+ervr5ebW1t6a0YWYGJIAAguaT3hEOhkJYsWRJ/\n7PF4FAwGVVJSomAwKI/HM2ZbZ2fnhO/ndheroCD1IzC9XlfK39OuZqpXK/7P9/XeR/8vwfPfU9X3\n5sxIDdPFfmUdvbKOXlln915NemDWyMjItD4wHB6Y1t9PxOt1KRiMpPx97Wgme/XL5T/UwOCl6yaC\n+OXyH2bFvxf7lXX0yjp6ZZ2dejXewUTSEPb5fAqFQvHHFy9elNfrTbitq6tLPp9vurXCJpgIAgAm\nlvSe8IoVK3To0CFJ0tmzZ+Xz+VRSUiJJqqqqUn9/v86dO6fLly/r6NGjWrFiRXorRtZhIggASCzp\nmfDNN9+sJUuWaP369crLy9O2bdv07rvvyuVyqampSdu3b9fmzZslSWvWrNH8+fPTXjQAAHaQNzLd\nm7yTlI7r+3a6b5Bu9Mo6emUdvbKOXllnp16Nd0+YuQMBADCEEAYAwBBCGAAAQwhhAAAMIYQBADCE\nEAYAwBBCGAAAQ2b8d8IAAOAqzoQBADCEEAYAwBBCGAAAQwhhAAAMIYQBADCEEAYAwJCk6wlnshde\neEGfffaZ8vLytHXrVt10002mS8pIH3/8sX73u99p4cKFkqSamhr95S9/MVxV5mlvb9fGjRv161//\nWg8++KAuXLigp59+WrFYTF6vV3/9619VVFRkusyM8N1ebdmyRWfPntWcOXMkSRs2bNCdd95ptsgM\nsWvXLn366ae6fPmyHnvsMf3kJz9hvxrHd3t15MgR2+9XWRvCn3zyib766isFAgF9+eWX2rp1qwKB\ngOmyMtbPfvYzvfbaa6bLyFgDAwN67rnntHz58vhzr732mpqbm3XPPffo5Zdf1oEDB9Tc3GywysyQ\nqFeS9Ic//EGrVq0yVFVmOnHihL744gsFAgGFw2GtXbtWy5cvZ79KIFGvbr/9dtvvV1l7ObqtrU2N\njY2SpOrqavX29qq/v99wVchWRUVFam1tlc/niz/38ccf6+c//7kkadWqVWprazNVXkZJ1CsktmzZ\nMr366quSpNLSUg0ODrJfjSNRr2KxmOGq0i9rQzgUCsntdscfezweBYNBgxVlto6ODv32t7/VAw88\noGPHjpkuJ+MUFBRo1qxZY54bHByMXyacO3cu+9d/JeqVJO3bt08PP/ywfv/736unp8dAZZnH4XCo\nuLhYknTgwAHV1dWxX40jUa8cDoft96usvRz9Xcy+Ob4f/ehHevzxx3XPPfeos7NTDz/8sD744APu\nQ00C+9fEfvWrX2nOnDlatGjyyhESAAAB3ElEQVSR3nrrLf3tb3/Ts88+a7qsjHH48GEdOHBAe/fu\n1V133RV/nv3qetf26syZM7bfr7L2TNjn8ykUCsUfX7x4UV6v12BFmauiokJr1qxRXl6efvjDH6q8\nvFxdXV2my8p4xcXFGhoakiR1dXVx+XUCy5cv16JFiyRJDQ0Nam9vN1xR5vjoo4+0Z88etba2yuVy\nsV9N4Lu9yoX9KmtDeMWKFTp06JAk6ezZs/L5fCopKTFcVWZ677339M4770iSgsGguru7VVFRYbiq\nzHfHHXfE97EPPvhAK1euNFxR5nriiSfU2dkp6eq99NGR+LkuEolo165devPNN+MjfNmvEkvUq1zY\nr7J6FaWWlhadOnVKeXl52rZtm3784x+bLikj9ff3649//KP6+vo0PDysxx9/XPX19abLyihnzpzR\nzp07df78eRUUFKiiokItLS3asmWLotGovve97+nFF19UYWGh6VKNS9SrBx98UG+99ZZuuOEGFRcX\n68UXX9TcuXNNl2pcIBDQ7t27NX/+/PhzL730kv785z+zX31Hol7dd9992rdvn633q6wOYQAAslnW\nXo4GACDbEcIAABhCCAMAYAghDACAIYQwAACGEMIAABhCCAMAYAghDACAIf8fI/Q3Zxex8MIAAAAA\nSUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sales = np.array([random() + (day % 7) * random() for day in days])\n",
    "scatter(days, sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LEADkcMzelxY"
   },
   "source": [
    "How does linear regression do at fitting this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "EV5kt69GenV3",
    "outputId": "2d0db3bf-4a66-4e05-cf2c-ccb40dfc1c17"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12974456415694935"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_ols = LinearRegression()\n",
    "sales_ols.fit(days, sales)\n",
    "sales_ols.score(days, sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7shN1eBMep9Q"
   },
   "source": [
    "That's not great - and the fix depends on the domain. Here, we know it'd be best to actually use \"day of week\" as a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Qo9eFlHIeqtA",
    "outputId": "c7f631a8-93b0-4c56-9354-243216a8040f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3134977673939594"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "day_of_week = days % 7\n",
    "sales_ols.fit(day_of_week, sales)\n",
    "sales_ols.score(day_of_week, sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9ooJIfIMex2G"
   },
   "source": [
    "Note that it's also important to have representative data across whatever seasonal feature(s) you use - don't predict retailers based only on Christmas, as that won't generalize well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "44QZgrPUe3-Y"
   },
   "source": [
    "## Recurrent Neural Networks\n",
    "\n",
    "There's plenty more to \"traditional\" time series, but the latest and greatest technique for sequence data is recurrent neural networks. A recurrence relation in math is an equation that uses recursion to define a sequence - a famous example is the Fibonacci numbers:\n",
    "\n",
    "$F_n = F_{n-1} + F_{n-2}$\n",
    "\n",
    "For formal math you also need a base case $F_0=1, F_1=1$, and then the rest builds from there. But for neural networks what we're really talking about are loops:\n",
    "\n",
    "![Recurrent neural network](https://upload.wikimedia.org/wikipedia/commons/b/b5/Recurrent_neural_network_unfold.svg)\n",
    "\n",
    "The hidden layers have edges (output) going back to their own input - this loop means that for any time `t` the training is at least partly based on the output from time `t-1`. The entire network is being represented on the left, and you can unfold the network explicitly to see how it behaves at any given `t`.\n",
    "\n",
    "Different units can have this \"loop\", but a particularly successful one is the long short-term memory unit (LSTM):\n",
    "\n",
    "![Long short-term memory unit](https://upload.wikimedia.org/wikipedia/commons/thumb/6/63/Long_Short-Term_Memory.svg/1024px-Long_Short-Term_Memory.svg.png)\n",
    "\n",
    "There's a lot going on here - in a nutshell, the calculus still works out and backpropagation can still be implemented. The advantage (ane namesake) of LSTM is that it can generally put more weight on recent (short-term) events while not completely losing older (long-term) information.\n",
    "\n",
    "After enough iterations, a typical neural network will start calculating prior gradients that are so small they effectively become zero - this is the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem), and is what RNN with LSTM addresses. Pay special attention to the $c_t$ parameters and how they pass through the unit to get an intuition for how this problem is solved.\n",
    "\n",
    "So why are these cool? One particularly compelling application is actually not time series but language modeling - language is inherently ordered data (letters/words go one after another, and the order *matters*). [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) is a famous and worth reading blog post on this topic.\n",
    "\n",
    "For our purposes, let's use TensorFlow and Keras to train RNNs with natural language. Resources:\n",
    "\n",
    "- https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py\n",
    "- https://keras.io/layers/recurrent/#lstm\n",
    "- http://adventuresinmachinelearning.com/keras-lstm-tutorial/\n",
    "\n",
    "Note that `tensorflow.contrib` [also has an implementation of RNN/LSTM](https://www.tensorflow.org/tutorials/sequences/recurrent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eWrQllf8WEd-"
   },
   "source": [
    "### RNN/LSTM Sentiment Classification with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 975
    },
    "colab_type": "code",
    "id": "Ti23G0gRe3kr",
    "outputId": "bba9ae40-a286-49ed-d87b-b2946fb60ddf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 1s 0us/step\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n",
      "Build model...\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Train...\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/15\n",
      "25000/25000 [==============================] - 163s 7ms/step - loss: 0.4654 - acc: 0.7795 - val_loss: 0.4157 - val_acc: 0.8150\n",
      "Epoch 2/15\n",
      "25000/25000 [==============================] - 160s 6ms/step - loss: 0.3031 - acc: 0.8758 - val_loss: 0.3851 - val_acc: 0.8359\n",
      "Epoch 3/15\n",
      "25000/25000 [==============================] - 159s 6ms/step - loss: 0.2200 - acc: 0.9137 - val_loss: 0.4080 - val_acc: 0.8263\n",
      "Epoch 4/15\n",
      "25000/25000 [==============================] - 160s 6ms/step - loss: 0.1560 - acc: 0.9413 - val_loss: 0.4448 - val_acc: 0.8232\n",
      "Epoch 5/15\n",
      "25000/25000 [==============================] - 161s 6ms/step - loss: 0.1135 - acc: 0.9598 - val_loss: 0.6829 - val_acc: 0.8190\n",
      "Epoch 6/15\n",
      "25000/25000 [==============================] - 160s 6ms/step - loss: 0.0848 - acc: 0.9694 - val_loss: 0.5962 - val_acc: 0.8197\n",
      "Epoch 7/15\n",
      "25000/25000 [==============================] - 160s 6ms/step - loss: 0.0576 - acc: 0.9810 - val_loss: 0.7397 - val_acc: 0.8176\n",
      "Epoch 8/15\n",
      "25000/25000 [==============================] - 160s 6ms/step - loss: 0.0383 - acc: 0.9863 - val_loss: 0.8178 - val_acc: 0.8124\n",
      "Epoch 9/15\n",
      "25000/25000 [==============================] - 160s 6ms/step - loss: 0.0340 - acc: 0.9883 - val_loss: 0.8594 - val_acc: 0.8185\n",
      "Epoch 10/15\n",
      "25000/25000 [==============================] - 159s 6ms/step - loss: 0.0260 - acc: 0.9913 - val_loss: 0.9963 - val_acc: 0.8114\n",
      "Epoch 11/15\n",
      "25000/25000 [==============================] - 159s 6ms/step - loss: 0.0223 - acc: 0.9938 - val_loss: 0.9134 - val_acc: 0.8100\n",
      "Epoch 12/15\n",
      "25000/25000 [==============================] - 161s 6ms/step - loss: 0.0179 - acc: 0.9943 - val_loss: 0.9299 - val_acc: 0.8082\n",
      "Epoch 13/15\n",
      "25000/25000 [==============================] - 160s 6ms/step - loss: 0.0180 - acc: 0.9940 - val_loss: 1.0536 - val_acc: 0.8138\n",
      "Epoch 14/15\n",
      "25000/25000 [==============================] - 160s 6ms/step - loss: 0.0112 - acc: 0.9962 - val_loss: 1.1474 - val_acc: 0.8075\n",
      "Epoch 15/15\n",
      "25000/25000 [==============================] - 160s 6ms/step - loss: 0.0126 - acc: 0.9964 - val_loss: 1.1362 - val_acc: 0.8106\n",
      "25000/25000 [==============================] - 29s 1ms/step\n",
      "Test score: 1.1362178895920516\n",
      "Test accuracy: 0.81064\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#Trains an LSTM model on the IMDB sentiment classification task.\n",
    "The dataset is actually too small for LSTM to be of any advantage\n",
    "compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
    "**Notes**\n",
    "- RNNs are tricky. Choice of batch size is important,\n",
    "choice of loss and optimizer is critical, etc.\n",
    "Some configurations won't converge.\n",
    "- LSTM loss decrease patterns during training can be quite different\n",
    "from what you see with CNNs/MLPs/etc.\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "\n",
    "max_features = 20000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7pETWPIe362y"
   },
   "source": [
    "### RNN Text generation with NumPy\n",
    "\n",
    "What else can we do with RNN? Since we're analyzing the *sequence*, we can do more than classify - we can *generate* text. We'll pull some news stories using [newspaper](https://github.com/codelucas/newspaper/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fz1m55G5WSrQ"
   },
   "source": [
    "#### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 836
    },
    "colab_type": "code",
    "id": "ahlHBeoZCaLX",
    "outputId": "7b1c5f93-3fa5-42db-acb6-3c894b0accef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting newspaper3k\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/b9/51afecb35bb61b188a4b44868001de348a0e8134b4dfa00ffc191567c4b9/newspaper3k-0.2.8-py3-none-any.whl (211kB)\n",
      "\u001b[K    100% |████████████████████████████████| 215kB 25.8MB/s \n",
      "\u001b[?25hCollecting feedfinder2>=0.0.4 (from newspaper3k)\n",
      "  Downloading https://files.pythonhosted.org/packages/35/82/1251fefec3bb4b03fd966c7e7f7a41c9fc2bb00d823a34c13f847fd61406/feedfinder2-0.0.4.tar.gz\n",
      "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/d8/7d37fec71ff7c9dbcdd80d2b48bcdd86d6af502156fc93846fb0102cb2c4/feedparser-5.2.1.tar.bz2 (192kB)\n",
      "\u001b[K    100% |████████████████████████████████| 194kB 25.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (4.6.3)\n",
      "Collecting cssselect>=0.9.2 (from newspaper3k)\n",
      "  Downloading https://files.pythonhosted.org/packages/7b/44/25b7283e50585f0b4156960691d951b05d061abf4a714078393e51929b30/cssselect-1.0.3-py2.py3-none-any.whl\n",
      "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/cb/2c8332bcdc14d33b0bedd18ae0a4981a069c3513e445120da3c3f23a8aaa/jieba3k-0.35.1.zip (7.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 7.4MB 6.6MB/s \n",
      "\u001b[?25hCollecting tldextract>=2.0.1 (from newspaper3k)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/90/18ac0e5340b6228c25cc8e79835c3811e7553b2b9ae87296dfeb62b7866d/tldextract-2.2.1-py2.py3-none-any.whl (48kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 18.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.18.4)\n",
      "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (4.2.6)\n",
      "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
      "  Downloading https://files.pythonhosted.org/packages/17/82/86982e4b6d16e4febc79c2a1d68ee3b707e8a020c5d2bc4af8052d0f136a/tinysegmenter-0.3.tar.gz\n",
      "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (3.2.5)\n",
      "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (4.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.5.3)\n",
      "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (3.13)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.11.0)\n",
      "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
      "  Downloading https://files.pythonhosted.org/packages/23/9c/6e63c23c39e53d3df41c77a3d05a49a42c4e1383a6d2a5e3233161b89dbf/requests_file-1.4.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.6/dist-packages (from tldextract>=2.0.1->newspaper3k) (2.6)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from tldextract>=2.0.1->newspaper3k) (40.9.0)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (1.22)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (2019.3.9)\n",
      "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow>=3.3.0->newspaper3k) (0.46)\n",
      "Building wheels for collected packages: feedfinder2, feedparser, jieba3k, tinysegmenter\n",
      "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/de/03/ca/778e3a7a627e3d98836cc890e7cb40c7575424cfd3340f40ed\n",
      "  Building wheel for feedparser (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/8c/69/b7/f52763c41c5471df57703a0ef718a32a5e81ee35dcf6d4f97f\n",
      "  Building wheel for jieba3k (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/83/15/9c/a3f1f67e7f7181170ad37d32e503c35da20627c013f438ed34\n",
      "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/81/2b/43/a02ede72324dd40cdd7ca53aad718c7710628e91b8b0dc0f02\n",
      "Successfully built feedfinder2 feedparser jieba3k tinysegmenter\n",
      "Installing collected packages: feedfinder2, feedparser, cssselect, jieba3k, requests-file, tldextract, tinysegmenter, newspaper3k\n",
      "Successfully installed cssselect-1.0.3 feedfinder2-0.0.4 feedparser-5.2.1 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-1.4.3 tinysegmenter-0.3 tldextract-2.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install newspaper3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fTPlziljCiNJ"
   },
   "outputs": [],
   "source": [
    "import newspaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bk9JF2zaCxoO",
    "outputId": "9e66fc15-a397-4b59-f810-d2182565c99a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ap = newspaper.build('https://www.apnews.com')\n",
    "len(ap.articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "Vc6JgAIJDF4E",
    "outputId": "44a13922-d86a-4668-c4fd-455c0d03b6c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "California Democrats again are trying to mandate the abortion pill be stocked in campus clinics across the Golden State’s university system, and this time they’ve got a friend in the governor’s office.\n"
     ]
    }
   ],
   "source": [
    "article_text = ''\n",
    "\n",
    "for article in ap.articles[:1]:\n",
    "  try:\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    article_text += '\\n\\n' + article.text\n",
    "  except:\n",
    "    print('Failed: ' + article.url)\n",
    "  \n",
    "article_text = article_text.split('\\n\\n')[1]\n",
    "print(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "rsMBBMcv_nRM",
    "outputId": "9f77b07b-4a5a-4ac8-f1b3-79e1a5331fad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique characters :  29\n",
      "txt_data_size :  201\n"
     ]
    }
   ],
   "source": [
    "# Based on \"The Unreasonable Effectiveness of RNN\" implementation\n",
    "import numpy as np\n",
    "\n",
    "chars = list(set(article_text)) # split and remove duplicate characters. convert to list.\n",
    "\n",
    "num_chars = len(chars) # the number of unique characters\n",
    "txt_data_size = len(article_text)\n",
    "\n",
    "print(\"unique characters : \", num_chars)\n",
    "print(\"txt_data_size : \", txt_data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "colab_type": "code",
    "id": "aQygqc_CAWRA",
    "outputId": "30c45e95-057a-4643-9cae-fc518b49c914"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{',': 0, 't': 1, 's': 2, 'm': 3, 'd': 4, 'h': 5, 'y': 6, 'o': 7, 'l': 8, 'S': 9, 'a': 10, 'i': 11, 'D': 12, 'p': 13, 'e': 14, 'k': 15, 'c': 16, 'g': 17, 'b': 18, 'v': 19, 'n': 20, 'G': 21, 'u': 22, 'C': 23, '’': 24, 'r': 25, 'f': 26, '.': 27, ' ': 28}\n",
      "----------------------------------------------------\n",
      "{0: ',', 1: 't', 2: 's', 3: 'm', 4: 'd', 5: 'h', 6: 'y', 7: 'o', 8: 'l', 9: 'S', 10: 'a', 11: 'i', 12: 'D', 13: 'p', 14: 'e', 15: 'k', 16: 'c', 17: 'g', 18: 'b', 19: 'v', 20: 'n', 21: 'G', 22: 'u', 23: 'C', 24: '’', 25: 'r', 26: 'f', 27: '.', 28: ' '}\n",
      "----------------------------------------------------\n",
      "[23, 10, 8, 11, 26, 7, 25, 20, 11, 10, 28, 12, 14, 3, 7, 16, 25, 10, 1, 2, 28, 10, 17, 10, 11, 20, 28, 10, 25, 14, 28, 1, 25, 6, 11, 20, 17, 28, 1, 7, 28, 3, 10, 20, 4, 10, 1, 14, 28, 1, 5, 14, 28, 10, 18, 7, 25, 1, 11, 7, 20, 28, 13, 11, 8, 8, 28, 18, 14, 28, 2, 1, 7, 16, 15, 14, 4, 28, 11, 20, 28, 16, 10, 3, 13, 22, 2, 28, 16, 8, 11, 20, 11, 16, 2, 28, 10, 16, 25, 7, 2, 2, 28, 1, 5, 14, 28, 21, 7, 8, 4, 14, 20, 28, 9, 1, 10, 1, 14, 24, 2, 28, 22, 20, 11, 19, 14, 25, 2, 11, 1, 6, 28, 2, 6, 2, 1, 14, 3, 0, 28, 10, 20, 4, 28, 1, 5, 11, 2, 28, 1, 11, 3, 14, 28, 1, 5, 14, 6, 24, 19, 14, 28, 17, 7, 1, 28, 10, 28, 26, 25, 11, 14, 20, 4, 28, 11, 20, 28, 1, 5, 14, 28, 17, 7, 19, 14, 25, 20, 7, 25, 24, 2, 28, 7, 26, 26, 11, 16, 14, 27]\n",
      "----------------------------------------------------\n",
      "data length :  201\n"
     ]
    }
   ],
   "source": [
    "# one hot encode\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars)) # \"enumerate\" retruns index and value. Convert it to dictionary\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "print(char_to_int)\n",
    "print(\"----------------------------------------------------\")\n",
    "print(int_to_char)\n",
    "print(\"----------------------------------------------------\")\n",
    "# integer encode input data\n",
    "integer_encoded = [char_to_int[i] for i in article_text] # \"integer_encoded\" is a list which has a sequence converted from an original data to integers.\n",
    "print(integer_encoded)\n",
    "print(\"----------------------------------------------------\")\n",
    "print(\"data length : \", len(integer_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bcpMSWDHFowT"
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "iteration = 1000\n",
    "sequence_length = 40\n",
    "batch_size = round((txt_data_size /sequence_length)+0.5) # = math.ceil\n",
    "hidden_size = 500  # size of hidden layer of neurons.  \n",
    "learning_rate = 1e-1\n",
    "\n",
    "\n",
    "# model parameters\n",
    "\n",
    "W_xh = np.random.randn(hidden_size, num_chars)*0.01     # weight input -> hidden. \n",
    "W_hh = np.random.randn(hidden_size, hidden_size)*0.01   # weight hidden -> hidden\n",
    "W_hy = np.random.randn(num_chars, hidden_size)*0.01     # weight hidden -> output\n",
    "\n",
    "b_h = np.zeros((hidden_size, 1)) # hidden bias\n",
    "b_y = np.zeros((num_chars, 1)) # output bias\n",
    "\n",
    "h_prev = np.zeros((hidden_size,1)) # h_(t-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bkqoN86qWaI4"
   },
   "source": [
    "#### Forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "imfg_Ew0WdDL"
   },
   "outputs": [],
   "source": [
    "def forwardprop(inputs, targets, h_prev):\n",
    "        \n",
    "    # Since the RNN receives the sequence, the weights are not updated during one sequence.\n",
    "    xs, hs, ys, ps = {}, {}, {}, {} # dictionary\n",
    "    hs[-1] = np.copy(h_prev) # Copy previous hidden state vector to -1 key value.\n",
    "    loss = 0 # loss initialization\n",
    "    \n",
    "    for t in range(len(inputs)): # t is a \"time step\" and is used as a key(dic).  \n",
    "        \n",
    "        xs[t] = np.zeros((num_chars,1)) \n",
    "        xs[t][inputs[t]] = 1\n",
    "        hs[t] = np.tanh(np.dot(W_xh, xs[t]) + np.dot(W_hh, hs[t-1]) + b_h) # hidden state. \n",
    "        ys[t] = np.dot(W_hy, hs[t]) + b_y # unnormalized log probabilities for next chars\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars. \n",
    "        # Softmax. -> The sum of probabilities is 1 even without the exp() function, but all of the elements are positive through the exp() function.\n",
    " \n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss). Efficient and simple code\n",
    "\n",
    "#         y_class = np.zeros((num_chars, 1)) \n",
    "#         y_class[targets[t]] =1\n",
    "#         loss += np.sum(y_class*(-np.log(ps[t]))) # softmax (cross-entropy loss)        \n",
    "\n",
    "    return loss, ps, hs, xs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zm6qwNiqWdMe"
   },
   "source": [
    "#### Backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "81qBiz_xWenI"
   },
   "outputs": [],
   "source": [
    "def backprop(ps, inputs, hs, xs):\n",
    "\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy) # make all zero matrices.\n",
    "    dbh, dby = np.zeros_like(b_h), np.zeros_like(b_y)\n",
    "    dhnext = np.zeros_like(hs[0]) # (hidden_size,1) \n",
    "\n",
    "    # reversed\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t]) # shape (num_chars,1).  \"dy\" means \"dloss/dy\"\n",
    "        dy[targets[t]] -= 1 # backprop into y. After taking the soft max in the input vector, subtract 1 from the value of the element corresponding to the correct label.\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy \n",
    "        dh = np.dot(W_hy.T, dy) + dhnext # backprop into h. \n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity #tanh'(x) = 1-tanh^2(x)\n",
    "        dbh += dhraw\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        dhnext = np.dot(W_hh.T, dhraw)\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]: \n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients.  \n",
    "    \n",
    "    return dWxh, dWhh, dWhy, dbh, dby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r8sBvcdbWfhi"
   },
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "colab_type": "code",
    "id": "iA4RM70LWgO_",
    "outputId": "0fd64bca-f1b5-4be1-9e80-076308365598"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 1.921729\n",
      "iter 100, loss: 0.000758\n",
      "iter 200, loss: 0.000377\n",
      "iter 300, loss: 0.000219\n",
      "iter 400, loss: 0.000141\n",
      "iter 500, loss: 0.000096\n",
      "iter 600, loss: 0.000062\n",
      "iter 700, loss: 0.000043\n",
      "iter 800, loss: 0.000031\n",
      "iter 900, loss: 0.000024\n",
      "CPU times: user 5min 53s, sys: 3min 26s, total: 9min 20s\n",
      "Wall time: 4min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data_pointer = 0\n",
    "\n",
    "# memory variables for Adagrad\n",
    "mWxh, mWhh, mWhy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy)\n",
    "mbh, mby = np.zeros_like(b_h), np.zeros_like(b_y) \n",
    "\n",
    "for i in range(iteration):\n",
    "    h_prev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    data_pointer = 0 # go from start of data\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        \n",
    "        inputs = [char_to_int[ch]\n",
    "                  for ch in article_text[data_pointer:data_pointer+sequence_length]]\n",
    "        targets = [char_to_int[ch]\n",
    "                   for ch in article_text[data_pointer+1:data_pointer+sequence_length+1]] # t+1        \n",
    "            \n",
    "        if (data_pointer+sequence_length+1 >= len(article_text) and b == batch_size-1): # processing of the last part of the input data. \n",
    "#             targets.append(char_to_int[txt_data[0]])   # When the data doesn't fit, add the first char to the back.\n",
    "            targets.append(char_to_int[\" \"])   # When the data doesn't fit, add space(\" \") to the back.\n",
    "\n",
    "\n",
    "        # forward\n",
    "        loss, ps, hs, xs = forwardprop(inputs, targets, h_prev)\n",
    "#         print(loss)\n",
    "    \n",
    "        # backward\n",
    "        dWxh, dWhh, dWhy, dbh, dby = backprop(ps, inputs, hs, xs) \n",
    "        \n",
    "        \n",
    "    # perform parameter update with Adagrad\n",
    "        for param, dparam, mem in zip([W_xh, W_hh, W_hy, b_h, b_y], \n",
    "                                    [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                    [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "            mem += dparam * dparam # elementwise\n",
    "            param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update      \n",
    "    \n",
    "        data_pointer += sequence_length # move data pointer\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        print ('iter %d, loss: %f' % (i, loss)) # print progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tjh8Ip68WgYV"
   },
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HDCxDNPG68Hx"
   },
   "outputs": [],
   "source": [
    "def predict(test_char, length):\n",
    "    x = np.zeros((num_chars, 1)) \n",
    "    x[char_to_int[test_char]] = 1\n",
    "    ixes = []\n",
    "    h = np.zeros((hidden_size,1))\n",
    "\n",
    "    for t in range(length):\n",
    "        h = np.tanh(np.dot(W_xh, x) + np.dot(W_hh, h) + b_h) \n",
    "        y = np.dot(W_hy, h) + b_y\n",
    "        p = np.exp(y) / np.sum(np.exp(y)) \n",
    "        ix = np.random.choice(range(num_chars), p=p.ravel()) # ravel -> rank0\n",
    "        # \"ix\" is a list of indexes selected according to the soft max probability.\n",
    "        x = np.zeros((num_chars, 1)) # init\n",
    "        x[ix] = 1 \n",
    "        ixes.append(ix) # list\n",
    "    txt = test_char + ''.join(int_to_char[i] for i in ixes)\n",
    "    print ('----\\n %s \\n----' % (txt, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "nGVhl-Gxh6N6",
    "outputId": "e0c8b70b-fb50-4000-f4f8-a572539513db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " Califor Gooffic te nofft ove yke Gocr’sdlliamoffico \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "predict('C', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xPsz-oefL1kP"
   },
   "source": [
    "Well... that's *vaguely* language-looking. Can you do better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0lfZdD_cp1t5"
   },
   "source": [
    "# Assignment\n",
    "\n",
    "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
    "\n",
    "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
    "\n",
    "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
    "\n",
    "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
    "\n",
    "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
    "\n",
    "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ltj1je1fp5rO"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "req = requests.get('https://www.gutenberg.org/files/100/100-0.txt')\n",
    "# Text string of all Shakespear's works (First work with title starting at character 2969)\n",
    "text = req.text[2969:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sonnets 1 through 31\n",
    "article_text = text[40:20570]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique Characters: 69\n",
      "Total Number of Characters: 20530\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# List of unique characters in text\n",
    "chars = list(set(article_text))\n",
    "# Number of unique characters in text\n",
    "num_chars = len(chars)\n",
    "# Total number of characters in text\n",
    "text_data_size = len(article_text)\n",
    "\n",
    "print(f\"Number of Unique Characters: {num_chars}\")\n",
    "print(f\"Total Number of Characters: {text_data_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'z': 0, 'A': 1, 'u': 2, '0': 3, ';': 4, '4': 5, 't': 6, 'f': 7, 'm': 8, 'h': 9, 'b': 10, 'G': 11, 'o': 12, 'd': 13, 'a': 14, '6': 15, ':': 16, 'B': 17, 'j': 18, 'w': 19, '‘': 20, 'T': 21, 'L': 22, 'N': 23, '9': 24, '’': 25, 'y': 26, 'p': 27, 'U': 28, '7': 29, 'e': 30, 'g': 31, 'V': 32, 'n': 33, 'D': 34, '5': 35, '(': 36, 'M': 37, '\\n': 38, 'l': 39, ' ': 40, 'x': 41, 'S': 42, 'i': 43, 'W': 44, 's': 45, 'r': 46, '-': 47, 'k': 48, '?': 49, 'Y': 50, '2': 51, 'O': 52, '\\r': 53, 'c': 54, ',': 55, '.': 56, 'I': 57, 'R': 58, 'F': 59, ')': 60, '1': 61, '3': 62, 'v': 63, 'q': 64, 'C': 65, 'H': 66, 'P': 67, '8': 68}\n",
      "//---------------------------------//----------------------------------//\n",
      "{0: 'z', 1: 'A', 2: 'u', 3: '0', 4: ';', 5: '4', 6: 't', 7: 'f', 8: 'm', 9: 'h', 10: 'b', 11: 'G', 12: 'o', 13: 'd', 14: 'a', 15: '6', 16: ':', 17: 'B', 18: 'j', 19: 'w', 20: '‘', 21: 'T', 22: 'L', 23: 'N', 24: '9', 25: '’', 26: 'y', 27: 'p', 28: 'U', 29: '7', 30: 'e', 31: 'g', 32: 'V', 33: 'n', 34: 'D', 35: '5', 36: '(', 37: 'M', 38: '\\n', 39: 'l', 40: ' ', 41: 'x', 42: 'S', 43: 'i', 44: 'W', 45: 's', 46: 'r', 47: '-', 48: 'k', 49: '?', 50: 'Y', 51: '2', 52: 'O', 53: '\\r', 54: 'c', 55: ',', 56: '.', 57: 'I', 58: 'R', 59: 'F', 60: ')', 61: '1', 62: '3', 63: 'v', 64: 'q', 65: 'C', 66: 'H', 67: 'P', 68: '8'}\n",
      "//---------------------------------//----------------------------------//\n",
      "[59, 46, 12, 8, 40, 7, 14, 43, 46, 30, 45, 6, 40, 54, 46, 30, 14, 6, 2, 46, 30, 45, 40, 19, 30, 40, 13, 30, 45, 43, 46, 30, 40, 43, 33, 54, 46, 30, 14, 45, 30, 55, 53, 38, 21, 9, 14, 6, 40, 6, 9, 30, 46, 30, 10, 26, 40, 10, 30, 14, 2, 6, 26, 25, 45, 40, 46, 12, 45, 30, 40, 8, 43, 31, 9, 6, 40, 33, 30, 63, 30, 46, 40, 13, 43, 30, 55, 53, 38, 17, 2, 6, 40, 14, 45, 40, 6, 9, 30, 40, 46, 43, 27, 30, 46, 40, 45, 9, 12, 2, 39, 13, 40, 10, 26, 40, 6, 43, 8, 30, 40, 13, 30, 54, 30, 14, 45, 30, 55, 53, 38, 66, 43, 45, 40, 6, 30, 33, 13, 30, 46, 40, 9, 30, 43, 46, 40, 8, 43, 31, 9, 6, 40, 10, 30, 14, 46, 40, 9, 43, 45, 40, 8, 30, 8, 12, 46, 26, 16, 53, 38, 17, 2, 6, 40, 6, 9, 12, 2, 40, 54, 12, 33, 6, 46, 14, 54, 6, 30, 13, 40, 6, 12, 40, 6, 9, 43, 33, 30, 40, 12, 19, 33, 40, 10, 46, 43, 31, 9, 6, 40, 30, 26, 30, 45, 55, 53, 38, 59, 30, 30, 13, 25, 45, 6, 40, 6, 9, 26, 40, 39, 43, 31, 9, 6, 25, 45, 40, 7, 39, 14, 8, 30, 40, 19, 43, 6, 9, 40, 45, 30, 39, 7, 47, 45, 2, 10, 45, 6, 14, 33, 6, 43, 14, 39, 40, 7, 2, 30, 39, 55, 53, 38, 37, 14, 48, 43, 33, 31, 40, 14, 40, 7, 14, 8, 43, 33, 30, 40, 19, 9, 30, 46, 30, 40, 14, 10, 2, 33, 13, 14, 33, 54, 30, 40, 39, 43, 30, 45, 55, 53, 38, 21, 9, 26, 40, 45, 30, 39, 7, 40, 6, 9, 26, 40, 7, 12, 30, 55, 40, 6, 12, 40, 6, 9, 26, 40, 45, 19, 30, 30, 6, 40, 45, 30, 39, 7, 40, 6, 12, 12, 40, 54, 46, 2, 30, 39, 16, 53, 38, 21, 9, 12, 2, 40, 6, 9, 14, 6, 40, 14, 46, 6, 40, 33, 12, 19, 40, 6, 9, 30, 40, 19, 12, 46, 39, 13, 25, 45, 40, 7, 46, 30, 45, 9, 40, 12, 46, 33, 14, 8, 30, 33, 6, 55, 53, 38, 1, 33, 13, 40, 12, 33, 39, 26, 40, 9, 30, 46, 14, 39, 13, 40, 6, 12, 40, 6, 9, 30, 40, 31, 14, 2, 13, 26, 40, 45, 27, 46, 43, 33, 31, 55, 53, 38, 44, 43, 6, 9, 43, 33, 40, 6, 9, 43, 33, 30, 40, 12, 19, 33, 40, 10, 2, 13, 40, 10, 2, 46, 43, 30, 45, 6, 40, 6, 9, 26, 40, 54, 12, 33, 6, 30, 33, 6, 55, 53, 38, 1, 33, 13, 55, 40, 6, 30, 33, 13, 30, 46, 40, 54, 9, 2, 46, 39, 55, 40, 8, 14, 48, 25, 45, 6, 40, 19, 14, 45, 6, 30, 40, 43, 33, 40, 33, 43, 31, 31, 14, 46, 13, 43, 33, 31, 16, 53, 38, 40, 40, 67, 43, 6, 26, 40, 6, 9, 30, 40, 19, 12, 46, 39, 13, 55, 40, 12, 46, 40, 30, 39, 45, 30, 40, 6, 9, 43, 45, 40, 31, 39, 2, 6, 6, 12, 33, 40, 10, 30, 55, 53, 38, 40, 40, 21, 12, 40, 30, 14, 6, 40, 6, 9, 30, 40, 19, 12, 46, 39, 13, 25, 45, 40, 13, 2, 30, 55, 40, 10, 26, 40, 6, 9, 30, 40, 31, 46, 14, 63, 30, 40, 14, 33, 13, 40, 6, 9, 30, 30, 56, 53, 38, 53, 38, 53, 38, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 51, 53, 38, 53, 38, 44, 9, 30, 33, 40, 7, 12, 46, 6, 26, 40, 19, 43, 33, 6, 30, 46, 45, 40, 45, 9, 14, 39, 39, 40, 10, 30, 45, 43, 30, 31, 30, 40, 6, 9, 26, 40, 10, 46, 12, 19, 55, 53, 38, 1, 33, 13, 40, 13, 43, 31, 40, 13, 30, 30, 27, 40, 6, 46, 30, 33, 54, 9, 30, 45, 40, 43, 33, 40, 6, 9, 26, 40, 10, 30, 14, 2, 6, 26, 25, 45, 40, 7, 43, 30, 39, 13, 55, 53, 38, 21, 9, 26, 40, 26, 12, 2, 6, 9, 25, 45, 40, 27, 46, 12, 2, 13, 40, 39, 43, 63, 30, 46, 26, 40, 45, 12, 40, 31, 14, 0, 30, 13, 40, 12, 33, 40, 33, 12, 19, 55, 53, 38, 44, 43, 39, 39, 40, 10, 30, 40, 14, 40, 6, 14, 6, 6, 30, 46, 30, 13, 40, 19, 30, 30, 13, 40, 12, 7, 40, 45, 8, 14, 39, 39, 40, 19, 12, 46, 6, 9, 40, 9, 30, 39, 13, 16, 53, 38, 21, 9, 30, 33, 40, 10, 30, 43, 33, 31, 40, 14, 45, 48, 30, 13, 55, 40, 19, 9, 30, 46, 30, 40, 14, 39, 39, 40, 6, 9, 26, 40, 10, 30, 14, 2, 6, 26, 40, 39, 43, 30, 45, 55, 53, 38, 44, 9, 30, 46, 30, 40, 14, 39, 39, 40, 6, 9, 30, 40, 6, 46, 30, 14, 45, 2, 46, 30, 40, 12, 7, 40, 6, 9, 26, 40, 39, 2, 45, 6, 26, 40, 13, 14, 26, 45, 4, 53, 38, 21, 12, 40, 45, 14, 26, 55, 40, 19, 43, 6, 9, 43, 33, 40, 6, 9, 43, 33, 30, 40, 12, 19, 33, 40, 13, 30, 30, 27, 40, 45, 2, 33, 48, 30, 33, 40, 30, 26, 30, 45, 55, 53, 38, 44, 30, 46, 30, 40, 14, 33, 40, 14, 39, 39, 47, 30, 14, 6, 43, 33, 31, 40, 45, 9, 14, 8, 30, 55, 40, 14, 33, 13, 40, 6, 9, 46, 43, 7, 6, 39, 30, 45, 45, 40, 27, 46, 14, 43, 45, 30, 56, 53, 38, 66, 12, 19, 40, 8, 2, 54, 9, 40, 8, 12, 46, 30, 40, 27, 46, 14, 43, 45, 30, 40, 13, 30, 45, 30, 46, 63, 25, 13, 40, 6, 9, 26, 40, 10, 30, 14, 2, 6, 26, 25, 45, 40, 2, 45, 30, 55, 53, 38, 57, 7, 40, 6, 9, 12, 2, 40, 54, 12, 2, 39, 13, 45, 6, 40, 14, 33, 45, 19, 30, 46, 40, 20, 21, 9, 43, 45, 40, 7, 14, 43, 46, 40, 54, 9, 43, 39, 13, 40, 12, 7, 40, 8, 43, 33, 30, 53, 38, 42, 9, 14, 39, 39, 40, 45, 2, 8, 40, 8, 26, 40, 54, 12, 2, 33, 6, 55, 40, 14, 33, 13, 40, 8, 14, 48, 30, 40, 8, 26, 40, 12, 39, 13, 40, 30, 41, 54, 2, 45, 30, 55, 25, 53, 38, 67, 46, 12, 63, 43, 33, 31, 40, 9, 43, 45, 40, 10, 30, 14, 2, 6, 26, 40, 10, 26, 40, 45, 2, 54, 54, 30, 45, 45, 43, 12, 33, 40, 6, 9, 43, 33, 30, 56, 53, 38, 40, 40, 21, 9, 43, 45, 40, 19, 30, 46, 30, 40, 6, 12, 40, 10, 30, 40, 33, 30, 19, 40, 8, 14, 13, 30, 40, 19, 9, 30, 33, 40, 6, 9, 12, 2, 40, 14, 46, 6, 40, 12, 39, 13, 55, 53, 38, 40, 40, 1, 33, 13, 40, 45, 30, 30, 40, 6, 9, 26, 40, 10, 39, 12, 12, 13, 40, 19, 14, 46, 8, 40, 19, 9, 30, 33, 40, 6, 9, 12, 2, 40, 7, 30, 30, 39, 25, 45, 6, 40, 43, 6, 40, 54, 12, 39, 13, 56, 53, 38, 53, 38, 53, 38, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 62, 53, 38, 53, 38, 22, 12, 12, 48, 40, 43, 33, 40, 6, 9, 26, 40, 31, 39, 14, 45, 45, 40, 14, 33, 13, 40, 6, 30, 39, 39, 40, 6, 9, 30, 40, 7, 14, 54, 30, 40, 6, 9, 12, 2, 40, 63, 43, 30, 19, 30, 45, 6, 55, 53, 38, 23, 12, 19, 40, 43, 45, 40, 6, 9, 30, 40, 6, 43, 8, 30, 40, 6, 9, 14, 6, 40, 7, 14, 54, 30, 40, 45, 9, 12, 2, 39, 13, 40, 7, 12, 46, 8, 40, 14, 33, 12, 6, 9, 30, 46, 55, 53, 38, 44, 9, 12, 45, 30, 40, 7, 46, 30, 45, 9, 40, 46, 30, 27, 14, 43, 46, 40, 43, 7, 40, 33, 12, 19, 40, 6, 9, 12, 2, 40, 33, 12, 6, 40, 46, 30, 33, 30, 19, 30, 45, 6, 55, 53, 38, 21, 9, 12, 2, 40, 13, 12, 45, 6, 40, 10, 30, 31, 2, 43, 39, 30, 40, 6, 9, 30, 40, 19, 12, 46, 39, 13, 55, 40, 2, 33, 10, 39, 30, 45, 45, 40, 45, 12, 8, 30, 40, 8, 12, 6, 9, 30, 46, 56, 53, 38, 59, 12, 46, 40, 19, 9, 30, 46, 30, 40, 43, 45, 40, 45, 9, 30, 40, 45, 12, 40, 7, 14, 43, 46, 40, 19, 9, 12, 45, 30, 40, 2, 33, 30, 14, 46, 30, 13, 40, 19, 12, 8, 10, 53, 38, 34, 43, 45, 13, 14, 43, 33, 45, 40, 6, 9, 30, 40, 6, 43, 39, 39, 14, 31, 30, 40, 12, 7, 40, 6, 9, 26, 40, 9, 2, 45, 10, 14, 33, 13, 46, 26, 49, 53, 38, 52, 46, 40, 19, 9, 12, 40, 43, 45, 40, 9, 30, 40, 45, 12, 40, 7, 12, 33, 13, 40, 19, 43, 39, 39, 40, 10, 30, 40, 6, 9, 30, 40, 6, 12, 8, 10, 53, 38, 52, 7, 40, 9, 43, 45, 40, 45, 30, 39, 7, 47, 39, 12, 63, 30, 40, 6, 12, 40, 45, 6, 12, 27, 40, 27, 12, 45, 6, 30, 46, 43, 6, 26, 49, 53, 38, 21, 9, 12, 2, 40, 14, 46, 6, 40, 6, 9, 26, 40, 8, 12, 6, 9, 30, 46, 25, 45, 40, 31, 39, 14, 45, 45, 40, 14, 33, 13, 40, 45, 9, 30, 40, 43, 33, 40, 6, 9, 30, 30, 53, 38, 65, 14, 39, 39, 45, 40, 10, 14, 54, 48, 40, 6, 9, 30, 40, 39, 12, 63, 30, 39, 26, 40, 1, 27, 46, 43, 39, 40, 12, 7, 40, 9, 30, 46, 40, 27, 46, 43, 8, 30, 55, 53, 38, 42, 12, 40, 6, 9, 12, 2, 40, 6, 9, 46, 12, 2, 31, 9, 40, 19, 43, 33, 13, 12, 19, 45, 40, 12, 7, 40, 6, 9, 43, 33, 30, 40, 14, 31, 30, 40, 45, 9, 14, 39, 6, 40, 45, 30, 30, 55, 53, 38, 34, 30, 45, 27, 43, 6, 30, 40, 12, 7, 40, 19, 46, 43, 33, 48, 39, 30, 45, 40, 6, 9, 43, 45, 40, 6, 9, 26, 40, 31, 12, 39, 13, 30, 33, 40, 6, 43, 8, 30, 56, 53, 38, 40, 40, 17, 2, 6, 40, 43, 7, 40, 6, 9, 12, 2, 40, 39, 43, 63, 30, 40, 46, 30, 8, 30, 8, 10, 30, 46, 30, 13, 40, 33, 12, 6, 40, 6, 12, 40, 10, 30, 55, 53, 38, 40, 40, 34, 43, 30, 40, 45, 43, 33, 31, 39, 30, 40, 14, 33, 13, 40, 6, 9, 43, 33, 30, 40, 43, 8, 14, 31, 30, 40, 13, 43, 30, 45, 40, 19, 43, 6, 9, 40, 6, 9, 30, 30, 56, 53, 38, 53, 38, 53, 38, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 5, 53, 38, 53, 38, 28, 33, 6, 9, 46, 43, 7, 6, 26, 40, 39, 12, 63, 30, 39, 43, 33, 30, 45, 45, 40, 19, 9, 26, 40, 13, 12, 45, 6, 40, 6, 9, 12, 2, 40, 45, 27, 30, 33, 13, 55, 53, 38, 28, 27, 12, 33, 40, 6, 9, 26, 40, 45, 30, 39, 7, 40, 6, 9, 26, 40, 10, 30, 14, 2, 6, 26, 25, 45, 40, 39, 30, 31, 14, 54, 26, 49, 53, 38, 23, 14, 6, 2, 46, 30, 25, 45, 40, 10, 30, 64, 2, 30, 45, 6, 40, 31, 43, 63, 30, 45, 40, 33, 12, 6, 9, 43, 33, 31, 40, 10, 2, 6, 40, 13, 12, 6, 9, 40, 39, 30, 33, 13, 55, 53, 38, 1, 33, 13, 40, 10, 30, 43, 33, 31, 40, 7, 46, 14, 33, 48, 40, 45, 9, 30, 40, 39, 30, 33, 13, 45, 40, 6, 12, 40, 6, 9, 12, 45, 30, 40, 14, 46, 30, 40, 7, 46, 30, 30, 16, 53, 38, 21, 9, 30, 33, 40, 10, 30, 14, 2, 6, 30, 12, 2, 45, 40, 33, 43, 31, 31, 14, 46, 13, 40, 19, 9, 26, 40, 13, 12, 45, 6, 40, 6, 9, 12, 2, 40, 14, 10, 2, 45, 30, 55, 53, 38, 21, 9, 30, 40, 10, 12, 2, 33, 6, 30, 12, 2, 45, 40, 39, 14, 46, 31, 30, 45, 45, 40, 31, 43, 63, 30, 33, 40, 6, 9, 30, 30, 40, 6, 12, 40, 31, 43, 63, 30, 49, 53, 38, 67, 46, 12, 7, 43, 6, 39, 30, 45, 45, 40, 2, 45, 2, 46, 30, 46, 40, 19, 9, 26, 40, 13, 12, 45, 6, 40, 6, 9, 12, 2, 40, 2, 45, 30, 53, 38, 42, 12, 40, 31, 46, 30, 14, 6, 40, 14, 40, 45, 2, 8, 40, 12, 7, 40, 45, 2, 8, 45, 40, 26, 30, 6, 40, 54, 14, 33, 45, 6, 40, 33, 12, 6, 40, 39, 43, 63, 30, 49, 53, 38, 59, 12, 46, 40, 9, 14, 63, 43, 33, 31, 40, 6, 46, 14, 7, 7, 43, 54, 40, 19, 43, 6, 9, 40, 6, 9, 26, 40, 45, 30, 39, 7, 40, 14, 39, 12, 33, 30, 55, 53, 38, 21, 9, 12, 2, 40, 12, 7, 40, 6, 9, 26, 40, 45, 30, 39, 7, 40, 6, 9, 26, 40, 45, 19, 30, 30, 6, 40, 45, 30, 39, 7, 40, 13, 12, 45, 6, 40, 13, 30, 54, 30, 43, 63, 30, 55, 53, 38, 21, 9, 30, 33, 40, 9, 12, 19, 40, 19, 9, 30, 33, 40, 33, 14, 6, 2, 46, 30, 40, 54, 14, 39, 39, 45, 40, 6, 9, 30, 30, 40, 6, 12, 40, 10, 30, 40, 31, 12, 33, 30, 55, 53, 38, 44, 9, 14, 6, 40, 14, 54, 54, 30, 27, 6, 14, 10, 39, 30, 40, 14, 2, 13, 43, 6, 40, 54, 14, 33, 45, 6, 40, 6, 9, 12, 2, 40, 39, 30, 14, 63, 30, 49, 53, 38, 40, 40, 21, 9, 26, 40, 2, 33, 2, 45, 30, 13, 40, 10, 30, 14, 2, 6, 26, 40, 8, 2, 45, 6, 40, 10, 30, 40, 6, 12, 8, 10, 30, 13, 40, 19, 43, 6, 9, 40, 6, 9, 30, 30, 55, 53, 38, 40, 40, 44, 9, 43, 54, 9, 40, 2, 45, 30, 13, 40, 39, 43, 63, 30, 45, 40, 6, 9, 25, 40, 30, 41, 30, 54, 2, 6, 12, 46, 40, 6, 12, 40, 10, 30, 56, 53, 38, 53, 38, 53, 38, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 35, 53, 38, 53, 38, 21, 9, 12, 45, 30, 40, 9, 12, 2, 46, 45, 40, 6, 9, 14, 6, 40, 19, 43, 6, 9, 40, 31, 30, 33, 6, 39, 30, 40, 19, 12, 46, 48, 40, 13, 43, 13, 40, 7, 46, 14, 8, 30, 53, 38, 21, 9, 30, 40, 39, 12, 63, 30, 39, 26, 40, 31, 14, 0, 30, 40, 19, 9, 30, 46, 30, 40, 30, 63, 30, 46, 26, 40, 30, 26, 30, 40, 13, 12, 6, 9, 40, 13, 19, 30, 39, 39, 53, 38, 44, 43, 39, 39, 40, 27, 39, 14, 26, 40, 6, 9, 30, 40, 6, 26, 46, 14, 33, 6, 45, 40, 6, 12, 40, 6, 9, 30, 40, 63, 30, 46, 26, 40, 45, 14, 8, 30, 55, 53, 38, 1, 33, 13, 40, 6, 9, 14, 6, 40, 2, 33, 7, 14, 43, 46, 40, 19, 9, 43, 54, 9, 40, 7, 14, 43, 46, 39, 26, 40, 13, 12, 6, 9, 40, 30, 41, 54, 30, 39, 16, 53, 38, 59, 12, 46, 40, 33, 30, 63, 30, 46, 47, 46, 30, 45, 6, 43, 33, 31, 40, 6, 43, 8, 30, 40, 39, 30, 14, 13, 45, 40, 45, 2, 8, 8, 30, 46, 40, 12, 33, 53, 38, 21, 12, 40, 9, 43, 13, 30, 12, 2, 45, 40, 19, 43, 33, 6, 30, 46, 40, 14, 33, 13, 40, 54, 12, 33, 7, 12, 2, 33, 13, 45, 40, 9, 43, 8, 40, 6, 9, 30, 46, 30, 55, 53, 38, 42, 14, 27, 40, 54, 9, 30, 54, 48, 30, 13, 40, 19, 43, 6, 9, 40, 7, 46, 12, 45, 6, 40, 14, 33, 13, 40, 39, 2, 45, 6, 26, 40, 39, 30, 14, 63, 30, 45, 40, 64, 2, 43, 6, 30, 40, 31, 12, 33, 30, 55, 53, 38, 17, 30, 14, 2, 6, 26, 40, 12, 25, 30, 46, 47, 45, 33, 12, 19, 30, 13, 40, 14, 33, 13, 40, 10, 14, 46, 30, 33, 30, 45, 45, 40, 30, 63, 30, 46, 26, 40, 19, 9, 30, 46, 30, 16, 53, 38, 21, 9, 30, 33, 40, 19, 30, 46, 30, 40, 33, 12, 6, 40, 45, 2, 8, 8, 30, 46, 25, 45, 40, 13, 43, 45, 6, 43, 39, 39, 14, 6, 43, 12, 33, 40, 39, 30, 7, 6, 53, 38, 1, 40, 39, 43, 64, 2, 43, 13, 40, 27, 46, 43, 45, 12, 33, 30, 46, 40, 27, 30, 33, 6, 40, 43, 33, 40, 19, 14, 39, 39, 45, 40, 12, 7, 40, 31, 39, 14, 45, 45, 55, 53, 38, 17, 30, 14, 2, 6, 26, 25, 45, 40, 30, 7, 7, 30, 54, 6, 40, 19, 43, 6, 9, 40, 10, 30, 14, 2, 6, 26, 40, 19, 30, 46, 30, 40, 10, 30, 46, 30, 7, 6, 55, 53, 38, 23, 12, 46, 40, 43, 6, 40, 33, 12, 46, 40, 33, 12, 40, 46, 30, 8, 30, 8, 10, 46, 14, 33, 54, 30, 40, 19, 9, 14, 6, 40, 43, 6, 40, 19, 14, 45, 56, 53, 38, 40, 40, 17, 2, 6, 40, 7, 39, 12, 19, 30, 46, 45, 40, 13, 43, 45, 6, 43, 39, 39, 30, 13, 40, 6, 9, 12, 2, 31, 9, 40, 6, 9, 30, 26, 40, 19, 43, 6, 9, 40, 19, 43, 33, 6, 30, 46, 40, 8, 30, 30, 6, 55, 53, 38, 40, 40, 22, 30, 30, 45, 30, 40, 10, 2, 6, 40, 6, 9, 30, 43, 46, 40, 45, 9, 12, 19, 55, 40, 6, 9, 30, 43, 46, 40, 45, 2, 10, 45, 6, 14, 33, 54, 30, 40, 45, 6, 43, 39, 39, 40, 39, 43, 63, 30, 45, 40, 45, 19, 30, 30, 6, 56, 53, 38, 53, 38, 53, 38, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 15, 53, 38, 53, 38, 21, 9, 30, 33, 40, 39, 30, 6, 40, 33, 12, 6, 40, 19, 43, 33, 6, 30, 46, 25, 45, 40, 46, 14, 31, 31, 30, 13, 40, 9, 14, 33, 13, 40, 13, 30, 7, 14, 54, 30, 55, 53, 38, 57, 33, 40, 6, 9, 30, 30, 40, 6, 9, 26, 40, 45, 2, 8, 8, 30, 46, 40, 30, 46, 30, 40, 6, 9, 12, 2, 40, 10, 30, 40, 13, 43, 45, 6, 43, 39, 39, 30, 13, 16, 53, 38, 37, 14, 48, 30, 40, 45, 19, 30, 30, 6, 40, 45, 12, 8, 30, 40, 63, 43, 14, 39, 4, 40, 6, 46, 30, 14, 45, 2, 46, 30, 40, 6, 9, 12, 2, 40, 45, 12, 8, 30, 40, 27, 39, 14, 54, 30, 55, 53, 38, 44, 43, 6, 9, 40, 10, 30, 14, 2, 6, 26, 25, 45, 40, 6, 46, 30, 14, 45, 2, 46, 30, 40, 30, 46, 30, 40, 43, 6, 40, 10, 30, 40, 45, 30, 39, 7, 47, 48, 43, 39, 39, 30, 13, 16, 53, 38, 21, 9, 14, 6, 40, 2, 45, 30, 40, 43, 45, 40, 33, 12, 6, 40, 7, 12, 46, 10, 43, 13, 13, 30, 33, 40, 2, 45, 2, 46, 26, 55, 53, 38, 44, 9, 43, 54, 9, 40, 9, 14, 27, 27, 43, 30, 45, 40, 6, 9, 12, 45, 30, 40, 6, 9, 14, 6, 40, 27, 14, 26, 40, 6, 9, 30, 40, 19, 43, 39, 39, 43, 33, 31, 40, 39, 12, 14, 33, 4, 53, 38, 21, 9, 14, 6, 25, 45, 40, 7, 12, 46, 40, 6, 9, 26, 40, 45, 30, 39, 7, 40, 6, 12, 40, 10, 46, 30, 30, 13, 40, 14, 33, 12, 6, 9, 30, 46, 40, 6, 9, 30, 30, 55, 53, 38, 52, 46, 40, 6, 30, 33, 40, 6, 43, 8, 30, 45, 40, 9, 14, 27, 27, 43, 30, 46, 40, 10, 30, 40, 43, 6, 40, 6, 30, 33, 40, 7, 12, 46, 40, 12, 33, 30, 55, 53, 38, 21, 30, 33, 40, 6, 43, 8, 30, 45, 40, 6, 9, 26, 40, 45, 30, 39, 7, 40, 19, 30, 46, 30, 40, 9, 14, 27, 27, 43, 30, 46, 40, 6, 9, 14, 33, 40, 6, 9, 12, 2, 40, 14, 46, 6, 55, 53, 38, 57, 7, 40, 6, 30, 33, 40, 12, 7, 40, 6, 9, 43, 33, 30, 40, 6, 30, 33, 40, 6, 43, 8, 30, 45, 40, 46, 30, 7, 43, 31, 2, 46, 30, 13, 40, 6, 9, 30, 30, 16, 53, 38, 21, 9, 30, 33, 40, 19, 9, 14, 6, 40, 54, 12, 2, 39, 13, 40, 13, 30, 14, 6, 9, 40, 13, 12, 40, 43, 7, 40, 6, 9, 12, 2, 40, 45, 9, 12, 2, 39, 13, 45, 6, 40, 13, 30, 27, 14, 46, 6, 55, 53, 38, 22, 30, 14, 63, 43, 33, 31, 40, 6, 9, 30, 30, 40, 39, 43, 63, 43, 33, 31, 40, 43, 33, 40, 27, 12, 45, 6, 30, 46, 43, 6, 26, 49, 53, 38, 40, 40, 17, 30, 40, 33, 12, 6, 40, 45, 30, 39, 7, 47, 19, 43, 39, 39, 30, 13, 40, 7, 12, 46, 40, 6, 9, 12, 2, 40, 14, 46, 6, 40, 8, 2, 54, 9, 40, 6, 12, 12, 40, 7, 14, 43, 46, 55, 53, 38, 40, 40, 21, 12, 40, 10, 30, 40, 13, 30, 14, 6, 9, 25, 45, 40, 54, 12, 33, 64, 2, 30, 45, 6, 40, 14, 33, 13, 40, 8, 14, 48, 30, 40, 19, 12, 46, 8, 45, 40, 6, 9, 43, 33, 30, 40, 9, 30, 43, 46, 56, 53, 38, 53, 38, 53, 38, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 29, 53, 38, 53, 38, 22, 12, 40, 43, 33, 40, 6, 9, 30, 40, 12, 46, 43, 30, 33, 6, 40, 19, 9, 30, 33, 40, 6, 9, 30, 40, 31, 46, 14, 54, 43, 12, 2, 45, 40, 39, 43, 31, 9, 6, 53, 38, 22, 43, 7, 6, 45, 40, 2, 27, 40, 9, 43, 45, 40, 10, 2, 46, 33, 43, 33, 31, 40, 9, 30, 14, 13, 55, 40, 30, 14, 54, 9, 40, 2, 33, 13, 30, 46, 40, 30, 26, 30, 53, 38, 34, 12, 6, 9, 40, 9, 12, 8, 14, 31, 30, 40, 6, 12, 40, 9, 43, 45, 40, 33, 30, 19, 47, 14, 27, 27, 30, 14, 46, 43, 33, 31, 40, 45, 43, 31, 9, 6, 55, 53, 38, 42, 30, 46, 63, 43, 33, 31, 40, 19, 43, 6, 9, 40, 39, 12, 12, 48, 45, 40, 9, 43, 45, 40, 45, 14, 54, 46, 30, 13, 40, 8, 14, 18, 30, 45, 6, 26, 55, 53, 38, 1, 33, 13, 40, 9, 14, 63, 43, 33, 31, 40, 54, 39, 43, 8, 10, 30, 13, 40, 6, 9, 30, 40, 45, 6, 30, 30, 27, 47, 2, 27, 40, 9, 30, 14, 63, 30, 33, 39, 26, 40, 9, 43, 39, 39, 55, 53, 38, 58, 30, 45, 30, 8, 10, 39, 43, 33, 31, 40, 45, 6, 46, 12, 33, 31, 40, 26, 12, 2, 6, 9, 40, 43, 33, 40, 9, 43, 45, 40, 8, 43, 13, 13, 39, 30, 40, 14, 31, 30, 55, 53, 38, 50, 30, 6, 40, 8, 12, 46, 6, 14, 39, 40, 39, 12, 12, 48, 45, 40, 14, 13, 12, 46, 30, 40, 9, 43, 45, 40, 10, 30, 14, 2, 6, 26, 40, 45, 6, 43, 39, 39, 55, 53, 38, 1, 6, 6, 30, 33, 13, 43, 33, 31, 40, 12, 33, 40, 9, 43, 45, 40, 31, 12, 39, 13, 30, 33, 40, 27, 43, 39, 31, 46, 43, 8, 14, 31, 30, 16, 53, 38, 17, 2, 6, 40, 19, 9, 30, 33, 40, 7, 46, 12, 8, 40, 9, 43, 31, 9, 8, 12, 45, 6, 40, 27, 43, 6, 54, 9, 40, 19, 43, 6, 9, 40, 19, 30, 14, 46, 26, 40, 54, 14, 46, 55, 53, 38, 22, 43, 48, 30, 40, 7, 30, 30, 10, 39, 30, 40, 14, 31, 30, 40, 9, 30, 40, 46, 30, 30, 39, 30, 6, 9, 40, 7, 46, 12, 8, 40, 6, 9, 30, 40, 13, 14, 26, 55, 53, 38, 21, 9, 30, 40, 30, 26, 30, 45, 40, 36, 7, 12, 46, 30, 40, 13, 2, 6, 30, 12, 2, 45, 60, 40, 33, 12, 19, 40, 54, 12, 33, 63, 30, 46, 6, 30, 13, 40, 14, 46, 30, 53, 38, 59, 46, 12, 8, 40, 9, 43, 45, 40, 39, 12, 19, 40, 6, 46, 14, 54, 6, 40, 14, 33, 13, 40, 39, 12, 12, 48, 40, 14, 33, 12, 6, 9, 30, 46, 40, 19, 14, 26, 16, 53, 38, 40, 40, 42, 12, 40, 6, 9, 12, 2, 55, 40, 6, 9, 26, 40, 45, 30, 39, 7, 40, 12, 2, 6, 47, 31, 12, 43, 33, 31, 40, 43, 33, 40, 6, 9, 26, 40, 33, 12, 12, 33, 16, 53, 38, 40, 40, 28, 33, 39, 12, 12, 48, 30, 13, 40, 12, 33, 40, 13, 43, 30, 45, 6, 40, 2, 33, 39, 30, 45, 45, 40, 6, 9, 12, 2, 40, 31, 30, 6, 40, 14, 40, 45, 12, 33, 56, 53, 38, 53, 38, 53, 38, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 68, 53, 38, 53, 38, 37, 2, 45, 43, 54, 40, 6, 12, 40, 9, 30, 14, 46, 55, 40, 19, 9, 26, 40, 9, 30, 14, 46, 25, 45, 6, 40, 6, 9, 12, 2, 40, 8, 2, 45, 43, 54, 40, 45, 14, 13, 39, 26, 49, 53, 38, 42, 19, 30, 30, 6, 45, 40, 19, 43, 6, 9, 40, 45, 19, 30, 30, 6, 45, 40, 19, 14, 46, 40, 33, 12, 6, 55, 40, 18, 12, 26, 40, 13, 30, 39, 43, 31, 9, 6, 45, 40, 43, 33, 40, 18, 12, 26, 16, 53, 38, 44, 9, 26, 40, 39, 12, 63, 25, 45, 6, 40, 6, 9, 12, 2, 40, 6, 9, 14, 6, 40, 19, 9, 43, 54, 9, 40, 6, 9, 12, 2, 40, 46, 30, 54, 30, 43, 63, 25, 45, 6, 40, 33, 12, 6, 40, 31, 39, 14, 13, 39, 26, 55, 53, 38, 52, 46, 40, 30, 39, 45, 30, 40, 46, 30, 54, 30, 43, 63, 25, 45, 6, 40, 19, 43, 6, 9, 40, 27, 39, 30, 14, 45, 2, 46, 30, 40, 6, 9, 43, 33, 30, 40, 14, 33, 33, 12, 26, 49, 53, 38, 57, 7, 40, 6, 9, 30, 40, 6, 46, 2, 30, 40, 54, 12, 33, 54, 12, 46, 13, 40, 12, 7, 40, 19, 30, 39, 39, 47, 6, 2, 33, 30, 13, 40, 45, 12, 2, 33, 13, 45, 55, 53, 38, 17, 26, 40, 2, 33, 43, 12, 33, 45, 40, 8, 14, 46, 46, 43, 30, 13, 40, 13, 12, 40, 12, 7, 7, 30, 33, 13, 40, 6, 9, 43, 33, 30, 40, 30, 14, 46, 55, 53, 38, 21, 9, 30, 26, 40, 13, 12, 40, 10, 2, 6, 40, 45, 19, 30, 30, 6, 39, 26, 40, 54, 9, 43, 13, 30, 40, 6, 9, 30, 30, 55, 40, 19, 9, 12, 40, 54, 12, 33, 7, 12, 2, 33, 13, 45, 53, 38, 57, 33, 40, 45, 43, 33, 31, 39, 30, 33, 30, 45, 45, 40, 6, 9, 30, 40, 27, 14, 46, 6, 45, 40, 6, 9, 14, 6, 40, 6, 9, 12, 2, 40, 45, 9, 12, 2, 39, 13, 45, 6, 40, 10, 30, 14, 46, 16, 53, 38, 37, 14, 46, 48, 40, 9, 12, 19, 40, 12, 33, 30, 40, 45, 6, 46, 43, 33, 31, 40, 45, 19, 30, 30, 6, 40, 9, 2, 45, 10, 14, 33, 13, 40, 6, 12, 40, 14, 33, 12, 6, 9, 30, 46, 55, 53, 38, 42, 6, 46, 43, 48, 30, 45, 40, 30, 14, 54, 9, 40, 43, 33, 40, 30, 14, 54, 9, 40, 10, 26, 40, 8, 2, 6, 2, 14, 39, 40, 12, 46, 13, 30, 46, 43, 33, 31, 4, 53, 38, 58, 30, 45, 30, 8, 10, 39, 43, 33, 31, 40, 45, 43, 46, 30, 55, 40, 14, 33, 13, 40, 54, 9, 43, 39, 13, 55, 40, 14, 33, 13, 40, 9, 14, 27, 27, 26, 40, 8, 12, 6, 9, 30, 46, 55, 53, 38, 44, 9, 12, 40, 14, 39, 39, 40, 43, 33, 40, 12, 33, 30, 55, 40, 12, 33, 30, 40, 27, 39, 30, 14, 45, 43, 33, 31, 40, 33, 12, 6, 30, 40, 13, 12, 40, 45, 43, 33, 31, 16, 53, 38, 40, 40, 44, 9, 12, 45, 30, 40, 45, 27, 30, 30, 54, 9, 39, 30, 45, 45, 40, 45, 12, 33, 31, 40, 10, 30, 43, 33, 31, 40, 8, 14, 33, 26, 55, 40, 45, 30, 30, 8, 43, 33, 31, 40, 12, 33, 30, 55, 53, 38, 40, 40, 42, 43, 33, 31, 45, 40, 6, 9, 43, 45, 40, 6, 12, 40, 6, 9, 30, 30, 55, 40, 20, 21, 9, 12, 2, 40, 45, 43, 33, 31, 39, 30, 40, 19, 43, 39, 6, 40, 27, 46, 12, 63, 30, 40, 33, 12, 33, 30, 25, 56, 53, 38, 53, 38, 53, 38, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 24, 53, 38, 53, 38, 57, 45, 40, 43, 6, 40, 7, 12, 46, 40, 7, 30, 14, 46, 40, 6, 12, 40, 19, 30, 6, 40, 14, 40, 19, 43, 13, 12, 19, 25, 45, 40, 30, 26, 30, 55, 53, 38, 21, 9, 14, 6, 40, 6, 9, 12, 2, 40, 54, 12, 33, 45, 2, 8, 25, 45, 6, 40, 6, 9, 26, 40, 45, 30, 39, 7, 40, 43, 33, 40, 45, 43, 33, 31, 39, 30, 40, 39, 43, 7, 30, 49, 53, 38, 1, 9, 55, 40, 43, 7, 40, 6, 9, 12, 2, 40, 43, 45, 45, 2, 30, 39, 30, 45, 45, 40, 45, 9, 14, 39, 6, 40, 9, 14, 27, 40, 6, 12, 40, 13, 43, 30, 55, 53, 38, 21, 9, 30, 40, 19, 12, 46, 39, 13, 40, 19, 43, 39, 39, 40, 19, 14, 43, 39, 40, 6, 9, 30, 30, 40, 39, 43, 48, 30, 40, 14, 40, 8, 14, 48, 30, 39, 30, 45, 45, 40, 19, 43, 7, 30, 55, 53, 38, 21, 9, 30, 40, 19, 12, 46, 39, 13, 40, 19, 43, 39, 39, 40, 10, 30, 40, 6, 9, 26, 40, 19, 43, 13, 12, 19, 40, 14, 33, 13, 40, 45, 6, 43, 39, 39, 40, 19, 30, 30, 27, 55, 53, 38, 21, 9, 14, 6, 40, 6, 9, 12, 2, 40, 33, 12, 40, 7, 12, 46, 8, 40, 12, 7, 40, 6, 9, 30, 30, 40, 9, 14, 45, 6, 40, 39, 30, 7, 6, 40, 10, 30, 9, 43, 33, 13, 55, 53, 38, 44, 9, 30, 33, 40, 30, 63, 30, 46, 26, 40, 27, 46, 43, 63, 14, 6, 30, 40, 19, 43, 13, 12, 19, 40, 19, 30, 39, 39, 40, 8, 14, 26, 40, 48, 30, 30, 27, 55, 53, 38, 17, 26, 40, 54, 9, 43, 39, 13, 46, 30, 33, 25, 45, 40, 30, 26, 30, 45, 55, 40, 9, 30, 46, 40, 9, 2, 45, 10, 14, 33, 13, 25, 45, 40, 45, 9, 14, 27, 30, 40, 43, 33, 40, 8, 43, 33, 13, 16, 53, 38, 22, 12, 12, 48, 40, 19, 9, 14, 6, 40, 14, 33, 40, 2, 33, 6, 9, 46, 43, 7, 6, 40, 43, 33, 40, 6, 9, 30, 40, 19, 12, 46, 39, 13, 40, 13, 12, 6, 9, 40, 45, 27, 30, 33, 13, 53, 38, 42, 9, 43, 7, 6, 45, 40, 10, 2, 6, 40, 9, 43, 45, 40, 27, 39, 14, 54, 30, 55, 40, 7, 12, 46, 40, 45, 6, 43, 39, 39, 40, 6, 9, 30, 40, 19, 12, 46, 39, 13, 40, 30, 33, 18, 12, 26, 45, 40, 43, 6, 4, 53, 38, 17, 2, 6, 40, 10, 30, 14, 2, 6, 26, 25, 45, 40, 19, 14, 45, 6, 30, 40, 9, 14, 6, 9, 40, 43, 33, 40, 6, 9, 30, 40, 19, 12, 46, 39, 13, 40, 14, 33, 40, 30, 33, 13, 55, 53, 38, 1, 33, 13, 40, 48, 30, 27, 6, 40, 2, 33, 2, 45, 30, 13, 40, 6, 9, 30, 40, 2, 45, 30, 46, 40, 45, 12, 40, 13, 30, 45, 6, 46, 12, 26, 45, 40, 43, 6, 16, 53, 38, 40, 40, 23, 12, 40, 39, 12, 63, 30, 40, 6, 12, 19, 14, 46, 13, 40, 12, 6, 9, 30, 46, 45, 40, 43, 33, 40, 6, 9, 14, 6, 40, 10, 12, 45, 12, 8, 40, 45, 43, 6, 45, 53, 38, 40, 40, 21, 9, 14, 6, 40, 12, 33, 40, 9, 43, 8, 45, 30, 39, 7, 40, 45, 2, 54, 9, 40, 8, 2, 46, 13, 25, 46, 12, 2, 45, 40, 45, 9, 14, 8, 30, 40, 54, 12, 8, 8, 43, 6, 45, 56, 53, 38, 53, 38, 53, 38, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 61, 3, 53, 38, 53, 38, 59, 12, 46, 40, 45, 9, 14, 8, 30, 40, 13, 30, 33, 26, 40, 6, 9, 14, 6, 40, 6, 9, 12, 2, 40, 10, 30, 14, 46, 25, 45, 6, 40, 39, 12, 63, 30, 40, 6, 12, 40, 14, 33, 26, 53, 38, 44, 9, 12, 40, 7, 12, 46, 40, 6, 9, 26, 40, 45, 30, 39, 7, 40, 14, 46, 6, 40, 45, 12, 40, 2, 33, 27, 46, 12, 63, 43, 13, 30, 33, 6, 56, 53, 38, 11, 46, 14, 33, 6, 40, 43, 7, 40, 6, 9, 12, 2, 40, 19, 43, 39, 6, 55, 40, 6, 9, 12, 2, 40, 14, 46, 6, 40, 10, 30, 39, 12, 63, 30, 13, 40, 12, 7, 40, 8, 14, 33, 26, 55, 53, 38, 17, 2, 6, 40, 6, 9, 14, 6, 40, 6, 9, 12, 2, 40, 33, 12, 33, 30, 40, 39, 12, 63, 25, 45, 6, 40, 43, 45, 40, 8, 12, 45, 6, 40, 30, 63, 43, 13, 30, 33, 6, 16, 53, 38, 59, 12, 46, 40, 6, 9, 12, 2, 40, 14, 46, 6, 40, 45, 12, 40, 27, 12, 45, 45, 30, 45, 45, 30, 13, 40, 19, 43, 6, 9, 40, 8, 2, 46, 13, 25, 46, 12, 2, 45, 40, 9, 14, 6, 30, 55, 53, 38, 21, 9, 14, 6, 40, 25, 31, 14, 43, 33, 45, 6, 40, 6, 9, 26, 40, 45, 30, 39, 7, 40, 6, 9, 12, 2, 40, 45, 6, 43, 54, 48, 25, 45, 6, 40, 33, 12, 6, 40, 6, 12, 40, 54, 12, 33, 45, 27, 43, 46, 30, 55, 53, 38, 42, 30, 30, 48, 43, 33, 31, 40, 6, 9, 14, 6, 40, 10, 30, 14, 2, 6, 30, 12, 2, 45, 40, 46, 12, 12, 7, 40, 6, 12, 40, 46, 2, 43, 33, 14, 6, 30, 53, 38, 44, 9, 43, 54, 9, 40, 6, 12, 40, 46, 30, 27, 14, 43, 46, 40, 45, 9, 12, 2, 39, 13, 40, 10, 30, 40, 6, 9, 26, 40, 54, 9, 43, 30, 7, 40, 13, 30, 45, 43, 46, 30, 16, 53, 38, 52, 40, 54, 9, 14, 33, 31, 30, 40, 6, 9, 26, 40, 6, 9, 12, 2, 31, 9, 6, 55, 40, 6, 9, 14, 6, 40, 57, 40, 8, 14, 26, 40, 54, 9, 14, 33, 31, 30, 40, 8, 26, 40, 8, 43, 33, 13, 55, 53, 38, 42, 9, 14, 39, 39, 40, 9, 14, 6, 30, 40, 10, 30, 40, 7, 14, 43, 46, 30, 46, 40, 39, 12, 13, 31, 30, 13, 40, 6, 9, 14, 33, 40, 31, 30, 33, 6, 39, 30, 40, 39, 12, 63, 30, 49, 53, 38, 17, 30, 40, 14, 45, 40, 6, 9, 26, 40, 27, 46, 30, 45, 30, 33, 54, 30, 40, 43, 45, 40, 31, 46, 14, 54, 43, 12, 2, 45, 40, 14, 33, 13, 40, 48, 43, 33, 13, 55, 53, 38, 52, 46, 40, 6, 12, 40, 6, 9, 26, 40, 45, 30, 39, 7, 40, 14, 6, 40, 39, 30, 14, 45, 6, 40, 48, 43, 33, 13, 47, 9, 30, 14, 46, 6, 30, 13, 40, 27, 46, 12, 63, 30, 55, 53, 38, 40, 40, 37, 14, 48, 30, 40, 6, 9, 30, 30, 40, 14, 33, 12, 6, 9, 30, 46, 40, 45, 30, 39, 7, 40, 7, 12, 46, 40, 39, 12, 63, 30, 40, 12, 7, 40, 8, 30, 55, 53, 38, 40, 40, 21, 9, 14, 6, 40, 10, 30, 14, 2, 6, 26, 40, 45, 6, 43, 39, 39, 40, 8, 14, 26, 40, 39, 43, 63, 30, 40, 43, 33, 40, 6, 9, 43, 33, 30, 40, 12, 46, 40, 6, 9, 30, 30, 56, 53, 38, 53, 38, 53, 38, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 61, 61, 53, 38, 53, 38, 1, 45, 40, 7, 14, 45, 6, 40, 14, 45, 40, 6, 9, 12, 2, 40, 45, 9, 14, 39, 6, 40, 19, 14, 33, 30, 40, 45, 12, 40, 7, 14, 45, 6, 40, 6, 9, 12, 2, 40, 31, 46, 12, 19, 25, 45, 6, 55, 53, 38, 57, 33, 40, 12, 33, 30, 40, 12, 7, 40, 6, 9, 43, 33, 30, 55, 40, 7, 46, 12, 8, 40, 6, 9, 14, 6, 40, 19, 9, 43, 54, 9, 40, 6, 9, 12, 2, 40, 13, 30, 27, 14, 46, 6, 30, 45, 6, 55, 53, 38, 1, 33, 13, 40, 6, 9, 14, 6, 40, 7, 46, 30, 45, 9, 40, 10, 39, 12, 12, 13, 40, 19, 9, 43, 54, 9, 40, 26, 12, 2, 33, 31, 39, 26, 40, 6, 9, 12, 2, 40, 10, 30, 45, 6, 12, 19, 25, 45, 6, 55, 53, 38, 21, 9, 12, 2, 40, 8, 14, 26, 45, 6, 40, 54, 14, 39, 39, 40, 6, 9, 43, 33, 30, 55, 40, 19, 9, 30, 33, 40, 6, 9, 12, 2, 40, 7, 46, 12, 8, 40, 26, 12, 2, 6, 9, 40, 54, 12, 33, 63, 30, 46, 6, 30, 45, 6, 55, 53, 38, 66, 30, 46, 30, 43, 33, 40, 39, 43, 63, 30, 45, 40, 19, 43, 45, 13, 12, 8, 55, 40, 10, 30, 14, 2, 6, 26, 55, 40, 14, 33, 13, 40, 43, 33, 54, 46, 30, 14, 45, 30, 55, 53, 38, 44, 43, 6, 9, 12, 2, 6, 40, 6, 9, 43, 45, 40, 7, 12, 39, 39, 26, 55, 40, 14, 31, 30, 55, 40, 14, 33, 13, 40, 54, 12, 39, 13, 40, 13, 30, 54, 14, 26, 55, 53, 38, 57, 7, 40, 14, 39, 39, 40, 19, 30, 46, 30, 40, 8, 43, 33, 13, 30, 13, 40, 45, 12, 55, 40, 6, 9, 30, 40, 6, 43, 8, 30, 45, 40, 45, 9, 12, 2, 39, 13, 40, 54, 30, 14, 45, 30, 55, 53, 38, 1, 33, 13, 40, 6, 9, 46, 30, 30, 45, 54, 12, 46, 30, 40, 26, 30, 14, 46, 40, 19, 12, 2, 39, 13, 40, 8, 14, 48, 30, 40, 6, 9, 30, 40, 19, 12, 46, 39, 13, 40, 14, 19, 14, 26, 16, 53, 38, 22, 30, 6, 40, 6, 9, 12, 45, 30, 40, 19, 9, 12, 8, 40, 33, 14, 6, 2, 46, 30, 40, 9, 14, 6, 9, 40, 33, 12, 6, 40, 8, 14, 13, 30, 40, 7, 12, 46, 40, 45, 6, 12, 46, 30, 55, 53, 38, 66, 14, 46, 45, 9, 55, 40, 7, 30, 14, 6, 2, 46, 30, 39, 30, 45, 45, 55, 40, 14, 33, 13, 40, 46, 2, 13, 30, 55, 40, 10, 14, 46, 46, 30, 33, 39, 26, 40, 27, 30, 46, 43, 45, 9, 16, 53, 38, 22, 12, 12, 48, 40, 19, 9, 12, 8, 40, 45, 9, 30, 40, 10, 30, 45, 6, 40, 30, 33, 13, 12, 19, 30, 13, 55, 40, 45, 9, 30, 40, 31, 14, 63, 30, 40, 6, 9, 30, 30, 40, 8, 12, 46, 30, 4, 53, 38, 44, 9, 43, 54, 9, 40, 10, 12, 2, 33, 6, 30, 12, 2, 45, 40, 31, 43, 7, 6, 40, 6, 9, 12, 2, 40, 45, 9, 12, 2, 39, 13, 45, 6, 40, 43, 33, 40, 10, 12, 2, 33, 6, 26, 40, 54, 9, 30, 46, 43, 45, 9, 16, 53, 38, 40, 40, 42, 9, 30, 40, 54, 14, 46, 63, 30, 13, 40, 6, 9, 30, 30, 40, 7, 12, 46, 40, 9, 30, 46, 40, 45, 30, 14, 39, 55, 40, 14, 33, 13, 40, 8, 30, 14, 33, 6, 40, 6, 9, 30, 46, 30, 10, 26, 55, 53, 38, 40, 40, 21, 9, 12, 2, 40, 45, 9, 12, 2, 39, 13, 45, 6, 40, 27, 46, 43, 33, 6, 40, 8, 12, 46, 30, 55, 40, 33, 12, 6, 40, 39, 30, 6, 40, 6, 9, 14, 6, 40, 54, 12, 27, 26, 40, 13, 43, 30, 56, 53, 38, 53, 38, 53, 38, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 61, 51, 53, 38, 53, 38, 44, 9, 30, 33, 40, 57, 40, 13, 12, 40, 54, 12, 2, 33, 6, 40, 6, 9, 30, 40, 54, 39, 12, 54, 48, 40, 6, 9, 14, 6, 40, 6, 30, 39, 39, 45, 40, 6, 9, 30, 40, 6, 43, 8, 30, 55, 53, 38, 1, 33, 13, 40, 45, 30, 30, 40, 6, 9, 30, 40, 10, 46, 14, 63, 30, 40, 13, 14, 26, 40, 45, 2, 33, 48, 40, 43, 33, 40, 9, 43, 13, 30, 12, 2, 45, 40, 33, 43, 31, 9, 6, 55, 53, 38, 44, 9, 30, 33, 40, 57, 40, 10, 30, 9, 12, 39, 13, 40, 6, 9, 30, 40, 63, 43, 12, 39, 30, 6, 40, 27, 14, 45, 6, 40, 27, 46, 43, 8, 30, 55, 53, 38, 1, 33, 13, 40, 45, 14, 10, 39, 30, 40, 54, 2, 46, 39, 45, 40, 14, 39, 39, 40, 45, 43, 39, 63, 30, 46, 30, 13, 40, 12, 25, 30, 46, 40, 19, 43, 6, 9, 40, 19, 9, 43, 6, 30, 16, 53, 38, 44, 9, 30, 33, 40, 39, 12, 7, 6, 26, 40, 6, 46, 30, 30, 45, 40, 57, 40, 45, 30, 30, 40, 10, 14, 46, 46, 30, 33, 40, 12, 7, 40, 39, 30, 14, 63, 30, 45, 55, 53, 38, 44, 9, 43, 54, 9, 40, 30, 46, 45, 6, 40, 7, 46, 12, 8, 40, 9, 30, 14, 6, 40, 13, 43, 13, 40, 54, 14, 33, 12, 27, 26, 40, 6, 9, 30, 40, 9, 30, 46, 13, 53, 38, 1, 33, 13, 40, 45, 2, 8, 8, 30, 46, 25, 45, 40, 31, 46, 30, 30, 33, 40, 14, 39, 39, 40, 31, 43, 46, 13, 30, 13, 40, 2, 27, 40, 43, 33, 40, 45, 9, 30, 14, 63, 30, 45, 53, 38, 17, 12, 46, 33, 30, 40, 12, 33, 40, 6, 9, 30, 40, 10, 43, 30, 46, 40, 19, 43, 6, 9, 40, 19, 9, 43, 6, 30, 40, 14, 33, 13, 40, 10, 46, 43, 45, 6, 39, 26, 40, 10, 30, 14, 46, 13, 16, 53, 38, 21, 9, 30, 33, 40, 12, 7, 40, 6, 9, 26, 40, 10, 30, 14, 2, 6, 26, 40, 13, 12, 40, 57, 40, 64, 2, 30, 45, 6, 43, 12, 33, 40, 8, 14, 48, 30, 53, 38, 21, 9, 14, 6, 40, 6, 9, 12, 2, 40, 14, 8, 12, 33, 31, 40, 6, 9, 30, 40, 19, 14, 45, 6, 30, 45, 40, 12, 7, 40, 6, 43, 8, 30, 40, 8, 2, 45, 6, 40, 31, 12, 55, 53, 38, 42, 43, 33, 54, 30, 40, 45, 19, 30, 30, 6, 45, 40, 14, 33, 13, 40, 10, 30, 14, 2, 6, 43, 30, 45, 40, 13, 12, 40, 6, 9, 30, 8, 45, 30, 39, 63, 30, 45, 40, 7, 12, 46, 45, 14, 48, 30, 55, 53, 38, 1, 33, 13, 40, 13, 43, 30, 40, 14, 45, 40, 7, 14, 45, 6, 40, 14, 45, 40, 6, 9, 30, 26, 40, 45, 30, 30, 40, 12, 6, 9, 30, 46, 45, 40, 31, 46, 12, 19, 55, 53, 38, 40, 40, 1, 33, 13, 40, 33, 12, 6, 9, 43, 33, 31, 40, 25, 31, 14, 43, 33, 45, 6, 40, 21, 43, 8, 30, 25, 45, 40, 45, 54, 26, 6, 9, 30, 40, 54, 14, 33, 40, 8, 14, 48, 30, 40, 13, 30, 7, 30, 33, 54, 30, 53, 38, 40, 40, 42, 14, 63, 30, 40, 10, 46, 30, 30, 13, 40, 6, 12, 40, 10, 46, 14, 63, 30, 40, 9, 43, 8, 55, 40, 19, 9, 30, 33, 40, 9, 30, 40, 6, 14, 48, 30, 45, 40, 6, 9, 30, 30, 40, 9, 30, 33, 54, 30, 56, 53, 38, 53, 38, 53, 38, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 61, 62, 53, 38, 53, 38, 52, 40, 6, 9, 14, 6, 40, 26, 12, 2, 40, 19, 30, 46, 30, 40, 26, 12, 2, 46, 40, 45, 30, 39, 7, 55, 40, 10, 2, 6, 40, 39, 12, 63, 30, 40, 26, 12, 2, 40, 14, 46, 30, 53, 38, 23, 12, 40, 39, 12, 33, 31, 30, 46, 40, 26, 12, 2, 46, 45, 55, 40, 6, 9, 14, 33, 40, 26, 12, 2, 40, 26, 12, 2, 46, 40, 45, 30, 39, 7, 40, 9, 30, 46, 30, 40, 39, 43, 63, 30, 55, 53, 38, 1, 31, 14, 43, 33, 45, 6, 40, 6, 9, 43, 45, 40, 54, 12, 8, 43, 33, 31, 40, 30, 33, 13, 40, 26, 12, 2, 40, 45, 9, 12, 2, 39, 13, 40, 27, 46, 30, 27, 14, 46, 30, 55, 53, 38, 1, 33, 13, 40, 26, 12, 2, 46, 40, 45, 19, 30, 30, 6, 40, 45, 30, 8, 10, 39, 14, 33, 54, 30, 40, 6, 12, 40, 45, 12, 8, 30, 40, 12, 6, 9, 30, 46, 40, 31, 43, 63, 30, 56, 53, 38, 42, 12, 40, 45, 9, 12, 2, 39, 13, 40, 6, 9, 14, 6, 40, 10, 30, 14, 2, 6, 26, 40, 19, 9, 43, 54, 9, 40, 26, 12, 2, 40, 9, 12, 39, 13, 40, 43, 33, 40, 39, 30, 14, 45, 30, 53, 38, 59, 43, 33, 13, 40, 33, 12, 40, 13, 30, 6, 30, 46, 8, 43, 33, 14, 6, 43, 12, 33, 55, 40, 6, 9, 30, 33, 40, 26, 12, 2, 40, 19, 30, 46, 30, 53, 38, 50, 12, 2, 46, 40, 45, 30, 39, 7, 40, 14, 31, 14, 43, 33, 40, 14, 7, 6, 30, 46, 40, 26, 12, 2, 46, 40, 45, 30, 39, 7, 25, 45, 40, 13, 30, 54, 30, 14, 45, 30, 55, 53, 38, 44, 9, 30, 33, 40, 26, 12, 2, 46, 40, 45, 19, 30, 30, 6, 40, 43, 45, 45, 2, 30, 40, 26, 12, 2, 46, 40, 45, 19, 30, 30, 6, 40, 7, 12, 46, 8, 40, 45, 9, 12, 2, 39, 13, 40, 10, 30, 14, 46, 56, 53, 38, 44, 9, 12, 40, 39, 30, 6, 45, 40, 45, 12, 40, 7, 14, 43, 46, 40, 14, 40, 9, 12, 2, 45, 30, 40, 7, 14, 39, 39, 40, 6, 12, 40, 13, 30, 54, 14, 26, 55, 53, 38, 44, 9, 43, 54, 9, 40, 9, 2, 45, 10, 14, 33, 13, 46, 26, 40, 43, 33, 40, 9, 12, 33, 12, 2, 46, 40, 8, 43, 31, 9, 6, 40, 2, 27, 9, 12, 39, 13, 55, 53, 38, 1, 31, 14, 43, 33, 45, 6, 40, 6, 9, 30, 40, 45, 6, 12, 46, 8, 26, 40, 31, 2, 45, 6, 45, 40, 12, 7, 40, 19, 43, 33, 6, 30, 46, 25, 45, 40, 13, 14, 26, 53, 38, 1, 33, 13, 40, 10, 14, 46, 46, 30, 33, 40, 46, 14, 31, 30, 40, 12, 7, 40, 13, 30, 14, 6, 9, 25, 45, 40, 30, 6, 30, 46, 33, 14, 39, 40, 54, 12, 39, 13, 49, 53, 38, 40, 40, 52, 40, 33, 12, 33, 30, 40, 10, 2, 6, 40, 2, 33, 6, 9, 46, 43, 7, 6, 45, 55, 40, 13, 30, 14, 46, 40, 8, 26, 40, 39, 12, 63, 30, 40, 26, 12, 2, 40, 48, 33, 12, 19, 55, 53, 38, 40, 40, 50, 12, 2, 40, 9, 14, 13, 40, 14, 40, 7, 14, 6, 9, 30, 46, 55, 40, 39, 30, 6, 40, 26, 12, 2, 46, 40, 45, 12, 33, 40, 45, 14, 26, 40, 45, 12, 56, 53, 38, 53, 38, 53, 38, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 61, 5, 53, 38, 53, 38, 23, 12, 6, 40, 7, 46, 12, 8, 40, 6, 9, 30, 40, 45, 6, 14, 46, 45, 40, 13, 12, 40, 57, 40, 8, 26, 40, 18, 2, 13, 31, 30, 8, 30, 33, 6, 40, 27, 39, 2, 54, 48, 55, 53, 38, 1, 33, 13, 40, 26, 30, 6, 40, 8, 30, 6, 9, 43, 33, 48, 45, 40, 57, 40, 9, 14, 63, 30, 40, 14, 45, 6, 46, 12, 33, 12, 8, 26, 55, 53, 38, 17, 2, 6, 40, 33, 12, 6, 40, 6, 12, 40, 6, 30, 39, 39, 40, 12, 7, 40, 31, 12, 12, 13, 55, 40, 12, 46, 40, 30, 63, 43, 39, 40, 39, 2, 54, 48, 55, 53, 38, 52, 7, 40, 27, 39, 14, 31, 2, 30, 45, 55, 40, 12, 7, 40, 13, 30, 14, 46, 6, 9, 45, 55, 40, 12, 46, 40, 45, 30, 14, 45, 12, 33, 45, 25, 40, 64, 2, 14, 39, 43, 6, 26, 55, 53, 38, 23, 12, 46, 40, 54, 14, 33, 40, 57, 40, 7, 12, 46, 6, 2, 33, 30, 40, 6, 12, 40, 10, 46, 43, 30, 7, 40, 8, 43, 33, 2, 6, 30, 45, 40, 6, 30, 39, 39, 4, 53, 38, 67, 12, 43, 33, 6, 43, 33, 31, 40, 6, 12, 40, 30, 14, 54, 9, 40, 9, 43, 45, 40, 6, 9, 2, 33, 13, 30, 46, 55, 40, 46, 14, 43, 33, 40, 14, 33, 13, 40, 19, 43, 33, 13, 55, 53, 38, 52, 46, 40, 45, 14, 26, 40, 19, 43, 6, 9, 40, 27, 46, 43, 33, 54, 30, 45, 40, 43, 7, 40, 43, 6, 40, 45, 9, 14, 39, 39, 40, 31, 12, 40, 19, 30, 39, 39, 53, 38, 17, 26, 40, 12, 7, 6, 40, 27, 46, 30, 13, 43, 54, 6, 40, 6, 9, 14, 6, 40, 57, 40, 43, 33, 40, 9, 30, 14, 63, 30, 33, 40, 7, 43, 33, 13, 56, 53, 38, 17, 2, 6, 40, 7, 46, 12, 8, 40, 6, 9, 43, 33, 30, 40, 30, 26, 30, 45, 40, 8, 26, 40, 48, 33, 12, 19, 39, 30, 13, 31, 30, 40, 57, 40, 13, 30, 46, 43, 63, 30, 55, 53, 38, 1, 33, 13, 40, 54, 12, 33, 45, 6, 14, 33, 6, 40, 45, 6, 14, 46, 45, 40, 43, 33, 40, 6, 9, 30, 8, 40, 57, 40, 46, 30, 14, 13, 40, 45, 2, 54, 9, 40, 14, 46, 6, 53, 38, 1, 45, 40, 6, 46, 2, 6, 9, 40, 14, 33, 13, 40, 10, 30, 14, 2, 6, 26, 40, 45, 9, 14, 39, 39, 40, 6, 12, 31, 30, 6, 9, 30, 46, 40, 6, 9, 46, 43, 63, 30, 53, 38, 57, 7, 40, 7, 46, 12, 8, 40, 6, 9, 26, 40, 45, 30, 39, 7, 55, 40, 6, 12, 40, 45, 6, 12, 46, 30, 40, 6, 9, 12, 2, 40, 19, 12, 2, 39, 13, 45, 6, 40, 54, 12, 33, 63, 30, 46, 6, 16, 53, 38, 40, 40, 52, 46, 40, 30, 39, 45, 30, 40, 12, 7, 40, 6, 9, 30, 30, 40, 6, 9, 43, 45, 40, 57, 40, 27, 46, 12, 31, 33, 12, 45, 6, 43, 54, 14, 6, 30, 55, 53, 38, 40, 40, 21, 9, 26, 40, 30, 33, 13, 40, 43, 45, 40, 6, 46, 2, 6, 9, 25, 45, 40, 14, 33, 13, 40, 10, 30, 14, 2, 6, 26, 25, 45, 40, 13, 12, 12, 8, 40, 14, 33, 13, 40, 13, 14, 6, 30, 56, 53, 38, 53, 38, 53, 38, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 61, 35, 53, 38, 53, 38, 44, 9, 30, 33, 40, 57, 40, 54, 12, 33, 45, 43, 13, 30, 46, 40, 30, 63, 30, 46, 26, 40, 6, 9, 43, 33, 31, 40, 6, 9, 14, 6, 40, 31, 46, 12, 19, 45, 53, 38, 66, 12, 39, 13, 45, 40, 43, 33, 40, 27, 30, 46, 7, 30, 54, 6, 43, 12, 33, 40, 10, 2, 6, 40, 14, 40, 39, 43, 6, 6, 39, 30, 40, 8, 12, 8, 30, 33, 6, 56, 53, 38, 21, 9, 14, 6, 40, 6, 9, 43, 45, 40, 9, 2, 31, 30, 40, 45, 6, 14, 31, 30, 40, 27, 46, 30, 45, 30, 33, 6, 30, 6, 9, 40, 33, 12, 2, 31, 9, 6, 40, 10, 2, 6, 40, 45, 9, 12, 19, 45, 53, 38, 44, 9, 30, 46, 30, 12, 33, 40, 6, 9, 30, 40, 45, 6, 14, 46, 45, 40, 43, 33, 40, 45, 30, 54, 46, 30, 6, 40, 43, 33, 7, 39, 2, 30, 33, 54, 30, 40, 54, 12, 8, 8, 30, 33, 6, 56, 53, 38, 44, 9, 30, 33, 40, 57, 40, 27, 30, 46, 54, 30, 43, 63, 30, 40, 6, 9, 14, 6, 40, 8, 30, 33, 40, 14, 45, 40, 27, 39, 14, 33, 6, 45, 40, 43, 33, 54, 46, 30, 14, 45, 30, 55, 53, 38, 65, 9, 30, 30, 46, 30, 13, 40, 14, 33, 13, 40, 54, 9, 30, 54, 48, 30, 13, 40, 30, 63, 30, 33, 40, 10, 26, 40, 6, 9, 30, 40, 45, 30, 39, 7, 47, 45, 14, 8, 30, 40, 45, 48, 26, 16, 53, 38, 32, 14, 2, 33, 6, 40, 43, 33, 40, 6, 9, 30, 43, 46, 40, 26, 12, 2, 6, 9, 7, 2, 39, 40, 45, 14, 27, 55, 40, 14, 6, 40, 9, 30, 43, 31, 9, 6, 40, 13, 30, 54, 46, 30, 14, 45, 30, 55, 53, 38, 1, 33, 13, 40, 19, 30, 14, 46, 40, 6, 9, 30, 43, 46, 40, 10, 46, 14, 63, 30, 40, 45, 6, 14, 6, 30, 40, 12, 2, 6, 40, 12, 7, 40, 8, 30, 8, 12, 46, 26, 56, 53, 38, 21, 9, 30, 33, 40, 6, 9, 30, 40, 54, 12, 33, 54, 30, 43, 6, 40, 12, 7, 40, 6, 9, 43, 45, 40, 43, 33, 54, 12, 33, 45, 6, 14, 33, 6, 40, 45, 6, 14, 26, 55, 53, 38, 42, 30, 6, 45, 40, 26, 12, 2, 40, 8, 12, 45, 6, 40, 46, 43, 54, 9, 40, 43, 33, 40, 26, 12, 2, 6, 9, 40, 10, 30, 7, 12, 46, 30, 40, 8, 26, 40, 45, 43, 31, 9, 6, 55, 53, 38, 44, 9, 30, 46, 30, 40, 19, 14, 45, 6, 30, 7, 2, 39, 40, 6, 43, 8, 30, 40, 13, 30, 10, 14, 6, 30, 6, 9, 40, 19, 43, 6, 9, 40, 13, 30, 54, 14, 26, 53, 38, 21, 12, 40, 54, 9, 14, 33, 31, 30, 40, 26, 12, 2, 46, 40, 13, 14, 26, 40, 12, 7, 40, 26, 12, 2, 6, 9, 40, 6, 12, 40, 45, 2, 39, 39, 43, 30, 13, 40, 33, 43, 31, 9, 6, 55, 53, 38, 40, 40, 1, 33, 13, 40, 14, 39, 39, 40, 43, 33, 40, 19, 14, 46, 40, 19, 43, 6, 9, 40, 21, 43, 8, 30, 40, 7, 12, 46, 40, 39, 12, 63, 30, 40, 12, 7, 40, 26, 12, 2, 55, 53, 38, 40, 40, 1, 45, 40, 9, 30, 40, 6, 14, 48, 30, 45, 40, 7, 46, 12, 8, 40, 26, 12, 2, 55, 40, 57, 40, 30, 33, 31, 46, 14, 7, 6, 40, 26, 12, 2, 40, 33, 30, 19, 56, 53, 38, 53, 38, 53, 38, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 61, 15, 53, 38, 53, 38, 17, 2, 6, 40, 19, 9, 30, 46, 30, 7, 12, 46, 30, 40, 13, 12, 40, 33, 12, 6, 40, 26, 12, 2, 40, 14, 40, 8, 43, 31, 9, 6, 43, 30, 46, 40, 19, 14, 26, 53, 38, 37, 14, 48, 30, 40, 19, 14, 46, 40, 2, 27, 12, 33, 40, 6, 9, 43, 45, 40, 10, 39, 12, 12, 13, 26, 40, 6, 26, 46, 14, 33, 6, 40, 21, 43, 8, 30, 49, 53, 38, 1, 33, 13, 40, 7, 12, 46, 6, 43, 7, 26, 40, 26, 12, 2, 46, 40, 45, 30, 39, 7, 40, 43, 33, 40, 26, 12, 2, 46, 40, 13, 30, 54, 14, 26, 53, 38, 44, 43, 6, 9, 40, 8, 30, 14, 33, 45, 40, 8, 12, 46, 30, 40, 10, 39, 30, 45, 45, 30, 13, 40, 6, 9, 14, 33, 40, 8, 26, 40, 10, 14, 46, 46, 30, 33, 40, 46, 9, 26, 8, 30, 49, 53, 38, 23, 12, 19, 40, 45, 6, 14, 33, 13, 40, 26, 12, 2, 40, 12, 33, 40, 6, 9, 30, 40, 6, 12, 27, 40, 12, 7, 40, 9, 14, 27, 27, 26, 40, 9, 12, 2, 46, 45, 55, 53, 38, 1, 33, 13, 40, 8, 14, 33, 26, 40, 8, 14, 43, 13, 30, 33, 40, 31, 14, 46, 13, 30, 33, 45, 40, 26, 30, 6, 40, 2, 33, 45, 30, 6, 55, 53, 38, 44, 43, 6, 9, 40, 63, 43, 46, 6, 2, 12, 2, 45, 40, 19, 43, 45, 9, 40, 19, 12, 2, 39, 13, 40, 10, 30, 14, 46, 40, 26, 12, 2, 40, 39, 43, 63, 43, 33, 31, 40, 7, 39, 12, 19, 30, 46, 45, 55, 53, 38, 37, 2, 54, 9, 40, 39, 43, 48, 30, 46, 40, 6, 9, 14, 33, 40, 26, 12, 2, 46, 40, 27, 14, 43, 33, 6, 30, 13, 40, 54, 12, 2, 33, 6, 30, 46, 7, 30, 43, 6, 16, 53, 38, 42, 12, 40, 45, 9, 12, 2, 39, 13, 40, 6, 9, 30, 40, 39, 43, 33, 30, 45, 40, 12, 7, 40, 39, 43, 7, 30, 40, 6, 9, 14, 6, 40, 39, 43, 7, 30, 40, 46, 30, 27, 14, 43, 46, 53, 38, 44, 9, 43, 54, 9, 40, 6, 9, 43, 45, 40, 36, 21, 43, 8, 30, 25, 45, 40, 27, 30, 33, 54, 43, 39, 60, 40, 12, 46, 40, 8, 26, 40, 27, 2, 27, 43, 39, 40, 27, 30, 33, 53, 38, 23, 30, 43, 6, 9, 30, 46, 40, 43, 33, 40, 43, 33, 19, 14, 46, 13, 40, 19, 12, 46, 6, 9, 40, 33, 12, 46, 40, 12, 2, 6, 19, 14, 46, 13, 40, 7, 14, 43, 46, 53, 38, 65, 14, 33, 40, 8, 14, 48, 30, 40, 26, 12, 2, 40, 39, 43, 63, 30, 40, 26, 12, 2, 46, 40, 45, 30, 39, 7, 40, 43, 33, 40, 30, 26, 30, 45, 40, 12, 7, 40, 8, 30, 33, 56, 53, 38, 40, 40, 21, 12, 40, 31, 43, 63, 30, 40, 14, 19, 14, 26, 40, 26, 12, 2, 46, 40, 45, 30, 39, 7, 55, 40, 48, 30, 30, 27, 45, 40, 26, 12, 2, 46, 40, 45, 30, 39, 7, 40, 45, 6, 43, 39, 39, 55, 53, 38, 40, 40, 1, 33, 13, 40, 26, 12, 2, 40, 8, 2, 45, 6, 40, 39, 43, 63, 30, 40, 13, 46, 14, 19, 33, 40, 10, 26, 40, 26, 12, 2, 46, 40, 12, 19, 33, 40, 45, 19, 30, 30, 6, 40, 45, 48, 43, 39, 39, 56, 53, 38, 53, 38, 53, 38, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 61, 29, 53, 38, 53, 38, 44, 9, 12, 40, 19, 43, 39, 39, 40, 10, 30, 39, 43, 30, 63, 30, 40, 8, 26, 40, 63, 30, 46, 45, 30, 40, 43, 33, 40, 6, 43, 8, 30, 40, 6, 12, 40, 54, 12, 8, 30, 53, 38, 57, 7, 40, 43, 6, 40, 19, 30, 46, 30, 40, 7, 43, 39, 39, 30, 13, 40, 19, 43, 6, 9, 40, 26, 12, 2, 46, 40, 8, 12, 45, 6, 40, 9, 43, 31, 9, 40, 13, 30, 45, 30, 46, 6, 45, 49, 53, 38, 21, 9, 12, 2, 31, 9, 40, 26, 30, 6, 40, 9, 30, 14, 63, 30, 33, 40, 48, 33, 12, 19, 45, 40, 43, 6, 40, 43, 45, 40, 10, 2, 6, 40, 14, 45, 40, 14, 40, 6, 12, 8, 10, 53, 38, 44, 9, 43, 54, 9, 40, 9, 43, 13, 30, 45, 40, 26, 12, 2, 46, 40, 39, 43, 7, 30, 55, 40, 14, 33, 13, 40, 45, 9, 12, 19, 45, 40, 33, 12, 6, 40, 9, 14, 39, 7, 40, 26, 12, 2, 46, 40, 27, 14, 46, 6, 45, 16, 53, 38, 57, 7, 40, 57, 40, 54, 12, 2, 39, 13, 40, 19, 46, 43, 6, 30, 40, 6, 9, 30, 40, 10, 30, 14, 2, 6, 26, 40, 12, 7, 40, 26, 12, 2, 46, 40, 30, 26, 30, 45, 55, 53, 38, 1, 33, 13, 40, 43, 33, 40, 7, 46, 30, 45, 9, 40, 33, 2, 8, 10, 30, 46, 45, 40, 33, 2, 8, 10, 30, 46, 40, 14, 39, 39, 40, 26, 12, 2, 46, 40, 31, 46, 14, 54, 30, 45, 55, 53, 38, 21, 9, 30, 40, 14, 31, 30, 40, 6, 12, 40, 54, 12, 8, 30, 40, 19, 12, 2, 39, 13, 40, 45, 14, 26, 40, 6, 9, 43, 45, 40, 27, 12, 30, 6, 40, 39, 43, 30, 45, 55, 53, 38, 42, 2, 54, 9, 40, 9, 30, 14, 63, 30, 33, 39, 26, 40, 6, 12, 2, 54, 9, 30, 45, 40, 33, 30, 25, 30, 46, 40, 6, 12, 2, 54, 9, 30, 13, 40, 30, 14, 46, 6, 9, 39, 26, 40, 7, 14, 54, 30, 45, 56, 53, 38, 42, 12, 40, 45, 9, 12, 2, 39, 13, 40, 8, 26, 40, 27, 14, 27, 30, 46, 45, 40, 36, 26, 30, 39, 39, 12, 19, 30, 13, 40, 19, 43, 6, 9, 40, 6, 9, 30, 43, 46, 40, 14, 31, 30, 60, 53, 38, 17, 30, 40, 45, 54, 12, 46, 33, 30, 13, 55, 40, 39, 43, 48, 30, 40, 12, 39, 13, 40, 8, 30, 33, 40, 12, 7, 40, 39, 30, 45, 45, 40, 6, 46, 2, 6, 9, 40, 6, 9, 14, 33, 40, 6, 12, 33, 31, 2, 30, 55, 53, 38, 1, 33, 13, 40, 26, 12, 2, 46, 40, 6, 46, 2, 30, 40, 46, 43, 31, 9, 6, 45, 40, 10, 30, 40, 6, 30, 46, 8, 30, 13, 40, 14, 40, 27, 12, 30, 6, 25, 45, 40, 46, 14, 31, 30, 55, 53, 38, 1, 33, 13, 40, 45, 6, 46, 30, 6, 54, 9, 30, 13, 40, 8, 30, 6, 46, 30, 40, 12, 7, 40, 14, 33, 40, 14, 33, 6, 43, 64, 2, 30, 40, 45, 12, 33, 31, 56, 53, 38, 40, 40, 17, 2, 6, 40, 19, 30, 46, 30, 40, 45, 12, 8, 30, 40, 54, 9, 43, 39, 13, 40, 12, 7, 40, 26, 12, 2, 46, 45, 40, 14, 39, 43, 63, 30, 40, 6, 9, 14, 6, 40, 6, 43, 8, 30, 55, 53, 38, 40, 40, 50, 12, 2, 40, 45, 9, 12, 2, 39, 13, 40, 39, 43, 63, 30, 40, 6, 19, 43, 54, 30, 40, 43, 33, 40, 43, 6, 55, 40, 14, 33, 13, 40, 43, 33, 40, 8, 26, 40, 46, 9, 26, 8, 30, 56, 53, 38, 53, 38, 53, 38, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 61, 68, 53, 38, 53, 38, 42, 9, 14, 39, 39, 40, 57, 40, 54, 12, 8, 27, 14, 46, 30, 40, 6, 9, 30, 30, 40, 6, 12, 40, 14, 40, 45, 2, 8, 8, 30, 46, 25, 45, 40, 13, 14, 26, 49, 53, 38, 21, 9, 12, 2, 40, 14, 46, 6, 40, 8, 12, 46, 30, 40, 39, 12, 63, 30, 39, 26, 40, 14, 33, 13, 40, 8, 12, 46, 30, 40, 6, 30, 8, 27, 30, 46, 14, 6, 30, 16, 53, 38, 58, 12, 2, 31, 9, 40, 19, 43, 33, 13, 45, 40, 13, 12, 40, 45, 9, 14, 48, 30, 40, 6, 9, 30, 40, 13, 14, 46, 39, 43, 33, 31, 40, 10, 2, 13, 45, 40, 12, 7, 40, 37, 14, 26, 55, 53, 38, 1, 33, 13, 40, 45, 2, 8, 8, 30, 46, 25, 45, 40, 39, 30, 14, 45, 30, 40, 9, 14, 6, 9, 40, 14, 39, 39, 40, 6, 12, 12, 40, 45, 9, 12, 46, 6, 40, 14, 40, 13, 14, 6, 30, 16, 53, 38, 42, 12, 8, 30, 6, 43, 8, 30, 40, 6, 12, 12, 40, 9, 12, 6, 40, 6, 9, 30, 40, 30, 26, 30, 40, 12, 7, 40, 9, 30, 14, 63, 30, 33, 40, 45, 9, 43, 33, 30, 45, 55, 53, 38, 1, 33, 13, 40, 12, 7, 6, 30, 33, 40, 43, 45, 40, 9, 43, 45, 40, 31, 12, 39, 13, 40, 54, 12, 8, 27, 39, 30, 41, 43, 12, 33, 40, 13, 43, 8, 8, 30, 13, 55, 53, 38, 1, 33, 13, 40, 30, 63, 30, 46, 26, 40, 7, 14, 43, 46, 40, 7, 46, 12, 8, 40, 7, 14, 43, 46, 40, 45, 12, 8, 30, 6, 43, 8, 30, 40, 13, 30, 54, 39, 43, 33, 30, 45, 55, 53, 38, 17, 26, 40, 54, 9, 14, 33, 54, 30, 55, 40, 12, 46, 40, 33, 14, 6, 2, 46, 30, 25, 45, 40, 54, 9, 14, 33, 31, 43, 33, 31, 40, 54, 12, 2, 46, 45, 30, 40, 2, 33, 6, 46, 43, 8, 8, 30, 13, 16, 53, 38, 17, 2, 6, 40, 6, 9, 26, 40, 30, 6, 30, 46, 33, 14, 39, 40, 45, 2, 8, 8, 30, 46, 40, 45, 9, 14, 39, 39, 40, 33, 12, 6, 40, 7, 14, 13, 30, 55, 53, 38, 23, 12, 46, 40, 39, 12, 45, 30, 40, 27, 12, 45, 45, 30, 45, 45, 43, 12, 33, 40, 12, 7, 40, 6, 9, 14, 6, 40, 7, 14, 43, 46, 40, 6, 9, 12, 2, 40, 12, 19, 25, 45, 6, 55, 53, 38, 23, 12, 46, 40, 45, 9, 14, 39, 39, 40, 13, 30, 14, 6, 9, 40, 10, 46, 14, 31, 40, 6, 9, 12, 2, 40, 19, 14, 33, 13, 25, 46, 30, 45, 6, 40, 43, 33, 40, 9, 43, 45, 40, 45, 9, 14, 13, 30, 55, 53, 38, 44, 9, 30, 33, 40, 43, 33, 40, 30, 6, 30, 46, 33, 14, 39, 40, 39, 43, 33, 30, 45, 40, 6, 12, 40, 6, 43, 8, 30, 40, 6, 9, 12, 2, 40, 31, 46, 12, 19, 25, 45, 6, 55, 53, 38, 40, 40, 42, 12, 40, 39, 12, 33, 31, 40, 14, 45, 40, 8, 30, 33, 40, 54, 14, 33, 40, 10, 46, 30, 14, 6, 9, 30, 40, 12, 46, 40, 30, 26, 30, 45, 40, 54, 14, 33, 40, 45, 30, 30, 55, 53, 38, 40, 40, 42, 12, 40, 39, 12, 33, 31, 40, 39, 43, 63, 30, 45, 40, 6, 9, 43, 45, 55, 40, 14, 33, 13, 40, 6, 9, 43, 45, 40, 31, 43, 63, 30, 45, 40, 39, 43, 7, 30, 40, 6, 12, 40, 6, 9, 30, 30, 56, 53, 38, 53, 38, 53, 38, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 61, 24, 53, 38, 53, 38, 34, 30, 63, 12, 2, 46, 43, 33, 31, 40, 21, 43, 8, 30, 40, 10, 39, 2, 33, 6, 40, 6, 9, 12, 2, 40, 6, 9, 30, 40, 39, 43, 12, 33, 25, 45, 40, 27, 14, 19, 45, 55, 53, 38, 1, 33, 13, 40, 8, 14, 48, 30, 40, 6, 9, 30, 40, 30, 14, 46, 6, 9, 40, 13, 30, 63, 12, 2, 46, 40, 9, 30, 46, 40, 12, 19, 33, 40, 45, 19, 30, 30, 6, 40, 10, 46, 12, 12, 13, 55, 53, 38, 67, 39, 2, 54, 48, 40, 6, 9, 30, 40, 48, 30, 30, 33, 40, 6, 30, 30, 6, 9, 40, 7, 46, 12, 8, 40, 6, 9, 30, 40, 7, 43, 30, 46, 54, 30, 40, 6, 43, 31, 30, 46, 25, 45, 40, 18, 14, 19, 45, 55, 53, 38, 1, 33, 13, 40, 10, 2, 46, 33, 40, 6, 9, 30, 40, 39, 12, 33, 31, 47, 39, 43, 63, 30, 13, 40, 27, 9, 12, 30, 33, 43, 41, 55, 40, 43, 33, 40, 9, 30, 46, 40, 10, 39, 12, 12, 13, 55, 53, 38, 37, 14, 48, 30, 40, 31, 39, 14, 13, 40, 14, 33, 13, 40, 45, 12, 46, 46, 26, 40, 45, 30, 14, 45, 12, 33, 45, 40, 14, 45, 40, 6, 9, 12, 2, 40, 7, 39, 30, 30, 6, 25, 45, 6, 55, 53, 38, 1, 33, 13, 40, 13, 12, 40, 19, 9, 14, 6, 30, 25, 30, 46, 40, 6, 9, 12, 2, 40, 19, 43, 39, 6, 40, 45, 19, 43, 7, 6, 47, 7, 12, 12, 6, 30, 13, 40, 21, 43, 8, 30, 53, 38, 21, 12, 40, 6, 9, 30, 40, 19, 43, 13, 30, 40, 19, 12, 46, 39, 13, 40, 14, 33, 13, 40, 14, 39, 39, 40, 9, 30, 46, 40, 7, 14, 13, 43, 33, 31, 40, 45, 19, 30, 30, 6, 45, 16, 53, 38, 17, 2, 6, 40, 57, 40, 7, 12, 46, 10, 43, 13, 40, 6, 9, 30, 30, 40, 12, 33, 30, 40, 8, 12, 45, 6, 40, 9, 30, 43, 33, 12, 2, 45, 40, 54, 46, 43, 8, 30, 55, 53, 38, 52, 40, 54, 14, 46, 63, 30, 40, 33, 12, 6, 40, 19, 43, 6, 9, 40, 6, 9, 26, 40, 9, 12, 2, 46, 45, 40, 8, 26, 40, 39, 12, 63, 30, 25, 45, 40, 7, 14, 43, 46, 40, 10, 46, 12, 19, 55, 53, 38, 23, 12, 46, 40, 13, 46, 14, 19, 40, 33, 12, 40, 39, 43, 33, 30, 45, 40, 6, 9, 30, 46, 30, 40, 19, 43, 6, 9, 40, 6, 9, 43, 33, 30, 40, 14, 33, 6, 43, 64, 2, 30, 40, 27, 30, 33, 55, 53, 38, 66, 43, 8, 40, 43, 33, 40, 6, 9, 26, 40, 54, 12, 2, 46, 45, 30, 40, 2, 33, 6, 14, 43, 33, 6, 30, 13, 40, 13, 12, 40, 14, 39, 39, 12, 19, 55, 53, 38, 59, 12, 46, 40, 10, 30, 14, 2, 6, 26, 25, 45, 40, 27, 14, 6, 6, 30, 46, 33, 40, 6, 12, 40, 45, 2, 54, 54, 30, 30, 13, 43, 33, 31, 40, 8, 30, 33, 56, 53, 38, 40, 40, 50, 30, 6, 40, 13, 12, 40, 6, 9, 26, 40, 19, 12, 46, 45, 6, 40, 12, 39, 13, 40, 21, 43, 8, 30, 16, 40, 13, 30, 45, 27, 43, 6, 30, 40, 6, 9, 26, 40, 19, 46, 12, 33, 31, 55, 53, 38, 40, 40, 37, 26, 40, 39, 12, 63, 30, 40, 45, 9, 14, 39, 39, 40, 43, 33, 40, 8, 26, 40, 63, 30, 46, 45, 30, 40, 30, 63, 30, 46, 40, 39, 43, 63, 30, 40, 26, 12, 2, 33, 31, 56, 53, 38, 53, 38, 53, 38, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 51, 3, 53, 38, 53, 38, 1, 40, 19, 12, 8, 14, 33, 25, 45, 40, 7, 14, 54, 30, 40, 19, 43, 6, 9, 40, 33, 14, 6, 2, 46, 30, 25, 45, 40, 12, 19, 33, 40, 9, 14, 33, 13, 40, 27, 14, 43, 33, 6, 30, 13, 55, 53, 38, 66, 14, 45, 6, 40, 6, 9, 12, 2, 40, 6, 9, 30, 40, 8, 14, 45, 6, 30, 46, 40, 8, 43, 45, 6, 46, 30, 45, 45, 40, 12, 7, 40, 8, 26, 40, 27, 14, 45, 45, 43, 12, 33, 55, 53, 38, 1, 40, 19, 12, 8, 14, 33, 25, 45, 40, 31, 30, 33, 6, 39, 30, 40, 9, 30, 14, 46, 6, 40, 10, 2, 6, 40, 33, 12, 6, 40, 14, 54, 64, 2, 14, 43, 33, 6, 30, 13, 53, 38, 44, 43, 6, 9, 40, 45, 9, 43, 7, 6, 43, 33, 31, 40, 54, 9, 14, 33, 31, 30, 40, 14, 45, 40, 43, 45, 40, 7, 14, 39, 45, 30, 40, 19, 12, 8, 30, 33, 25, 45, 40, 7, 14, 45, 9, 43, 12, 33, 55, 53, 38, 1, 33, 40, 30, 26, 30, 40, 8, 12, 46, 30, 40, 10, 46, 43, 31, 9, 6, 40, 6, 9, 14, 33, 40, 6, 9, 30, 43, 46, 45, 55, 40, 39, 30, 45, 45, 40, 7, 14, 39, 45, 30, 40, 43, 33, 40, 46, 12, 39, 39, 43, 33, 31, 16, 53, 38, 11, 43, 39, 13, 43, 33, 31, 40, 6, 9, 30, 40, 12, 10, 18, 30, 54, 6, 40, 19, 9, 30, 46, 30, 2, 27, 12, 33, 40, 43, 6, 40, 31, 14, 0, 30, 6, 9, 55, 53, 38, 1, 40, 8, 14, 33, 40, 43, 33, 40, 9, 2, 30, 40, 14, 39, 39, 40, 9, 2, 30, 45, 40, 43, 33, 40, 9, 43, 45, 40, 54, 12, 33, 6, 46, 12, 39, 39, 43, 33, 31, 55, 53, 38, 44, 9, 43, 54, 9, 40, 45, 6, 30, 14, 39, 45, 40, 8, 30, 33, 25, 45, 40, 30, 26, 30, 45, 40, 14, 33, 13, 40, 19, 12, 8, 30, 33, 25, 45, 40, 45, 12, 2, 39, 45, 40, 14, 8, 14, 0, 30, 6, 9, 56, 53, 38, 1, 33, 13, 40, 7, 12, 46, 40, 14, 40, 19, 12, 8, 14, 33, 40, 19, 30, 46, 6, 40, 6, 9, 12, 2, 40, 7, 43, 46, 45, 6, 40, 54, 46, 30, 14, 6, 30, 13, 55, 53, 38, 21, 43, 39, 39, 40, 33, 14, 6, 2, 46, 30, 40, 14, 45, 40, 45, 9, 30, 40, 19, 46, 12, 2, 31, 9, 6, 40, 6, 9, 30, 30, 40, 7, 30, 39, 39, 40, 14, 47, 13, 12, 6, 43, 33, 31, 55, 53, 38, 1, 33, 13, 40, 10, 26, 40, 14, 13, 13, 43, 6, 43, 12, 33, 40, 8, 30, 40, 12, 7, 40, 6, 9, 30, 30, 40, 13, 30, 7, 30, 14, 6, 30, 13, 55, 53, 38, 17, 26, 40, 14, 13, 13, 43, 33, 31, 40, 12, 33, 30, 40, 6, 9, 43, 33, 31, 40, 6, 12, 40, 8, 26, 40, 27, 2, 46, 27, 12, 45, 30, 40, 33, 12, 6, 9, 43, 33, 31, 56, 53, 38, 40, 40, 17, 2, 6, 40, 45, 43, 33, 54, 30, 40, 45, 9, 30, 40, 27, 46, 43, 54, 48, 30, 13, 40, 6, 9, 30, 30, 40, 12, 2, 6, 40, 7, 12, 46, 40, 19, 12, 8, 30, 33, 25, 45, 40, 27, 39, 30, 14, 45, 2, 46, 30, 55, 53, 38, 40, 40, 37, 43, 33, 30, 40, 10, 30, 40, 6, 9, 26, 40, 39, 12, 63, 30, 40, 14, 33, 13, 40, 6, 9, 26, 40, 39, 12, 63, 30, 25, 45, 40, 2, 45, 30, 40, 6, 9, 30, 43, 46, 40, 6, 46, 30, 14, 45, 2, 46, 30, 56, 53, 38, 53, 38, 53, 38, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 51, 61, 53, 38, 53, 38, 42, 12, 40, 43, 45, 40, 43, 6, 40, 33, 12, 6, 40, 19, 43, 6, 9, 40, 8, 30, 40, 14, 45, 40, 19, 43, 6, 9, 40, 6, 9, 14, 6, 40, 8, 2, 45, 30, 55, 53, 38, 42, 6, 43, 46, 46, 30, 13, 40, 10, 26, 40, 14, 40, 27, 14, 43, 33, 6, 30, 13, 40, 10, 30, 14, 2, 6, 26, 40, 6, 12, 40, 9, 43, 45, 40, 63, 30, 46, 45, 30, 55, 53, 38, 44, 9, 12, 40, 9, 30, 14, 63, 30, 33, 40, 43, 6, 40, 45, 30, 39, 7, 40, 7, 12, 46, 40, 12, 46, 33, 14, 8, 30, 33, 6, 40, 13, 12, 6, 9, 40, 2, 45, 30, 55, 53, 38, 1, 33, 13, 40, 30, 63, 30, 46, 26, 40, 7, 14, 43, 46, 40, 19, 43, 6, 9, 40, 9, 43, 45, 40, 7, 14, 43, 46, 40, 13, 12, 6, 9, 40, 46, 30, 9, 30, 14, 46, 45, 30, 55, 53, 38, 37, 14, 48, 43, 33, 31, 40, 14, 40, 54, 12, 2, 27, 39, 30, 8, 30, 33, 6, 40, 12, 7, 40, 27, 46, 12, 2, 13, 40, 54, 12, 8, 27, 14, 46, 30, 53, 38, 44, 43, 6, 9, 40, 45, 2, 33, 40, 14, 33, 13, 40, 8, 12, 12, 33, 55, 40, 19, 43, 6, 9, 40, 30, 14, 46, 6, 9, 40, 14, 33, 13, 40, 45, 30, 14, 25, 45, 40, 46, 43, 54, 9, 40, 31, 30, 8, 45, 16, 53, 38, 44, 43, 6, 9, 40, 1, 27, 46, 43, 39, 25, 45, 40, 7, 43, 46, 45, 6, 47, 10, 12, 46, 33, 40, 7, 39, 12, 19, 30, 46, 45, 40, 14, 33, 13, 40, 14, 39, 39, 40, 6, 9, 43, 33, 31, 45, 40, 46, 14, 46, 30, 55, 53, 38, 21, 9, 14, 6, 40, 9, 30, 14, 63, 30, 33, 25, 45, 40, 14, 43, 46, 40, 43, 33, 40, 6, 9, 43, 45, 40, 9, 2, 31, 30, 40, 46, 12, 33, 13, 2, 46, 30, 40, 9, 30, 8, 45, 56, 53, 38, 52, 40, 39, 30, 6, 40, 8, 30, 40, 6, 46, 2, 30, 40, 43, 33, 40, 39, 12, 63, 30, 40, 10, 2, 6, 40, 6, 46, 2, 39, 26, 40, 19, 46, 43, 6, 30, 55, 53, 38, 1, 33, 13, 40, 6, 9, 30, 33, 40, 10, 30, 39, 43, 30, 63, 30, 40, 8, 30, 55, 40, 8, 26, 40, 39, 12, 63, 30, 40, 43, 45, 40, 14, 45, 40, 7, 14, 43, 46, 55, 53, 38, 1, 45, 40, 14, 33, 26, 40, 8, 12, 6, 9, 30, 46, 25, 45, 40, 54, 9, 43, 39, 13, 55, 40, 6, 9, 12, 2, 31, 9, 40, 33, 12, 6, 40, 45, 12, 40, 10, 46, 43, 31, 9, 6, 53, 38, 1, 45, 40, 6, 9, 12, 45, 30, 40, 31, 12, 39, 13, 40, 54, 14, 33, 13, 39, 30, 45, 40, 7, 43, 41, 30, 13, 40, 43, 33, 40, 9, 30, 14, 63, 30, 33, 25, 45, 40, 14, 43, 46, 16, 53, 38, 40, 40, 22, 30, 6, 40, 6, 9, 30, 8, 40, 45, 14, 26, 40, 8, 12, 46, 30, 40, 6, 9, 14, 6, 40, 39, 43, 48, 30, 40, 12, 7, 40, 9, 30, 14, 46, 45, 14, 26, 40, 19, 30, 39, 39, 55, 53, 38, 40, 40, 57, 40, 19, 43, 39, 39, 40, 33, 12, 6, 40, 27, 46, 14, 43, 45, 30, 40, 6, 9, 14, 6, 40, 27, 2, 46, 27, 12, 45, 30, 40, 33, 12, 6, 40, 6, 12, 40, 45, 30, 39, 39, 56, 53, 38, 53, 38, 53, 38, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 51, 51, 53, 38, 53, 38, 37, 26, 40, 31, 39, 14, 45, 45, 40, 45, 9, 14, 39, 39, 40, 33, 12, 6, 40, 27, 30, 46, 45, 2, 14, 13, 30, 40, 8, 30, 40, 57, 40, 14, 8, 40, 12, 39, 13, 55, 53, 38, 42, 12, 40, 39, 12, 33, 31, 40, 14, 45, 40, 26, 12, 2, 6, 9, 40, 14, 33, 13, 40, 6, 9, 12, 2, 40, 14, 46, 30, 40, 12, 7, 40, 12, 33, 30, 40, 13, 14, 6, 30, 55, 53, 38, 17, 2, 6, 40, 19, 9, 30, 33, 40, 43, 33, 40, 6, 9, 30, 30, 40, 6, 43, 8, 30, 25, 45, 40, 7, 2, 46, 46, 12, 19, 45, 40, 57, 40, 10, 30, 9, 12, 39, 13, 55, 53, 38, 21, 9, 30, 33, 40, 39, 12, 12, 48, 40, 57, 40, 13, 30, 14, 6, 9, 40, 8, 26, 40, 13, 14, 26, 45, 40, 45, 9, 12, 2, 39, 13, 40, 30, 41, 27, 43, 14, 6, 30, 56, 53, 38, 59, 12, 46, 40, 14, 39, 39, 40, 6, 9, 14, 6, 40, 10, 30, 14, 2, 6, 26, 40, 6, 9, 14, 6, 40, 13, 12, 6, 9, 40, 54, 12, 63, 30, 46, 40, 6, 9, 30, 30, 55, 53, 38, 57, 45, 40, 10, 2, 6, 40, 6, 9, 30, 40, 45, 30, 30, 8, 39, 26, 40, 46, 14, 43, 8, 30, 33, 6, 40, 12, 7, 40, 8, 26, 40, 9, 30, 14, 46, 6, 55, 53, 38, 44, 9, 43, 54, 9, 40, 43, 33, 40, 6, 9, 26, 40, 10, 46, 30, 14, 45, 6, 40, 13, 12, 6, 9, 40, 39, 43, 63, 30, 55, 40, 14, 45, 40, 6, 9, 43, 33, 30, 40, 43, 33, 40, 8, 30, 55, 53, 38, 66, 12, 19, 40, 54, 14, 33, 40, 57, 40, 6, 9, 30, 33, 40, 10, 30, 40, 30, 39, 13, 30, 46, 40, 6, 9, 14, 33, 40, 6, 9, 12, 2, 40, 14, 46, 6, 49, 53, 38, 52, 40, 6, 9, 30, 46, 30, 7, 12, 46, 30, 40, 39, 12, 63, 30, 40, 10, 30, 40, 12, 7, 40, 6, 9, 26, 45, 30, 39, 7, 40, 45, 12, 40, 19, 14, 46, 26, 55, 53, 38, 1, 45, 40, 57, 40, 33, 12, 6, 40, 7, 12, 46, 40, 8, 26, 40, 45, 30, 39, 7, 55, 40, 10, 2, 6, 40, 7, 12, 46, 40, 6, 9, 30, 30, 40, 19, 43, 39, 39, 55, 53, 38, 17, 30, 14, 46, 43, 33, 31, 40, 6, 9, 26, 40, 9, 30, 14, 46, 6, 40, 19, 9, 43, 54, 9, 40, 57, 40, 19, 43, 39, 39, 40, 48, 30, 30, 27, 40, 45, 12, 40, 54, 9, 14, 46, 26, 53, 38, 1, 45, 40, 6, 30, 33, 13, 30, 46, 40, 33, 2, 46, 45, 30, 40, 9, 30, 46, 40, 10, 14, 10, 30, 40, 7, 46, 12, 8, 40, 7, 14, 46, 43, 33, 31, 40, 43, 39, 39, 56, 53, 38, 40, 40, 67, 46, 30, 45, 2, 8, 30, 40, 33, 12, 6, 40, 12, 33, 40, 6, 9, 26, 40, 9, 30, 14, 46, 6, 40, 19, 9, 30, 33, 40, 8, 43, 33, 30, 40, 43, 45, 40, 45, 39, 14, 43, 33, 55, 53, 38, 40, 40, 21, 9, 12, 2, 40, 31, 14, 63, 25, 45, 6, 40, 8, 30, 40, 6, 9, 43, 33, 30, 40, 33, 12, 6, 40, 6, 12, 40, 31, 43, 63, 30, 40, 10, 14, 54, 48, 40, 14, 31, 14, 43, 33, 56, 53, 38, 53, 38, 53, 38, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 51, 62, 53, 38, 53, 38, 1, 45, 40, 14, 33, 40, 2, 33, 27, 30, 46, 7, 30, 54, 6, 40, 14, 54, 6, 12, 46, 40, 12, 33, 40, 6, 9, 30, 40, 45, 6, 14, 31, 30, 55, 53, 38, 44, 9, 12, 40, 19, 43, 6, 9, 40, 9, 43, 45, 40, 7, 30, 14, 46, 40, 43, 45, 40, 27, 2, 6, 40, 10, 30, 45, 43, 13, 30, 40, 9, 43, 45, 40, 27, 14, 46, 6, 55, 53, 38, 52, 46, 40, 45, 12, 8, 30, 40, 7, 43, 30, 46, 54, 30, 40, 6, 9, 43, 33, 31, 40, 46, 30, 27, 39, 30, 6, 30, 40, 19, 43, 6, 9, 40, 6, 12, 12, 40, 8, 2, 54, 9, 40, 46, 14, 31, 30, 55, 53, 38, 44, 9, 12, 45, 30, 40, 45, 6, 46, 30, 33, 31, 6, 9, 25, 45, 40, 14, 10, 2, 33, 13, 14, 33, 54, 30, 40, 19, 30, 14, 48, 30, 33, 45, 40, 9, 43, 45, 40, 12, 19, 33, 40, 9, 30, 14, 46, 6, 4, 53, 38, 42, 12, 40, 57, 40, 7, 12, 46, 40, 7, 30, 14, 46, 40, 12, 7, 40, 6, 46, 2, 45, 6, 55, 40, 7, 12, 46, 31, 30, 6, 40, 6, 12, 40, 45, 14, 26, 55, 53, 38, 21, 9, 30, 40, 27, 30, 46, 7, 30, 54, 6, 40, 54, 30, 46, 30, 8, 12, 33, 26, 40, 12, 7, 40, 39, 12, 63, 30, 25, 45, 40, 46, 43, 6, 30, 55, 53, 38, 1, 33, 13, 40, 43, 33, 40, 8, 43, 33, 30, 40, 12, 19, 33, 40, 39, 12, 63, 30, 25, 45, 40, 45, 6, 46, 30, 33, 31, 6, 9, 40, 45, 30, 30, 8, 40, 6, 12, 40, 13, 30, 54, 14, 26, 55, 53, 38, 52, 25, 30, 46, 54, 9, 14, 46, 31, 30, 13, 40, 19, 43, 6, 9, 40, 10, 2, 46, 6, 9, 30, 33, 40, 12, 7, 40, 8, 43, 33, 30, 40, 12, 19, 33, 40, 39, 12, 63, 30, 25, 45, 40, 8, 43, 31, 9, 6, 16, 53, 38, 52, 40, 39, 30, 6, 40, 8, 26, 40, 39, 12, 12, 48, 45, 40, 10, 30, 40, 6, 9, 30, 33, 40, 6, 9, 30, 40, 30, 39, 12, 64, 2, 30, 33, 54, 30, 55, 53, 38, 1, 33, 13, 40, 13, 2, 8, 10, 40, 27, 46, 30, 45, 14, 31, 30, 46, 45, 40, 12, 7, 40, 8, 26, 40, 45, 27, 30, 14, 48, 43, 33, 31, 40, 10, 46, 30, 14, 45, 6, 55, 53, 38, 44, 9, 12, 40, 27, 39, 30, 14, 13, 40, 7, 12, 46, 40, 39, 12, 63, 30, 55, 40, 14, 33, 13, 40, 39, 12, 12, 48, 40, 7, 12, 46, 40, 46, 30, 54, 12, 8, 27, 30, 33, 45, 30, 55, 53, 38, 37, 12, 46, 30, 40, 6, 9, 14, 33, 40, 6, 9, 14, 6, 40, 6, 12, 33, 31, 2, 30, 40, 6, 9, 14, 6, 40, 8, 12, 46, 30, 40, 9, 14, 6, 9, 40, 8, 12, 46, 30, 40, 30, 41, 27, 46, 30, 45, 45, 30, 13, 56, 53, 38, 40, 40, 52, 40, 39, 30, 14, 46, 33, 40, 6, 12, 40, 46, 30, 14, 13, 40, 19, 9, 14, 6, 40, 45, 43, 39, 30, 33, 6, 40, 39, 12, 63, 30, 40, 9, 14, 6, 9, 40, 19, 46, 43, 6, 55, 53, 38, 40, 40, 21, 12, 40, 9, 30, 14, 46, 40, 19, 43, 6, 9, 40, 30, 26, 30, 45, 40, 10, 30, 39, 12, 33, 31, 45, 40, 6, 12, 40, 39, 12, 63, 30, 25, 45, 40, 7, 43, 33, 30, 40, 19, 43, 6, 56, 53, 38, 53, 38, 53, 38, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 51, 5, 53, 38, 53, 38, 37, 43, 33, 30, 40, 30, 26, 30, 40, 9, 14, 6, 9, 40, 27, 39, 14, 26, 30, 13, 40, 6, 9, 30, 40, 27, 14, 43, 33, 6, 30, 46, 40, 14, 33, 13, 40, 9, 14, 6, 9, 40, 45, 6, 30, 39, 39, 30, 13, 55, 53, 38, 21, 9, 26, 40, 10, 30, 14, 2, 6, 26, 25, 45, 40, 7, 12, 46, 8, 40, 43, 33, 40, 6, 14, 10, 39, 30, 40, 12, 7, 40, 8, 26, 40, 9, 30, 14, 46, 6, 55, 53, 38, 37, 26, 40, 10, 12, 13, 26, 40, 43, 45, 40, 6, 9, 30, 40, 7, 46, 14, 8, 30, 40, 19, 9, 30, 46, 30, 43, 33, 40, 25, 6, 43, 45, 40, 9, 30, 39, 13, 55, 53, 38, 1, 33, 13, 40, 27, 30, 46, 45, 27, 30, 54, 6, 43, 63, 30, 40, 43, 6, 40, 43, 45, 40, 10, 30, 45, 6, 40, 27, 14, 43, 33, 6, 30, 46, 25, 45, 40, 14, 46, 6, 56, 53, 38, 59, 12, 46, 40, 6, 9, 46, 12, 2, 31, 9, 40, 6, 9, 30, 40, 27, 14, 43, 33, 6, 30, 46, 40, 8, 2, 45, 6, 40, 26, 12, 2, 40, 45, 30, 30, 40, 9, 43, 45, 40, 45, 48, 43, 39, 39, 55, 53, 38, 21, 12, 40, 7, 43, 33, 13, 40, 19, 9, 30, 46, 30, 40, 26, 12, 2, 46, 40, 6, 46, 2, 30, 40, 43, 8, 14, 31, 30, 40, 27, 43, 54, 6, 2, 46, 30, 13, 40, 39, 43, 30, 45, 55, 53, 38, 44, 9, 43, 54, 9, 40, 43, 33, 40, 8, 26, 40, 10, 12, 45, 12, 8, 25, 45, 40, 45, 9, 12, 27, 40, 43, 45, 40, 9, 14, 33, 31, 43, 33, 31, 40, 45, 6, 43, 39, 39, 55, 53, 38, 21, 9, 14, 6, 40, 9, 14, 6, 9, 40, 9, 43, 45, 40, 19, 43, 33, 13, 12, 19, 45, 40, 31, 39, 14, 0, 30, 13, 40, 19, 43, 6, 9, 40, 6, 9, 43, 33, 30, 40, 30, 26, 30, 45, 16, 53, 38, 23, 12, 19, 40, 45, 30, 30, 40, 19, 9, 14, 6, 40, 31, 12, 12, 13, 40, 6, 2, 46, 33, 45, 40, 30, 26, 30, 45, 40, 7, 12, 46, 40, 30, 26, 30, 45, 40, 9, 14, 63, 30, 40, 13, 12, 33, 30, 55, 53, 38, 37, 43, 33, 30, 40, 30, 26, 30, 45, 40, 9, 14, 63, 30, 40, 13, 46, 14, 19, 33, 40, 6, 9, 26, 40, 45, 9, 14, 27, 30, 55, 40, 14, 33, 13, 40, 6, 9, 43, 33, 30, 40, 7, 12, 46, 40, 8, 30, 53, 38, 1, 46, 30, 40, 19, 43, 33, 13, 12, 19, 45, 40, 6, 12, 40, 8, 26, 40, 10, 46, 30, 14, 45, 6, 55, 40, 19, 9, 30, 46, 30, 47, 6, 9, 46, 12, 2, 31, 9, 40, 6, 9, 30, 40, 45, 2, 33, 53, 38, 34, 30, 39, 43, 31, 9, 6, 45, 40, 6, 12, 40, 27, 30, 30, 27, 55, 40, 6, 12, 40, 31, 14, 0, 30, 40, 6, 9, 30, 46, 30, 43, 33, 40, 12, 33, 40, 6, 9, 30, 30, 4, 53, 38, 40, 40, 50, 30, 6, 40, 30, 26, 30, 45, 40, 6, 9, 43, 45, 40, 54, 2, 33, 33, 43, 33, 31, 40, 19, 14, 33, 6, 40, 6, 12, 40, 31, 46, 14, 54, 30, 40, 6, 9, 30, 43, 46, 40, 14, 46, 6, 55, 53, 38, 40, 40, 21, 9, 30, 26, 40, 13, 46, 14, 19, 40, 10, 2, 6, 40, 19, 9, 14, 6, 40, 6, 9, 30, 26, 40, 45, 30, 30, 55, 40, 48, 33, 12, 19, 40, 33, 12, 6, 40, 6, 9, 30, 40, 9, 30, 14, 46, 6, 56, 53, 38, 53, 38, 53, 38, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 51, 35, 53, 38, 53, 38, 22, 30, 6, 40, 6, 9, 12, 45, 30, 40, 19, 9, 12, 40, 14, 46, 30, 40, 43, 33, 40, 7, 14, 63, 12, 2, 46, 40, 19, 43, 6, 9, 40, 6, 9, 30, 43, 46, 40, 45, 6, 14, 46, 45, 55, 53, 38, 52, 7, 40, 27, 2, 10, 39, 43, 54, 40, 9, 12, 33, 12, 2, 46, 40, 14, 33, 13, 40, 27, 46, 12, 2, 13, 40, 6, 43, 6, 39, 30, 45, 40, 10, 12, 14, 45, 6, 55, 53, 38, 44, 9, 43, 39, 45, 6, 40, 57, 40, 19, 9, 12, 8, 40, 7, 12, 46, 6, 2, 33, 30, 40, 12, 7, 40, 45, 2, 54, 9, 40, 6, 46, 43, 2, 8, 27, 9, 40, 10, 14, 46, 45, 53, 38, 28, 33, 39, 12, 12, 48, 30, 13, 40, 7, 12, 46, 40, 18, 12, 26, 40, 43, 33, 40, 6, 9, 14, 6, 40, 57, 40, 9, 12, 33, 12, 2, 46, 40, 8, 12, 45, 6, 4, 53, 38, 11, 46, 30, 14, 6, 40, 27, 46, 43, 33, 54, 30, 45, 25, 40, 7, 14, 63, 12, 2, 46, 43, 6, 30, 45, 40, 6, 9, 30, 43, 46, 40, 7, 14, 43, 46, 40, 39, 30, 14, 63, 30, 45, 40, 45, 27, 46, 30, 14, 13, 55, 53, 38, 17, 2, 6, 40, 14, 45, 40, 6, 9, 30, 40, 8, 14, 46, 43, 31, 12, 39, 13, 40, 14, 6, 40, 6, 9, 30, 40, 45, 2, 33, 25, 45, 40, 30, 26, 30, 55, 53, 38, 1, 33, 13, 40, 43, 33, 40, 6, 9, 30, 8, 45, 30, 39, 63, 30, 45, 40, 6, 9, 30, 43, 46, 40, 27, 46, 43, 13, 30, 40, 39, 43, 30, 45, 40, 10, 2, 46, 43, 30, 13, 55, 53, 38, 59, 12, 46, 40, 14, 6, 40, 14, 40, 7, 46, 12, 19, 33, 40, 6, 9, 30, 26, 40, 43, 33, 40, 6, 9, 30, 43, 46, 40, 31, 39, 12, 46, 26, 40, 13, 43, 30, 56, 53, 38, 21, 9, 30, 40, 27, 14, 43, 33, 7, 2, 39, 40, 19, 14, 46, 46, 43, 12, 46, 40, 7, 14, 8, 12, 2, 45, 30, 13, 40, 7, 12, 46, 40, 7, 43, 31, 9, 6, 55, 53, 38, 1, 7, 6, 30, 46, 40, 14, 40, 6, 9, 12, 2, 45, 14, 33, 13, 40, 63, 43, 54, 6, 12, 46, 43, 30, 45, 40, 12, 33, 54, 30, 40, 7, 12, 43, 39, 30, 13, 55, 53, 38, 57, 45, 40, 7, 46, 12, 8, 40, 6, 9, 30, 40, 10, 12, 12, 48, 40, 12, 7, 40, 9, 12, 33, 12, 2, 46, 40, 46, 14, 0, 30, 13, 40, 64, 2, 43, 6, 30, 55, 53, 38, 1, 33, 13, 40, 14, 39, 39, 40, 6, 9, 30, 40, 46, 30, 45, 6, 40, 7, 12, 46, 31, 12, 6, 40, 7, 12, 46, 40, 19, 9, 43, 54, 9, 40, 9, 30, 40, 6, 12, 43, 39, 30, 13, 16, 53, 38, 40, 40, 21, 9, 30, 33, 40, 9, 14, 27, 27, 26, 40, 57, 40, 6, 9, 14, 6, 40, 39, 12, 63, 30, 40, 14, 33, 13, 40, 14, 8, 40, 10, 30, 39, 12, 63, 30, 13, 53, 38, 40, 40, 44, 9, 30, 46, 30, 40, 57, 40, 8, 14, 26, 40, 33, 12, 6, 40, 46, 30, 8, 12, 63, 30, 40, 33, 12, 46, 40, 10, 30, 40, 46, 30, 8, 12, 63, 30, 13, 56, 53, 38, 53, 38, 53, 38, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 51, 15, 53, 38, 53, 38, 22, 12, 46, 13, 40, 12, 7, 40, 8, 26, 40, 39, 12, 63, 30, 55, 40, 6, 12, 40, 19, 9, 12, 8, 40, 43, 33, 40, 63, 14, 45, 45, 14, 39, 14, 31, 30, 53, 38, 21, 9, 26, 40, 8, 30, 46, 43, 6, 40, 9, 14, 6, 9, 40, 8, 26, 40, 13, 2, 6, 26, 40, 45, 6, 46, 12, 33, 31, 39, 26, 40, 48, 33, 43, 6, 4, 53, 38, 21, 12, 40, 6, 9, 30, 30, 40, 57, 40, 45, 30, 33, 13, 40, 6, 9, 43, 45, 40, 19, 46, 43, 6, 6, 30, 33, 40, 30, 8, 10, 14, 45, 45, 14, 31, 30, 53, 38, 21, 12, 40, 19, 43, 6, 33, 30, 45, 45, 40, 13, 2, 6, 26, 55, 40, 33, 12, 6, 40, 6, 12, 40, 45, 9, 12, 19, 40, 8, 26, 40, 19, 43, 6, 56, 53, 38, 34, 2, 6, 26, 40, 45, 12, 40, 31, 46, 30, 14, 6, 55, 40, 19, 9, 43, 54, 9, 40, 19, 43, 6, 40, 45, 12, 40, 27, 12, 12, 46, 40, 14, 45, 40, 8, 43, 33, 30, 53, 38, 37, 14, 26, 40, 8, 14, 48, 30, 40, 45, 30, 30, 8, 40, 10, 14, 46, 30, 55, 40, 43, 33, 40, 19, 14, 33, 6, 43, 33, 31, 40, 19, 12, 46, 13, 45, 40, 6, 12, 40, 45, 9, 12, 19, 40, 43, 6, 4, 53, 38, 17, 2, 6, 40, 6, 9, 14, 6, 40, 57, 40, 9, 12, 27, 30, 40, 45, 12, 8, 30, 40, 31, 12, 12, 13, 40, 54, 12, 33, 54, 30, 43, 6, 40, 12, 7, 40, 6, 9, 43, 33, 30, 53, 38, 57, 33, 40, 6, 9, 26, 40, 45, 12, 2, 39, 25, 45, 40, 6, 9, 12, 2, 31, 9, 6, 40, 36, 14, 39, 39, 40, 33, 14, 48, 30, 13, 60, 40, 19, 43, 39, 39, 40, 10, 30, 45, 6, 12, 19, 40, 43, 6, 16, 53, 38, 21, 43, 39, 39, 40, 19, 9, 14, 6, 45, 12, 30, 63, 30, 46, 40, 45, 6, 14, 46, 40, 6, 9, 14, 6, 40, 31, 2, 43, 13, 30, 45, 40, 8, 26, 40, 8, 12, 63, 43, 33, 31, 55, 53, 38, 67, 12, 43, 33, 6, 45, 40, 12, 33, 40, 8, 30, 40, 31, 46, 14, 54, 43, 12, 2, 45, 39, 26, 40, 19, 43, 6, 9, 40, 7, 14, 43, 46, 40, 14, 45, 27, 30, 54, 6, 55, 53, 38, 1, 33, 13, 40, 27, 2, 6, 45, 40, 14, 27, 27, 14, 46, 30, 39, 40, 12, 33, 40, 8, 26, 40, 6, 14, 6, 6, 30, 46, 30, 13, 40, 39, 12, 63, 43, 33, 31, 55, 53, 38, 21, 12, 40, 45, 9, 12, 19, 40, 8, 30, 40, 19, 12, 46, 6, 9, 26, 40, 12, 7, 40, 6, 9, 26, 40, 45, 19, 30, 30, 6, 40, 46, 30, 45, 27, 30, 54, 6, 55, 53, 38, 40, 40, 21, 9, 30, 33, 40, 8, 14, 26, 40, 57, 40, 13, 14, 46, 30, 40, 6, 12, 40, 10, 12, 14, 45, 6, 40, 9, 12, 19, 40, 57, 40, 13, 12, 40, 39, 12, 63, 30, 40, 6, 9, 30, 30, 55, 53, 38, 40, 40, 21, 43, 39, 39, 40, 6, 9, 30, 33, 55, 40, 33, 12, 6, 40, 45, 9, 12, 19, 40, 8, 26, 40, 9, 30, 14, 13, 40, 19, 9, 30, 46, 30, 40, 6, 9, 12, 2, 40, 8, 14, 26, 45, 6, 40, 27, 46, 12, 63, 30, 40, 8, 30, 56, 53, 38, 53, 38, 53, 38, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 51, 29, 53, 38, 53, 38, 44, 30, 14, 46, 26, 40, 19, 43, 6, 9, 40, 6, 12, 43, 39, 55, 40, 57, 40, 9, 14, 45, 6, 30, 40, 8, 30, 40, 6, 12, 40, 8, 26, 40, 10, 30, 13, 55, 53, 38, 21, 9, 30, 40, 13, 30, 14, 46, 40, 46, 30, 45, 27, 12, 45, 30, 40, 7, 12, 46, 40, 39, 43, 8, 10, 45, 40, 19, 43, 6, 9, 40, 6, 46, 14, 63, 30, 39, 40, 6, 43, 46, 30, 13, 55, 53, 38, 17, 2, 6, 40, 6, 9, 30, 33, 40, 10, 30, 31, 43, 33, 45, 40, 14, 40, 18, 12, 2, 46, 33, 30, 26, 40, 43, 33, 40, 8, 26, 40, 9, 30, 14, 13, 53, 38, 21, 12, 40, 19, 12, 46, 48, 40, 8, 26, 40, 8, 43, 33, 13, 55, 40, 19, 9, 30, 33, 40, 10, 12, 13, 26, 25, 45, 40, 19, 12, 46, 48, 25, 45, 40, 30, 41, 27, 43, 46, 30, 13, 56, 53, 38, 59, 12, 46, 40, 6, 9, 30, 33, 40, 8, 26, 40, 6, 9, 12, 2, 31, 9, 6, 45, 40, 36, 7, 46, 12, 8, 40, 7, 14, 46, 40, 19, 9, 30, 46, 30, 40, 57, 40, 14, 10, 43, 13, 30, 60, 53, 38, 57, 33, 6, 30, 33, 13, 40, 14, 40, 0, 30, 14, 39, 12, 2, 45, 40, 27, 43, 39, 31, 46, 43, 8, 14, 31, 30, 40, 6, 12, 40, 6, 9, 30, 30, 55, 53, 38, 1, 33, 13, 40, 48, 30, 30, 27, 40, 8, 26, 40, 13, 46, 12, 12, 27, 43, 33, 31, 40, 30, 26, 30, 39, 43, 13, 45, 40, 12, 27, 30, 33, 40, 19, 43, 13, 30, 55, 53, 38, 22, 12, 12, 48, 43, 33, 31, 40, 12, 33, 40, 13, 14, 46, 48, 33, 30, 45, 45, 40, 19, 9, 43, 54, 9, 40, 6, 9, 30, 40, 10, 39, 43, 33, 13, 40, 13, 12, 40, 45, 30, 30, 56, 53, 38, 42, 14, 63, 30, 40, 6, 9, 14, 6, 40, 8, 26, 40, 45, 12, 2, 39, 25, 45, 40, 43, 8, 14, 31, 43, 33, 14, 46, 26, 40, 45, 43, 31, 9, 6, 53, 38, 67, 46, 30, 45, 30, 33, 6, 45, 40, 6, 9, 26, 40, 45, 9, 14, 13, 12, 19, 40, 6, 12, 40, 8, 26, 40, 45, 43, 31, 9, 6, 39, 30, 45, 45, 40, 63, 43, 30, 19, 55, 53, 38, 44, 9, 43, 54, 9, 40, 39, 43, 48, 30, 40, 14, 40, 18, 30, 19, 30, 39, 40, 36, 9, 2, 33, 31, 40, 43, 33, 40, 31, 9, 14, 45, 6, 39, 26, 40, 33, 43, 31, 9, 6, 60, 53, 38, 37, 14, 48, 30, 45, 40, 10, 39, 14, 54, 48, 40, 33, 43, 31, 9, 6, 40, 10, 30, 14, 2, 6, 30, 12, 2, 45, 55, 40, 14, 33, 13, 40, 9, 30, 46, 40, 12, 39, 13, 40, 7, 14, 54, 30, 40, 33, 30, 19, 56, 53, 38, 40, 40, 22, 12, 40, 6, 9, 2, 45, 40, 10, 26, 40, 13, 14, 26, 40, 8, 26, 40, 39, 43, 8, 10, 45, 55, 40, 10, 26, 40, 33, 43, 31, 9, 6, 40, 8, 26, 40, 8, 43, 33, 13, 55, 53, 38, 40, 40, 59, 12, 46, 40, 6, 9, 30, 30, 55, 40, 14, 33, 13, 40, 7, 12, 46, 40, 8, 26, 40, 45, 30, 39, 7, 55, 40, 33, 12, 40, 64, 2, 43, 30, 6, 40, 7, 43, 33, 13, 56, 53, 38, 53, 38, 53, 38, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 51, 68, 53, 38, 53, 38, 66, 12, 19, 40, 54, 14, 33, 40, 57, 40, 6, 9, 30, 33, 40, 46, 30, 6, 2, 46, 33, 40, 43, 33, 40, 9, 14, 27, 27, 26, 40, 27, 39, 43, 31, 9, 6, 53, 38, 21, 9, 14, 6, 40, 14, 8, 40, 13, 30, 10, 14, 46, 46, 30, 13, 40, 6, 9, 30, 40, 10, 30, 33, 30, 7, 43, 6, 40, 12, 7, 40, 46, 30, 45, 6, 49, 53, 38, 44, 9, 30, 33, 40, 13, 14, 26, 25, 45, 40, 12, 27, 27, 46, 30, 45, 45, 43, 12, 33, 40, 43, 45, 40, 33, 12, 6, 40, 30, 14, 45, 30, 13, 40, 10, 26, 40, 33, 43, 31, 9, 6, 55, 53, 38, 17, 2, 6, 40, 13, 14, 26, 40, 10, 26, 40, 33, 43, 31, 9, 6, 40, 14, 33, 13, 40, 33, 43, 31, 9, 6, 40, 10, 26, 40, 13, 14, 26, 40, 12, 27, 27, 46, 30, 45, 45, 30, 13, 56, 53, 38, 1, 33, 13, 40, 30, 14, 54, 9, 40, 36, 6, 9, 12, 2, 31, 9, 40, 30, 33, 30, 8, 43, 30, 45, 40, 6, 12, 40, 30, 43, 6, 9, 30, 46, 25, 45, 40, 46, 30, 43, 31, 33, 60, 53, 38, 34, 12, 40, 43, 33, 40, 54, 12, 33, 45, 30, 33, 6, 40, 45, 9, 14, 48, 30, 40, 9, 14, 33, 13, 45, 40, 6, 12, 40, 6, 12, 46, 6, 2, 46, 30, 40, 8, 30, 55, 53, 38, 21, 9, 30, 40, 12, 33, 30, 40, 10, 26, 40, 6, 12, 43, 39, 55, 40, 6, 9, 30, 40, 12, 6, 9, 30, 46, 40, 6, 12, 40, 54, 12, 8, 27, 39, 14, 43, 33, 53, 38, 66, 12, 19, 40, 7, 14, 46, 40, 57, 40, 6, 12, 43, 39, 55, 40, 45, 6, 43, 39, 39, 40, 7, 14, 46, 6, 9, 30, 46, 40, 12, 7, 7, 40, 7, 46, 12, 8, 40, 6, 9, 30, 30, 56, 53, 38, 57, 40, 6, 30, 39, 39, 40, 6, 9, 30, 40, 13, 14, 26, 40, 6, 12, 40, 27, 39, 30, 14, 45, 30, 40, 9, 43, 8, 40, 6, 9, 12, 2, 40, 14, 46, 6, 40, 10, 46, 43, 31, 9, 6, 55, 53, 38, 1, 33, 13, 40, 13, 12, 45, 6, 40, 9, 43, 8, 40, 31, 46, 14, 54, 30, 40, 19, 9, 30, 33, 40, 54, 39, 12, 2, 13, 45, 40, 13, 12, 40, 10, 39, 12, 6, 40, 6, 9, 30, 40, 9, 30, 14, 63, 30, 33, 16, 53, 38, 42, 12, 40, 7, 39, 14, 6, 6, 30, 46, 40, 57, 40, 6, 9, 30, 40, 45, 19, 14, 46, 6, 47, 54, 12, 8, 27, 39, 30, 41, 43, 12, 33, 30, 13, 40, 33, 43, 31, 9, 6, 55, 53, 38, 44, 9, 30, 33, 40, 45, 27, 14, 46, 48, 39, 43, 33, 31, 40, 45, 6, 14, 46, 45, 40, 6, 19, 43, 46, 30, 40, 33, 12, 6, 40, 6, 9, 12, 2, 40, 31, 43, 39, 13, 25, 45, 6, 40, 6, 9, 30, 40, 30, 63, 30, 33, 56, 53, 38, 40, 40, 17, 2, 6, 40, 13, 14, 26, 40, 13, 12, 6, 9, 40, 13, 14, 43, 39, 26, 40, 13, 46, 14, 19, 40, 8, 26, 40, 45, 12, 46, 46, 12, 19, 45, 40, 39, 12, 33, 31, 30, 46, 55, 53, 38, 40, 40, 1, 33, 13, 40, 33, 43, 31, 9, 6, 40, 13, 12, 6, 9, 40, 33, 43, 31, 9, 6, 39, 26, 40, 8, 14, 48, 30, 40, 31, 46, 43, 30, 7, 25, 45, 40, 39, 30, 33, 31, 6, 9, 40, 45, 30, 30, 8, 40, 45, 6, 46, 12, 33, 31, 30, 46, 53, 38, 53, 38, 53, 38, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 51, 24, 53, 38, 53, 38, 44, 9, 30, 33, 40, 43, 33, 40, 13, 43, 45, 31, 46, 14, 54, 30, 40, 19, 43, 6, 9, 40, 59, 12, 46, 6, 2, 33, 30, 40, 14, 33, 13, 40, 8, 30, 33, 25, 45, 40, 30, 26, 30, 45, 55, 53, 38, 57, 40, 14, 39, 39, 40, 14, 39, 12, 33, 30, 40, 10, 30, 19, 30, 30, 27, 40, 8, 26, 40, 12, 2, 6, 54, 14, 45, 6, 40, 45, 6, 14, 6, 30, 55, 53, 38, 1, 33, 13, 40, 6, 46, 12, 2, 10, 39, 30, 40, 13, 30, 14, 7, 40, 9, 30, 14, 63, 30, 33, 40, 19, 43, 6, 9, 40, 8, 26, 40, 10, 12, 12, 6, 39, 30, 45, 45, 40, 54, 46, 43, 30, 45, 55, 53, 38, 1, 33, 13, 40, 39, 12, 12, 48, 40, 2, 27, 12, 33, 40, 8, 26, 40, 45, 30, 39, 7, 40, 14, 33, 13, 40, 54, 2, 46, 45, 30, 40, 8, 26, 40, 7, 14, 6, 30, 55, 53, 38, 44, 43, 45, 9, 43, 33, 31, 40, 8, 30, 40, 39, 43, 48, 30, 40, 6, 12, 40, 12, 33, 30, 40, 8, 12, 46, 30, 40, 46, 43, 54, 9, 40, 43, 33, 40, 9, 12, 27, 30, 55, 53, 38, 59, 30, 14, 6, 2, 46, 30, 13, 40, 39, 43, 48, 30, 40, 9, 43, 8, 55, 40, 39, 43, 48, 30, 40, 9, 43, 8, 40, 19, 43, 6, 9, 40, 7, 46, 43, 30, 33, 13, 45, 40, 27, 12, 45, 45, 30, 45, 45, 30, 13, 55, 53, 38, 34, 30, 45, 43, 46, 43, 33, 31, 40, 6, 9, 43, 45, 40, 8, 14, 33, 25, 45, 40, 14, 46, 6, 55, 40, 14, 33, 13, 40, 6, 9, 14, 6, 40, 8, 14, 33, 25, 45, 40, 45, 54, 12, 27, 30, 55, 53, 38, 44, 43, 6, 9, 40, 19, 9, 14, 6, 40, 57, 40, 8, 12, 45, 6, 40, 30, 33, 18, 12, 26, 40, 54, 12, 33, 6, 30, 33, 6, 30, 13, 40, 39, 30, 14, 45, 6, 55, 53, 38, 50, 30, 6, 40, 43, 33, 40, 6, 9, 30, 45, 30, 40, 6, 9, 12, 2, 31, 9, 6, 45, 40, 8, 26, 40, 45, 30, 39, 7, 40, 14, 39, 8, 12, 45, 6, 40, 13, 30, 45, 27, 43, 45, 43, 33, 31, 55, 53, 38, 66, 14, 27, 39, 26, 40, 57, 40, 6, 9, 43, 33, 48, 40, 12, 33, 40, 6, 9, 30, 30, 55, 40, 14, 33, 13, 40, 6, 9, 30, 33, 40, 8, 26, 40, 45, 6, 14, 6, 30, 55, 53, 38, 36, 22, 43, 48, 30, 40, 6, 12, 40, 6, 9, 30, 40, 39, 14, 46, 48, 40, 14, 6, 40, 10, 46, 30, 14, 48, 40, 12, 7, 40, 13, 14, 26, 40, 14, 46, 43, 45, 43, 33, 31, 53, 38, 59, 46, 12, 8, 40, 45, 2, 39, 39, 30, 33, 40, 30, 14, 46, 6, 9, 60, 40, 45, 43, 33, 31, 45, 40, 9, 26, 8, 33, 45, 40, 14, 6, 40, 9, 30, 14, 63, 30, 33, 25, 45, 40, 31, 14, 6, 30, 55, 53, 38, 40, 40, 59, 12, 46, 40, 6, 9, 26, 40, 45, 19, 30, 30, 6, 40, 39, 12, 63, 30, 40, 46, 30, 8, 30, 8, 10, 30, 46, 30, 13, 40, 45, 2, 54, 9, 40, 19, 30, 14, 39, 6, 9, 40, 10, 46, 43, 33, 31, 45, 55, 53, 38, 40, 40, 21, 9, 14, 6, 40, 6, 9, 30, 33, 40, 57, 40, 45, 54, 12, 46, 33, 40, 6, 12, 40, 54, 9, 14, 33, 31, 30, 40, 8, 26, 40, 45, 6, 14, 6, 30, 40, 19, 43, 6, 9, 40, 48, 43, 33, 31, 45, 56, 53, 38, 53, 38, 53, 38, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 62, 3, 53, 38, 53, 38, 44, 9, 30, 33, 40, 6, 12, 40, 6, 9, 30, 40, 45, 30, 45, 45, 43, 12, 33, 45, 40, 12, 7, 40, 45, 19, 30, 30, 6, 40, 45, 43, 39, 30, 33, 6, 40, 6, 9, 12, 2, 31, 9, 6, 55, 53, 38, 57, 40, 45, 2, 8, 8, 12, 33, 40, 2, 27, 40, 46, 30, 8, 30, 8, 10, 46, 14, 33, 54, 30, 40, 12, 7, 40, 6, 9, 43, 33, 31, 45, 40, 27, 14, 45, 6, 55, 53, 38, 57, 40, 45, 43, 31, 9, 40, 6, 9, 30, 40, 39, 14, 54, 48, 40, 12, 7, 40, 8, 14, 33, 26, 40, 14, 40, 6, 9, 43, 33, 31, 40, 57, 40, 45, 12, 2, 31, 9, 6, 55, 53, 38, 1, 33, 13, 40, 19, 43, 6, 9, 40, 12, 39, 13, 40, 19, 12, 30, 45, 40, 33, 30, 19, 40, 19, 14, 43, 39, 40, 8, 26, 40, 13, 30, 14, 46, 40, 6, 43, 8, 30, 25, 45, 40, 19, 14, 45, 6, 30, 16, 53, 38, 21, 9, 30, 33, 40, 54, 14, 33, 40, 57, 40, 13, 46, 12, 19, 33, 40, 14, 33, 40, 30, 26, 30, 40, 36, 2, 33, 2, 45, 30, 13, 40, 6, 12, 40, 7, 39, 12, 19, 60, 53, 38, 59, 12, 46, 40, 27, 46, 30, 54, 43, 12, 2, 45, 40, 7, 46, 43, 30, 33, 13, 45, 40, 9, 43, 13, 40, 43, 33, 40, 13, 30, 14, 6, 9, 25, 45, 40, 13, 14, 6, 30, 39, 30, 45, 45, 40, 33, 43, 31, 9, 6, 55, 53, 38, 1, 33, 13, 40, 19, 30, 30, 27, 40, 14, 7, 46, 30, 45, 9, 40, 39, 12, 63, 30, 25, 45, 40, 39, 12, 33, 31, 40, 45, 43, 33, 54, 30, 40, 54, 14, 33, 54, 30, 39, 39, 30, 13, 40, 19, 12, 30, 55, 53, 38, 1, 33, 13, 40, 8, 12, 14, 33, 40, 6, 9, 25, 40, 30, 41, 27, 30, 33, 45, 30, 40, 12, 7, 40, 8, 14, 33, 26, 40, 14, 40, 63, 14, 33, 43, 45, 9, 30, 13, 40, 45, 43, 31, 9, 6, 56, 53, 38, 21, 9, 30, 33, 40, 54, 14, 33, 40, 57, 40, 31, 46, 43, 30, 63, 30, 40, 14, 6, 40, 31, 46, 43, 30, 63, 14, 33, 54, 30, 45, 40, 7, 12, 46, 30, 31, 12, 33, 30, 55, 53, 38, 1, 33, 13, 40, 9, 30, 14, 63, 43, 39, 26, 40, 7, 46, 12, 8, 40, 19, 12, 30, 40, 6, 12, 40, 19, 12, 30, 40, 6, 30, 39, 39, 40, 12, 25, 30, 46, 53, 38, 21, 9, 30, 40, 45, 14, 13, 40, 14, 54, 54, 12, 2, 33, 6, 40, 12, 7, 40, 7, 12, 46, 30, 47, 10, 30, 8, 12, 14, 33, 30, 13, 40, 8, 12, 14, 33, 55, 53, 38, 44, 9, 43, 54, 9, 40, 57, 40, 33, 30, 19, 40, 27, 14, 26, 40, 14, 45, 40, 43, 7, 40, 33, 12, 6, 40, 27, 14, 43, 13, 40, 10, 30, 7, 12, 46, 30, 56, 53, 38, 40, 40, 17, 2, 6, 40, 43, 7, 40, 6, 9, 30, 40, 19, 9, 43, 39, 30, 40, 57, 40, 6, 9, 43, 33, 48, 40, 12, 33, 40, 6, 9, 30, 30, 40, 36, 13, 30, 14, 46, 40, 7, 46, 43, 30, 33, 13, 60, 53, 38, 40, 40, 1, 39, 39, 40, 39, 12, 45, 45, 30, 45, 40, 14, 46, 30, 40, 46, 30, 45, 6, 12, 46, 30, 13, 55, 40, 14, 33, 13, 40, 45, 12, 46, 46, 12, 19, 45, 40, 30, 33, 13, 56, 53, 38, 53, 38, 53, 38, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 62, 61, 53, 38, 53, 38, 21, 9, 26, 40, 10, 12, 45, 12, 8, 40, 43, 45, 40, 30, 33, 13, 30, 14, 46, 30, 13, 40, 19, 43, 6, 9, 40, 14, 39, 39, 40, 9, 30, 14, 46, 6, 45, 55, 53, 38, 44, 9, 43, 54, 9, 40, 57, 40, 10, 26, 40, 39, 14, 54, 48, 43, 33, 31, 40, 9, 14, 63, 30, 40, 45, 2, 27, 27, 12, 45, 30, 13, 40, 13, 30, 14, 13, 55, 53, 38, 1, 33, 13, 40, 6, 9, 30, 46, 30, 40, 46, 30, 43, 31, 33, 45, 40, 39, 12, 63, 30, 40, 14, 33, 13, 40, 14, 39, 39, 40, 39, 12, 63, 30, 25, 45, 40, 39, 12, 63, 43, 33, 31, 40, 27, 14, 46, 6, 45, 55, 53, 38, 1, 33, 13, 40, 14, 39, 39, 40, 6, 9, 12, 45, 30, 40, 7, 46, 43, 30, 33, 13, 45, 40, 19, 9, 43, 54, 9, 40, 57, 40, 6, 9, 12, 2, 31, 9, 6, 40, 10, 2, 46, 43, 30, 13, 56, 53, 38, 66, 12, 19, 40, 8, 14, 33, 26, 40, 14, 40, 9, 12, 39, 26, 40, 14, 33, 13, 40, 12, 10, 45, 30, 64, 2, 43, 12, 2, 45, 40, 6, 30, 14, 46, 53, 38, 66, 14, 6, 9, 40, 13, 30, 14, 46, 40, 46, 30, 39, 43, 31, 43, 12, 2, 45, 40, 39, 12, 63, 30, 40, 45, 6, 12, 39, 25, 33, 40, 7, 46, 12, 8, 40, 8, 43, 33, 30, 40, 30, 26, 30, 55, 53, 38, 1, 45, 40, 43, 33, 6, 30, 46, 30, 45, 6, 40, 12, 7, 40, 6, 9, 30, 40, 13, 30, 14, 13, 55, 40, 19, 9, 43, 54, 9, 40, 33, 12, 19, 40, 14, 27, 27, 30, 14, 46, 55, 53, 38, 17, 2, 6, 40, 6, 9, 43, 33, 31, 45, 40, 46, 30, 8, 12, 63, 30, 13, 40, 6, 9, 14, 6, 40, 9, 43, 13, 13, 30, 33, 40, 43, 33, 40, 6, 9, 30, 30, 40, 39, 43, 30, 56, 53, 38, 21, 9, 12, 2, 40, 14, 46, 6, 40, 6, 9, 30, 40, 31, 46, 14, 63, 30, 40, 19, 9, 30, 46, 30, 40, 10, 2, 46, 43, 30, 13, 40, 39, 12, 63, 30, 40, 13, 12, 6, 9, 40, 39, 43, 63, 30, 55, 53, 38, 66, 2, 33, 31, 40, 19, 43, 6, 9, 40, 6, 9, 30, 40, 6, 46, 12, 27, 9, 43, 30, 45, 40, 12, 7, 40, 8, 26, 40, 39, 12, 63, 30, 46, 45, 40, 31, 12, 33, 30, 55, 53, 38, 44, 9, 12, 40, 14, 39, 39, 40, 6, 9, 30, 43, 46, 40, 27, 14, 46, 6, 45, 40, 12, 7, 40, 8, 30, 40, 6, 12, 40, 6, 9, 30, 30, 40, 13, 43, 13, 40, 31, 43, 63, 30, 55, 53, 38, 21, 9, 14, 6, 40, 13, 2, 30, 40, 12, 7, 40, 8, 14, 33, 26, 55, 40, 33, 12, 19, 40, 43, 45, 40, 6, 9, 43, 33, 30, 40, 14, 39, 12, 33, 30, 56, 53, 38, 40, 40, 21, 9, 30, 43, 46, 40, 43, 8, 14, 31, 30, 45, 40, 57, 40, 39, 12, 63, 30, 13, 55, 40, 57, 40, 63, 43, 30, 19, 40, 43, 33, 40, 6, 9, 30, 30, 55, 53, 38, 40, 40, 1, 33, 13, 40, 6, 9, 12, 2, 40, 36, 14, 39, 39, 40, 6, 9, 30, 26, 60, 40, 9, 14, 45, 6, 40, 14, 39, 39, 40, 6, 9, 30, 40, 14, 39, 39, 40, 12, 7, 40, 8, 30, 56, 53, 38, 53, 38]\n",
      "//---------------------------------//----------------------------------//\n",
      "Data Length (Total number of chars): 20530\n"
     ]
    }
   ],
   "source": [
    "# One hot encoding unique characters\n",
    "\n",
    "# Dictionary of unique characters (key) and index number (value)\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "# Dictionary of index nubmer (key) and unique character (value)\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "print(char_to_int)\n",
    "print(\"//---------------------------------//----------------------------------//\")\n",
    "print(int_to_char)\n",
    "print(\"//---------------------------------//----------------------------------//\")\n",
    "\n",
    "# List of all characters in 'sonnet_blob' converted to econcoded characters \n",
    "# (each unique character encoded to an interger)\n",
    "integer_encoded = [char_to_int[i] for i in article_text]\n",
    "print(integer_encoded)\n",
    "print(\"//---------------------------------//----------------------------------//\")\n",
    "print(f\"Data Length (Total number of chars): {len(integer_encoded)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "iteration = 250\n",
    "sequence_length = 40\n",
    "batch_size = round((text_data_size / sequence_length) + 0.5) # = math.ceil\n",
    "hidden_size = 500  # number of neurons in hidden layer\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# Model Parameters\n",
    "\n",
    "# Weight: input -> hidden\n",
    "W_xh = np.random.randn(hidden_size, num_chars) * 0.01\n",
    "# Weight: hidden -> hidden\n",
    "W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "# Weight: input -> output\n",
    "W_hy = np.random.randn(num_chars, hidden_size) * 0.01\n",
    "\n",
    "# Bias: for hidden layer\n",
    "b_h =  np.zeros((hidden_size, 1))\n",
    "# Bias: for output\n",
    "b_y = np.zeros((num_chars, 1))\n",
    "\n",
    "# Previous state (h_(t-1))\n",
    "h_prev = np.zeros((hidden_size, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardprop(inputs, targets, h_prev):\n",
    "        \n",
    "    # Since the RNN receives the sequence, the weights are not updated during one sequence.\n",
    "    xs, hs, ys, ps = {}, {}, {}, {} # dictionary\n",
    "    hs[-1] = np.copy(h_prev) # Copy previous hidden state vector to -1 key value.\n",
    "    loss = 0 # loss initialization\n",
    "    \n",
    "    for t in range(len(inputs)): # t is a \"time step\" and is used as a key(dic).  \n",
    "        \n",
    "        xs[t] = np.zeros((num_chars,1)) \n",
    "        xs[t][inputs[t]] = 1\n",
    "        hs[t] = np.tanh(np.dot(W_xh, xs[t]) + np.dot(W_hh, hs[t-1]) + b_h) # hidden state. \n",
    "        ys[t] = np.dot(W_hy, hs[t]) + b_y # unnormalized log probabilities for next chars\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars. \n",
    "        # Softmax. -> The sum of probabilities is 1 even without the exp() function, but all of the elements are positive through the exp() function.\n",
    " \n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss). Efficient and simple code\n",
    "\n",
    "#         y_class = np.zeros((num_chars, 1)) \n",
    "#         y_class[targets[t]] =1\n",
    "#         loss += np.sum(y_class*(-np.log(ps[t]))) # softmax (cross-entropy loss)        \n",
    "\n",
    "    return loss, ps, hs, xs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Back propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(ps, inputs, hs, xs):\n",
    "\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy) # make all zero matrices.\n",
    "    dbh, dby = np.zeros_like(b_h), np.zeros_like(b_y)\n",
    "    dhnext = np.zeros_like(hs[0]) # (hidden_size,1) \n",
    "\n",
    "    # reversed\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t]) # shape (num_chars,1).  \"dy\" means \"dloss/dy\"\n",
    "        dy[targets[t]] -= 1 # backprop into y. After taking the soft max in the input vector, subtract 1 from the value of the element corresponding to the correct label.\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy \n",
    "        dh = np.dot(W_hy.T, dy) + dhnext # backprop into h. \n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity #tanh'(x) = 1-tanh^2(x)\n",
    "        dbh += dhraw\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        dhnext = np.dot(W_hh.T, dhraw)\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]: \n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients.  \n",
    "    \n",
    "    return dWxh, dWhh, dWhy, dbh, dby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 34.278856\n",
      "iter 1, loss: 34.613622\n",
      "iter 2, loss: 33.339764\n",
      "iter 3, loss: 33.750673\n",
      "iter 4, loss: 28.966298\n",
      "iter 5, loss: 33.137154\n",
      "iter 6, loss: 32.006217\n",
      "iter 7, loss: 34.405405\n",
      "iter 8, loss: 34.849736\n",
      "iter 9, loss: 35.016177\n",
      "iter 10, loss: 34.024311\n",
      "iter 11, loss: 33.661345\n",
      "iter 12, loss: 32.612589\n",
      "iter 13, loss: 31.395771\n",
      "iter 14, loss: 30.869206\n",
      "iter 15, loss: 30.538661\n",
      "iter 16, loss: 29.937937\n",
      "iter 17, loss: 29.231809\n",
      "iter 18, loss: 28.733053\n",
      "iter 19, loss: 29.544348\n",
      "iter 20, loss: 27.659229\n",
      "iter 21, loss: 27.727882\n",
      "iter 22, loss: 28.733026\n",
      "iter 23, loss: 27.007627\n",
      "iter 24, loss: 26.055063\n",
      "iter 25, loss: 24.556070\n",
      "iter 26, loss: 27.848597\n",
      "iter 27, loss: 23.613158\n",
      "iter 28, loss: 23.218200\n",
      "iter 29, loss: 22.823439\n",
      "iter 30, loss: 21.835354\n",
      "iter 31, loss: 19.164787\n",
      "iter 32, loss: 18.379229\n",
      "iter 33, loss: 17.068048\n",
      "iter 34, loss: 17.598674\n",
      "iter 35, loss: 18.010270\n",
      "iter 36, loss: 19.164117\n",
      "iter 37, loss: 16.651883\n",
      "iter 38, loss: 19.978308\n",
      "iter 39, loss: 19.072200\n",
      "iter 40, loss: 17.601972\n",
      "iter 41, loss: 17.700840\n",
      "iter 42, loss: 17.625795\n",
      "iter 43, loss: 18.333082\n",
      "iter 44, loss: 16.623070\n",
      "iter 45, loss: 16.716992\n",
      "iter 46, loss: 16.798560\n",
      "iter 47, loss: 16.965366\n",
      "iter 48, loss: 19.634322\n",
      "iter 49, loss: 17.487455\n",
      "iter 50, loss: 17.375331\n",
      "iter 51, loss: 17.585014\n",
      "iter 52, loss: 16.945442\n",
      "iter 53, loss: 16.447112\n",
      "iter 54, loss: 14.835111\n",
      "iter 55, loss: 14.848916\n",
      "iter 56, loss: 14.575416\n",
      "iter 57, loss: 14.711474\n",
      "iter 58, loss: 14.554805\n",
      "iter 59, loss: 14.183882\n",
      "iter 60, loss: 14.047333\n",
      "iter 61, loss: 14.207545\n",
      "iter 62, loss: 14.211910\n",
      "iter 63, loss: 14.046597\n",
      "iter 64, loss: 14.387660\n",
      "iter 65, loss: 14.086042\n",
      "iter 66, loss: 14.008632\n",
      "iter 67, loss: 13.734288\n",
      "iter 68, loss: 13.393012\n",
      "iter 69, loss: 13.445570\n",
      "iter 70, loss: 13.302888\n",
      "iter 71, loss: 13.219262\n",
      "iter 72, loss: 13.098218\n",
      "iter 73, loss: 13.023123\n",
      "iter 74, loss: 13.013758\n",
      "iter 75, loss: 13.036451\n",
      "iter 76, loss: 12.998708\n",
      "iter 77, loss: 12.937733\n",
      "iter 78, loss: 13.432631\n",
      "iter 79, loss: 12.733359\n",
      "iter 80, loss: 13.774885\n",
      "iter 81, loss: 13.120945\n",
      "iter 82, loss: 13.118749\n",
      "iter 83, loss: 13.006764\n",
      "iter 84, loss: 12.884857\n",
      "iter 85, loss: 12.967940\n",
      "iter 86, loss: 12.263006\n",
      "iter 87, loss: 12.047594\n",
      "iter 88, loss: 13.391418\n",
      "iter 89, loss: 12.710384\n",
      "iter 90, loss: 12.925482\n",
      "iter 91, loss: 12.528457\n",
      "iter 92, loss: 13.048581\n",
      "iter 93, loss: 12.489698\n",
      "iter 94, loss: 12.302587\n",
      "iter 95, loss: 12.176024\n",
      "iter 96, loss: 12.502752\n",
      "iter 97, loss: 12.464279\n",
      "iter 98, loss: 12.335684\n",
      "iter 99, loss: 12.125500\n",
      "iter 100, loss: 11.994931\n",
      "iter 101, loss: 12.030785\n",
      "iter 102, loss: 11.952897\n",
      "iter 103, loss: 11.511712\n",
      "iter 104, loss: 11.908571\n",
      "iter 105, loss: 11.330989\n",
      "iter 106, loss: 11.971530\n",
      "iter 107, loss: 11.109229\n",
      "iter 108, loss: 11.661059\n",
      "iter 109, loss: 11.231171\n",
      "iter 110, loss: 11.566484\n",
      "iter 111, loss: 11.087329\n",
      "iter 112, loss: 11.210212\n",
      "iter 113, loss: 10.802197\n",
      "iter 114, loss: 11.215542\n",
      "iter 115, loss: 10.958352\n",
      "iter 116, loss: 11.087562\n",
      "iter 117, loss: 11.072669\n",
      "iter 118, loss: 11.087330\n",
      "iter 119, loss: 11.757941\n",
      "iter 120, loss: 10.989783\n",
      "iter 121, loss: 11.329195\n",
      "iter 122, loss: 11.302694\n",
      "iter 123, loss: 10.833800\n",
      "iter 124, loss: 11.077221\n",
      "iter 125, loss: 10.583547\n",
      "iter 126, loss: 10.529725\n",
      "iter 127, loss: 10.648360\n",
      "iter 128, loss: 10.415627\n",
      "iter 129, loss: 10.456769\n",
      "iter 130, loss: 10.412555\n",
      "iter 131, loss: 10.399965\n",
      "iter 132, loss: 10.613047\n",
      "iter 133, loss: 10.315357\n",
      "iter 134, loss: 10.432769\n",
      "iter 135, loss: 10.435002\n",
      "iter 136, loss: 10.289701\n",
      "iter 137, loss: 10.309674\n",
      "iter 138, loss: 10.447610\n",
      "iter 139, loss: 10.451511\n",
      "iter 140, loss: 10.340161\n",
      "iter 141, loss: 10.393530\n",
      "iter 142, loss: 10.347909\n",
      "iter 143, loss: 10.252942\n",
      "iter 144, loss: 10.201662\n",
      "iter 145, loss: 10.110024\n",
      "iter 146, loss: 10.055419\n",
      "iter 147, loss: 9.850189\n",
      "iter 148, loss: 10.003312\n",
      "iter 149, loss: 10.162623\n",
      "iter 150, loss: 9.950454\n",
      "iter 151, loss: 9.737161\n",
      "iter 152, loss: 9.957828\n",
      "iter 153, loss: 9.649440\n",
      "iter 154, loss: 9.774575\n",
      "iter 155, loss: 9.692479\n",
      "iter 156, loss: 9.562786\n",
      "iter 157, loss: 9.957433\n",
      "iter 158, loss: 9.652638\n",
      "iter 159, loss: 9.552465\n",
      "iter 160, loss: 9.624413\n",
      "iter 161, loss: 9.611449\n",
      "iter 162, loss: 10.098779\n",
      "iter 163, loss: 9.834290\n",
      "iter 164, loss: 10.453765\n",
      "iter 165, loss: 10.140978\n",
      "iter 166, loss: 9.837549\n",
      "iter 167, loss: 10.258659\n",
      "iter 168, loss: 9.981966\n",
      "iter 169, loss: 9.741393\n",
      "iter 170, loss: 9.787849\n",
      "iter 171, loss: 9.851088\n",
      "iter 172, loss: 9.720282\n",
      "iter 173, loss: 9.902107\n",
      "iter 174, loss: 9.846799\n",
      "iter 175, loss: 10.262464\n",
      "iter 176, loss: 10.020309\n",
      "iter 177, loss: 9.850856\n",
      "iter 178, loss: 9.957652\n",
      "iter 179, loss: 10.098836\n",
      "iter 180, loss: 9.713162\n",
      "iter 181, loss: 9.647935\n",
      "iter 182, loss: 9.706911\n",
      "iter 183, loss: 9.568866\n",
      "iter 184, loss: 9.671103\n",
      "iter 185, loss: 9.630645\n",
      "iter 186, loss: 9.829390\n",
      "iter 187, loss: 9.679770\n",
      "iter 188, loss: 11.327515\n",
      "iter 189, loss: 10.779997\n",
      "iter 190, loss: 10.284307\n",
      "iter 191, loss: 10.589961\n",
      "iter 192, loss: 10.729095\n",
      "iter 193, loss: 10.598593\n",
      "iter 194, loss: 10.660714\n",
      "iter 195, loss: 10.691068\n",
      "iter 196, loss: 10.662554\n",
      "iter 197, loss: 10.445102\n",
      "iter 198, loss: 10.559158\n",
      "iter 199, loss: 11.013212\n",
      "iter 200, loss: 10.485818\n",
      "iter 201, loss: 10.409780\n",
      "iter 202, loss: 10.976804\n",
      "iter 203, loss: 10.585131\n",
      "iter 204, loss: 10.502303\n",
      "iter 205, loss: 11.059348\n",
      "iter 206, loss: 11.042725\n",
      "iter 207, loss: 10.456796\n",
      "iter 208, loss: 10.360433\n",
      "iter 209, loss: 10.339890\n",
      "iter 210, loss: 10.276020\n",
      "iter 211, loss: 10.248826\n",
      "iter 212, loss: 10.254607\n",
      "iter 213, loss: 10.441152\n",
      "iter 214, loss: 10.118522\n",
      "iter 215, loss: 10.269254\n",
      "iter 216, loss: 10.173396\n",
      "iter 217, loss: 10.072816\n",
      "iter 218, loss: 9.933639\n",
      "iter 219, loss: 10.001076\n",
      "iter 220, loss: 9.927708\n",
      "iter 221, loss: 9.857020\n",
      "iter 222, loss: 9.767823\n",
      "iter 223, loss: 9.774427\n",
      "iter 224, loss: 9.709867\n",
      "iter 225, loss: 9.734792\n",
      "iter 226, loss: 9.601717\n",
      "iter 227, loss: 9.492810\n",
      "iter 228, loss: 9.621308\n",
      "iter 229, loss: 9.586761\n",
      "iter 230, loss: 9.405092\n",
      "iter 231, loss: 9.430545\n",
      "iter 232, loss: 9.612549\n",
      "iter 233, loss: 9.390722\n",
      "iter 234, loss: 9.129309\n",
      "iter 235, loss: 9.228779\n",
      "iter 236, loss: 9.402473\n",
      "iter 237, loss: 9.368269\n",
      "iter 238, loss: 9.821960\n",
      "iter 239, loss: 9.373222\n",
      "iter 240, loss: 9.313788\n",
      "iter 241, loss: 9.288641\n",
      "iter 242, loss: 9.667395\n",
      "iter 243, loss: 9.246996\n",
      "iter 244, loss: 9.347019\n",
      "iter 245, loss: 9.267702\n",
      "iter 246, loss: 9.184917\n",
      "iter 247, loss: 9.221086\n",
      "iter 248, loss: 9.225160\n",
      "iter 249, loss: 9.191253\n",
      "CPU times: user 7h 31min 48s, sys: 17h 42min 50s, total: 1d 1h 14min 38s\n",
      "Wall time: 1h 34min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data_pointer = 0\n",
    "\n",
    "# memory variables for Adagrad\n",
    "mWxh, mWhh, mWhy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy)\n",
    "mbh, mby = np.zeros_like(b_h), np.zeros_like(b_y) \n",
    "\n",
    "for i in range(iteration):\n",
    "    h_prev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    data_pointer = 0 # go from start of data\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        \n",
    "        inputs = [char_to_int[ch] for ch in article_text[data_pointer:data_pointer+sequence_length]]\n",
    "        targets = [char_to_int[ch] for ch in article_text[data_pointer+1:data_pointer+sequence_length+1]] # t+1        \n",
    "            \n",
    "        if (data_pointer+sequence_length+1 >= len(article_text) and b == batch_size-1): # processing of the last part of the input data. \n",
    "#             targets.append(char_to_int[txt_data[0]])   # When the data doesn't fit, add the first char to the back.\n",
    "            targets.append(char_to_int[\" \"])   # When the data doesn't fit, add space(\" \") to the back.\n",
    "\n",
    "\n",
    "        # forward\n",
    "        loss, ps, hs, xs = forwardprop(inputs, targets, h_prev)\n",
    "#         print(loss)\n",
    "    \n",
    "        # backward\n",
    "        dWxh, dWhh, dWhy, dbh, dby = backprop(ps, inputs, hs, xs) \n",
    "        \n",
    "        \n",
    "    # perform parameter update with Adagrad\n",
    "        for param, dparam, mem in zip([W_xh, W_hh, W_hy, b_h, b_y], \n",
    "                                    [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                    [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "            mem += dparam * dparam # elementwise\n",
    "            param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update      \n",
    "    \n",
    "        data_pointer += sequence_length # move data pointer\n",
    "        \n",
    "    if i % 1 == 0:\n",
    "        print ('iter %d, loss: %f' % (i, loss)) # print progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_char, length):\n",
    "    x = np.zeros((num_chars, 1)) \n",
    "    x[char_to_int[test_char]] = 1\n",
    "    ixes = []\n",
    "    h = np.zeros((hidden_size,1))\n",
    "\n",
    "    for t in range(length):\n",
    "        h = np.tanh(np.dot(W_xh, x) + np.dot(W_hh, h) + b_h) \n",
    "        y = np.dot(W_hy, h) + b_y\n",
    "        p = np.exp(y) / np.sum(np.exp(y)) \n",
    "        ix = np.random.choice(range(num_chars), p=p.ravel()) # ravel -> rank0\n",
    "        # \"ix\" is a list of indexes selected according to the soft max probability.\n",
    "        x = np.zeros((num_chars, 1)) # init\n",
    "        x[ix] = 1 \n",
    "        ixes.append(ix) # list\n",
    "    txt = test_char + ''.join(int_to_char[i] for i in ixes)\n",
    "    print ('-------\\n %s \\n-------' % (txt, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      " Care hoft-ome tis sheale biels,\n",
      "  you devete\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " so thte ey’s forl why’sbint storthovom commer thou nowl’s thy hy sathiene et ham, arthoo or elath that sell the.\n",
      "\n",
      "Whiglinm I my cay ot bone urtes bacuy umasbelde.rar qt sut then suim-sut,\n",
      "\n",
      "\n",
      "I gove corse at shtazoucted bpeat sresen the titst wens eopl thy unt lit\n",
      "On o  baghun.\n",
      "\n",
      "Ase.\n",
      "  T Oe lngistildest heat?\n",
      "Ur.\n",
      " hauth my hy my yeliln haty pon ind dich gart that fome tiph nen wher oThy seer arfiosm divsess and tarperist ailt, owrer sind to to I cheogh tie, laming n waute\n",
      "  Prapt, shy fot,\n",
      "Mare where try dy the pavke and anthrs,\n",
      "Thighe.\n",
      "nle beare.\n",
      "Brese of non cang and un frak et my tiths wuege whe rueght,\n",
      "Alderr niefs an wint will arer anoAste ere dir’s an thou erld printe whautes Soe eld an dume, facl:\n",
      "S\n",
      "Butely fdan spese,\n",
      "I ards’deld om of I hith on the the thess a gautts cork id seve\n",
      "Hruet are, duege,\n",
      "Wham wom’sd linmen chesinirour fich whesands, all, fore it momt thige?\n",
      "The arsinled, y pey,\n",
      "Thispired\n",
      "An.\n",
      "  Btim lrage I   a hilt iny chase thige,\n",
      "Whour beart show fase she stostleir koreind eivly tixel asmny tho seanl eees the I to thou rong the rrawl.\n",
      "  Fair swimiandimes oses fonthed,\n",
      "\n",
      "And leport an noscein;\n",
      "On thics tho eeres buth fner hovune I in whig wis widgen thy I these kosics no geous bonfs moce,\n",
      "And garemeil’s\n",
      "O llivuthest\n",
      "oow gere\n",
      "\n",
      "\n",
      "Wheates\n",
      "Meart,\n",
      "Ane thy of here,\n",
      "For bo nore buths doel, nneir bare lont reane so gonp,,\n",
      "  fe a hom  w eathrs exe ise psates ghem rechen too  \n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "predict('C', 1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zE4a4O7Bp5x1"
   },
   "source": [
    "# Resources and Stretch Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uT3UV3gap9H6"
   },
   "source": [
    "## Stretch goals:\n",
    "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
    "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
    "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
    "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
    "- Run on bigger, better data\n",
    "\n",
    "## Resources:\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
    "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
    "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
    "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
    "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN\n",
    "- [Using Dropout with LSTM](https://machinelearningmastery.com/use-dropout-lstm-networks-time-series-forecasting/)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS1_441_RNN_and_LSTM.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
