{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "U4-S2-NN (Python3)",
      "language": "python",
      "name": "u4-sprint2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "LS_DS_432_Convolution_Neural_Networks_Assignment.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdnJH5c11I69",
        "colab_type": "text"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "<br></br>\n",
        "\n",
        "## *Data Science Unit 4 Sprint 3 Assignment 2*\n",
        "# Convolutional Neural Networks (CNNs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0lfZdD_cp1t5"
      },
      "source": [
        "# Assignment\n",
        "\n",
        "- <a href=\"#p1\">Part 1:</a> Pre-Trained Model\n",
        "- <a href=\"#p2\">Part 2:</a> Custom CNN Model\n",
        "- <a href=\"#p3\">Part 3:</a> CNN with Data Augmentation\n",
        "\n",
        "\n",
        "You will apply three different CNN models to a binary image classification model using Keras. Classify images of Mountains (`./data/train/mountain/*`) and images of forests (`./data/train/forest/*`). Treat mountains as the positive class (1) and the forest images as the negative (zero). \n",
        "\n",
        "|Mountain (+)|Forest (-)|\n",
        "|---|---|\n",
        "|![](./data/train/mountain/art1131.jpg)|![](./data/validation/forest/cdmc317.jpg)|\n",
        "\n",
        "The problem is relatively difficult given that the sample is tiny: there are about 350 observations per class. This sample size might be something that you can expect with prototyping an image classification problem/solution at work. Get accustomed to evaluating several different possible models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3XOC5C-01I7E"
      },
      "source": [
        "# Pre - Trained Model\n",
        "<a id=\"p1\"></a>\n",
        "\n",
        "Load a pretrained network from Keras, [ResNet50](https://tfhub.dev/google/imagenet/resnet_v1_50/classification/1) - a 50 layer deep network trained to recognize [1000 objects](https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt). Starting usage:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
        "\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model # This is the functional API\n",
        "\n",
        "resnet = ResNet50(weights='imagenet', include_top=False)\n",
        "\n",
        "```\n",
        "\n",
        "The `include_top` parameter in `ResNet50` will remove the full connected layers from the ResNet model. The next step is to turn off the training of the ResNet layers. We want to use the learned parameters without updating them in future training passes. \n",
        "\n",
        "```python\n",
        "for layer in resnet.layers:\n",
        "    layer.trainable = False\n",
        "```\n",
        "\n",
        "Using the Keras functional API, we will need to additional additional full connected layers to our model. We we removed the top layers, we removed all preivous fully connected layers. In other words, we kept only the feature processing portions of our network. You can expert with additional layers beyond what's listed here. The `GlobalAveragePooling2D` layer functions as a really fancy flatten function by taking the average of each of the last convolutional layer outputs (which is two dimensional still). \n",
        "\n",
        "```python\n",
        "x = resnet.output\n",
        "x = GlobalAveragePooling2D()(x) # This layer is a really fancy flatten\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(1, activation='sigmoid')(x)\n",
        "model = Model(resnet.input, predictions)\n",
        "```\n",
        "\n",
        "Your assignment is to apply the transfer learning above to classify images of Mountains (`./data/train/mountain/*`) and images of forests (`./data/train/forest/*`). Treat mountains as the positive class (1) and the forest images as the negative (zero). \n",
        "\n",
        "Steps to complete assignment: \n",
        "1. Load in Image Data into numpy arrays (`X`) \n",
        "2. Create a `y` for the labels\n",
        "3. Train your model with pre-trained layers from resnet\n",
        "4. Report your model's accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgOkTKic1I7H",
        "colab_type": "text"
      },
      "source": [
        "## Load in Data\n",
        "\n",
        "This surprisingly more difficult than it seems, because you are working with directories of images instead of a single file. This boiler plate will help you download a zipped version of the directory of images. The directory is organized into \"train\" and \"validation\" which you can use inside an `ImageGenerator` class to stream batches of images thru your model.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADneeb7c1I7K",
        "colab_type": "text"
      },
      "source": [
        "### Download & Summarize the Data\n",
        "\n",
        "This step is completed for you. Just run the cells and review the results. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfTYd9bR1I7N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e23ebb81-626d-48b7-b771-e9aa210035cf"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "_URL = 'https://github.com/LambdaSchool/DS-Unit-4-Sprint-3-Deep-Learning/blob/master/module2-convolutional-neural-networks/data.zip?raw=true'\n",
        "\n",
        "path_to_zip = tf.keras.utils.get_file('./data.zip', origin=_URL, extract=True)\n",
        "PATH = os.path.join(os.path.dirname(path_to_zip), 'data')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/LambdaSchool/DS-Unit-4-Sprint-3-Deep-Learning/blob/master/module2-convolutional-neural-networks/data.zip?raw=true\n",
            "42172416/42170838 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDEsfTNB1I7h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dir = os.path.join(PATH, 'train')\n",
        "validation_dir = os.path.join(PATH, 'validation')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCe2G9RD1I72",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_mountain_dir = os.path.join(train_dir, 'mountain')  # directory with our training cat pictures\n",
        "train_forest_dir = os.path.join(train_dir, 'forest')  # directory with our training dog pictures\n",
        "validation_mountain_dir = os.path.join(validation_dir, 'mountain')  # directory with our validation cat pictures\n",
        "validation_forest_dir = os.path.join(validation_dir, 'forest')  # directory with our validation dog pictures"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLSEfzQq1I7-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_mountain_tr = len(os.listdir(train_mountain_dir))\n",
        "num_forest_tr = len(os.listdir(train_forest_dir))\n",
        "\n",
        "num_mountain_val = len(os.listdir(validation_mountain_dir))\n",
        "num_forest_val = len(os.listdir(validation_forest_dir))\n",
        "\n",
        "total_train = num_mountain_tr + num_forest_tr\n",
        "total_val = num_mountain_val + num_forest_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoMRZ2Vv1I8E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "a5aaf9b0-71dc-4c63-d200-25feb74362a4"
      },
      "source": [
        "print('total training mountain images:', num_mountain_tr)\n",
        "print('total training forest images:', num_forest_tr)\n",
        "\n",
        "print('total validation mountain images:', num_mountain_val)\n",
        "print('total validation forest images:', num_forest_val)\n",
        "print(\"--\")\n",
        "print(\"Total training images:\", total_train)\n",
        "print(\"Total validation images:\", total_val)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total training mountain images: 254\n",
            "total training forest images: 270\n",
            "total validation mountain images: 125\n",
            "total validation forest images: 62\n",
            "--\n",
            "Total training images: 524\n",
            "Total validation images: 187\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjHrMrjt1I8M",
        "colab_type": "text"
      },
      "source": [
        "### Keras `ImageGenerator` to Process the Data\n",
        "\n",
        "This step is completed for you, but please review the code. The `ImageGenerator` class reads in batches of data from a directory and pass them to the model one batch at a time. Just like large text files, this method is advantageous, because it stifles the need to load a bunch of images into memory. \n",
        "\n",
        "Check out the documentation for this class method: [Keras `ImageGenerator` Class](https://keras.io/preprocessing/image/#imagedatagenerator-class). You'll expand it's use in the third assignment objective."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoobruNr1I8O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 16\n",
        "epochs = 50\n",
        "IMG_HEIGHT = 224\n",
        "IMG_WIDTH = 224\n",
        "# CHNLS = 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPs8wVio1I8T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our training data\n",
        "validation_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our validation data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xE6u0Ek91I8X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a1daaeed-904d-4c21-82d0-2d8b83c54c7e"
      },
      "source": [
        "train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,\n",
        "                                                           directory=train_dir,\n",
        "                                                           shuffle=True,\n",
        "                                                           target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                           class_mode='binary')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 533 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELe8D7KJ1I8e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7ed9bb65-b158-4424-84c3-b4eacf113588"
      },
      "source": [
        "val_data_gen = validation_image_generator.flow_from_directory(batch_size=batch_size,\n",
        "                                                              directory=validation_dir,\n",
        "                                                              target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                              class_mode='binary')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 195 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kW_4jU8j1I8j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2358b7da-91da-4e3a-ee8b-49921df6f1d1"
      },
      "source": [
        "train_data_gen.image_shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(224, 224, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tUBphhA1I8v",
        "colab_type": "text"
      },
      "source": [
        "## Instatiate Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_S-MFPkA1I8x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "3cf48a79-559c-44e8-8e32-c6071239b83e"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
        "\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model # This is the functional API\n",
        "\n",
        "resnet = ResNet50(weights='imagenet', include_top=False)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZJQ8jEj1I84",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for layer in resnet.layers:\n",
        "    layer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bD2KFbNA1I8_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = resnet.output\n",
        "x = GlobalAveragePooling2D()(x) # This layer is a really fancy flatten\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(1, activation='sigmoid')(x)\n",
        "model = Model(resnet.input, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8cFnCmy1I9D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from tensorflow.keras.optimizers import Adam\n",
        "# lr = tf.keras.optimizers.Adam(learning_rate=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdIZKqku1I9K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7d023c78-56e9-4465-c7d5-7dfee9977d81"
      },
      "source": [
        "import tensorflow.keras.backend as K\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print(K.eval(model.optimizer.lr))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsH4H1zp1I9O",
        "colab_type": "text"
      },
      "source": [
        "## Fit Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-XFaztQ1I9P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ff9e45d9-d4c2-4664-de15-01af477230eb"
      },
      "source": [
        "history = model.fit(\n",
        "    train_data_gen,\n",
        "    steps_per_epoch=total_train // batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_data_gen,\n",
        "    validation_steps=total_val // batch_size\n",
        ")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "32/32 [==============================] - 4s 129ms/step - loss: 0.8427 - accuracy: 0.5684 - val_loss: 0.6720 - val_accuracy: 0.6364\n",
            "Epoch 2/50\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.5919 - accuracy: 0.7066 - val_loss: 0.5089 - val_accuracy: 0.7546\n",
            "Epoch 3/50\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 0.5650 - accuracy: 0.6707 - val_loss: 0.7996 - val_accuracy: 0.4356\n",
            "Epoch 4/50\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 0.5153 - accuracy: 0.7445 - val_loss: 0.4600 - val_accuracy: 0.8160\n",
            "Epoch 5/50\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 0.4673 - accuracy: 0.7804 - val_loss: 0.4447 - val_accuracy: 0.8037\n",
            "Epoch 6/50\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 0.4099 - accuracy: 0.8343 - val_loss: 0.4905 - val_accuracy: 0.7914\n",
            "Epoch 7/50\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 0.3907 - accuracy: 0.8623 - val_loss: 0.5342 - val_accuracy: 0.7669\n",
            "Epoch 8/50\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 0.3314 - accuracy: 0.8822 - val_loss: 0.4483 - val_accuracy: 0.8098\n",
            "Epoch 9/50\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 0.3669 - accuracy: 0.8443 - val_loss: 0.2942 - val_accuracy: 0.8589\n",
            "Epoch 10/50\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 0.2671 - accuracy: 0.9082 - val_loss: 0.3591 - val_accuracy: 0.8466\n",
            "Epoch 11/50\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 0.3425 - accuracy: 0.8503 - val_loss: 0.4549 - val_accuracy: 0.7914\n",
            "Epoch 12/50\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 0.2861 - accuracy: 0.8802 - val_loss: 0.3500 - val_accuracy: 0.8589\n",
            "Epoch 13/50\n",
            "32/32 [==============================] - 3s 103ms/step - loss: 0.2620 - accuracy: 0.9142 - val_loss: 0.3145 - val_accuracy: 0.8750\n",
            "Epoch 14/50\n",
            "32/32 [==============================] - 3s 103ms/step - loss: 0.3071 - accuracy: 0.8802 - val_loss: 0.3234 - val_accuracy: 0.8068\n",
            "Epoch 15/50\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 0.2494 - accuracy: 0.9202 - val_loss: 0.2439 - val_accuracy: 0.9018\n",
            "Epoch 16/50\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 0.2135 - accuracy: 0.9202 - val_loss: 0.3254 - val_accuracy: 0.9018\n",
            "Epoch 17/50\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 0.2135 - accuracy: 0.9222 - val_loss: 0.2009 - val_accuracy: 0.9202\n",
            "Epoch 18/50\n",
            "32/32 [==============================] - 3s 103ms/step - loss: 0.1870 - accuracy: 0.9453 - val_loss: 0.4463 - val_accuracy: 0.8344\n",
            "Epoch 19/50\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 0.2619 - accuracy: 0.8882 - val_loss: 0.3828 - val_accuracy: 0.7975\n",
            "Epoch 20/50\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 0.2634 - accuracy: 0.8782 - val_loss: 0.3224 - val_accuracy: 0.8466\n",
            "Epoch 21/50\n",
            "32/32 [==============================] - 3s 103ms/step - loss: 0.1759 - accuracy: 0.9421 - val_loss: 0.2279 - val_accuracy: 0.9387\n",
            "Epoch 22/50\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 0.1420 - accuracy: 0.9541 - val_loss: 0.1921 - val_accuracy: 0.9387\n",
            "Epoch 23/50\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 0.1781 - accuracy: 0.9301 - val_loss: 0.5517 - val_accuracy: 0.8344\n",
            "Epoch 24/50\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 0.1702 - accuracy: 0.9401 - val_loss: 0.1808 - val_accuracy: 0.9387\n",
            "Epoch 25/50\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 0.1728 - accuracy: 0.9401 - val_loss: 0.5026 - val_accuracy: 0.8221\n",
            "Epoch 26/50\n",
            "32/32 [==============================] - 3s 103ms/step - loss: 0.1239 - accuracy: 0.9601 - val_loss: 0.2363 - val_accuracy: 0.9205\n",
            "Epoch 27/50\n",
            "32/32 [==============================] - 3s 103ms/step - loss: 0.1706 - accuracy: 0.9341 - val_loss: 0.1931 - val_accuracy: 0.9148\n",
            "Epoch 28/50\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 0.2128 - accuracy: 0.9082 - val_loss: 0.5317 - val_accuracy: 0.7853\n",
            "Epoch 29/50\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 0.1256 - accuracy: 0.9621 - val_loss: 0.2395 - val_accuracy: 0.8896\n",
            "Epoch 30/50\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 0.1242 - accuracy: 0.9601 - val_loss: 0.1671 - val_accuracy: 0.9448\n",
            "Epoch 31/50\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 0.1567 - accuracy: 0.9321 - val_loss: 0.2082 - val_accuracy: 0.9325\n",
            "Epoch 32/50\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 0.0983 - accuracy: 0.9681 - val_loss: 0.2068 - val_accuracy: 0.9448\n",
            "Epoch 33/50\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 0.0941 - accuracy: 0.9780 - val_loss: 0.2045 - val_accuracy: 0.9141\n",
            "Epoch 34/50\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 0.1910 - accuracy: 0.9182 - val_loss: 0.3014 - val_accuracy: 0.8650\n",
            "Epoch 35/50\n",
            "32/32 [==============================] - 3s 103ms/step - loss: 0.1906 - accuracy: 0.9277 - val_loss: 0.2271 - val_accuracy: 0.9264\n",
            "Epoch 36/50\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 0.1187 - accuracy: 0.9581 - val_loss: 0.2434 - val_accuracy: 0.8896\n",
            "Epoch 37/50\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 0.1004 - accuracy: 0.9701 - val_loss: 0.1943 - val_accuracy: 0.9387\n",
            "Epoch 38/50\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 0.1396 - accuracy: 0.9441 - val_loss: 0.3158 - val_accuracy: 0.8589\n",
            "Epoch 39/50\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 0.1036 - accuracy: 0.9541 - val_loss: 0.2094 - val_accuracy: 0.9318\n",
            "Epoch 40/50\n",
            "32/32 [==============================] - 3s 103ms/step - loss: 0.0854 - accuracy: 0.9721 - val_loss: 0.1972 - val_accuracy: 0.9375\n",
            "Epoch 41/50\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 0.1241 - accuracy: 0.9541 - val_loss: 0.2631 - val_accuracy: 0.9018\n",
            "Epoch 42/50\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 0.0785 - accuracy: 0.9800 - val_loss: 0.2099 - val_accuracy: 0.9387\n",
            "Epoch 43/50\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 0.0855 - accuracy: 0.9721 - val_loss: 0.1483 - val_accuracy: 0.9448\n",
            "Epoch 44/50\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 0.0697 - accuracy: 0.9820 - val_loss: 0.1914 - val_accuracy: 0.9387\n",
            "Epoch 45/50\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 0.1048 - accuracy: 0.9541 - val_loss: 0.4216 - val_accuracy: 0.8528\n",
            "Epoch 46/50\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 0.2179 - accuracy: 0.9102 - val_loss: 0.3202 - val_accuracy: 0.8834\n",
            "Epoch 47/50\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 0.1100 - accuracy: 0.9621 - val_loss: 0.2883 - val_accuracy: 0.8957\n",
            "Epoch 48/50\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 0.0727 - accuracy: 0.9721 - val_loss: 0.1220 - val_accuracy: 0.9632\n",
            "Epoch 49/50\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 0.0876 - accuracy: 0.9621 - val_loss: 0.4594 - val_accuracy: 0.8282\n",
            "Epoch 50/50\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 0.0725 - accuracy: 0.9800 - val_loss: 0.3323 - val_accuracy: 0.9509\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fm8BDxHP1I9U",
        "colab_type": "text"
      },
      "source": [
        "# Custom CNN Model\n",
        "\n",
        "In this step, write and train your own convolutional neural network using Keras. You can use any architecture that suits you as long as it has at least one convolutional and one pooling layer at the beginning of the network - you can add more if you want. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRWhw1M11I9U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "190dee54-ea8b-4e7f-b887-a62aa74c253e"
      },
      "source": [
        "# # Define the Model\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
        "# model = Sequential()\n",
        "# model.add(Conv2D(224, (3,3), activation='relu', input_shape=(224,224,3)))\n",
        "# model.add(MaxPooling2D((2,2)))\n",
        "# model.add(Conv2D(448, (3,3), activation='relu'))\n",
        "# model.add(MaxPooling2D((2,2)))\n",
        "# model.add(Conv2D(448, (3,3), activation='relu'))\n",
        "# model.add(Flatten())\n",
        "# model.add(Dense(448, activation='relu'))\n",
        "# model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# model.summary()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-3e9060bfd783>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m448\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m448\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    211\u001b[0m       \u001b[0;31m# If the model is being built continuously on top of an input layer:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m       \u001b[0;31m# refresh its output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m       \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSINGLE_LAYER_OUTPUT_ERROR_MSG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    895\u001b[0m           \u001b[0;31m# Build layer if applicable (if the `build` method has been\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m           \u001b[0;31m# overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 897\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m           \u001b[0mcast_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2414\u001b[0m         \u001b[0;31m# operations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2415\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2416\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2417\u001b[0m       \u001b[0;31m# We must set also ensure that the layer is marked as built, and the build\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2418\u001b[0m       \u001b[0;31m# shape is stored since user defined build functions may not be calling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m   1163\u001b[0m         \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_constraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1165\u001b[0;31m         trainable=True)\n\u001b[0m\u001b[1;32m   1166\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m       self.bias = self.add_weight(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, synchronization, aggregation, **kwargs)\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m         caching_device=caching_device)\n\u001b[0m\u001b[1;32m    578\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mregularizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m       \u001b[0;31m# TODO(fchollet): in the future, this should be handled at the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_add_variable_with_custom_getter\u001b[0;34m(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 743\u001b[0;31m         **kwargs_for_getter)\n\u001b[0m\u001b[1;32m    744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m     \u001b[0;31m# If we set an initializer and the variable processed it, tracking will not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer_utils.py\u001b[0m in \u001b[0;36mmake_variable\u001b[0;34m(name, shape, dtype, initializer, trainable, caching_device, validate_shape, constraint, use_resource, collections, synchronization, aggregation, partitioner)\u001b[0m\n\u001b[1;32m    139\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m       \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m       shape=variable_shape if variable_shape else None)\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariableV1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v1_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m_variable_v1_call\u001b[0;34m(cls, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint, use_resource, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         shape=shape)\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m   def _variable_v2_call(cls,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m                         shape=None):\n\u001b[1;32m    197\u001b[0m     \u001b[0;34m\"\"\"Call on Variable class. Useful to force the signature.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdefault_variable_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creator_stack\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m       \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_getter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mdefault_variable_creator\u001b[0;34m(next_creator, **kwargs)\u001b[0m\n\u001b[1;32m   2596\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2597\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2598\u001b[0;31m         shape=shape)\n\u001b[0m\u001b[1;32m   2599\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2600\u001b[0m     return variables.RefVariable(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariableMetaclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m   1432\u001b[0m           \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m           \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1434\u001b[0;31m           distribute_strategy=distribute_strategy)\n\u001b[0m\u001b[1;32m   1435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1436\u001b[0m   def _init_from_args(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m_init_from_args\u001b[0;34m(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape)\u001b[0m\n\u001b[1;32m   1565\u001b[0m           \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initializer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1566\u001b[0m             initial_value = ops.convert_to_tensor(\n\u001b[0;32m-> 1567\u001b[0;31m                 \u001b[0minitial_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minit_from_fn\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minitial_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1568\u001b[0m                 name=\"initial_value\", dtype=dtype)\n\u001b[1;32m   1569\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer_utils.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m         (type(init_ops.Initializer), type(init_ops_v2.Initializer))):\n\u001b[1;32m    120\u001b[0m       \u001b[0minitializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0minit_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m     \u001b[0mvariable_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0muse_resource\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops_v2.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, shape, dtype)\u001b[0m\n\u001b[1;32m    556\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m       \u001b[0mlimit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_random_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_uniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops_v2.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[0;34m(self, shape, minval, maxval, dtype)\u001b[0m\n\u001b[1;32m   1066\u001b[0m       \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_uniform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m     return op(\n\u001b[0;32m-> 1068\u001b[0;31m         shape=shape, minval=minval, maxval=maxval, dtype=dtype, seed=self.seed)\n\u001b[0m\u001b[1;32m   1069\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtruncated_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstddev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/random_ops.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[0;34m(shape, minval, maxval, dtype, seed, name)\u001b[0m\n\u001b[1;32m    299\u001b[0m           \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmaxval\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mminval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m     \u001b[0;31m# TODO(b/132092188): C++ shape inference inside functional ops does not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;31m# cross FuncGraph boundaries since that information is only available in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6651\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6652\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6653\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6654\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[1211392,448] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Add]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETp8AGKk1I9X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "outputId": "93fdfe35-6de7-430e-c05f-099f321a5a50"
      },
      "source": [
        "# Define the Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3,3), activation='relu', input_shape=(224,224,3)))\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "model.add(Conv2D(64, (3,3), activation='relu'))\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "model.add(Conv2D(64, (3,3), activation='relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_12 (Conv2D)           (None, 222, 222, 32)      896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 111, 111, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 109, 109, 64)      18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 54, 54, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 52, 52, 64)        36928     \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 173056)            0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 64)                11075648  \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 11,132,033\n",
            "Trainable params: 11,132,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sL8efkUa1I9b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from tensorflow.keras.optimizers import Adam\n",
        "# lr = tf.keras.optimizers.Adam(learning_rate=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QHsSKri1I9h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compile Model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bzi1o2nH1I9k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "61f13b1c-9747-4927-898f-92325f8f48b0"
      },
      "source": [
        "# Fit Model\n",
        "history = model.fit(\n",
        "    train_data_gen,\n",
        "    steps_per_epoch=total_train // batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_data_gen,\n",
        "    validation_steps=total_val // batch_size\n",
        ")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "32/32 [==============================] - 2s 68ms/step - loss: 0.6149 - accuracy: 0.7924 - val_loss: 0.1771 - val_accuracy: 0.9141\n",
            "Epoch 2/50\n",
            "32/32 [==============================] - 2s 64ms/step - loss: 0.2354 - accuracy: 0.8902 - val_loss: 0.2329 - val_accuracy: 0.9264\n",
            "Epoch 3/50\n",
            "32/32 [==============================] - 2s 65ms/step - loss: 0.1997 - accuracy: 0.9301 - val_loss: 0.1570 - val_accuracy: 0.9325\n",
            "Epoch 4/50\n",
            "32/32 [==============================] - 2s 64ms/step - loss: 0.1346 - accuracy: 0.9401 - val_loss: 0.3245 - val_accuracy: 0.8773\n",
            "Epoch 5/50\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0882 - accuracy: 0.9621 - val_loss: 0.3005 - val_accuracy: 0.9325\n",
            "Epoch 6/50\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.1010 - accuracy: 0.9641 - val_loss: 0.2197 - val_accuracy: 0.9325\n",
            "Epoch 7/50\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.1034 - accuracy: 0.9641 - val_loss: 0.2443 - val_accuracy: 0.8957\n",
            "Epoch 8/50\n",
            "32/32 [==============================] - 2s 65ms/step - loss: 0.0480 - accuracy: 0.9800 - val_loss: 0.2545 - val_accuracy: 0.9141\n",
            "Epoch 9/50\n",
            "32/32 [==============================] - 2s 64ms/step - loss: 0.1482 - accuracy: 0.9441 - val_loss: 0.1625 - val_accuracy: 0.9448\n",
            "Epoch 10/50\n",
            "32/32 [==============================] - 2s 64ms/step - loss: 0.1770 - accuracy: 0.9501 - val_loss: 0.4849 - val_accuracy: 0.8068\n",
            "Epoch 11/50\n",
            "32/32 [==============================] - 2s 64ms/step - loss: 0.0719 - accuracy: 0.9721 - val_loss: 0.1965 - val_accuracy: 0.9261\n",
            "Epoch 12/50\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0691 - accuracy: 0.9760 - val_loss: 0.4790 - val_accuracy: 0.8650\n",
            "Epoch 13/50\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0120 - accuracy: 0.9960 - val_loss: 0.3061 - val_accuracy: 0.9018\n",
            "Epoch 14/50\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0577 - accuracy: 0.9820 - val_loss: 0.2695 - val_accuracy: 0.9325\n",
            "Epoch 15/50\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0239 - accuracy: 0.9880 - val_loss: 0.9427 - val_accuracy: 0.7914\n",
            "Epoch 16/50\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.5003 - val_accuracy: 0.9018\n",
            "Epoch 17/50\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 4.6007e-04 - accuracy: 1.0000 - val_loss: 0.2838 - val_accuracy: 0.9325\n",
            "Epoch 18/50\n",
            "32/32 [==============================] - 2s 64ms/step - loss: 6.2551e-04 - accuracy: 1.0000 - val_loss: 0.3923 - val_accuracy: 0.9264\n",
            "Epoch 19/50\n",
            "32/32 [==============================] - 2s 65ms/step - loss: 2.2688e-04 - accuracy: 1.0000 - val_loss: 0.3900 - val_accuracy: 0.9325\n",
            "Epoch 20/50\n",
            "32/32 [==============================] - 2s 64ms/step - loss: 1.8905e-04 - accuracy: 1.0000 - val_loss: 0.3585 - val_accuracy: 0.9264\n",
            "Epoch 21/50\n",
            "32/32 [==============================] - 2s 64ms/step - loss: 1.1923e-04 - accuracy: 1.0000 - val_loss: 0.5878 - val_accuracy: 0.9080\n",
            "Epoch 22/50\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 1.4021e-04 - accuracy: 1.0000 - val_loss: 0.4292 - val_accuracy: 0.9202\n",
            "Epoch 23/50\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 1.0241e-04 - accuracy: 1.0000 - val_loss: 0.4540 - val_accuracy: 0.9261\n",
            "Epoch 24/50\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 8.5195e-05 - accuracy: 1.0000 - val_loss: 0.5413 - val_accuracy: 0.9261\n",
            "Epoch 25/50\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 7.7332e-05 - accuracy: 1.0000 - val_loss: 0.4700 - val_accuracy: 0.9141\n",
            "Epoch 26/50\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 6.2801e-05 - accuracy: 1.0000 - val_loss: 0.4805 - val_accuracy: 0.9264\n",
            "Epoch 27/50\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 5.2785e-05 - accuracy: 1.0000 - val_loss: 0.4467 - val_accuracy: 0.9264\n",
            "Epoch 28/50\n",
            "32/32 [==============================] - 2s 64ms/step - loss: 4.6327e-05 - accuracy: 1.0000 - val_loss: 0.7031 - val_accuracy: 0.9080\n",
            "Epoch 29/50\n",
            "32/32 [==============================] - 2s 64ms/step - loss: 4.3107e-05 - accuracy: 1.0000 - val_loss: 0.3952 - val_accuracy: 0.9387\n",
            "Epoch 30/50\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 3.4982e-05 - accuracy: 1.0000 - val_loss: 0.4786 - val_accuracy: 0.9264\n",
            "Epoch 31/50\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 3.3889e-05 - accuracy: 1.0000 - val_loss: 0.5651 - val_accuracy: 0.9264\n",
            "Epoch 32/50\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 2.3851e-05 - accuracy: 1.0000 - val_loss: 0.5944 - val_accuracy: 0.9141\n",
            "Epoch 33/50\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 2.2638e-05 - accuracy: 1.0000 - val_loss: 0.4785 - val_accuracy: 0.9448\n",
            "Epoch 34/50\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 2.0615e-05 - accuracy: 1.0000 - val_loss: 0.9660 - val_accuracy: 0.9080\n",
            "Epoch 35/50\n",
            "32/32 [==============================] - 2s 65ms/step - loss: 1.8018e-05 - accuracy: 1.0000 - val_loss: 0.6271 - val_accuracy: 0.9202\n",
            "Epoch 36/50\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 1.5810e-05 - accuracy: 1.0000 - val_loss: 0.6193 - val_accuracy: 0.9261\n",
            "Epoch 37/50\n",
            "32/32 [==============================] - 2s 64ms/step - loss: 1.0789e-05 - accuracy: 1.0000 - val_loss: 0.7168 - val_accuracy: 0.9148\n",
            "Epoch 38/50\n",
            "32/32 [==============================] - 2s 64ms/step - loss: 1.4652e-05 - accuracy: 1.0000 - val_loss: 0.7732 - val_accuracy: 0.9080\n",
            "Epoch 39/50\n",
            "32/32 [==============================] - 2s 64ms/step - loss: 1.1208e-05 - accuracy: 1.0000 - val_loss: 0.3237 - val_accuracy: 0.9448\n",
            "Epoch 40/50\n",
            "32/32 [==============================] - 2s 64ms/step - loss: 8.9329e-06 - accuracy: 1.0000 - val_loss: 0.6917 - val_accuracy: 0.9141\n",
            "Epoch 41/50\n",
            "32/32 [==============================] - 2s 64ms/step - loss: 8.1994e-06 - accuracy: 1.0000 - val_loss: 0.5255 - val_accuracy: 0.9387\n",
            "Epoch 42/50\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 7.2880e-06 - accuracy: 1.0000 - val_loss: 1.0171 - val_accuracy: 0.8834\n",
            "Epoch 43/50\n",
            "32/32 [==============================] - 2s 65ms/step - loss: 5.2828e-06 - accuracy: 1.0000 - val_loss: 0.3693 - val_accuracy: 0.9448\n",
            "Epoch 44/50\n",
            "32/32 [==============================] - 2s 64ms/step - loss: 7.1501e-06 - accuracy: 1.0000 - val_loss: 0.7409 - val_accuracy: 0.9202\n",
            "Epoch 45/50\n",
            "32/32 [==============================] - 2s 64ms/step - loss: 6.2007e-06 - accuracy: 1.0000 - val_loss: 0.6094 - val_accuracy: 0.9325\n",
            "Epoch 46/50\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 4.4795e-06 - accuracy: 1.0000 - val_loss: 0.6018 - val_accuracy: 0.9264\n",
            "Epoch 47/50\n",
            "32/32 [==============================] - 2s 64ms/step - loss: 5.2353e-06 - accuracy: 1.0000 - val_loss: 0.6975 - val_accuracy: 0.9264\n",
            "Epoch 48/50\n",
            "32/32 [==============================] - 2s 65ms/step - loss: 4.3797e-06 - accuracy: 1.0000 - val_loss: 1.2799 - val_accuracy: 0.9202\n",
            "Epoch 49/50\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 5.9197e-06 - accuracy: 1.0000 - val_loss: 0.7570 - val_accuracy: 0.9261\n",
            "Epoch 50/50\n",
            "32/32 [==============================] - 2s 64ms/step - loss: 3.9018e-06 - accuracy: 1.0000 - val_loss: 0.8876 - val_accuracy: 0.9091\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTrp93Q01I9q",
        "colab_type": "text"
      },
      "source": [
        "# Custom CNN Model with Image Manipulations\n",
        "\n",
        "To simulate an increase in a sample of image, you can apply image manipulation techniques: cropping, rotation, stretching, etc. Luckily Keras has some handy functions for us to apply these techniques to our mountain and forest example. Simply, you should be able to modify our image generator for the problem. Check out these resources to help you get started: \n",
        "\n",
        "1. [Keras `ImageGenerator` Class](https://keras.io/preprocessing/image/#imagedatagenerator-class)\n",
        "2. [Building a powerful image classifier with very little data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5DC2Cuq1I9q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# State Code for Image Manipulation Here\n",
        "# State Code for Image Manipulation Here\n",
        "train_image_generator = ImageDataGenerator(rescale=1./255, rotation_range=90, zoom_range=[0.5,1.0]) # Generator for our training data\n",
        "validation_image_generator = ImageDataGenerator(rescale=1./255, rotation_range=90, zoom_range=[0.5,1.0]) # Generator for our validation data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfte20bV8XbK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3edb7db0-c002-4372-cac2-82243291baf4"
      },
      "source": [
        "train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,\n",
        "                                                           directory=train_dir,\n",
        "                                                           shuffle=True,\n",
        "                                                           target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                           class_mode='binary')"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 533 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LM4cfKca8jPu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bd0dc7e7-dd3f-43d6-ea35-f7fb5bd8ab75"
      },
      "source": [
        "val_data_gen = validation_image_generator.flow_from_directory(batch_size=batch_size,\n",
        "                                                              directory=validation_dir,\n",
        "                                                              target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                              class_mode='binary')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 195 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhjJgLPz8nEJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1338ac86-6626-4fcd-bb2c-46f298773c33"
      },
      "source": [
        "train_data_gen.image_shape"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(224, 224, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RAF12tF9I4O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "32d6e1cc-df5d-4c7a-abf3-1cec1053b21d"
      },
      "source": [
        "# Fit Model\n",
        "history = model.fit(\n",
        "    train_data_gen,\n",
        "    steps_per_epoch=total_train // batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_data_gen,\n",
        "    validation_steps=total_val // batch_size\n",
        ")"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "32/32 [==============================] - 8s 243ms/step - loss: 0.6575 - accuracy: 0.7445 - val_loss: 0.5400 - val_accuracy: 0.7362\n",
            "Epoch 2/50\n",
            "32/32 [==============================] - 8s 241ms/step - loss: 0.2845 - accuracy: 0.8828 - val_loss: 0.2173 - val_accuracy: 0.9205\n",
            "Epoch 3/50\n",
            "32/32 [==============================] - 8s 249ms/step - loss: 0.2467 - accuracy: 0.9022 - val_loss: 0.2213 - val_accuracy: 0.9034\n",
            "Epoch 4/50\n",
            "32/32 [==============================] - 8s 241ms/step - loss: 0.2050 - accuracy: 0.9082 - val_loss: 0.2997 - val_accuracy: 0.9080\n",
            "Epoch 5/50\n",
            "32/32 [==============================] - 8s 243ms/step - loss: 0.2614 - accuracy: 0.9042 - val_loss: 0.2542 - val_accuracy: 0.9018\n",
            "Epoch 6/50\n",
            "32/32 [==============================] - 8s 244ms/step - loss: 0.2459 - accuracy: 0.9122 - val_loss: 0.2721 - val_accuracy: 0.8712\n",
            "Epoch 7/50\n",
            "32/32 [==============================] - 8s 243ms/step - loss: 0.2120 - accuracy: 0.9341 - val_loss: 0.2529 - val_accuracy: 0.8957\n",
            "Epoch 8/50\n",
            "32/32 [==============================] - 8s 242ms/step - loss: 0.1835 - accuracy: 0.9281 - val_loss: 0.1794 - val_accuracy: 0.9141\n",
            "Epoch 9/50\n",
            "32/32 [==============================] - 8s 242ms/step - loss: 0.2170 - accuracy: 0.9122 - val_loss: 0.1672 - val_accuracy: 0.9264\n",
            "Epoch 10/50\n",
            "32/32 [==============================] - 8s 243ms/step - loss: 0.1864 - accuracy: 0.9242 - val_loss: 0.2884 - val_accuracy: 0.8528\n",
            "Epoch 11/50\n",
            "32/32 [==============================] - 8s 242ms/step - loss: 0.1865 - accuracy: 0.9222 - val_loss: 0.1610 - val_accuracy: 0.9202\n",
            "Epoch 12/50\n",
            "32/32 [==============================] - 8s 241ms/step - loss: 0.1700 - accuracy: 0.9242 - val_loss: 0.4737 - val_accuracy: 0.8221\n",
            "Epoch 13/50\n",
            "32/32 [==============================] - 8s 241ms/step - loss: 0.2227 - accuracy: 0.9062 - val_loss: 0.1957 - val_accuracy: 0.9325\n",
            "Epoch 14/50\n",
            "32/32 [==============================] - 8s 243ms/step - loss: 0.2121 - accuracy: 0.8982 - val_loss: 0.3731 - val_accuracy: 0.8589\n",
            "Epoch 15/50\n",
            "32/32 [==============================] - 8s 242ms/step - loss: 0.2261 - accuracy: 0.9122 - val_loss: 0.2186 - val_accuracy: 0.8864\n",
            "Epoch 16/50\n",
            "32/32 [==============================] - 8s 248ms/step - loss: 0.1853 - accuracy: 0.9222 - val_loss: 0.3199 - val_accuracy: 0.8750\n",
            "Epoch 17/50\n",
            "32/32 [==============================] - 8s 242ms/step - loss: 0.1671 - accuracy: 0.9341 - val_loss: 0.2778 - val_accuracy: 0.8650\n",
            "Epoch 18/50\n",
            "32/32 [==============================] - 8s 242ms/step - loss: 0.1482 - accuracy: 0.9441 - val_loss: 0.2191 - val_accuracy: 0.9018\n",
            "Epoch 19/50\n",
            "32/32 [==============================] - 8s 243ms/step - loss: 0.1804 - accuracy: 0.9238 - val_loss: 0.2398 - val_accuracy: 0.9018\n",
            "Epoch 20/50\n",
            "32/32 [==============================] - 8s 245ms/step - loss: 0.2262 - accuracy: 0.9042 - val_loss: 0.3029 - val_accuracy: 0.8466\n",
            "Epoch 21/50\n",
            "32/32 [==============================] - 8s 246ms/step - loss: 0.2047 - accuracy: 0.9281 - val_loss: 0.2748 - val_accuracy: 0.8957\n",
            "Epoch 22/50\n",
            "32/32 [==============================] - 8s 246ms/step - loss: 0.1281 - accuracy: 0.9381 - val_loss: 0.3124 - val_accuracy: 0.9141\n",
            "Epoch 23/50\n",
            "32/32 [==============================] - 8s 242ms/step - loss: 0.1777 - accuracy: 0.9321 - val_loss: 0.3366 - val_accuracy: 0.8589\n",
            "Epoch 24/50\n",
            "32/32 [==============================] - 8s 241ms/step - loss: 0.1466 - accuracy: 0.9541 - val_loss: 0.3003 - val_accuracy: 0.8712\n",
            "Epoch 25/50\n",
            "32/32 [==============================] - 8s 241ms/step - loss: 0.1321 - accuracy: 0.9461 - val_loss: 0.2132 - val_accuracy: 0.9141\n",
            "Epoch 26/50\n",
            "32/32 [==============================] - 8s 243ms/step - loss: 0.1681 - accuracy: 0.9481 - val_loss: 0.2286 - val_accuracy: 0.9141\n",
            "Epoch 27/50\n",
            "32/32 [==============================] - 8s 242ms/step - loss: 0.2138 - accuracy: 0.9082 - val_loss: 0.2316 - val_accuracy: 0.9080\n",
            "Epoch 28/50\n",
            "32/32 [==============================] - 8s 242ms/step - loss: 0.1504 - accuracy: 0.9501 - val_loss: 0.1582 - val_accuracy: 0.9375\n",
            "Epoch 29/50\n",
            "32/32 [==============================] - 8s 245ms/step - loss: 0.1405 - accuracy: 0.9381 - val_loss: 0.2527 - val_accuracy: 0.9034\n",
            "Epoch 30/50\n",
            "32/32 [==============================] - 8s 241ms/step - loss: 0.1159 - accuracy: 0.9561 - val_loss: 0.2214 - val_accuracy: 0.9202\n",
            "Epoch 31/50\n",
            "32/32 [==============================] - 8s 240ms/step - loss: 0.1326 - accuracy: 0.9541 - val_loss: 0.2860 - val_accuracy: 0.8834\n",
            "Epoch 32/50\n",
            "32/32 [==============================] - 8s 240ms/step - loss: 0.1308 - accuracy: 0.9561 - val_loss: 0.2144 - val_accuracy: 0.9448\n",
            "Epoch 33/50\n",
            "32/32 [==============================] - 8s 240ms/step - loss: 0.1591 - accuracy: 0.9441 - val_loss: 0.1553 - val_accuracy: 0.9202\n",
            "Epoch 34/50\n",
            "32/32 [==============================] - 8s 241ms/step - loss: 0.1769 - accuracy: 0.9321 - val_loss: 0.2962 - val_accuracy: 0.8773\n",
            "Epoch 35/50\n",
            "32/32 [==============================] - 8s 240ms/step - loss: 0.1528 - accuracy: 0.9481 - val_loss: 0.1175 - val_accuracy: 0.9693\n",
            "Epoch 36/50\n",
            "32/32 [==============================] - 8s 244ms/step - loss: 0.1358 - accuracy: 0.9551 - val_loss: 0.2628 - val_accuracy: 0.8773\n",
            "Epoch 37/50\n",
            "32/32 [==============================] - 8s 251ms/step - loss: 0.1639 - accuracy: 0.9222 - val_loss: 0.2018 - val_accuracy: 0.9387\n",
            "Epoch 38/50\n",
            "32/32 [==============================] - 8s 240ms/step - loss: 0.2344 - accuracy: 0.9142 - val_loss: 0.2819 - val_accuracy: 0.8896\n",
            "Epoch 39/50\n",
            "32/32 [==============================] - 8s 241ms/step - loss: 0.1309 - accuracy: 0.9501 - val_loss: 0.2172 - val_accuracy: 0.9202\n",
            "Epoch 40/50\n",
            "32/32 [==============================] - 8s 241ms/step - loss: 0.0901 - accuracy: 0.9681 - val_loss: 0.1910 - val_accuracy: 0.9202\n",
            "Epoch 41/50\n",
            "32/32 [==============================] - 8s 241ms/step - loss: 0.1608 - accuracy: 0.9441 - val_loss: 0.5147 - val_accuracy: 0.7784\n",
            "Epoch 42/50\n",
            "32/32 [==============================] - 8s 244ms/step - loss: 0.1904 - accuracy: 0.9361 - val_loss: 0.2200 - val_accuracy: 0.8977\n",
            "Epoch 43/50\n",
            "32/32 [==============================] - 8s 242ms/step - loss: 0.1111 - accuracy: 0.9561 - val_loss: 0.3142 - val_accuracy: 0.8834\n",
            "Epoch 44/50\n",
            "32/32 [==============================] - 8s 241ms/step - loss: 0.1460 - accuracy: 0.9501 - val_loss: 0.3872 - val_accuracy: 0.8589\n",
            "Epoch 45/50\n",
            "32/32 [==============================] - 8s 241ms/step - loss: 0.1335 - accuracy: 0.9401 - val_loss: 0.2832 - val_accuracy: 0.9264\n",
            "Epoch 46/50\n",
            "32/32 [==============================] - 8s 240ms/step - loss: 0.1400 - accuracy: 0.9541 - val_loss: 0.2271 - val_accuracy: 0.9018\n",
            "Epoch 47/50\n",
            "32/32 [==============================] - 8s 244ms/step - loss: 0.1198 - accuracy: 0.9601 - val_loss: 0.1566 - val_accuracy: 0.9202\n",
            "Epoch 48/50\n",
            "32/32 [==============================] - 8s 242ms/step - loss: 0.1458 - accuracy: 0.9541 - val_loss: 0.1818 - val_accuracy: 0.9571\n",
            "Epoch 49/50\n",
            "32/32 [==============================] - 8s 240ms/step - loss: 0.1518 - accuracy: 0.9481 - val_loss: 0.3315 - val_accuracy: 0.8466\n",
            "Epoch 50/50\n",
            "32/32 [==============================] - 8s 241ms/step - loss: 0.1118 - accuracy: 0.9621 - val_loss: 0.1636 - val_accuracy: 0.9264\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uT3UV3gap9H6"
      },
      "source": [
        "# Resources and Stretch Goals\n",
        "\n",
        "Stretch goals\n",
        "- Enhance your code to use classes/functions and accept terms to search and classes to look for in recognizing the downloaded images (e.g. download images of parties, recognize all that contain balloons)\n",
        "- Check out [other available pretrained networks](https://tfhub.dev), try some and compare\n",
        "- Image recognition/classification is somewhat solved, but *relationships* between entities and describing an image is not - check out some of the extended resources (e.g. [Visual Genome](https://visualgenome.org/)) on the topic\n",
        "- Transfer learning - using images you source yourself, [retrain a classifier](https://www.tensorflow.org/hub/tutorials/image_retraining) with a new category\n",
        "- (Not CNN related) Use [piexif](https://pypi.org/project/piexif/) to check out the metadata of images passed in to your system - see if they're from a national park! (Note - many images lack GPS metadata, so this won't work in most cases, but still cool)\n",
        "\n",
        "Resources\n",
        "- [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) - influential paper (introduced ResNet)\n",
        "- [YOLO: Real-Time Object Detection](https://pjreddie.com/darknet/yolo/) - an influential convolution based object detection system, focused on inference speed (for applications to e.g. self driving vehicles)\n",
        "- [R-CNN, Fast R-CNN, Faster R-CNN, YOLO](https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e) - comparison of object detection systems\n",
        "- [Common Objects in Context](http://cocodataset.org/) - a large-scale object detection, segmentation, and captioning dataset\n",
        "- [Visual Genome](https://visualgenome.org/) - a dataset, a knowledge base, an ongoing effort to connect structured image concepts to language"
      ]
    }
  ]
}