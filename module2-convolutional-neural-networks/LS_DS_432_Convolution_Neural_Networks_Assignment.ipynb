{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "U4-S2-NNF-DS10",
      "language": "python",
      "name": "u4-s2-nnf-ds10"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "nteract": {
      "version": "0.23.1"
    },
    "colab": {
      "name": "LS_DS_432_Convolution_Neural_Networks_Assignment.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Logan-Stark/DS-Unit-4-Sprint-3-Deep-Learning/blob/main/module2-convolutional-neural-networks/LS_DS_432_Convolution_Neural_Networks_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc4yMj7mtCAZ",
        "colab_type": "text"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "<br></br>\n",
        "\n",
        "## *Data Science Unit 4 Sprint 3 Assignment 2*\n",
        "# Convolutional Neural Networks (CNNs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0lfZdD_cp1t5"
      },
      "source": [
        "# Assignment\n",
        "\n",
        "- <a href=\"#p1\">Part 1:</a> Pre-Trained Model\n",
        "- <a href=\"#p2\">Part 2:</a> Custom CNN Model\n",
        "- <a href=\"#p3\">Part 3:</a> CNN with Data Augmentation\n",
        "\n",
        "\n",
        "You will apply three different CNN models to a binary image classification model using Keras. Classify images of Mountains (`./data/train/mountain/*`) and images of forests (`./data/train/forest/*`). Treat mountains as the positive class (1) and the forest images as the negative (zero). \n",
        "\n",
        "|Mountain (+)|Forest (-)|\n",
        "|---|---|\n",
        "|![](https://github.com/LambdaSchool/DS-Unit-4-Sprint-3-Deep-Learning/blob/main/module2-convolutional-neural-networks/data/train/mountain/art1131.jpg?raw=1)|![](https://github.com/LambdaSchool/DS-Unit-4-Sprint-3-Deep-Learning/blob/main/module2-convolutional-neural-networks/data/validation/forest/cdmc317.jpg?raw=1)|\n",
        "\n",
        "The problem is relatively difficult given that the sample is tiny: there are about 350 observations per class. This sample size might be something that you can expect with prototyping an image classification problem/solution at work. Get accustomed to evaluating several different possible models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1eawBP-otCAb"
      },
      "source": [
        "# Pre - Trained Model\n",
        "<a id=\"p1\"></a>\n",
        "\n",
        "Load a pretrained network from Keras, [ResNet50](https://tfhub.dev/google/imagenet/resnet_v1_50/classification/1) - a 50 layer deep network trained to recognize [1000 objects](https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt). Starting usage:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
        "\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model # This is the functional API\n",
        "\n",
        "resnet = ResNet50(weights='imagenet', include_top=False)\n",
        "\n",
        "```\n",
        "\n",
        "The `include_top` parameter in `ResNet50` will remove the full connected layers from the ResNet model. The next step is to turn off the training of the ResNet layers. We want to use the learned parameters without updating them in future training passes. \n",
        "\n",
        "```python\n",
        "for layer in resnet.layers:\n",
        "    layer.trainable = False\n",
        "```\n",
        "\n",
        "Using the Keras functional API, we will need to additional additional full connected layers to our model. We we removed the top layers, we removed all preivous fully connected layers. In other words, we kept only the feature processing portions of our network. You can expert with additional layers beyond what's listed here. The `GlobalAveragePooling2D` layer functions as a really fancy flatten function by taking the average of each of the last convolutional layer outputs (which is two dimensional still). \n",
        "\n",
        "```python\n",
        "x = resnet.output\n",
        "x = GlobalAveragePooling2D()(x) # This layer is a really fancy flatten\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(1, activation='sigmoid')(x)\n",
        "model = Model(resnet.input, predictions)\n",
        "```\n",
        "\n",
        "Your assignment is to apply the transfer learning above to classify images of Mountains (`./data/train/mountain/*`) and images of forests (`./data/train/forest/*`). Treat mountains as the positive class (1) and the forest images as the negative (zero). \n",
        "\n",
        "Steps to complete assignment: \n",
        "1. Load in Image Data into numpy arrays (`X`) \n",
        "2. Create a `y` for the labels\n",
        "3. Train your model with pre-trained layers from resnet\n",
        "4. Report your model's accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLdGdXCatCAb",
        "colab_type": "text"
      },
      "source": [
        "## Load in Data\n",
        "\n",
        "This surprisingly more difficult than it seems, because you are working with directories of images instead of a single file. This boiler plate will help you download a zipped version of the directory of images. The directory is organized into \"train\" and \"validation\" which you can use inside an `ImageGenerator` class to stream batches of images thru your model.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moRVuHUqtCAc",
        "colab_type": "text"
      },
      "source": [
        "### Download & Summarize the Data\n",
        "\n",
        "This step is completed for you. Just run the cells and review the results. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AR66H8o9tCAc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "_URL = 'https://github.com/LambdaSchool/DS-Unit-4-Sprint-3-Deep-Learning/blob/main/module2-convolutional-neural-networks/data.zip?raw=true'\n",
        "\n",
        "path_to_zip = tf.keras.utils.get_file('./data.zip', origin=_URL, extract=True)\n",
        "PATH = os.path.join(os.path.dirname(path_to_zip), 'data')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNFsIu_KtCAg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dir = os.path.join(PATH, 'train')\n",
        "validation_dir = os.path.join(PATH, 'validation')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsI9BQLotCAj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_mountain_dir = os.path.join(train_dir, 'mountain')  # directory with our training cat pictures\n",
        "train_forest_dir = os.path.join(train_dir, 'forest')  # directory with our training dog pictures\n",
        "validation_mountain_dir = os.path.join(validation_dir, 'mountain')  # directory with our validation cat pictures\n",
        "validation_forest_dir = os.path.join(validation_dir, 'forest')  # directory with our validation dog pictures"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUs1e5-XtCAl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_mountain_tr = len(os.listdir(train_mountain_dir))\n",
        "num_forest_tr = len(os.listdir(train_forest_dir))\n",
        "\n",
        "num_mountain_val = len(os.listdir(validation_mountain_dir))\n",
        "num_forest_val = len(os.listdir(validation_forest_dir))\n",
        "\n",
        "total_train = num_mountain_tr + num_forest_tr\n",
        "total_val = num_mountain_val + num_forest_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmklbgSMtCAn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "569faba2-eefc-47f1-f5b6-0ca8565931f3"
      },
      "source": [
        "print('total training mountain images:', num_mountain_tr)\n",
        "print('total training forest images:', num_forest_tr)\n",
        "\n",
        "print('total validation mountain images:', num_mountain_val)\n",
        "print('total validation forest images:', num_forest_val)\n",
        "print(\"--\")\n",
        "print(\"Total training images:\", total_train)\n",
        "print(\"Total validation images:\", total_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total training mountain images: 254\n",
            "total training forest images: 270\n",
            "total validation mountain images: 125\n",
            "total validation forest images: 62\n",
            "--\n",
            "Total training images: 524\n",
            "Total validation images: 187\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQ4ag4ultCAq",
        "colab_type": "text"
      },
      "source": [
        "### Keras `ImageGenerator` to Process the Data\n",
        "\n",
        "This step is completed for you, but please review the code. The `ImageGenerator` class reads in batches of data from a directory and pass them to the model one batch at a time. Just like large text files, this method is advantageous, because it stifles the need to load a bunch of images into memory. \n",
        "\n",
        "Check out the documentation for this class method: [Keras `ImageGenerator` Class](https://keras.io/preprocessing/image/#imagedatagenerator-class). You'll expand it's use in the third assignment objective."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67i9IW49tCAq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 16\n",
        "epochs = 50\n",
        "IMG_HEIGHT = 224\n",
        "IMG_WIDTH = 224"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1wNKMo1tCAt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our training data\n",
        "validation_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our validation data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndsuM4L9tCAv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "58b1e184-f40a-4c4e-fdae-16bf439e22d5"
      },
      "source": [
        "train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,\n",
        "                                                           directory=train_dir,\n",
        "                                                           shuffle=True,\n",
        "                                                           target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                           class_mode='binary')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 533 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kxlk3optCAy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "64259250-3446-4082-e5bc-a408e2042ea7"
      },
      "source": [
        "val_data_gen = validation_image_generator.flow_from_directory(batch_size=batch_size,\n",
        "                                                              directory=validation_dir,\n",
        "                                                              target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                              class_mode='binary')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 195 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2l7ue6NutCA0",
        "colab_type": "text"
      },
      "source": [
        "## Instatiate Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFjtPNEnJ8mt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import datasets\n",
        "from tensorflow.keras.models import Sequential, Model # <- May Use\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKQUCHuh8tRo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        " \n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
        " \n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model # This is the functional API\n",
        " \n",
        "resnet = ResNet50(weights='imagenet', include_top=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifv7J8Yy9BsG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for layer in resnet.layers:\n",
        "    layer.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78PE8esD9PeC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = resnet.output\n",
        "x = GlobalAveragePooling2D()(x) # This layer is a really fancy flatten\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(1, activation='sigmoid')(x)\n",
        "model = Model(resnet.input, predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNuAuOXIMQMS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVPBWYG7tCA2",
        "colab_type": "text"
      },
      "source": [
        "## Fit Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4XdvWA5tCA3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "45c4f743-689c-4e67-bfd8-56c52840bdb4"
      },
      "source": [
        "history = model.fit(\n",
        "    train_data_gen,\n",
        "    steps_per_epoch=total_train // batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_data_gen,\n",
        "    validation_steps=total_val // batch_size\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "32/32 [==============================] - 4s 111ms/step - loss: 0.7466 - accuracy: 0.5629 - val_loss: 0.6867 - val_accuracy: 0.4943\n",
            "Epoch 2/50\n",
            "32/32 [==============================] - 3s 78ms/step - loss: 0.6580 - accuracy: 0.6128 - val_loss: 0.8431 - val_accuracy: 0.3750\n",
            "Epoch 3/50\n",
            "32/32 [==============================] - 3s 78ms/step - loss: 0.5338 - accuracy: 0.7605 - val_loss: 0.7685 - val_accuracy: 0.4318\n",
            "Epoch 4/50\n",
            "32/32 [==============================] - 3s 78ms/step - loss: 0.4651 - accuracy: 0.7944 - val_loss: 0.4660 - val_accuracy: 0.8295\n",
            "Epoch 5/50\n",
            "32/32 [==============================] - 3s 78ms/step - loss: 0.4429 - accuracy: 0.8004 - val_loss: 0.5131 - val_accuracy: 0.7841\n",
            "Epoch 6/50\n",
            "32/32 [==============================] - 3s 79ms/step - loss: 0.4023 - accuracy: 0.8438 - val_loss: 0.3672 - val_accuracy: 0.8068\n",
            "Epoch 7/50\n",
            "32/32 [==============================] - 3s 79ms/step - loss: 0.3624 - accuracy: 0.8703 - val_loss: 0.4948 - val_accuracy: 0.7841\n",
            "Epoch 8/50\n",
            "32/32 [==============================] - 3s 79ms/step - loss: 0.3195 - accuracy: 0.8882 - val_loss: 0.3433 - val_accuracy: 0.8523\n",
            "Epoch 9/50\n",
            "32/32 [==============================] - 3s 79ms/step - loss: 0.3561 - accuracy: 0.8483 - val_loss: 0.3588 - val_accuracy: 0.8466\n",
            "Epoch 10/50\n",
            "32/32 [==============================] - 3s 79ms/step - loss: 0.3139 - accuracy: 0.9002 - val_loss: 0.3012 - val_accuracy: 0.9034\n",
            "Epoch 11/50\n",
            "32/32 [==============================] - 3s 80ms/step - loss: 0.3004 - accuracy: 0.8906 - val_loss: 0.5579 - val_accuracy: 0.7557\n",
            "Epoch 12/50\n",
            "32/32 [==============================] - 3s 79ms/step - loss: 0.4424 - accuracy: 0.7904 - val_loss: 0.2925 - val_accuracy: 0.8807\n",
            "Epoch 13/50\n",
            "32/32 [==============================] - 3s 79ms/step - loss: 0.2577 - accuracy: 0.9180 - val_loss: 0.3416 - val_accuracy: 0.8466\n",
            "Epoch 14/50\n",
            "32/32 [==============================] - 3s 79ms/step - loss: 0.2570 - accuracy: 0.9162 - val_loss: 0.2816 - val_accuracy: 0.9034\n",
            "Epoch 15/50\n",
            "32/32 [==============================] - 3s 79ms/step - loss: 0.2238 - accuracy: 0.9281 - val_loss: 0.3493 - val_accuracy: 0.8352\n",
            "Epoch 16/50\n",
            "32/32 [==============================] - 3s 79ms/step - loss: 0.2122 - accuracy: 0.9321 - val_loss: 0.3573 - val_accuracy: 0.8466\n",
            "Epoch 17/50\n",
            "32/32 [==============================] - 3s 80ms/step - loss: 0.2355 - accuracy: 0.9042 - val_loss: 0.2481 - val_accuracy: 0.8977\n",
            "Epoch 18/50\n",
            "32/32 [==============================] - 3s 80ms/step - loss: 0.2228 - accuracy: 0.9242 - val_loss: 0.2637 - val_accuracy: 0.9034\n",
            "Epoch 19/50\n",
            "32/32 [==============================] - 3s 81ms/step - loss: 0.2154 - accuracy: 0.9301 - val_loss: 0.2976 - val_accuracy: 0.8636\n",
            "Epoch 20/50\n",
            "32/32 [==============================] - 3s 81ms/step - loss: 0.2235 - accuracy: 0.9042 - val_loss: 0.2368 - val_accuracy: 0.9091\n",
            "Epoch 21/50\n",
            "32/32 [==============================] - 3s 79ms/step - loss: 0.1888 - accuracy: 0.9441 - val_loss: 0.3168 - val_accuracy: 0.8750\n",
            "Epoch 22/50\n",
            "32/32 [==============================] - 3s 79ms/step - loss: 0.1818 - accuracy: 0.9281 - val_loss: 0.2652 - val_accuracy: 0.8977\n",
            "Epoch 23/50\n",
            "32/32 [==============================] - 3s 79ms/step - loss: 0.1610 - accuracy: 0.9481 - val_loss: 0.3281 - val_accuracy: 0.8636\n",
            "Epoch 24/50\n",
            "32/32 [==============================] - 3s 78ms/step - loss: 0.1382 - accuracy: 0.9621 - val_loss: 0.2867 - val_accuracy: 0.8807\n",
            "Epoch 25/50\n",
            "32/32 [==============================] - 3s 78ms/step - loss: 0.1479 - accuracy: 0.9501 - val_loss: 0.2354 - val_accuracy: 0.9148\n",
            "Epoch 26/50\n",
            "32/32 [==============================] - 2s 78ms/step - loss: 0.1543 - accuracy: 0.9421 - val_loss: 0.3117 - val_accuracy: 0.8580\n",
            "Epoch 27/50\n",
            "32/32 [==============================] - 3s 78ms/step - loss: 0.1910 - accuracy: 0.9142 - val_loss: 0.2465 - val_accuracy: 0.9091\n",
            "Epoch 28/50\n",
            "32/32 [==============================] - 3s 79ms/step - loss: 0.1449 - accuracy: 0.9461 - val_loss: 0.2747 - val_accuracy: 0.9091\n",
            "Epoch 29/50\n",
            "32/32 [==============================] - 3s 79ms/step - loss: 0.1362 - accuracy: 0.9492 - val_loss: 0.5399 - val_accuracy: 0.7898\n",
            "Epoch 30/50\n",
            "32/32 [==============================] - 3s 80ms/step - loss: 0.1568 - accuracy: 0.9361 - val_loss: 0.1935 - val_accuracy: 0.9261\n",
            "Epoch 31/50\n",
            "32/32 [==============================] - 3s 79ms/step - loss: 0.1747 - accuracy: 0.9361 - val_loss: 0.2248 - val_accuracy: 0.9205\n",
            "Epoch 32/50\n",
            "32/32 [==============================] - 3s 79ms/step - loss: 0.1207 - accuracy: 0.9621 - val_loss: 0.2010 - val_accuracy: 0.9375\n",
            "Epoch 33/50\n",
            "32/32 [==============================] - 3s 80ms/step - loss: 0.1391 - accuracy: 0.9501 - val_loss: 0.2099 - val_accuracy: 0.9318\n",
            "Epoch 34/50\n",
            "32/32 [==============================] - 3s 79ms/step - loss: 0.1606 - accuracy: 0.9341 - val_loss: 0.3341 - val_accuracy: 0.8636\n",
            "Epoch 35/50\n",
            "32/32 [==============================] - 3s 80ms/step - loss: 0.1008 - accuracy: 0.9641 - val_loss: 0.2236 - val_accuracy: 0.9261\n",
            "Epoch 36/50\n",
            "32/32 [==============================] - 3s 79ms/step - loss: 0.0898 - accuracy: 0.9741 - val_loss: 0.1943 - val_accuracy: 0.9432\n",
            "Epoch 37/50\n",
            "32/32 [==============================] - 3s 80ms/step - loss: 0.0877 - accuracy: 0.9701 - val_loss: 0.2001 - val_accuracy: 0.9432\n",
            "Epoch 38/50\n",
            "32/32 [==============================] - 3s 79ms/step - loss: 0.0985 - accuracy: 0.9641 - val_loss: 0.2317 - val_accuracy: 0.9375\n",
            "Epoch 39/50\n",
            "32/32 [==============================] - 3s 79ms/step - loss: 0.0940 - accuracy: 0.9641 - val_loss: 0.1766 - val_accuracy: 0.9375\n",
            "Epoch 40/50\n",
            "32/32 [==============================] - 3s 79ms/step - loss: 0.0972 - accuracy: 0.9621 - val_loss: 0.3302 - val_accuracy: 0.8750\n",
            "Epoch 41/50\n",
            "32/32 [==============================] - 3s 79ms/step - loss: 0.1106 - accuracy: 0.9601 - val_loss: 0.1776 - val_accuracy: 0.9432\n",
            "Epoch 42/50\n",
            "32/32 [==============================] - 3s 80ms/step - loss: 0.0848 - accuracy: 0.9681 - val_loss: 0.1929 - val_accuracy: 0.9375\n",
            "Epoch 43/50\n",
            "32/32 [==============================] - 3s 80ms/step - loss: 0.1227 - accuracy: 0.9541 - val_loss: 0.1829 - val_accuracy: 0.9432\n",
            "Epoch 44/50\n",
            "32/32 [==============================] - 3s 79ms/step - loss: 0.0957 - accuracy: 0.9641 - val_loss: 0.1558 - val_accuracy: 0.9545\n",
            "Epoch 45/50\n",
            "32/32 [==============================] - 2s 78ms/step - loss: 0.0817 - accuracy: 0.9760 - val_loss: 0.1821 - val_accuracy: 0.9602\n",
            "Epoch 46/50\n",
            "32/32 [==============================] - 3s 78ms/step - loss: 0.1233 - accuracy: 0.9561 - val_loss: 0.1689 - val_accuracy: 0.9432\n",
            "Epoch 47/50\n",
            "32/32 [==============================] - 3s 80ms/step - loss: 0.0773 - accuracy: 0.9668 - val_loss: 0.2284 - val_accuracy: 0.9318\n",
            "Epoch 48/50\n",
            "32/32 [==============================] - 3s 79ms/step - loss: 0.2129 - accuracy: 0.9162 - val_loss: 0.4969 - val_accuracy: 0.8182\n",
            "Epoch 49/50\n",
            "32/32 [==============================] - 3s 79ms/step - loss: 0.0796 - accuracy: 0.9780 - val_loss: 0.1691 - val_accuracy: 0.9375\n",
            "Epoch 50/50\n",
            "32/32 [==============================] - 3s 78ms/step - loss: 0.1241 - accuracy: 0.9521 - val_loss: 0.1499 - val_accuracy: 0.9432\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vu363OOWT_Ja",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a4190e2e-9fa4-4eec-b457-a049db48ecd9"
      },
      "source": [
        "train_data_gen.image_shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(224, 224, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ln87-SPURH8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "516234e7-fbf0-43f8-9f46-ab3b91cebc03"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Graph accuracy over time\n",
        "eps = range(len(history.history['accuracy']))\n",
        "plt.plot(eps, history.history['accuracy'], label='Train')\n",
        "plt.plot(eps, history.history['val_accuracy'], label='Test')\n",
        "\n",
        "plt.title('Accuracy over time')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Classification Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPzsgS94tCA5",
        "colab_type": "text"
      },
      "source": [
        "# Custom CNN Model\n",
        "\n",
        "In this step, write and train your own convolutional neural network using Keras. You can use any architecture that suits you as long as it has at least one convolutional and one pooling layer at the beginning of the network - you can add more if you want. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0npkzIzXVM90",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "b43c66da-a069-486e-81d5-5392114cca67"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "# Define the Model\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3,3), activation='relu', \n",
        "                 input_shape=(IMG_HEIGHT,IMG_WIDTH,3)))\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "model.add(Conv2D(64, (3,3), activation='relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_3 (Conv2D)            (None, 222, 222, 32)      896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 111, 111, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 109, 109, 64)      18496     \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 760384)            0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 64)                48664640  \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 48,684,097\n",
            "Trainable params: 48,684,097\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnbJJie3tCA5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "outputId": "9a7f1e8a-bb09-4544-9687-ee2a0a715e5a"
      },
      "source": [
        "# Define the Model\n",
        "# model = Sequential()\n",
        "# model.add(Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)))\n",
        "# model.add(MaxPooling2D((2,2)))\n",
        "# model.add(Conv2D(64, (3,3), activation='relu'))\n",
        "# model.add(MaxPooling2D((2,2)))\n",
        "# model.add(Conv2D(64, (3,3), activation='relu'))\n",
        "# model.add(Flatten())\n",
        "# model.add(Dense(64, activation='relu'))\n",
        "# model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 30, 30, 32)        896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 13, 13, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 4, 4, 64)          36928     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 64)                65600     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 122,570\n",
            "Trainable params: 122,570\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1P_mRtoutCA9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compile Model\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwM4GsaetCA_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "efc509e0-1c3e-49f3-ab0f-de347f40dc7c"
      },
      "source": [
        "# Fit Model\n",
        "history = model.fit(\n",
        "    train_data_gen,\n",
        "    steps_per_epoch=total_train // batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_data_gen,\n",
        "    validation_steps=total_val // batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "32/32 [==============================] - 2s 69ms/step - loss: nan - accuracy: 0.5190 - val_loss: nan - val_accuracy: 0.3295\n",
            "Epoch 2/50\n",
            "32/32 [==============================] - 2s 66ms/step - loss: nan - accuracy: 0.5130 - val_loss: nan - val_accuracy: 0.3352\n",
            "Epoch 3/50\n",
            "32/32 [==============================] - 2s 66ms/step - loss: nan - accuracy: 0.5230 - val_loss: nan - val_accuracy: 0.3352\n",
            "Epoch 4/50\n",
            "32/32 [==============================] - 2s 67ms/step - loss: nan - accuracy: 0.5309 - val_loss: nan - val_accuracy: 0.3523\n",
            "Epoch 5/50\n",
            "32/32 [==============================] - 2s 67ms/step - loss: nan - accuracy: 0.5210 - val_loss: nan - val_accuracy: 0.3409\n",
            "Epoch 6/50\n",
            "32/32 [==============================] - 2s 68ms/step - loss: nan - accuracy: 0.5210 - val_loss: nan - val_accuracy: 0.3523\n",
            "Epoch 7/50\n",
            "32/32 [==============================] - 2s 67ms/step - loss: nan - accuracy: 0.5190 - val_loss: nan - val_accuracy: 0.3523\n",
            "Epoch 8/50\n",
            "32/32 [==============================] - 2s 67ms/step - loss: nan - accuracy: 0.4990 - val_loss: nan - val_accuracy: 0.3466\n",
            "Epoch 9/50\n",
            "32/32 [==============================] - 2s 66ms/step - loss: nan - accuracy: 0.5210 - val_loss: nan - val_accuracy: 0.3409\n",
            "Epoch 10/50\n",
            "32/32 [==============================] - 2s 68ms/step - loss: nan - accuracy: 0.5170 - val_loss: nan - val_accuracy: 0.3523\n",
            "Epoch 11/50\n",
            "32/32 [==============================] - 2s 67ms/step - loss: nan - accuracy: 0.5190 - val_loss: nan - val_accuracy: 0.3750\n",
            "Epoch 12/50\n",
            "32/32 [==============================] - 2s 68ms/step - loss: nan - accuracy: 0.5170 - val_loss: nan - val_accuracy: 0.3409\n",
            "Epoch 13/50\n",
            "32/32 [==============================] - 2s 67ms/step - loss: nan - accuracy: 0.5150 - val_loss: nan - val_accuracy: 0.3580\n",
            "Epoch 14/50\n",
            "32/32 [==============================] - 2s 68ms/step - loss: nan - accuracy: 0.5230 - val_loss: nan - val_accuracy: 0.3466\n",
            "Epoch 15/50\n",
            "32/32 [==============================] - 2s 69ms/step - loss: nan - accuracy: 0.5269 - val_loss: nan - val_accuracy: 0.3409\n",
            "Epoch 16/50\n",
            "32/32 [==============================] - 2s 68ms/step - loss: nan - accuracy: 0.5070 - val_loss: nan - val_accuracy: 0.3409\n",
            "Epoch 17/50\n",
            "32/32 [==============================] - 2s 68ms/step - loss: nan - accuracy: 0.5190 - val_loss: nan - val_accuracy: 0.3466\n",
            "Epoch 18/50\n",
            "32/32 [==============================] - 2s 68ms/step - loss: nan - accuracy: 0.5269 - val_loss: nan - val_accuracy: 0.3523\n",
            "Epoch 19/50\n",
            "32/32 [==============================] - 2s 68ms/step - loss: nan - accuracy: 0.5130 - val_loss: nan - val_accuracy: 0.3466\n",
            "Epoch 20/50\n",
            "32/32 [==============================] - 2s 66ms/step - loss: nan - accuracy: 0.5170 - val_loss: nan - val_accuracy: 0.3466\n",
            "Epoch 21/50\n",
            "32/32 [==============================] - 2s 67ms/step - loss: nan - accuracy: 0.5250 - val_loss: nan - val_accuracy: 0.3409\n",
            "Epoch 22/50\n",
            "32/32 [==============================] - 2s 65ms/step - loss: nan - accuracy: 0.5130 - val_loss: nan - val_accuracy: 0.3523\n",
            "Epoch 23/50\n",
            "32/32 [==============================] - 2s 66ms/step - loss: nan - accuracy: 0.5090 - val_loss: nan - val_accuracy: 0.3352\n",
            "Epoch 24/50\n",
            "32/32 [==============================] - 2s 66ms/step - loss: nan - accuracy: 0.5215 - val_loss: nan - val_accuracy: 0.3352\n",
            "Epoch 25/50\n",
            "32/32 [==============================] - 2s 66ms/step - loss: nan - accuracy: 0.5150 - val_loss: nan - val_accuracy: 0.3409\n",
            "Epoch 26/50\n",
            "32/32 [==============================] - 2s 66ms/step - loss: nan - accuracy: 0.5150 - val_loss: nan - val_accuracy: 0.3409\n",
            "Epoch 27/50\n",
            "32/32 [==============================] - 2s 66ms/step - loss: nan - accuracy: 0.5150 - val_loss: nan - val_accuracy: 0.3295\n",
            "Epoch 28/50\n",
            "32/32 [==============================] - 2s 66ms/step - loss: nan - accuracy: 0.5250 - val_loss: nan - val_accuracy: 0.3239\n",
            "Epoch 29/50\n",
            "32/32 [==============================] - 2s 66ms/step - loss: nan - accuracy: 0.5210 - val_loss: nan - val_accuracy: 0.3523\n",
            "Epoch 30/50\n",
            "32/32 [==============================] - 2s 65ms/step - loss: nan - accuracy: 0.5130 - val_loss: nan - val_accuracy: 0.3523\n",
            "Epoch 31/50\n",
            "32/32 [==============================] - 2s 65ms/step - loss: nan - accuracy: 0.5090 - val_loss: nan - val_accuracy: 0.3352\n",
            "Epoch 32/50\n",
            "32/32 [==============================] - 2s 67ms/step - loss: nan - accuracy: 0.5215 - val_loss: nan - val_accuracy: 0.3352\n",
            "Epoch 33/50\n",
            "32/32 [==============================] - 2s 66ms/step - loss: nan - accuracy: 0.5190 - val_loss: nan - val_accuracy: 0.3466\n",
            "Epoch 34/50\n",
            "32/32 [==============================] - 2s 66ms/step - loss: nan - accuracy: 0.5090 - val_loss: nan - val_accuracy: 0.3580\n",
            "Epoch 35/50\n",
            "32/32 [==============================] - 2s 67ms/step - loss: nan - accuracy: 0.5150 - val_loss: nan - val_accuracy: 0.3295\n",
            "Epoch 36/50\n",
            "32/32 [==============================] - 2s 65ms/step - loss: nan - accuracy: 0.5190 - val_loss: nan - val_accuracy: 0.3295\n",
            "Epoch 37/50\n",
            "32/32 [==============================] - 2s 66ms/step - loss: nan - accuracy: 0.5030 - val_loss: nan - val_accuracy: 0.3295\n",
            "Epoch 38/50\n",
            "32/32 [==============================] - 2s 65ms/step - loss: nan - accuracy: 0.5190 - val_loss: nan - val_accuracy: 0.3466\n",
            "Epoch 39/50\n",
            "32/32 [==============================] - 2s 67ms/step - loss: nan - accuracy: 0.5090 - val_loss: nan - val_accuracy: 0.3466\n",
            "Epoch 40/50\n",
            "32/32 [==============================] - 2s 66ms/step - loss: nan - accuracy: 0.5230 - val_loss: nan - val_accuracy: 0.3409\n",
            "Epoch 41/50\n",
            "32/32 [==============================] - 2s 66ms/step - loss: nan - accuracy: 0.5250 - val_loss: nan - val_accuracy: 0.3239\n",
            "Epoch 42/50\n",
            "32/32 [==============================] - 2s 67ms/step - loss: nan - accuracy: 0.5150 - val_loss: nan - val_accuracy: 0.3523\n",
            "Epoch 43/50\n",
            "32/32 [==============================] - 2s 66ms/step - loss: nan - accuracy: 0.5250 - val_loss: nan - val_accuracy: 0.3523\n",
            "Epoch 44/50\n",
            "32/32 [==============================] - 2s 66ms/step - loss: nan - accuracy: 0.5190 - val_loss: nan - val_accuracy: 0.3466\n",
            "Epoch 45/50\n",
            "32/32 [==============================] - 2s 65ms/step - loss: nan - accuracy: 0.5130 - val_loss: nan - val_accuracy: 0.3523\n",
            "Epoch 46/50\n",
            "32/32 [==============================] - 2s 66ms/step - loss: nan - accuracy: 0.5210 - val_loss: nan - val_accuracy: 0.3580\n",
            "Epoch 47/50\n",
            "32/32 [==============================] - 2s 66ms/step - loss: nan - accuracy: 0.5250 - val_loss: nan - val_accuracy: 0.3580\n",
            "Epoch 48/50\n",
            "32/32 [==============================] - 2s 66ms/step - loss: nan - accuracy: 0.5070 - val_loss: nan - val_accuracy: 0.3580\n",
            "Epoch 49/50\n",
            "32/32 [==============================] - 2s 67ms/step - loss: nan - accuracy: 0.5050 - val_loss: nan - val_accuracy: 0.3239\n",
            "Epoch 50/50\n",
            "32/32 [==============================] - 2s 66ms/step - loss: nan - accuracy: 0.5150 - val_loss: nan - val_accuracy: 0.3580\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNTHjUddtCBB",
        "colab_type": "text"
      },
      "source": [
        "# Custom CNN Model with Image Manipulations\n",
        "\n",
        "To simulate an increase in a sample of image, you can apply image manipulation techniques: cropping, rotation, stretching, etc. Luckily Keras has some handy functions for us to apply these techniques to our mountain and forest example. Simply, you should be able to modify our image generator for the problem. Check out these resources to help you get started: \n",
        "\n",
        "1. [Keras `ImageGenerator` Class](https://keras.io/preprocessing/image/#imagedatagenerator-class)\n",
        "2. [Building a powerful image classifier with very little data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKioBv3WtCBB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uT3UV3gap9H6"
      },
      "source": [
        "# Resources and Stretch Goals\n",
        "\n",
        "Stretch goals\n",
        "- Enhance your code to use classes/functions and accept terms to search and classes to look for in recognizing the downloaded images (e.g. download images of parties, recognize all that contain balloons)\n",
        "- Check out [other available pretrained networks](https://tfhub.dev), try some and compare\n",
        "- Image recognition/classification is somewhat solved, but *relationships* between entities and describing an image is not - check out some of the extended resources (e.g. [Visual Genome](https://visualgenome.org/)) on the topic\n",
        "- Transfer learning - using images you source yourself, [retrain a classifier](https://www.tensorflow.org/hub/tutorials/image_retraining) with a new category\n",
        "- (Not CNN related) Use [piexif](https://pypi.org/project/piexif/) to check out the metadata of images passed in to your system - see if they're from a national park! (Note - many images lack GPS metadata, so this won't work in most cases, but still cool)\n",
        "\n",
        "Resources\n",
        "- [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) - influential paper (introduced ResNet)\n",
        "- [YOLO: Real-Time Object Detection](https://pjreddie.com/darknet/yolo/) - an influential convolution based object detection system, focused on inference speed (for applications to e.g. self driving vehicles)\n",
        "- [R-CNN, Fast R-CNN, Faster R-CNN, YOLO](https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e) - comparison of object detection systems\n",
        "- [Common Objects in Context](http://cocodataset.org/) - a large-scale object detection, segmentation, and captioning dataset\n",
        "- [Visual Genome](https://visualgenome.org/) - a dataset, a knowledge base, an ongoing effort to connect structured image concepts to language"
      ]
    }
  ]
}