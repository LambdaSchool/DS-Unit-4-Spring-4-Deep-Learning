{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "U4-S3-DNN (Python 3.7)",
      "language": "python",
      "name": "u4-s3-dnn"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "NDoshi_LS_DS_431_RNN_and_LSTM_Assignment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ndoshi83/DS-Unit-4-Sprint-3-Deep-Learning/blob/master/NDoshi_LS_DS_431_RNN_and_LSTM_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Had6i4n9A6lL",
        "colab_type": "text"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "<br></br>\n",
        "\n",
        "## *Data Science Unit 4 Sprint 3 Assignment 1*\n",
        "\n",
        "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
        "\n",
        "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
        "\n",
        "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
        "\n",
        "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
        "\n",
        "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
        "\n",
        "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
        "\n",
        "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ltj1je1fp5rO",
        "colab": {}
      },
      "source": [
        "# TODO - Words, words, mere words, no matter from the heart.\n",
        "# Import data\n",
        "fileurl = '100-0.txt'\n",
        "with open(fileurl, 'r') as file:\n",
        "  data = file.read().replace('\\n', '')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpoceRPiXhGi",
        "colab_type": "code",
        "outputId": "3abf1252-00bc-4abe-ff6a-3ef5631e7a03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Format data in to unique characters\n",
        "# Based on \"The Unreasonable Effectiveness of RNN\" implementation\n",
        "import numpy as np\n",
        "\n",
        "chars = list(set(data)) # split and remove duplicate characters. convert to list.\n",
        "\n",
        "num_chars = len(chars) # the number of unique characters\n",
        "txt_data_size = len(data)\n",
        "\n",
        "print(\"unique characters : \", num_chars)\n",
        "print(\"txt_data_size : \", txt_data_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unique characters :  106\n",
            "txt_data_size :  5422155\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxwNjwKOYQwN",
        "colab_type": "code",
        "outputId": "b720280f-8d55-4791-bba4-5148b7d0813d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "# one hot encode\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars)) # \"enumerate\" retruns index and value. Convert it to dictionary\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "print(char_to_int)\n",
        "print(\"----------------------------------------------------\")\n",
        "print(int_to_char)\n",
        "print(\"----------------------------------------------------\")\n",
        "# integer encode input data\n",
        "integer_encoded = [char_to_int[i] for i in data] # \"integer_encoded\" is a list which has a sequence converted from an original data to integers.\n",
        "print(integer_encoded)\n",
        "print(\"----------------------------------------------------\")\n",
        "print(\"data length : \", len(integer_encoded))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'1': 0, ' ': 1, '“': 2, 'J': 3, 'S': 4, '6': 5, 'w': 6, 'P': 7, 'i': 8, '\\\\': 9, 'é': 10, 'E': 11, 'b': 12, ':': 13, 'L': 14, '0': 15, 'æ': 16, 'U': 17, ']': 18, \"'\": 19, '?': 20, 'p': 21, 'Q': 22, 'X': 23, 'n': 24, 'u': 25, 'y': 26, 'x': 27, ')': 28, 'W': 29, 'O': 30, 'B': 31, '|': 32, '`': 33, 'j': 34, 'C': 35, '2': 36, 'R': 37, '_': 38, 'T': 39, '\\ufeff': 40, 'A': 41, '[': 42, 'è': 43, 'ç': 44, 'm': 45, 'a': 46, '7': 47, 'N': 48, 'g': 49, '5': 50, 'G': 51, 'â': 52, '}': 53, 'f': 54, 'e': 55, '\"': 56, '’': 57, 'D': 58, '!': 59, 'q': 60, '%': 61, 'I': 62, 'Æ': 63, 'o': 64, 'ê': 65, '9': 66, 'r': 67, 'V': 68, 'v': 69, 'Y': 70, '3': 71, 't': 72, 'F': 73, 'k': 74, 'É': 75, ',': 76, 'c': 77, 'l': 78, '”': 79, 's': 80, '@': 81, '4': 82, '-': 83, 'M': 84, 'î': 85, '(': 86, '&': 87, 'œ': 88, '.': 89, 'H': 90, '8': 91, 'à': 92, ';': 93, 'd': 94, '*': 95, '\\t': 96, '#': 97, 'K': 98, 'Z': 99, '‘': 100, '$': 101, '/': 102, 'h': 103, '—': 104, 'z': 105}\n",
            "----------------------------------------------------\n",
            "{0: '1', 1: ' ', 2: '“', 3: 'J', 4: 'S', 5: '6', 6: 'w', 7: 'P', 8: 'i', 9: '\\\\', 10: 'é', 11: 'E', 12: 'b', 13: ':', 14: 'L', 15: '0', 16: 'æ', 17: 'U', 18: ']', 19: \"'\", 20: '?', 21: 'p', 22: 'Q', 23: 'X', 24: 'n', 25: 'u', 26: 'y', 27: 'x', 28: ')', 29: 'W', 30: 'O', 31: 'B', 32: '|', 33: '`', 34: 'j', 35: 'C', 36: '2', 37: 'R', 38: '_', 39: 'T', 40: '\\ufeff', 41: 'A', 42: '[', 43: 'è', 44: 'ç', 45: 'm', 46: 'a', 47: '7', 48: 'N', 49: 'g', 50: '5', 51: 'G', 52: 'â', 53: '}', 54: 'f', 55: 'e', 56: '\"', 57: '’', 58: 'D', 59: '!', 60: 'q', 61: '%', 62: 'I', 63: 'Æ', 64: 'o', 65: 'ê', 66: '9', 67: 'r', 68: 'V', 69: 'v', 70: 'Y', 71: '3', 72: 't', 73: 'F', 74: 'k', 75: 'É', 76: ',', 77: 'c', 78: 'l', 79: '”', 80: 's', 81: '@', 82: '4', 83: '-', 84: 'M', 85: 'î', 86: '(', 87: '&', 88: 'œ', 89: '.', 90: 'H', 91: '8', 92: 'à', 93: ';', 94: 'd', 95: '*', 96: '\\t', 97: '#', 98: 'K', 99: 'Z', 100: '‘', 101: '$', 102: '/', 103: 'h', 104: '—', 105: 'z'}\n",
            "----------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVzr2BWwYjTv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# hyperparameters\n",
        "\n",
        "iteration = 1000\n",
        "sequence_length = 40\n",
        "batch_size = round((txt_data_size /sequence_length)+0.5) # = math.ceil\n",
        "hidden_size = 500  # size of hidden layer of neurons.  \n",
        "learning_rate = 1e-1\n",
        "\n",
        "\n",
        "# model parameters\n",
        "\n",
        "W_xh = np.random.randn(hidden_size, num_chars)*0.01     # weight input -> hidden. \n",
        "W_hh = np.random.randn(hidden_size, hidden_size)*0.01   # weight hidden -> hidden\n",
        "W_hy = np.random.randn(num_chars, hidden_size)*0.01     # weight hidden -> output\n",
        "\n",
        "b_h = np.zeros((hidden_size, 1)) # hidden bias\n",
        "b_y = np.zeros((num_chars, 1)) # output bias\n",
        "\n",
        "h_prev = np.zeros((hidden_size,1)) # h_(t-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJAfkEsTY4HU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Forward propagation\n",
        "def forwardprop(inputs, targets, h_prev):\n",
        "        \n",
        "    # Since the RNN receives the sequence, the weights are not updated during one sequence.\n",
        "    xs, hs, ys, ps = {}, {}, {}, {} # dictionary\n",
        "    hs[-1] = np.copy(h_prev) # Copy previous hidden state vector to -1 key value.\n",
        "    loss = 0 # loss initialization\n",
        "    \n",
        "    for t in range(len(inputs)): # t is a \"time step\" and is used as a key(dic).  \n",
        "        \n",
        "        xs[t] = np.zeros((num_chars,1)) \n",
        "        xs[t][inputs[t]] = 1\n",
        "        hs[t] = np.tanh(np.dot(W_xh, xs[t]) + np.dot(W_hh, hs[t-1]) + b_h) # hidden state. \n",
        "        ys[t] = np.dot(W_hy, hs[t]) + b_y # unnormalized log probabilities for next chars\n",
        "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars. \n",
        "        \n",
        "        # Softmax. -> The sum of probabilities is 1 even without the exp() function, but all of the elements are positive through the exp() function.\n",
        "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss). Efficient and simple code\n",
        "\n",
        "#         y_class = np.zeros((num_chars, 1)) \n",
        "#         y_class[targets[t]] =1\n",
        "#         loss += np.sum(y_class*(-np.log(ps[t]))) # softmax (cross-entropy loss)        \n",
        "\n",
        "    return loss, ps, hs, xs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWf25YIkZBF6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Backward propagation\n",
        "def backprop(ps, inputs, hs, xs, targets):\n",
        "\n",
        "    dWxh, dWhh, dWhy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy) # make all zero matrices.\n",
        "    dbh, dby = np.zeros_like(b_h), np.zeros_like(b_y)\n",
        "    dhnext = np.zeros_like(hs[0]) # (hidden_size,1) \n",
        "\n",
        "    # reversed\n",
        "    for t in reversed(range(len(inputs))):\n",
        "        dy = np.copy(ps[t]) # shape (num_chars,1).  \"dy\" means \"dloss/dy\"\n",
        "        dy[targets[t]] -= 1 # backprop into y. After taking the soft max in the input vector, subtract 1 from the value of the element corresponding to the correct label.\n",
        "        dWhy += np.dot(dy, hs[t].T)\n",
        "        dby += dy \n",
        "        dh = np.dot(W_hy.T, dy) + dhnext # backprop into h. \n",
        "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity #tanh'(x) = 1-tanh^2(x)\n",
        "        dbh += dhraw\n",
        "        dWxh += np.dot(dhraw, xs[t].T)\n",
        "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
        "        dhnext = np.dot(W_hh.T, dhraw)\n",
        "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]: \n",
        "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients.  \n",
        "    \n",
        "    return dWxh, dWhh, dWhy, dbh, dby"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aDYxczrZHsr",
        "colab_type": "code",
        "outputId": "15d33fd9-3ce0-49f6-997c-538ae057d162",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "source": [
        "# Train RNN model\n",
        "%%time\n",
        "\n",
        "data_pointer = 0\n",
        "\n",
        "# memory variables for Adagrad\n",
        "mWxh, mWhh, mWhy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy)\n",
        "mbh, mby = np.zeros_like(b_h), np.zeros_like(b_y) \n",
        "\n",
        "for i in range(iteration):\n",
        "    h_prev = np.zeros((hidden_size,1)) # reset RNN memory\n",
        "    data_pointer = 0 # go from start of data\n",
        "    \n",
        "    for b in range(batch_size):\n",
        "        \n",
        "        inputs = [char_to_int[ch] for ch in data[data_pointer:data_pointer+sequence_length]]\n",
        "        targets = [char_to_int[ch] for ch in data[data_pointer+1:data_pointer+sequence_length+1]] # t+1        \n",
        "            \n",
        "        if (data_pointer+sequence_length+1 >= len(data) and b == batch_size-1): # processing of the last part of the input data. \n",
        "#             targets.append(char_to_int[txt_data[0]])   # When the data doesn't fit, add the first char to the back.\n",
        "            targets.append(char_to_int[\" \"])   # When the data doesn't fit, add space(\" \") to the back.\n",
        "\n",
        "\n",
        "        # forward\n",
        "        loss, ps, hs, xs = forwardprop(inputs, targets, h_prev)\n",
        "#         print(loss)\n",
        "    \n",
        "        # backward\n",
        "        dWxh, dWhh, dWhy, dbh, dby = backprop(ps, inputs, hs, xs, targets) \n",
        "        \n",
        "        \n",
        "    # perform parameter update with Adagrad\n",
        "        for param, dparam, mem in zip([W_xh, W_hh, W_hy, b_h, b_y], \n",
        "                                    [dWxh, dWhh, dWhy, dbh, dby], \n",
        "                                    [mWxh, mWhh, mWhy, mbh, mby]):\n",
        "            mem += dparam * dparam # elementwise\n",
        "            param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update      \n",
        "    \n",
        "        data_pointer += sequence_length # move data pointer\n",
        "        \n",
        "    if i % 100 == 0:\n",
        "        print ('iter %d, loss: %f' % (i, loss)) # print progress"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-3bb1b3317096>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\ndata_pointer = 0\\n\\n# memory variables for Adagrad\\nmWxh, mWhh, mWhy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy)\\nmbh, mby = np.zeros_like(b_h), np.zeros_like(b_y) \\n\\nfor i in range(iteration):\\n    h_prev = np.zeros((hidden_size,1)) # reset RNN memory\\n    data_pointer = 0 # go from start of data\\n    \\n    for b in range(batch_size):\\n        \\n        inputs = [char_to_int[ch] for ch in data[data_pointer:data_pointer+sequence_length]]\\n        targets = [char_to_int[ch] for ch in data[data_pointer+1:data_pointer+sequence_length+1]] # t+1        \\n            \\n        if (data_pointer+sequence_length+1 >= len(data) and b == batch_size-1): # processing of the last part of the input data. \\n#             targets.append(char_to_int[txt_data[0]])   # When the data doesn\\'t fit, add the first char to the back.\\n            targets.append(char_to_int[\" \"])   # When the data doesn\\'t fit, add space(\" \") to the back.\\n\\n\\n        # forward\\n        loss, ps, hs, xs = forwardprop(inputs, targets, h_prev)\\n#         print(loss)\\n    \\n        # backward\\n        dWxh, dWhh, dWhy, dbh, dby = backprop(ps, inputs, hs, xs, targets) \\n        \\n        \\n    # perform parameter update with Adagrad\\n        for param, dparam, mem in zip([W_xh, W_hh, W_hy, b_h, b_y], \\n                       ...\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-1f9a29128c6d>\u001b[0m in \u001b[0;36mbackprop\u001b[0;34m(ps, inputs, hs, xs, targets)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mdWhy\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mdby\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mdh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW_hy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdy\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdhnext\u001b[0m \u001b[0;31m# backprop into h.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mdhraw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdh\u001b[0m \u001b[0;31m# backprop through tanh nonlinearity #tanh'(x) = 1-tanh^2(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mdbh\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdhraw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zE4a4O7Bp5x1"
      },
      "source": [
        "# Resources and Stretch Goals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uT3UV3gap9H6"
      },
      "source": [
        "## Stretch goals:\n",
        "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
        "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
        "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
        "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
        "- Run on bigger, better data\n",
        "\n",
        "## Resources:\n",
        "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
        "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
        "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
        "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
        "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
      ]
    }
  ]
}