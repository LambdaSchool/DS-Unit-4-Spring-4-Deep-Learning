{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "LS_DS_433_Autoencoders_Lecture.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nckflannery/DS-Unit-4-Sprint-3-Deep-Learning/blob/master/module3-autoencoders/LS_DS_433_Autoencoders_Lecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpxJFSEn-aBW",
        "colab_type": "text"
      },
      "source": [
        "Lambda School Data Science\n",
        "\n",
        "*Unit 4, Sprint 3, Module 3*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6YaZ0eU-aBZ",
        "colab_type": "text"
      },
      "source": [
        "# Autoencoders\n",
        "\n",
        "> An autoencoder is a type of artificial neural network used to learn efficient data codings in an unsupervised manner.[1][2] The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore signal “noise”. Along with the reduction side, a reconstructing side is learnt, where the autoencoder tries to generate from the reduced encoding a representation as close as possible to its original input, hence its name. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUHTw6rW-aBZ",
        "colab_type": "text"
      },
      "source": [
        "## Learning Objectives\n",
        "*At the end of the lecture you should be to*:\n",
        "* <a href=\"#p1\">Part 1</a>: Describe the componenets of an autoencoder\n",
        "* <a href=\"#p2\">Part 2</a>: Train an autoencoder\n",
        "* <a href=\"#p3\">Part 3</a>: Apply an autoenocder to a basic information retrieval problem\n",
        "\n",
        "__Problem:__ Is it possible to automatically represent an image as a fixed-sized vector even if it isn’t labeled?\n",
        "\n",
        "__Solution:__ Use an autoencoder\n",
        "\n",
        "Why do we need to represent an image as a fixed-sized vector do you ask? \n",
        "\n",
        "* __Information Retrieval__\n",
        "    - [Reverse Image Search](https://en.wikipedia.org/wiki/Reverse_image_search)\n",
        "    - [Recommendation Systems - Content Based Filtering](https://en.wikipedia.org/wiki/Recommender_system#Content-based_filtering)\n",
        "* __Dimensionality Reduction__\n",
        "    - [Feature Extraction](https://www.kaggle.com/c/vsb-power-line-fault-detection/discussion/78285)\n",
        "    - [Manifold Learning](https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction)\n",
        "\n",
        "We've already seen *representation learning* when we talked about word embedding modelings during our NLP week. Today we're going to achieve a similiar goal on images using *autoencoders*. An autoencoder is a neural network that is trained to attempt to copy its input to its output. Usually they are restricted in ways that allow them to copy only approximately. The model often learns useful properties of the data, because it is forced to prioritize which aspecs of the input should be copied. The properties of autoencoders have made them an important part of modern generative modeling approaches. Consider autoencoders a special case of feed-forward networks (the kind we've been studying); backpropagation and gradient descent still work. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XH_-rfH-aBa",
        "colab_type": "text"
      },
      "source": [
        "# Autoencoder Architecture (Learn)\n",
        "<a id=\"p1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqjUt2fS-aBb",
        "colab_type": "text"
      },
      "source": [
        "## Overview\n",
        "\n",
        "The *encoder* compresses the input data and the *decoder* does the reverse to produce the uncompressed version of the data to create a reconstruction of the input as accurately as possible:\n",
        "\n",
        "<img src='https://miro.medium.com/max/1400/1*44eDEuZBEsmG_TCAKRI3Kw@2x.png' width=800/>\n",
        "\n",
        "The learning process gis described simply as minimizing a loss function: \n",
        "$ L(x, g(f(x))) $\n",
        "\n",
        "- $L$ is a loss function penalizing $g(f(x))$ for being dissimiliar from $x$ (such as mean squared error)\n",
        "- $f$ is the encoder function\n",
        "- $g$ is the decoder function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muFV30D9-aBb",
        "colab_type": "text"
      },
      "source": [
        "## Follow Along\n",
        "### Extremely Simple Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mozeeZMaCs-5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b88c6718-22dc-403e-fd57-109b7958e91e"
      },
      "source": [
        "!pip install --upgrade wandb"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/b7/8f54fbb4b0490764657b89ef760332d0c64e5090d3c22880f86c8471817f/wandb-0.8.18-py2.py3-none-any.whl (1.3MB)\n",
            "\r\u001b[K     |▎                               | 10kB 24.6MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 27.6MB/s eta 0:00:01\r\u001b[K     |▊                               | 30kB 33.4MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 37.8MB/s eta 0:00:01\r\u001b[K     |█▎                              | 51kB 26.3MB/s eta 0:00:01\r\u001b[K     |█▌                              | 61kB 28.8MB/s eta 0:00:01\r\u001b[K     |█▊                              | 71kB 23.8MB/s eta 0:00:01\r\u001b[K     |██                              | 81kB 24.9MB/s eta 0:00:01\r\u001b[K     |██▏                             | 92kB 26.7MB/s eta 0:00:01\r\u001b[K     |██▌                             | 102kB 24.8MB/s eta 0:00:01\r\u001b[K     |██▊                             | 112kB 24.8MB/s eta 0:00:01\r\u001b[K     |███                             | 122kB 24.8MB/s eta 0:00:01\r\u001b[K     |███▏                            | 133kB 24.8MB/s eta 0:00:01\r\u001b[K     |███▍                            | 143kB 24.8MB/s eta 0:00:01\r\u001b[K     |███▊                            | 153kB 24.8MB/s eta 0:00:01\r\u001b[K     |████                            | 163kB 24.8MB/s eta 0:00:01\r\u001b[K     |████▏                           | 174kB 24.8MB/s eta 0:00:01\r\u001b[K     |████▍                           | 184kB 24.8MB/s eta 0:00:01\r\u001b[K     |████▋                           | 194kB 24.8MB/s eta 0:00:01\r\u001b[K     |█████                           | 204kB 24.8MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 215kB 24.8MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 225kB 24.8MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 235kB 24.8MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 245kB 24.8MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 256kB 24.8MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 266kB 24.8MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 276kB 24.8MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 286kB 24.8MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 296kB 24.8MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 307kB 24.8MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 317kB 24.8MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 327kB 24.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 337kB 24.8MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 348kB 24.8MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 358kB 24.8MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 368kB 24.8MB/s eta 0:00:01\r\u001b[K     |█████████                       | 378kB 24.8MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 389kB 24.8MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 399kB 24.8MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 409kB 24.8MB/s eta 0:00:01\r\u001b[K     |██████████                      | 419kB 24.8MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 430kB 24.8MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 440kB 24.8MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 450kB 24.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 460kB 24.8MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 471kB 24.8MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 481kB 24.8MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 491kB 24.8MB/s eta 0:00:01\r\u001b[K     |████████████                    | 501kB 24.8MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 512kB 24.8MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 522kB 24.8MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 532kB 24.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 542kB 24.8MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 552kB 24.8MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 563kB 24.8MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 573kB 24.8MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 583kB 24.8MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 593kB 24.8MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 604kB 24.8MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 614kB 24.8MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 624kB 24.8MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 634kB 24.8MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 645kB 24.8MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 655kB 24.8MB/s eta 0:00:01\r\u001b[K     |████████████████                | 665kB 24.8MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 675kB 24.8MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 686kB 24.8MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 696kB 24.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 706kB 24.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 716kB 24.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 727kB 24.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 737kB 24.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 747kB 24.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 757kB 24.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 768kB 24.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 778kB 24.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 788kB 24.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 798kB 24.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 808kB 24.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 819kB 24.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 829kB 24.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 839kB 24.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 849kB 24.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 860kB 24.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 870kB 24.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 880kB 24.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 890kB 24.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 901kB 24.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 911kB 24.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 921kB 24.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 931kB 24.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 942kB 24.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 952kB 24.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 962kB 24.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 972kB 24.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 983kB 24.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 993kB 24.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.0MB 24.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.0MB 24.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.0MB 24.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.0MB 24.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.0MB 24.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.1MB 24.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.1MB 24.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.1MB 24.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.1MB 24.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.1MB 24.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.1MB 24.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.1MB 24.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.1MB 24.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.1MB 24.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.1MB 24.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.2MB 24.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.2MB 24.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.2MB 24.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.2MB 24.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.2MB 24.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.2MB 24.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.2MB 24.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.2MB 24.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.2MB 24.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.2MB 24.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.3MB 24.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.3MB 24.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.3MB 24.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.3MB 24.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.3MB 24.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.3MB 24.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.3MB 24.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.3MB 24.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.3MB 24.8MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/80/d7/2bfc9332e68d3e15ea97b9b1588b3899ad565120253d3fd71c8f7f13b4fe/shortuuid-0.5.0.tar.gz\n",
            "Requirement already satisfied, skipping upgrade: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (1.12.0)\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/48/a38ff26af174ef2d03c5754c93b7a18818951734fdaa53bf3c2b80e497e7/sentry_sdk-0.13.5-py2.py3-none-any.whl (91kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 12.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.352.0)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.0)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/7a/2a/95ed0501cf5d8709490b1d3a3f9b5cf340da6c433f896bbe9ce08dbe6785/configparser-4.0.2-py2.py3-none-any.whl\n",
            "Collecting graphql-core<3.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6a/11/bc4a7eb440124271289d93e4d208bd07d94196038fabbe2a52435a07d3d3/graphql_core-2.2.1-py2.py3-none-any.whl (250kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 56.2MB/s \n",
            "\u001b[?25hCollecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/8c/4543981439d23c4ff65b2e62dddd767ebc84a8e664a9b67e840d1e2730d3/GitPython-3.0.5-py3-none-any.whl (455kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 62.5MB/s \n",
            "\u001b[?25hCollecting watchdog>=0.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/e3/5a55d48a29300160779f0a0d2776d17c1b762a2039b36de528b093b87d5b/watchdog-0.9.0.tar.gz (85kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 14.6MB/s \n",
            "\u001b[?25hCollecting gql>=0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/aa/9c/2933b7791210e00f5c26a6243198cc03af9132c29cf85e4c22cb007f171e/gql-0.1.0.tar.gz\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.6.1)\n",
            "Collecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 15.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: certifi in /usr/local/lib/python3.6/dist-packages (from sentry-sdk>=0.4.0->wandb) (2019.11.28)\n",
            "Requirement already satisfied, skipping upgrade: urllib3>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from sentry-sdk>=0.4.0->wandb) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: promise>=2.1 in /usr/local/lib/python3.6/dist-packages (from graphql-core<3.0.0->wandb) (2.2.1)\n",
            "Collecting rx<3,>=1.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/0f/5ef4ac78e2a538cc1b054eb86285fe0bf7a5dbaeaac2c584757c300515e2/Rx-1.6.1-py2.py3-none-any.whl (179kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 46.1MB/s \n",
            "\u001b[?25hCollecting gitdb2>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/6c/99296f89bad2ef85626e1df9f677acbee8885bb043ad82ad3ed4746d2325/gitdb2-2.0.6-py2.py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 11.9MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from watchdog>=0.8.3->wandb) (3.13)\n",
            "Collecting argh>=0.24.1\n",
            "  Downloading https://files.pythonhosted.org/packages/06/1c/e667a7126f0b84aaa1c56844337bf0ac12445d1beb9c8a6199a7314944bf/argh-0.26.2-py2.py3-none-any.whl\n",
            "Collecting pathtools>=0.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Collecting smmap2>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/55/d2/866d45e3a121ee15a1dc013824d58072fd5c7799c9c34d01378eb262ca8f/smmap2-2.0.5-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: shortuuid, watchdog, gql, subprocess32, pathtools\n",
            "  Building wheel for shortuuid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for shortuuid: filename=shortuuid-0.5.0-cp36-none-any.whl size=5499 sha256=4fe5c0967fcf8cb40a64901035b89e0eab2fb13f5e8fa4519e8cb62457fc5a94\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/eb/fd/69e5177f67b505e44acbd1aedfbe44b91768ee0c4cd5636576\n",
            "  Building wheel for watchdog (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for watchdog: filename=watchdog-0.9.0-cp36-none-any.whl size=73652 sha256=e2ad26d2af2bb42383698513dac8d35ec8b3babaadd9ab938b8e7793e42d26f8\n",
            "  Stored in directory: /root/.cache/pip/wheels/61/1d/d0/04cfe495619be2095eb8d89a31c42adb4e42b76495bc8f784c\n",
            "  Building wheel for gql (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gql: filename=gql-0.1.0-cp36-none-any.whl size=5541 sha256=237bc69ab3029d6bad6fd393df4bceed1d6446cb889e224b4aead6ab3a39d35a\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/8d/65/a3247f500d675d80a01e4d2f0ee44fe99f1faef575bc2a1664\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp36-none-any.whl size=6489 sha256=baf39664546c1e7f55ed1bba9ae31fd7a282d0bf77bad3ccf2557dadef33ea13\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp36-none-any.whl size=8786 sha256=537887cf8f1df1acfe1acb635f5faeac7b4e6f3955dcedadb40ec8f2a6375bfc\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "Successfully built shortuuid watchdog gql subprocess32 pathtools\n",
            "Installing collected packages: shortuuid, sentry-sdk, docker-pycreds, configparser, rx, graphql-core, smmap2, gitdb2, GitPython, argh, pathtools, watchdog, gql, subprocess32, wandb\n",
            "Successfully installed GitPython-3.0.5 argh-0.26.2 configparser-4.0.2 docker-pycreds-0.4.0 gitdb2-2.0.6 gql-0.1.0 graphql-core-2.2.1 pathtools-0.1.2 rx-1.6.1 sentry-sdk-0.13.5 shortuuid-0.5.0 smmap2-2.0.5 subprocess32-3.5.4 wandb-0.8.18 watchdog-0.9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tojgos6tDn35",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "094e870a-7021-4f7f-b787-f6fcc151f41b"
      },
      "source": [
        "!wandb login 46c02f2598d6b20efbf9a0ea2d6c3541debfbfe6"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[32mSuccessfully logged in to Weights & Biases!\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFK-blWS-aBc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "68c215d2-637d-4bd6-bb05-c873cf59abb7"
      },
      "source": [
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "\n",
        "# this is the size of our encoded representations\n",
        "encoding_dim = 32  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
        "\n",
        "# this is our input placeholder\n",
        "input_img = Input(shape=(784,))\n",
        "\n",
        "# \"encoded\" is the encoded representation of the input\n",
        "encoded = Dense(encoding_dim, activation='relu')(input_img)\n",
        "\n",
        "# \"decoded\" is the lossy reconstruction of the input\n",
        "decoded = Dense(784, activation='sigmoid')(encoded)\n",
        "\n",
        "# this model maps an input to its reconstruction\n",
        "autoencoder = Model(input_img, decoded)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_v60B54M-aBe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this model maps an input to its encoded representation\n",
        "encoder = Model(input_img, encoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8I8DVqb-aBg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a placeholder for an encoded (32-dimensional) input\n",
        "encoded_input = Input(shape=(encoding_dim,))\n",
        "\n",
        "# retrieve the last layer of the autoencoder model\n",
        "decoder_layer = autoencoder.layers[-1]\n",
        "\n",
        "# create the decoder model\n",
        "decoder = Model(encoded_input, decoder_layer(encoded_input))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qi8bskpV-aBi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "dd533aa3-4ff8-471d-b0dc-b3e92e10b70e"
      },
      "source": [
        "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oLEYI87-aBk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "996c7822-4b71-4859-8dab-898a62b471d4"
      },
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "import numpy as np\n",
        "(x_train, _), (x_test, _) = mnist.load_data()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_xWby0_-aBm",
        "colab_type": "code",
        "outputId": "6159f52f-596a-4787-a85a-a7f4c2583c9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
        "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 784)\n",
            "(10000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLNc9H24-aBp",
        "colab_type": "code",
        "outputId": "32cbe07b-398f-4c99-d4e3-dd31c6b14e10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "wandb.init(project=\"autoencoder\", entity=\"ds8\")\n",
        "\n",
        "autoencoder.fit(x_train, x_train,\n",
        "                epochs=500,\n",
        "                batch_size=256,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test, x_test),\n",
        "                verbose = False,\n",
        "                callbacks=[WandbCallback()])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/ds8/autoencoder\" target=\"_blank\">https://app.wandb.ai/ds8/autoencoder</a><br/>\n",
              "                Run page: <a href=\"https://app.wandb.ai/ds8/autoencoder/runs/574qqgae\" target=\"_blank\">https://app.wandb.ai/ds8/autoencoder/runs/574qqgae</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f487349c630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjuQ2M-e-aBr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# encode and decode some digits\n",
        "# note that we take them from the *test* set\n",
        "\n",
        "encoded_imgs = encoder.predict(x_test)\n",
        "decoded_imgs = decoder.predict(encoded_imgs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZ-8OmYk-aBs",
        "colab_type": "code",
        "outputId": "e5662138-683a-4625-d41b-86d36fb43b3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "source": [
        "# use Matplotlib (don't ask)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "n = 10  # how many digits we will display\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "    # display original\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(x_test[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # display reconstruction\n",
        "    ax = plt.subplot(2, n, i + 1 + n)\n",
        "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABG0AAADnCAYAAACkCqtqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\naAMAAAAAAAAA0EL+P9a5rQG10KGCAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1440x288 with 20 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IzyIARE-aBu",
        "colab_type": "text"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "Expected to talk about the components of autoencoder and their purpose. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TQqQZBB-aBv",
        "colab_type": "text"
      },
      "source": [
        "# Train an Autoencoder (Learn)\n",
        "<a id=\"p2\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhlgAdas-aBv",
        "colab_type": "text"
      },
      "source": [
        "## Overview\n",
        "\n",
        "As long as our architecture maintains an hourglass shape, we can continue to add layers and create a deeper network. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "toc-hr-collapsed": true,
        "id": "GRxNZmgz-aBw",
        "colab_type": "text"
      },
      "source": [
        "## Follow Along"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXXj1ZPD-aBx",
        "colab_type": "text"
      },
      "source": [
        "### Deep Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgjbcZZY-aBx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_img = Input(shape=(784,))\n",
        "\n",
        "encoded = Dense(128, activation='relu')(input_img)\n",
        "encoded = Dense(64, activation='relu')(encoded)\n",
        "encoded = Dense(32, activation='relu')(encoded) #dry strawberry\n",
        "\n",
        "decoded = Dense(64, activation='relu')(encoded)\n",
        "decoded = Dense(128, activation='relu')(decoded)\n",
        "decoded = Dense(784, activation='sigmoid')(decoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7n4-PPF2-aBz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e6c7bd29-3b44-4fb2-9f1e-88a6b2d59fee"
      },
      "source": [
        "# compile & fit model\n",
        "\n",
        "autoencoder = Model(input_img, decoded)\n",
        "\n",
        "autoencoder.compile(loss='binary_crossentropy',\n",
        "                    optimizer='nadam')\n",
        "\n",
        "autoencoder.fit(x_train, x_train,\n",
        "                epochs = 500,\n",
        "                batch_size=256,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test,x_test),\n",
        "                verbose=True,\n",
        "                callbacks=[WandbCallback()])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/500\n",
            "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (2.482962). Check your callbacks.\n",
            "  256/60000 [..............................] - ETA: 10:55 - loss: 0.1147WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (1.241494). Check your callbacks.\n",
            "60000/60000 [==============================] - 4s 70us/sample - loss: 0.1138 - val_loss: 0.1096\n",
            "Epoch 2/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.1114 - val_loss: 0.1102\n",
            "Epoch 3/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.1091 - val_loss: 0.1090\n",
            "Epoch 4/500\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.1071 - val_loss: 0.1038\n",
            "Epoch 5/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.1054 - val_loss: 0.1043\n",
            "Epoch 6/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.1037 - val_loss: 0.1011\n",
            "Epoch 7/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.1022 - val_loss: 0.1002\n",
            "Epoch 8/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.1010 - val_loss: 0.0992\n",
            "Epoch 9/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0997 - val_loss: 0.1005\n",
            "Epoch 10/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0987 - val_loss: 0.0989\n",
            "Epoch 11/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0977 - val_loss: 0.0984\n",
            "Epoch 12/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0967 - val_loss: 0.0962\n",
            "Epoch 13/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0960 - val_loss: 0.0949\n",
            "Epoch 14/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0953 - val_loss: 0.0945\n",
            "Epoch 15/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0945 - val_loss: 0.0944\n",
            "Epoch 16/500\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0938 - val_loss: 0.0933\n",
            "Epoch 17/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0932 - val_loss: 0.0935\n",
            "Epoch 18/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0925 - val_loss: 0.0918\n",
            "Epoch 19/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0920 - val_loss: 0.0907\n",
            "Epoch 20/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0915 - val_loss: 0.0923\n",
            "Epoch 21/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0910 - val_loss: 0.0907\n",
            "Epoch 22/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0904 - val_loss: 0.0903\n",
            "Epoch 23/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0900 - val_loss: 0.0901\n",
            "Epoch 24/500\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0895 - val_loss: 0.0888\n",
            "Epoch 25/500\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0891 - val_loss: 0.0875\n",
            "Epoch 26/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0887 - val_loss: 0.0897\n",
            "Epoch 27/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0883 - val_loss: 0.0873\n",
            "Epoch 28/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0879 - val_loss: 0.0865\n",
            "Epoch 29/500\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0875 - val_loss: 0.0870\n",
            "Epoch 30/500\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0872 - val_loss: 0.0863\n",
            "Epoch 31/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0868 - val_loss: 0.0864\n",
            "Epoch 32/500\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0865 - val_loss: 0.0862\n",
            "Epoch 33/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0862 - val_loss: 0.0868\n",
            "Epoch 34/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0859 - val_loss: 0.0862\n",
            "Epoch 35/500\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0857 - val_loss: 0.0854\n",
            "Epoch 36/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0854 - val_loss: 0.0853\n",
            "Epoch 37/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0852 - val_loss: 0.0841\n",
            "Epoch 38/500\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0848 - val_loss: 0.0846\n",
            "Epoch 39/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0847 - val_loss: 0.0850\n",
            "Epoch 40/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0844 - val_loss: 0.0862\n",
            "Epoch 41/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0842 - val_loss: 0.0847\n",
            "Epoch 42/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0840 - val_loss: 0.0833\n",
            "Epoch 43/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0837 - val_loss: 0.0831\n",
            "Epoch 44/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0836 - val_loss: 0.0851\n",
            "Epoch 45/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0834 - val_loss: 0.0827\n",
            "Epoch 46/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0831 - val_loss: 0.0830\n",
            "Epoch 47/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0829 - val_loss: 0.0826\n",
            "Epoch 48/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0827 - val_loss: 0.0827\n",
            "Epoch 49/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0826 - val_loss: 0.0822\n",
            "Epoch 50/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0823 - val_loss: 0.0825\n",
            "Epoch 51/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0822 - val_loss: 0.0820\n",
            "Epoch 52/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0820 - val_loss: 0.0825\n",
            "Epoch 53/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0818 - val_loss: 0.0816\n",
            "Epoch 54/500\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0816 - val_loss: 0.0813\n",
            "Epoch 55/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0815 - val_loss: 0.0810\n",
            "Epoch 56/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0813 - val_loss: 0.0815\n",
            "Epoch 57/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0812 - val_loss: 0.0811\n",
            "Epoch 58/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0810 - val_loss: 0.0816\n",
            "Epoch 59/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0809 - val_loss: 0.0810\n",
            "Epoch 60/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0807 - val_loss: 0.0804\n",
            "Epoch 61/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0806 - val_loss: 0.0801\n",
            "Epoch 62/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0805 - val_loss: 0.0805\n",
            "Epoch 63/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0803 - val_loss: 0.0808\n",
            "Epoch 64/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0803 - val_loss: 0.0807\n",
            "Epoch 65/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0801 - val_loss: 0.0796\n",
            "Epoch 66/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0800 - val_loss: 0.0805\n",
            "Epoch 67/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0799 - val_loss: 0.0797\n",
            "Epoch 68/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0797 - val_loss: 0.0796\n",
            "Epoch 69/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0797 - val_loss: 0.0803\n",
            "Epoch 70/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0796 - val_loss: 0.0795\n",
            "Epoch 71/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0794 - val_loss: 0.0794\n",
            "Epoch 72/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0793 - val_loss: 0.0792\n",
            "Epoch 73/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0792 - val_loss: 0.0795\n",
            "Epoch 74/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0792 - val_loss: 0.0790\n",
            "Epoch 75/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0791 - val_loss: 0.0796\n",
            "Epoch 76/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0790 - val_loss: 0.0793\n",
            "Epoch 77/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0789 - val_loss: 0.0787\n",
            "Epoch 78/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0788 - val_loss: 0.0788\n",
            "Epoch 79/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0788 - val_loss: 0.0788\n",
            "Epoch 80/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0787 - val_loss: 0.0785\n",
            "Epoch 81/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0786 - val_loss: 0.0788\n",
            "Epoch 82/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0786 - val_loss: 0.0787\n",
            "Epoch 83/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0785 - val_loss: 0.0787\n",
            "Epoch 84/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0785 - val_loss: 0.0786\n",
            "Epoch 85/500\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0784 - val_loss: 0.0783\n",
            "Epoch 86/500\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0783 - val_loss: 0.0782\n",
            "Epoch 87/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0783 - val_loss: 0.0787\n",
            "Epoch 88/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0782 - val_loss: 0.0785\n",
            "Epoch 89/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0782 - val_loss: 0.0783\n",
            "Epoch 90/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0781 - val_loss: 0.0785\n",
            "Epoch 91/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0781 - val_loss: 0.0782\n",
            "Epoch 92/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0780 - val_loss: 0.0780\n",
            "Epoch 93/500\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0780 - val_loss: 0.0784\n",
            "Epoch 94/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0780 - val_loss: 0.0786\n",
            "Epoch 95/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0779 - val_loss: 0.0785\n",
            "Epoch 96/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0779 - val_loss: 0.0779\n",
            "Epoch 97/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0778 - val_loss: 0.0778\n",
            "Epoch 98/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0778 - val_loss: 0.0777\n",
            "Epoch 99/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0778 - val_loss: 0.0777\n",
            "Epoch 100/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0777 - val_loss: 0.0789\n",
            "Epoch 101/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0777 - val_loss: 0.0784\n",
            "Epoch 102/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0777 - val_loss: 0.0781\n",
            "Epoch 103/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0776 - val_loss: 0.0776\n",
            "Epoch 104/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0776 - val_loss: 0.0781\n",
            "Epoch 105/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0776 - val_loss: 0.0779\n",
            "Epoch 106/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0775 - val_loss: 0.0779\n",
            "Epoch 107/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0775 - val_loss: 0.0778\n",
            "Epoch 108/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0775 - val_loss: 0.0776\n",
            "Epoch 109/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0774 - val_loss: 0.0778\n",
            "Epoch 110/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0774 - val_loss: 0.0776\n",
            "Epoch 111/500\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0774 - val_loss: 0.0774\n",
            "Epoch 112/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0774 - val_loss: 0.0776\n",
            "Epoch 113/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0774 - val_loss: 0.0776\n",
            "Epoch 114/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0773 - val_loss: 0.0775\n",
            "Epoch 115/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0773 - val_loss: 0.0778\n",
            "Epoch 116/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0773 - val_loss: 0.0773\n",
            "Epoch 117/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0773 - val_loss: 0.0775\n",
            "Epoch 118/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0773 - val_loss: 0.0775\n",
            "Epoch 119/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0772 - val_loss: 0.0780\n",
            "Epoch 120/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0772 - val_loss: 0.0778\n",
            "Epoch 121/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0772 - val_loss: 0.0778\n",
            "Epoch 122/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0772 - val_loss: 0.0772\n",
            "Epoch 123/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0771 - val_loss: 0.0781\n",
            "Epoch 124/500\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0771 - val_loss: 0.0772\n",
            "Epoch 125/500\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0771 - val_loss: 0.0776\n",
            "Epoch 126/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0771 - val_loss: 0.0777\n",
            "Epoch 127/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0771 - val_loss: 0.0773\n",
            "Epoch 128/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0770 - val_loss: 0.0772\n",
            "Epoch 129/500\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0771 - val_loss: 0.0774\n",
            "Epoch 130/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0770 - val_loss: 0.0771\n",
            "Epoch 131/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0770 - val_loss: 0.0773\n",
            "Epoch 132/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0770 - val_loss: 0.0774\n",
            "Epoch 133/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0770 - val_loss: 0.0774\n",
            "Epoch 134/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0769 - val_loss: 0.0776\n",
            "Epoch 135/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0769 - val_loss: 0.0772\n",
            "Epoch 136/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0769 - val_loss: 0.0771\n",
            "Epoch 137/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0769 - val_loss: 0.0772\n",
            "Epoch 138/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0769 - val_loss: 0.0774\n",
            "Epoch 139/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0769 - val_loss: 0.0774\n",
            "Epoch 140/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0769 - val_loss: 0.0776\n",
            "Epoch 141/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0768 - val_loss: 0.0773\n",
            "Epoch 142/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0768 - val_loss: 0.0772\n",
            "Epoch 143/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0768 - val_loss: 0.0770\n",
            "Epoch 144/500\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0768 - val_loss: 0.0770\n",
            "Epoch 145/500\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0768 - val_loss: 0.0773\n",
            "Epoch 146/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0768 - val_loss: 0.0771\n",
            "Epoch 147/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0768 - val_loss: 0.0769\n",
            "Epoch 148/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0767 - val_loss: 0.0771\n",
            "Epoch 149/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0767 - val_loss: 0.0772\n",
            "Epoch 150/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0767 - val_loss: 0.0770\n",
            "Epoch 151/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0767 - val_loss: 0.0766\n",
            "Epoch 152/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0767 - val_loss: 0.0771\n",
            "Epoch 153/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0767 - val_loss: 0.0771\n",
            "Epoch 154/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0767 - val_loss: 0.0767\n",
            "Epoch 155/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0766 - val_loss: 0.0769\n",
            "Epoch 156/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0767 - val_loss: 0.0770\n",
            "Epoch 157/500\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0766 - val_loss: 0.0768\n",
            "Epoch 158/500\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0766 - val_loss: 0.0776\n",
            "Epoch 159/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0766 - val_loss: 0.0769\n",
            "Epoch 160/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0766 - val_loss: 0.0768\n",
            "Epoch 161/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0766 - val_loss: 0.0773\n",
            "Epoch 162/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0766 - val_loss: 0.0768\n",
            "Epoch 163/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0766 - val_loss: 0.0771\n",
            "Epoch 164/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0765 - val_loss: 0.0769\n",
            "Epoch 165/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0766 - val_loss: 0.0767\n",
            "Epoch 166/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0765 - val_loss: 0.0770\n",
            "Epoch 167/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0765 - val_loss: 0.0767\n",
            "Epoch 168/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0765 - val_loss: 0.0766\n",
            "Epoch 169/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0765 - val_loss: 0.0769\n",
            "Epoch 170/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0765 - val_loss: 0.0764\n",
            "Epoch 171/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0765 - val_loss: 0.0770\n",
            "Epoch 172/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0765 - val_loss: 0.0772\n",
            "Epoch 173/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0765 - val_loss: 0.0767\n",
            "Epoch 174/500\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0765 - val_loss: 0.0766\n",
            "Epoch 175/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0764 - val_loss: 0.0767\n",
            "Epoch 176/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0764 - val_loss: 0.0771\n",
            "Epoch 177/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0765 - val_loss: 0.0768\n",
            "Epoch 178/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0764 - val_loss: 0.0765\n",
            "Epoch 179/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0764 - val_loss: 0.0768\n",
            "Epoch 180/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0764 - val_loss: 0.0766\n",
            "Epoch 181/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0764 - val_loss: 0.0766\n",
            "Epoch 182/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0764 - val_loss: 0.0768\n",
            "Epoch 183/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0764 - val_loss: 0.0766\n",
            "Epoch 184/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0764 - val_loss: 0.0766\n",
            "Epoch 185/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0764 - val_loss: 0.0768\n",
            "Epoch 186/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0764 - val_loss: 0.0766\n",
            "Epoch 187/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0763 - val_loss: 0.0767\n",
            "Epoch 188/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0763 - val_loss: 0.0769\n",
            "Epoch 189/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0764 - val_loss: 0.0771\n",
            "Epoch 190/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0763 - val_loss: 0.0767\n",
            "Epoch 191/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0763 - val_loss: 0.0770\n",
            "Epoch 192/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0763 - val_loss: 0.0767\n",
            "Epoch 193/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0763 - val_loss: 0.0764\n",
            "Epoch 194/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0763 - val_loss: 0.0767\n",
            "Epoch 195/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0763 - val_loss: 0.0768\n",
            "Epoch 196/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0763 - val_loss: 0.0768\n",
            "Epoch 197/500\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0763 - val_loss: 0.0764\n",
            "Epoch 198/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0763 - val_loss: 0.0765\n",
            "Epoch 199/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0763 - val_loss: 0.0769\n",
            "Epoch 200/500\n",
            "60000/60000 [==============================] - 1s 22us/sample - loss: 0.0763 - val_loss: 0.0763\n",
            "Epoch 201/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0762 - val_loss: 0.0765\n",
            "Epoch 202/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0762 - val_loss: 0.0769\n",
            "Epoch 203/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0762 - val_loss: 0.0764\n",
            "Epoch 204/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0762 - val_loss: 0.0765\n",
            "Epoch 205/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0762 - val_loss: 0.0768\n",
            "Epoch 206/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0762 - val_loss: 0.0766\n",
            "Epoch 207/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0762 - val_loss: 0.0763\n",
            "Epoch 208/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0762 - val_loss: 0.0767\n",
            "Epoch 209/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0762 - val_loss: 0.0765\n",
            "Epoch 210/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0762 - val_loss: 0.0765\n",
            "Epoch 211/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0761 - val_loss: 0.0766\n",
            "Epoch 212/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0762 - val_loss: 0.0762\n",
            "Epoch 213/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0761 - val_loss: 0.0762\n",
            "Epoch 214/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0762 - val_loss: 0.0766\n",
            "Epoch 215/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0762 - val_loss: 0.0764\n",
            "Epoch 216/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0761 - val_loss: 0.0768\n",
            "Epoch 217/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0761 - val_loss: 0.0764\n",
            "Epoch 218/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0761 - val_loss: 0.0769\n",
            "Epoch 219/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0761 - val_loss: 0.0764\n",
            "Epoch 220/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0761 - val_loss: 0.0766\n",
            "Epoch 221/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0761 - val_loss: 0.0767\n",
            "Epoch 222/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0761 - val_loss: 0.0765\n",
            "Epoch 223/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0761 - val_loss: 0.0763\n",
            "Epoch 224/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0761 - val_loss: 0.0777\n",
            "Epoch 225/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0761 - val_loss: 0.0768\n",
            "Epoch 226/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0761 - val_loss: 0.0763\n",
            "Epoch 227/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0761 - val_loss: 0.0767\n",
            "Epoch 228/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0761 - val_loss: 0.0761\n",
            "Epoch 229/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0761 - val_loss: 0.0764\n",
            "Epoch 230/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0761 - val_loss: 0.0764\n",
            "Epoch 231/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0761 - val_loss: 0.0766\n",
            "Epoch 232/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0760 - val_loss: 0.0766\n",
            "Epoch 233/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0761 - val_loss: 0.0763\n",
            "Epoch 234/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0760 - val_loss: 0.0764\n",
            "Epoch 235/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0761 - val_loss: 0.0764\n",
            "Epoch 236/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0761 - val_loss: 0.0763\n",
            "Epoch 237/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0760 - val_loss: 0.0765\n",
            "Epoch 238/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0760 - val_loss: 0.0764\n",
            "Epoch 239/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0760 - val_loss: 0.0771\n",
            "Epoch 240/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0760 - val_loss: 0.0765\n",
            "Epoch 241/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0760 - val_loss: 0.0763\n",
            "Epoch 242/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0760 - val_loss: 0.0766\n",
            "Epoch 243/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0760 - val_loss: 0.0769\n",
            "Epoch 244/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0760 - val_loss: 0.0764\n",
            "Epoch 245/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0760 - val_loss: 0.0764\n",
            "Epoch 246/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0760 - val_loss: 0.0764\n",
            "Epoch 247/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0760 - val_loss: 0.0766\n",
            "Epoch 248/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0760 - val_loss: 0.0762\n",
            "Epoch 249/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0760 - val_loss: 0.0761\n",
            "Epoch 250/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0760 - val_loss: 0.0770\n",
            "Epoch 251/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0760 - val_loss: 0.0767\n",
            "Epoch 252/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0760 - val_loss: 0.0762\n",
            "Epoch 253/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0759 - val_loss: 0.0768\n",
            "Epoch 254/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0759 - val_loss: 0.0764\n",
            "Epoch 255/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0759 - val_loss: 0.0764\n",
            "Epoch 256/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0759 - val_loss: 0.0760\n",
            "Epoch 257/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0759 - val_loss: 0.0763\n",
            "Epoch 258/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0759 - val_loss: 0.0764\n",
            "Epoch 259/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0759 - val_loss: 0.0762\n",
            "Epoch 260/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0759 - val_loss: 0.0765\n",
            "Epoch 261/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0760 - val_loss: 0.0763\n",
            "Epoch 262/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0759 - val_loss: 0.0766\n",
            "Epoch 263/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0759 - val_loss: 0.0762\n",
            "Epoch 264/500\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0759 - val_loss: 0.0765\n",
            "Epoch 265/500\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0759 - val_loss: 0.0764\n",
            "Epoch 266/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0759 - val_loss: 0.0762\n",
            "Epoch 267/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0759 - val_loss: 0.0761\n",
            "Epoch 268/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0759 - val_loss: 0.0763\n",
            "Epoch 269/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0759 - val_loss: 0.0762\n",
            "Epoch 270/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0759 - val_loss: 0.0762\n",
            "Epoch 271/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0759 - val_loss: 0.0764\n",
            "Epoch 272/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0759 - val_loss: 0.0761\n",
            "Epoch 273/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0758 - val_loss: 0.0768\n",
            "Epoch 274/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0759 - val_loss: 0.0773\n",
            "Epoch 275/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0759 - val_loss: 0.0760\n",
            "Epoch 276/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0758 - val_loss: 0.0766\n",
            "Epoch 277/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0759 - val_loss: 0.0761\n",
            "Epoch 278/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0758 - val_loss: 0.0765\n",
            "Epoch 279/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0759 - val_loss: 0.0761\n",
            "Epoch 280/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0758 - val_loss: 0.0768\n",
            "Epoch 281/500\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0758 - val_loss: 0.0764\n",
            "Epoch 282/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0759 - val_loss: 0.0762\n",
            "Epoch 283/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0758 - val_loss: 0.0761\n",
            "Epoch 284/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0759 - val_loss: 0.0759\n",
            "Epoch 285/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0758 - val_loss: 0.0767\n",
            "Epoch 286/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0758 - val_loss: 0.0761\n",
            "Epoch 287/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0758 - val_loss: 0.0763\n",
            "Epoch 288/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0758 - val_loss: 0.0764\n",
            "Epoch 289/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0758 - val_loss: 0.0762\n",
            "Epoch 290/500\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0758 - val_loss: 0.0768\n",
            "Epoch 291/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0758 - val_loss: 0.0762\n",
            "Epoch 292/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0758 - val_loss: 0.0761\n",
            "Epoch 293/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0758 - val_loss: 0.0761\n",
            "Epoch 294/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0758 - val_loss: 0.0764\n",
            "Epoch 295/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0758 - val_loss: 0.0763\n",
            "Epoch 296/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0758 - val_loss: 0.0768\n",
            "Epoch 297/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0758 - val_loss: 0.0764\n",
            "Epoch 298/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0758 - val_loss: 0.0760\n",
            "Epoch 299/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0758 - val_loss: 0.0761\n",
            "Epoch 300/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0758 - val_loss: 0.0763\n",
            "Epoch 301/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0758 - val_loss: 0.0766\n",
            "Epoch 302/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0758 - val_loss: 0.0762\n",
            "Epoch 303/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0758 - val_loss: 0.0762\n",
            "Epoch 304/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0758 - val_loss: 0.0766\n",
            "Epoch 305/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0758 - val_loss: 0.0767\n",
            "Epoch 306/500\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0758 - val_loss: 0.0764\n",
            "Epoch 307/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0758 - val_loss: 0.0773\n",
            "Epoch 308/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0758 - val_loss: 0.0762\n",
            "Epoch 309/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0757 - val_loss: 0.0759\n",
            "Epoch 310/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0757 - val_loss: 0.0762\n",
            "Epoch 311/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0758 - val_loss: 0.0762\n",
            "Epoch 312/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0757 - val_loss: 0.0762\n",
            "Epoch 313/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0757 - val_loss: 0.0763\n",
            "Epoch 314/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0757 - val_loss: 0.0768\n",
            "Epoch 315/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0757 - val_loss: 0.0763\n",
            "Epoch 316/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0757 - val_loss: 0.0760\n",
            "Epoch 317/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0757 - val_loss: 0.0766\n",
            "Epoch 318/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0757 - val_loss: 0.0763\n",
            "Epoch 319/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0757 - val_loss: 0.0760\n",
            "Epoch 320/500\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0757 - val_loss: 0.0763\n",
            "Epoch 321/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0757 - val_loss: 0.0760\n",
            "Epoch 322/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0757 - val_loss: 0.0760\n",
            "Epoch 323/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0757 - val_loss: 0.0768\n",
            "Epoch 324/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0757 - val_loss: 0.0762\n",
            "Epoch 325/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0757 - val_loss: 0.0762\n",
            "Epoch 326/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0757 - val_loss: 0.0760\n",
            "Epoch 327/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0757 - val_loss: 0.0762\n",
            "Epoch 328/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0757 - val_loss: 0.0760\n",
            "Epoch 329/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0757 - val_loss: 0.0761\n",
            "Epoch 330/500\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0757 - val_loss: 0.0763\n",
            "Epoch 331/500\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0757 - val_loss: 0.0761\n",
            "Epoch 332/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0757 - val_loss: 0.0762\n",
            "Epoch 333/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0757 - val_loss: 0.0760\n",
            "Epoch 334/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0757 - val_loss: 0.0762\n",
            "Epoch 335/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0757 - val_loss: 0.0763\n",
            "Epoch 336/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0757 - val_loss: 0.0761\n",
            "Epoch 337/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0757 - val_loss: 0.0762\n",
            "Epoch 338/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0757 - val_loss: 0.0772\n",
            "Epoch 339/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0757 - val_loss: 0.0762\n",
            "Epoch 340/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0756 - val_loss: 0.0763\n",
            "Epoch 341/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0757 - val_loss: 0.0758\n",
            "Epoch 342/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0757 - val_loss: 0.0761\n",
            "Epoch 343/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0756 - val_loss: 0.0764\n",
            "Epoch 344/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0757 - val_loss: 0.0758\n",
            "Epoch 345/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0757 - val_loss: 0.0760\n",
            "Epoch 346/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0756 - val_loss: 0.0762\n",
            "Epoch 347/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0757 - val_loss: 0.0758\n",
            "Epoch 348/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0756 - val_loss: 0.0759\n",
            "Epoch 349/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0756 - val_loss: 0.0765\n",
            "Epoch 350/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0756 - val_loss: 0.0767\n",
            "Epoch 351/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0757 - val_loss: 0.0763\n",
            "Epoch 352/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0756 - val_loss: 0.0760\n",
            "Epoch 353/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0756 - val_loss: 0.0762\n",
            "Epoch 354/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0756 - val_loss: 0.0760\n",
            "Epoch 355/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0757 - val_loss: 0.0766\n",
            "Epoch 356/500\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0756 - val_loss: 0.0763\n",
            "Epoch 357/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0756 - val_loss: 0.0759\n",
            "Epoch 358/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0756 - val_loss: 0.0761\n",
            "Epoch 359/500\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0756 - val_loss: 0.0764\n",
            "Epoch 360/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0756 - val_loss: 0.0760\n",
            "Epoch 361/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0756 - val_loss: 0.0761\n",
            "Epoch 362/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0756 - val_loss: 0.0760\n",
            "Epoch 363/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0756 - val_loss: 0.0765\n",
            "Epoch 364/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0756 - val_loss: 0.0762\n",
            "Epoch 365/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0756 - val_loss: 0.0765\n",
            "Epoch 366/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0756 - val_loss: 0.0762\n",
            "Epoch 367/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0756 - val_loss: 0.0759\n",
            "Epoch 368/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0756 - val_loss: 0.0767\n",
            "Epoch 369/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0756 - val_loss: 0.0762\n",
            "Epoch 370/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0756 - val_loss: 0.0758\n",
            "Epoch 371/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0756 - val_loss: 0.0761\n",
            "Epoch 372/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0756 - val_loss: 0.0762\n",
            "Epoch 373/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0756 - val_loss: 0.0771\n",
            "Epoch 374/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0756 - val_loss: 0.0758\n",
            "Epoch 375/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0756 - val_loss: 0.0761\n",
            "Epoch 376/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0756 - val_loss: 0.0761\n",
            "Epoch 377/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0756 - val_loss: 0.0758\n",
            "Epoch 378/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0756 - val_loss: 0.0759\n",
            "Epoch 379/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0756 - val_loss: 0.0761\n",
            "Epoch 380/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0756 - val_loss: 0.0758\n",
            "Epoch 381/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0756 - val_loss: 0.0760\n",
            "Epoch 382/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0756 - val_loss: 0.0762\n",
            "Epoch 383/500\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0756 - val_loss: 0.0763\n",
            "Epoch 384/500\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0756 - val_loss: 0.0762\n",
            "Epoch 385/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0756 - val_loss: 0.0759\n",
            "Epoch 386/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0755 - val_loss: 0.0765\n",
            "Epoch 387/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0756 - val_loss: 0.0762\n",
            "Epoch 388/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0755 - val_loss: 0.0762\n",
            "Epoch 389/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0756 - val_loss: 0.0758\n",
            "Epoch 390/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0755 - val_loss: 0.0758\n",
            "Epoch 391/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0756 - val_loss: 0.0761\n",
            "Epoch 392/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0755 - val_loss: 0.0761\n",
            "Epoch 393/500\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0755 - val_loss: 0.0762\n",
            "Epoch 394/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0755 - val_loss: 0.0761\n",
            "Epoch 395/500\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0756 - val_loss: 0.0757\n",
            "Epoch 396/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0756 - val_loss: 0.0759\n",
            "Epoch 397/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0755 - val_loss: 0.0758\n",
            "Epoch 398/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0755 - val_loss: 0.0760\n",
            "Epoch 399/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0755 - val_loss: 0.0760\n",
            "Epoch 400/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0755 - val_loss: 0.0763\n",
            "Epoch 401/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0755 - val_loss: 0.0761\n",
            "Epoch 402/500\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0755 - val_loss: 0.0761\n",
            "Epoch 403/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0755 - val_loss: 0.0762\n",
            "Epoch 404/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0755 - val_loss: 0.0761\n",
            "Epoch 405/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0755 - val_loss: 0.0759\n",
            "Epoch 406/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0755 - val_loss: 0.0759\n",
            "Epoch 407/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0755 - val_loss: 0.0763\n",
            "Epoch 408/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0755 - val_loss: 0.0763\n",
            "Epoch 409/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0755 - val_loss: 0.0760\n",
            "Epoch 410/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0755 - val_loss: 0.0760\n",
            "Epoch 411/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0755 - val_loss: 0.0764\n",
            "Epoch 412/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0755 - val_loss: 0.0762\n",
            "Epoch 413/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0755 - val_loss: 0.0758\n",
            "Epoch 414/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0755 - val_loss: 0.0762\n",
            "Epoch 415/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0755 - val_loss: 0.0760\n",
            "Epoch 416/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0755 - val_loss: 0.0762\n",
            "Epoch 417/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0755 - val_loss: 0.0759\n",
            "Epoch 418/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0755 - val_loss: 0.0764\n",
            "Epoch 419/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0755 - val_loss: 0.0760\n",
            "Epoch 420/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0755 - val_loss: 0.0760\n",
            "Epoch 421/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0755 - val_loss: 0.0760\n",
            "Epoch 422/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0755 - val_loss: 0.0760\n",
            "Epoch 423/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0755 - val_loss: 0.0760\n",
            "Epoch 424/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0755 - val_loss: 0.0759\n",
            "Epoch 425/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0755 - val_loss: 0.0760\n",
            "Epoch 426/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0755 - val_loss: 0.0762\n",
            "Epoch 427/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0755 - val_loss: 0.0759\n",
            "Epoch 428/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0755 - val_loss: 0.0759\n",
            "Epoch 429/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0755 - val_loss: 0.0760\n",
            "Epoch 430/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0755 - val_loss: 0.0757\n",
            "Epoch 431/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0755 - val_loss: 0.0758\n",
            "Epoch 432/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0755 - val_loss: 0.0759\n",
            "Epoch 433/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0755 - val_loss: 0.0759\n",
            "Epoch 434/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0755 - val_loss: 0.0760\n",
            "Epoch 435/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0755 - val_loss: 0.0765\n",
            "Epoch 436/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0755 - val_loss: 0.0759\n",
            "Epoch 437/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0755 - val_loss: 0.0762\n",
            "Epoch 438/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0755 - val_loss: 0.0759\n",
            "Epoch 439/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0755 - val_loss: 0.0760\n",
            "Epoch 440/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0755 - val_loss: 0.0758\n",
            "Epoch 441/500\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0754 - val_loss: 0.0758\n",
            "Epoch 442/500\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0755 - val_loss: 0.0760\n",
            "Epoch 443/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0754 - val_loss: 0.0758\n",
            "Epoch 444/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0755 - val_loss: 0.0761\n",
            "Epoch 445/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0755 - val_loss: 0.0759\n",
            "Epoch 446/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0754 - val_loss: 0.0761\n",
            "Epoch 447/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0754 - val_loss: 0.0759\n",
            "Epoch 448/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0754 - val_loss: 0.0759\n",
            "Epoch 449/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0754 - val_loss: 0.0759\n",
            "Epoch 450/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0754 - val_loss: 0.0762\n",
            "Epoch 451/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0754 - val_loss: 0.0758\n",
            "Epoch 452/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0754 - val_loss: 0.0763\n",
            "Epoch 453/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0754 - val_loss: 0.0760\n",
            "Epoch 454/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0754 - val_loss: 0.0761\n",
            "Epoch 455/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0754 - val_loss: 0.0762\n",
            "Epoch 456/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0755 - val_loss: 0.0765\n",
            "Epoch 457/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0754 - val_loss: 0.0759\n",
            "Epoch 458/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0754 - val_loss: 0.0766\n",
            "Epoch 459/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0754 - val_loss: 0.0760\n",
            "Epoch 460/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0754 - val_loss: 0.0759\n",
            "Epoch 461/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0754 - val_loss: 0.0756\n",
            "Epoch 462/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0754 - val_loss: 0.0761\n",
            "Epoch 463/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0754 - val_loss: 0.0759\n",
            "Epoch 464/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0754 - val_loss: 0.0763\n",
            "Epoch 465/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0755 - val_loss: 0.0761\n",
            "Epoch 466/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0754 - val_loss: 0.0765\n",
            "Epoch 467/500\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0754 - val_loss: 0.0759\n",
            "Epoch 468/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0754 - val_loss: 0.0760\n",
            "Epoch 469/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0754 - val_loss: 0.0757\n",
            "Epoch 470/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0754 - val_loss: 0.0760\n",
            "Epoch 471/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0754 - val_loss: 0.0759\n",
            "Epoch 472/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0754 - val_loss: 0.0760\n",
            "Epoch 473/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0754 - val_loss: 0.0760\n",
            "Epoch 474/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0754 - val_loss: 0.0760\n",
            "Epoch 475/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0754 - val_loss: 0.0757\n",
            "Epoch 476/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0754 - val_loss: 0.0763\n",
            "Epoch 477/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0754 - val_loss: 0.0764\n",
            "Epoch 478/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0754 - val_loss: 0.0759\n",
            "Epoch 479/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0754 - val_loss: 0.0760\n",
            "Epoch 480/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0754 - val_loss: 0.0758\n",
            "Epoch 481/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0754 - val_loss: 0.0758\n",
            "Epoch 482/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0754 - val_loss: 0.0760\n",
            "Epoch 483/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0754 - val_loss: 0.0756\n",
            "Epoch 484/500\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0754 - val_loss: 0.0760\n",
            "Epoch 485/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0754 - val_loss: 0.0759\n",
            "Epoch 486/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0754 - val_loss: 0.0763\n",
            "Epoch 487/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0754 - val_loss: 0.0758\n",
            "Epoch 488/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0754 - val_loss: 0.0759\n",
            "Epoch 489/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0754 - val_loss: 0.0759\n",
            "Epoch 490/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0754 - val_loss: 0.0759\n",
            "Epoch 491/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0754 - val_loss: 0.0760\n",
            "Epoch 492/500\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0754 - val_loss: 0.0759\n",
            "Epoch 493/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0754 - val_loss: 0.0759\n",
            "Epoch 494/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0754 - val_loss: 0.0757\n",
            "Epoch 495/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0754 - val_loss: 0.0757\n",
            "Epoch 496/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0754 - val_loss: 0.0759\n",
            "Epoch 497/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0754 - val_loss: 0.0757\n",
            "Epoch 498/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0754 - val_loss: 0.0760\n",
            "Epoch 499/500\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0754 - val_loss: 0.0757\n",
            "Epoch 500/500\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0753 - val_loss: 0.0762\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f486167bef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMilVr86TE85",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#encode_imgs = encoder.predict(x_test)\n",
        "#decoded_imgs = decoder.predict(encoded_imgs)\n",
        "\n",
        "decoded_imgs = autoencoder.predict(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-kDm-3O-aB1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "673f95ba-463e-4b7e-c60c-0124407f77e2"
      },
      "source": [
        "# use Matplotlib (don't ask)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "n = 10  # how many digits we will display\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "    # display original\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(x_test[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # display reconstruction\n",
        "    ax = plt.subplot(2, n, i + 1 + n)\n",
        "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABG0AAADnCAYAAACkCqtqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBgAAAAAAIIB4aAMAAAAAABBA/w8u9CMGDJOUswAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1440x288 with 20 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "toc-hr-collapsed": true,
        "id": "eDUQMpEb-aB3",
        "colab_type": "text"
      },
      "source": [
        "### Convolutional autoencoder\n",
        "\n",
        "> Since our inputs are images, it makes sense to use convolutional neural networks (convnets) as encoders and decoders. In practical settings, autoencoders applied to images are always convolutional autoencoders --they simply perform much better.\n",
        "\n",
        "> Let's implement one. The encoder will consist in a stack of Conv2D and MaxPooling2D layers (max pooling being used for spatial down-sampling), while the decoder will consist in a stack of Conv2D and UpSampling2D layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YflTAree-aB4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        },
        "outputId": "ac14d7aa-5ab6-46f4-f83e-f7fc409db7bd"
      },
      "source": [
        "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "# Create Model\n",
        "input_img = Input(shape=(28,28,1))\n",
        "l1 = Conv2D(32, (3,3), activation='relu', padding='same')(input_img)\n",
        "l2 = MaxPooling2D((2,2), padding='same')(l1)\n",
        "l3 = Conv2D(16, (3, 3), activation='relu', padding='same')(l2)\n",
        "l4 = MaxPooling2D((2, 2), padding='same')(l3)\n",
        "l5 = Conv2D(16, (3, 3), activation='relu', padding='same')(l4)\n",
        "encoded = MaxPooling2D((2, 2), padding='same')(l5)\n",
        "# at this point the representation is (4, 4, 8i.e. 128-dimensional representation\n",
        "x = Conv2D(16, (3, 3), activation='relu', padding='same')(encoded)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "x = Conv2D(32, (3, 3), activation='relu')(x)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "autoencoder = Model(input_img, decoded)\n",
        "autoencoder.compile(optimizer='nadam', loss='binary_crossentropy')\n",
        "autoencoder.summary()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         (None, 28, 28, 1)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_29 (Conv2D)           (None, 28, 28, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_30 (Conv2D)           (None, 14, 14, 16)        4624      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling (None, 7, 7, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_31 (Conv2D)           (None, 7, 7, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_15 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_32 (Conv2D)           (None, 4, 4, 16)          2320      \n",
            "_________________________________________________________________\n",
            "up_sampling2d_13 (UpSampling (None, 8, 8, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_33 (Conv2D)           (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "up_sampling2d_14 (UpSampling (None, 16, 16, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_34 (Conv2D)           (None, 14, 14, 32)        4640      \n",
            "_________________________________________________________________\n",
            "up_sampling2d_15 (UpSampling (None, 28, 28, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_35 (Conv2D)           (None, 28, 28, 1)         289       \n",
            "=================================================================\n",
            "Total params: 16,833\n",
            "Trainable params: 16,833\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAOJXRLm-aB5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "\n",
        "(x_train, _), (x_test, _) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))  # adapt this if using `channels_first` image data format\n",
        "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))  # adapt this if using `channels_first` image data format"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVUpmdNj-aB7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "afb94965-b779-4f4f-9c48-decbdfa85db3"
      },
      "source": [
        "wandb.init(project=\"autoencoder\", entity=\"ds8\")\n",
        "\n",
        "autoencoder.fit(x_train, x_train,\n",
        "                epochs=100,\n",
        "                batch_size=256,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test, x_test),\n",
        "                verbose=False,\n",
        "                callbacks=[WandbCallback()])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/ds8/autoencoder\" target=\"_blank\">https://app.wandb.ai/ds8/autoencoder</a><br/>\n",
              "                Run page: <a href=\"https://app.wandb.ai/ds8/autoencoder/runs/08v67ucp\" target=\"_blank\">https://app.wandb.ai/ds8/autoencoder/runs/08v67ucp</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f486209bf28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApObQQ5H-aB9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "cab291fd-2d94-4d84-897a-eec5556618a0"
      },
      "source": [
        "decoded_imgs = autoencoder.predict(x_test)\n",
        "\n",
        "n = 10\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "    # display original\n",
        "    ax = plt.subplot(2, n, i+1)\n",
        "    plt.imshow(x_test[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # display reconstruction\n",
        "    ax = plt.subplot(2, n, i + n + 1)\n",
        "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABG0AAADnCAYAAACkCqtqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 1440x288 with 20 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWmOQVAV-aB_",
        "colab_type": "text"
      },
      "source": [
        "#### Visualization of the Representations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbTslCUD-aB_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 699
        },
        "outputId": "82d71717-826f-46d5-bdae-597f1374b0fb"
      },
      "source": [
        "encoder = Model(input_img, encoded)\n",
        "encoder.predict(x_train)\n",
        "\n",
        "n = 10\n",
        "plt.figure(figsize=(20, 8))\n",
        "for i in range(n):\n",
        "    ax = plt.subplot(1, n, i + 1)\n",
        "    plt.imshow(encoded_imgs[i].reshape(4, 4 * 8).T)\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-fc80d405c2ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_imgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_xaxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_visible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 32 into shape (4,32)"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIsAAAHWCAYAAABHfnpiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1440x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dmJrALr-aCD",
        "colab_type": "text"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "You will train an autoencoder at some point in the near future. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4CQFdSg-aCD",
        "colab_type": "text"
      },
      "source": [
        "# Information Retrieval with Autoencoders (Learn)\n",
        "<a id=\"p3\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwZGf10w-aCE",
        "colab_type": "text"
      },
      "source": [
        "## Overview\n",
        "\n",
        "A common usecase for autoencoders is for reverse image search. Let's try to draw an image and see what's most similiar in our dataset. \n",
        "\n",
        "To accomplish this we will need to slice our autoendoer in half to extract our reduced features. :) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Mn5BCZF-aCF",
        "colab_type": "text"
      },
      "source": [
        "## Follow Along"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5No95mIT-aCF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = Model(input_img, encoded)\n",
        "encoded_imgs = encoder.predict(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAcu5CeI-aCH",
        "colab_type": "code",
        "outputId": "234db09c-8ded-4e48-ba33-8a1baae5c76f",
        "colab": {}
      },
      "source": [
        "encoded_imgs[0].T"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.39389127,  1.0158104 ,  0.        ,  0.        ,  0.06517133,\n",
              "        2.450819  ,  0.        ,  5.1117034 ,  0.74338543,  2.3620906 ,\n",
              "        0.        ,  0.        ,  0.        ,  2.0215404 ,  6.1629906 ,\n",
              "        0.6670714 ,  4.66508   ,  2.5439487 , 17.914988  ,  0.        ,\n",
              "        7.9524546 ,  5.3824563 ,  1.0916216 ,  6.234546  ,  0.        ,\n",
              "        0.8884269 ,  7.485719  ,  3.44194   ,  8.927442  ,  0.        ,\n",
              "        0.23894644,  0.        ], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-UnZMDK-aCJ",
        "colab_type": "code",
        "outputId": "633ba476-fe23-4aa9-8f23-b5294b04cba8",
        "colab": {}
      },
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "nn = NearestNeighbors(n_neighbors=10, algorithm='ball_tree')\n",
        "nn.fit(encoded_imgs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NearestNeighbors(algorithm='ball_tree', leaf_size=30, metric='minkowski',\n",
              "                 metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
              "                 radius=1.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqByoQU7-aCL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nn.kneighbors(...)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58uJmGzz-aCN",
        "colab_type": "text"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "You should already be familiar with KNN and similarity queries, so the key component of this section is know what to 'slice' from your autoencoder (the encoder) to extract features from your data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HV5Otyc-aCN",
        "colab_type": "text"
      },
      "source": [
        "# Review\n",
        "\n",
        "* <a href=\"#p1\">Part 1</a>: Describe the componenets of an autoencoder\n",
        "    - Enocder\n",
        "    - Decoder\n",
        "* <a href=\"#p2\">Part 2</a>: Train an autoencoder\n",
        "    - Can do in Keras Easily\n",
        "    - Can use a variety of architectures\n",
        "    - Architectures must follow hourglass shape\n",
        "* <a href=\"#p3\">Part 3</a>: Apply an autoenocder to a basic information retrieval problem\n",
        "    - Extract just the encoder to use for various tasks\n",
        "    - AE ares good for dimensionality reduction, reverse image search, and may more things. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p68U5mye-aCO",
        "colab_type": "text"
      },
      "source": [
        "# Sources\n",
        "\n",
        "__References__\n",
        "- [Building Autoencoders in Keras](https://blog.keras.io/building-autoencoders-in-keras.html)\n",
        "- [Deep Learning Cookbook](http://shop.oreilly.com/product/0636920097471.do)\n",
        "\n",
        "__Additional Material__"
      ]
    }
  ]
}