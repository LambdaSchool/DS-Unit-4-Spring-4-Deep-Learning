{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "LS_DS_433_Autoencoders_Lecture.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y494JHvTc60S",
        "colab_type": "text"
      },
      "source": [
        "Lambda School Data Science\n",
        "\n",
        "*Unit 4, Sprint 3, Module 3*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "By6c_KJWc60X",
        "colab_type": "text"
      },
      "source": [
        "# Autoencoders\n",
        "\n",
        "> An autoencoder is a type of artificial neural network used to learn efficient data codings in an unsupervised manner.[1][2] The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore signal “noise”. Along with the reduction side, a reconstructing side is learnt, where the autoencoder tries to generate from the reduced encoding a representation as close as possible to its original input, hence its name. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8IBborXc60c",
        "colab_type": "text"
      },
      "source": [
        "## Learning Objectives\n",
        "*At the end of the lecture you should be to*:\n",
        "* <a href=\"#p1\">Part 1</a>: Describe the componenets of an autoencoder\n",
        "* <a href=\"#p2\">Part 2</a>: Train an autoencoder\n",
        "* <a href=\"#p3\">Part 3</a>: Apply an autoenocder to a basic information retrieval problem\n",
        "\n",
        "__Problem:__ Is it possible to automatically represent an image as a fixed-sized vector even if it isn’t labeled?\n",
        "\n",
        "__Solution:__ Use an autoencoder\n",
        "\n",
        "Why do we need to represent an image as a fixed-sized vector do you ask? \n",
        "\n",
        "* __Information Retrieval__\n",
        "    - [Reverse Image Search](https://en.wikipedia.org/wiki/Reverse_image_search)\n",
        "    - [Recommendation Systems - Content Based Filtering](https://en.wikipedia.org/wiki/Recommender_system#Content-based_filtering)\n",
        "* __Dimensionality Reduction__\n",
        "    - [Feature Extraction](https://www.kaggle.com/c/vsb-power-line-fault-detection/discussion/78285)\n",
        "    - [Manifold Learning](https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction)\n",
        "\n",
        "We've already seen *representation learning* when we talked about word embedding modelings during our NLP week. Today we're going to achieve a similiar goal on images using *autoencoders*. An autoencoder is a neural network that is trained to attempt to copy its input to its output. Usually they are restricted in ways that allow them to copy only approximately. The model often learns useful properties of the data, because it is forced to prioritize which aspecs of the input should be copied. The properties of autoencoders have made them an important part of modern generative modeling approaches. Consider autoencoders a special case of feed-forward networks (the kind we've been studying); backpropagation and gradient descent still work. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaVuj6ADc60k",
        "colab_type": "text"
      },
      "source": [
        "# Autoencoder Architecture (Learn)\n",
        "<a id=\"p1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKNJU09Uc60m",
        "colab_type": "text"
      },
      "source": [
        "## Overview\n",
        "\n",
        "The *encoder* compresses the input data and the *decoder* does the reverse to produce the uncompressed version of the data to create a reconstruction of the input as accurately as possible:\n",
        "\n",
        "<img src='https://miro.medium.com/max/1400/1*44eDEuZBEsmG_TCAKRI3Kw@2x.png' width=800/>\n",
        "\n",
        "The learning process gis described simply as minimizing a loss function: \n",
        "$ L(x, g(f(x))) $\n",
        "\n",
        "- $L$ is a loss function penalizing $g(f(x))$ for being dissimiliar from $x$ (such as mean squared error)\n",
        "- $f$ is the encoder function\n",
        "- $g$ is the decoder function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGS8O6Nxc60n",
        "colab_type": "text"
      },
      "source": [
        "## Follow Along\n",
        "### Extremely Simple Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rIjPLsedfaF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "outputId": "c8e8cefc-8232-405c-b8c6-a6b2bb4b3cb9"
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 7308862284642268071\n",
            ", name: \"/device:XLA_CPU:0\"\n",
            "device_type: \"XLA_CPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 6038382861715628670\n",
            "physical_device_desc: \"device: XLA_CPU device\"\n",
            ", name: \"/device:XLA_GPU:0\"\n",
            "device_type: \"XLA_GPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 4929374347065904686\n",
            "physical_device_desc: \"device: XLA_GPU device\"\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 14912199066\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 8834225937001702791\n",
            "physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKuconOJdqSt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 989
        },
        "outputId": "3a8381b2-7cdd-47e0-a93e-f0826662adb1"
      },
      "source": [
        "!pip install tensorflow==2.0.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/0f/7bd55361168bb32796b360ad15a25de6966c9c1beb58a8e30c01c8279862/tensorflow-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (86.3MB)\n",
            "\u001b[K     |████████████████████████████████| 86.3MB 52kB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.34.2)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.2.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.8.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.9.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.17.5)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (3.10.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (3.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.1.0)\n",
            "Collecting tensorboard<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/54/99b9d5d52d5cb732f099baaaf7740403e83fe6b0cedde940fabd2b13d75a/tensorboard-2.0.2-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 46.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.27.1)\n",
            "Collecting tensorflow-estimator<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 55.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.12.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.1.8)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.11.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0.0) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==2.0.0) (45.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.2.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.7.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.21.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.1)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.3.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.0)\n",
            "Installing collected packages: tensorboard, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: tensorflow 1.15.0\n",
            "    Uninstalling tensorflow-1.15.0:\n",
            "      Successfully uninstalled tensorflow-1.15.0\n",
            "Successfully installed tensorboard-2.0.2 tensorflow-2.0.0 tensorflow-estimator-2.0.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow",
                  "tensorflow_core",
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5x2WDpkc60p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "2c66d0cb-0288-4491-86bb-e2c56ff7c2c7"
      },
      "source": [
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "# import wandb\n",
        "# from wandb.keras import WandbCallback\n",
        "\n",
        "# this is the size of our encoded representations\n",
        "encoding_dim = 32  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
        "\n",
        "# this is our input placeholder\n",
        "input_img = Input(shape=(784,))\n",
        "\n",
        "# \"encoded\" is the encoded representation of the input\n",
        "encoded = Dense(encoding_dim, activation='relu')(input_img)\n",
        "\n",
        "# \"decoded\" is the lossy reconstruction of the input\n",
        "decoded = Dense(784, activation='sigmoid')(encoded)\n",
        "\n",
        "# this model maps an input to its reconstruction\n",
        "autoencoder = Model(input_img, decoded)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naIH_NFwc60w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this model maps an input to its encoded representation\n",
        "encoder = Model(input_img, encoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsSb26BWc604",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a Seperate Decoder Model (Demo only)\n",
        "\n",
        "# create a placeholder for an encoded (32-dimensional) input\n",
        "encoded_input = Input(shape=(encoding_dim,))\n",
        "\n",
        "# retrieve the last layer of the autoencoder model\n",
        "decoded_layer = autoencoder.layers[-1]\n",
        "\n",
        "# create the decoder model\n",
        "decoder = Model(encoded_input, decoded_layer(encoded_input))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCojueDPc61D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "33aab91c-0afd-4cd5-db8d-4c97d51d2817"
      },
      "source": [
        "autoencoder.compile(optimizer='nadam', loss='binary_crossentropy')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xa7bO3-ec61M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6e42341b-cfa0-4364-e139-3e8af7f9408a"
      },
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "import numpy as np\n",
        "(x_train, _), (x_test, _) = mnist.load_data()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ML3m-g-Yc61c",
        "colab_type": "code",
        "outputId": "8551b0d5-2e3e-4857-93a2-4d70250bb266",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
        "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 784)\n",
            "(10000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-pf4J7vc61m",
        "colab_type": "code",
        "outputId": "7ab1fe77-b3d0-49cf-e4a1-3db3ace492d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# wandb.init(project=\"mnist_autoencoder\", entity=\"ds5\")\n",
        "\n",
        "autoencoder.fit(x_train, x_train,\n",
        "                epochs=1000,\n",
        "                batch_size=500,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test, x_test),\n",
        "                verbose = True)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f2f67414940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i86gH08Lc61t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# encode and decode some digits\n",
        "# note that we take them from the *test* set\n",
        "decoded_imgs = autoencoder.predict(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEAAi3vwmjRb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternative approach\n",
        "encoded_imgs = encoder.predict(x_test)\n",
        "decoced_imgs = decoder.predict(encoded_imgs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgpDngMsmx9p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "c1a5626a-3e52-420b-de53-b920c842bb2e"
      },
      "source": [
        "encoded_imgs[0]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.59904784, 0.91343904, 6.84451   , 0.7761507 , 0.        ,\n",
              "       6.043824  , 1.1888195 , 8.241451  , 0.1232598 , 4.338213  ,\n",
              "       0.71050394, 0.        , 0.        , 7.6470156 , 0.82148266,\n",
              "       0.055572  , 0.        , 4.360263  , 0.        , 0.15048245,\n",
              "       3.3027027 , 0.        , 1.5447372 , 1.8212687 , 0.        ,\n",
              "       0.6137589 , 2.449291  , 0.        , 0.        , 3.8101184 ,\n",
              "       6.6349897 , 0.        ], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eabRbs-c61_",
        "colab_type": "code",
        "outputId": "d4633a16-0e80-4d1b-9af1-26c0302f4cb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        }
      },
      "source": [
        "# use Matplotlib (don't ask)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "n = 10  # how many digits we will display\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "    # display original\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(x_test[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # display reconstruction\n",
        "    ax = plt.subplot(2, n, i + 1 + n)\n",
        "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABG0AAADnCAYAAACkCqtqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nD5gp5qElXK3yAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1440x288 with 20 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEYwycXvc62O",
        "colab_type": "text"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "Expected to talk about the components of autoencoder and their purpose. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RF0pP-gBc62P",
        "colab_type": "text"
      },
      "source": [
        "# Train an Autoencoder (Learn)\n",
        "<a id=\"p2\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxV_d1moc62Q",
        "colab_type": "text"
      },
      "source": [
        "## Overview\n",
        "\n",
        "As long as our architecture maintains an hourglass shape, we can continue to add layers and create a deeper network. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "toc-hr-collapsed": true,
        "id": "D2jIdr5lc62S",
        "colab_type": "text"
      },
      "source": [
        "## Follow Along"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8IYbqUmc62U",
        "colab_type": "text"
      },
      "source": [
        "### Deep Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ok-xMF5qc62X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_img = Input(shape=(784,))\n",
        "\n",
        "encoded = Dense(128, activation='relu')(input_img)\n",
        "encoded = Dense(64, activation='relu')(encoded)\n",
        "encoded = Dense(32, activation='relu')(encoded) # => Our dry strawberry\n",
        "\n",
        "decoded = Dense(64, activation='relu')(encoded)\n",
        "decoded = Dense(128, activation='relu')(decoded)\n",
        "decoded = Dense(784, activation='sigmoid')(decoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B56A5J0xc62d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "52dd99ff-0438-4b42-d00e-7c1fc4950666"
      },
      "source": [
        "# compile & fit model\n",
        "\n",
        "autoencoder = Model(input_img, decoded)\n",
        "\n",
        "autoencoder.compile(optimizer='nadam',\n",
        "                    loss='binary_crossentropy')\n",
        "\n",
        "autoencoder.fit(x_train, x_train,\n",
        "                epochs=1000,\n",
        "                batch_size=500,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test, x_test),\n",
        "                verbose = True)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1000\n",
            "60000/60000 [==============================] - 1s 22us/sample - loss: 0.2794 - val_loss: 0.2008\n",
            "Epoch 2/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.1830 - val_loss: 0.1714\n",
            "Epoch 3/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.1650 - val_loss: 0.1572\n",
            "Epoch 4/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.1511 - val_loss: 0.1430\n",
            "Epoch 5/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.1429 - val_loss: 0.1374\n",
            "Epoch 6/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.1374 - val_loss: 0.1342\n",
            "Epoch 7/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.1324 - val_loss: 0.1305\n",
            "Epoch 8/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.1287 - val_loss: 0.1256\n",
            "Epoch 9/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.1257 - val_loss: 0.1265\n",
            "Epoch 10/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.1229 - val_loss: 0.1203\n",
            "Epoch 11/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.1206 - val_loss: 0.1194\n",
            "Epoch 12/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.1183 - val_loss: 0.1164\n",
            "Epoch 13/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.1168 - val_loss: 0.1148\n",
            "Epoch 14/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.1149 - val_loss: 0.1146\n",
            "Epoch 15/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.1134 - val_loss: 0.1109\n",
            "Epoch 16/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.1121 - val_loss: 0.1111\n",
            "Epoch 17/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.1108 - val_loss: 0.1083\n",
            "Epoch 18/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.1097 - val_loss: 0.1086\n",
            "Epoch 19/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.1086 - val_loss: 0.1077\n",
            "Epoch 20/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.1076 - val_loss: 0.1062\n",
            "Epoch 21/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.1065 - val_loss: 0.1052\n",
            "Epoch 22/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.1056 - val_loss: 0.1056\n",
            "Epoch 23/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.1047 - val_loss: 0.1042\n",
            "Epoch 24/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.1040 - val_loss: 0.1059\n",
            "Epoch 25/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.1031 - val_loss: 0.1009\n",
            "Epoch 26/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.1025 - val_loss: 0.1007\n",
            "Epoch 27/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.1016 - val_loss: 0.1007\n",
            "Epoch 28/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.1010 - val_loss: 0.0987\n",
            "Epoch 29/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.1004 - val_loss: 0.0989\n",
            "Epoch 30/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0996 - val_loss: 0.0994\n",
            "Epoch 31/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0990 - val_loss: 0.0990\n",
            "Epoch 32/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0985 - val_loss: 0.0963\n",
            "Epoch 33/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0979 - val_loss: 0.0971\n",
            "Epoch 34/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0975 - val_loss: 0.0974\n",
            "Epoch 35/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0969 - val_loss: 0.0960\n",
            "Epoch 36/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0964 - val_loss: 0.0950\n",
            "Epoch 37/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0960 - val_loss: 0.0944\n",
            "Epoch 38/1000\n",
            "60000/60000 [==============================] - 1s 16us/sample - loss: 0.0956 - val_loss: 0.0935\n",
            "Epoch 39/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0951 - val_loss: 0.0935\n",
            "Epoch 40/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0949 - val_loss: 0.0935\n",
            "Epoch 41/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0943 - val_loss: 0.0927\n",
            "Epoch 42/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0938 - val_loss: 0.0928\n",
            "Epoch 43/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0937 - val_loss: 0.0925\n",
            "Epoch 44/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0933 - val_loss: 0.0917\n",
            "Epoch 45/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0930 - val_loss: 0.0922\n",
            "Epoch 46/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0926 - val_loss: 0.0916\n",
            "Epoch 47/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0922 - val_loss: 0.0906\n",
            "Epoch 48/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0921 - val_loss: 0.0914\n",
            "Epoch 49/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0917 - val_loss: 0.0905\n",
            "Epoch 50/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0914 - val_loss: 0.0905\n",
            "Epoch 51/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0911 - val_loss: 0.0895\n",
            "Epoch 52/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0909 - val_loss: 0.0900\n",
            "Epoch 53/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0906 - val_loss: 0.0906\n",
            "Epoch 54/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0904 - val_loss: 0.0893\n",
            "Epoch 55/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0901 - val_loss: 0.0890\n",
            "Epoch 56/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0899 - val_loss: 0.0889\n",
            "Epoch 57/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0897 - val_loss: 0.0894\n",
            "Epoch 58/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0895 - val_loss: 0.0882\n",
            "Epoch 59/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0892 - val_loss: 0.0897\n",
            "Epoch 60/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0891 - val_loss: 0.0888\n",
            "Epoch 61/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0888 - val_loss: 0.0882\n",
            "Epoch 62/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0888 - val_loss: 0.0873\n",
            "Epoch 63/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0885 - val_loss: 0.0883\n",
            "Epoch 64/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0884 - val_loss: 0.0892\n",
            "Epoch 65/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0881 - val_loss: 0.0884\n",
            "Epoch 66/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0879 - val_loss: 0.0880\n",
            "Epoch 67/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0878 - val_loss: 0.0869\n",
            "Epoch 68/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0876 - val_loss: 0.0866\n",
            "Epoch 69/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0875 - val_loss: 0.0875\n",
            "Epoch 70/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0873 - val_loss: 0.0872\n",
            "Epoch 71/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0873 - val_loss: 0.0857\n",
            "Epoch 72/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0871 - val_loss: 0.0861\n",
            "Epoch 73/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0868 - val_loss: 0.0859\n",
            "Epoch 74/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0868 - val_loss: 0.0863\n",
            "Epoch 75/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0866 - val_loss: 0.0866\n",
            "Epoch 76/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0864 - val_loss: 0.0853\n",
            "Epoch 77/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0864 - val_loss: 0.0869\n",
            "Epoch 78/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0862 - val_loss: 0.0856\n",
            "Epoch 79/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0860 - val_loss: 0.0851\n",
            "Epoch 80/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0859 - val_loss: 0.0857\n",
            "Epoch 81/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0858 - val_loss: 0.0850\n",
            "Epoch 82/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0857 - val_loss: 0.0856\n",
            "Epoch 83/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0854 - val_loss: 0.0853\n",
            "Epoch 84/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0855 - val_loss: 0.0850\n",
            "Epoch 85/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0853 - val_loss: 0.0853\n",
            "Epoch 86/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0851 - val_loss: 0.0846\n",
            "Epoch 87/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0851 - val_loss: 0.0852\n",
            "Epoch 88/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0850 - val_loss: 0.0866\n",
            "Epoch 89/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0849 - val_loss: 0.0841\n",
            "Epoch 90/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0847 - val_loss: 0.0844\n",
            "Epoch 91/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0847 - val_loss: 0.0837\n",
            "Epoch 92/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0845 - val_loss: 0.0839\n",
            "Epoch 93/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0844 - val_loss: 0.0839\n",
            "Epoch 94/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0844 - val_loss: 0.0839\n",
            "Epoch 95/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0842 - val_loss: 0.0838\n",
            "Epoch 96/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0842 - val_loss: 0.0843\n",
            "Epoch 97/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0841 - val_loss: 0.0834\n",
            "Epoch 98/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0840 - val_loss: 0.0834\n",
            "Epoch 99/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0839 - val_loss: 0.0833\n",
            "Epoch 100/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0838 - val_loss: 0.0838\n",
            "Epoch 101/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0837 - val_loss: 0.0837\n",
            "Epoch 102/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0836 - val_loss: 0.0834\n",
            "Epoch 103/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0836 - val_loss: 0.0834\n",
            "Epoch 104/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0835 - val_loss: 0.0832\n",
            "Epoch 105/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0835 - val_loss: 0.0834\n",
            "Epoch 106/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0834 - val_loss: 0.0827\n",
            "Epoch 107/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0833 - val_loss: 0.0829\n",
            "Epoch 108/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0832 - val_loss: 0.0833\n",
            "Epoch 109/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0832 - val_loss: 0.0829\n",
            "Epoch 110/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0831 - val_loss: 0.0825\n",
            "Epoch 111/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0831 - val_loss: 0.0831\n",
            "Epoch 112/1000\n",
            "60000/60000 [==============================] - 1s 16us/sample - loss: 0.0831 - val_loss: 0.0826\n",
            "Epoch 113/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0829 - val_loss: 0.0831\n",
            "Epoch 114/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0829 - val_loss: 0.0828\n",
            "Epoch 115/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0828 - val_loss: 0.0821\n",
            "Epoch 116/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0828 - val_loss: 0.0822\n",
            "Epoch 117/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0827 - val_loss: 0.0823\n",
            "Epoch 118/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0827 - val_loss: 0.0823\n",
            "Epoch 119/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0826 - val_loss: 0.0828\n",
            "Epoch 120/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0826 - val_loss: 0.0833\n",
            "Epoch 121/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0825 - val_loss: 0.0823\n",
            "Epoch 122/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0825 - val_loss: 0.0823\n",
            "Epoch 123/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0824 - val_loss: 0.0829\n",
            "Epoch 124/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0823 - val_loss: 0.0819\n",
            "Epoch 125/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0823 - val_loss: 0.0827\n",
            "Epoch 126/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0823 - val_loss: 0.0820\n",
            "Epoch 127/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0822 - val_loss: 0.0816\n",
            "Epoch 128/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0822 - val_loss: 0.0820\n",
            "Epoch 129/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0821 - val_loss: 0.0823\n",
            "Epoch 130/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0821 - val_loss: 0.0816\n",
            "Epoch 131/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0820 - val_loss: 0.0816\n",
            "Epoch 132/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0819 - val_loss: 0.0817\n",
            "Epoch 133/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0819 - val_loss: 0.0819\n",
            "Epoch 134/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0819 - val_loss: 0.0814\n",
            "Epoch 135/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0818 - val_loss: 0.0819\n",
            "Epoch 136/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0818 - val_loss: 0.0819\n",
            "Epoch 137/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0817 - val_loss: 0.0815\n",
            "Epoch 138/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0817 - val_loss: 0.0815\n",
            "Epoch 139/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0817 - val_loss: 0.0816\n",
            "Epoch 140/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0816 - val_loss: 0.0812\n",
            "Epoch 141/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0816 - val_loss: 0.0814\n",
            "Epoch 142/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0815 - val_loss: 0.0813\n",
            "Epoch 143/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0815 - val_loss: 0.0810\n",
            "Epoch 144/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0814 - val_loss: 0.0816\n",
            "Epoch 145/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0814 - val_loss: 0.0815\n",
            "Epoch 146/1000\n",
            "60000/60000 [==============================] - 1s 17us/sample - loss: 0.0814 - val_loss: 0.0811\n",
            "Epoch 147/1000\n",
            "60000/60000 [==============================] - 1s 16us/sample - loss: 0.0813 - val_loss: 0.0813\n",
            "Epoch 148/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0813 - val_loss: 0.0814\n",
            "Epoch 149/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0812 - val_loss: 0.0810\n",
            "Epoch 150/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0811 - val_loss: 0.0810\n",
            "Epoch 151/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0812 - val_loss: 0.0812\n",
            "Epoch 152/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0811 - val_loss: 0.0813\n",
            "Epoch 153/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0810 - val_loss: 0.0805\n",
            "Epoch 154/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0811 - val_loss: 0.0812\n",
            "Epoch 155/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0810 - val_loss: 0.0808\n",
            "Epoch 156/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0810 - val_loss: 0.0806\n",
            "Epoch 157/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0809 - val_loss: 0.0808\n",
            "Epoch 158/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0809 - val_loss: 0.0809\n",
            "Epoch 159/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0808 - val_loss: 0.0805\n",
            "Epoch 160/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0808 - val_loss: 0.0807\n",
            "Epoch 161/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0807 - val_loss: 0.0806\n",
            "Epoch 162/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0808 - val_loss: 0.0808\n",
            "Epoch 163/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0807 - val_loss: 0.0806\n",
            "Epoch 164/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0806 - val_loss: 0.0804\n",
            "Epoch 165/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0807 - val_loss: 0.0810\n",
            "Epoch 166/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0806 - val_loss: 0.0810\n",
            "Epoch 167/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0806 - val_loss: 0.0803\n",
            "Epoch 168/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0805 - val_loss: 0.0802\n",
            "Epoch 169/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0805 - val_loss: 0.0806\n",
            "Epoch 170/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0805 - val_loss: 0.0815\n",
            "Epoch 171/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0805 - val_loss: 0.0804\n",
            "Epoch 172/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0804 - val_loss: 0.0807\n",
            "Epoch 173/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0804 - val_loss: 0.0806\n",
            "Epoch 174/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0803 - val_loss: 0.0806\n",
            "Epoch 175/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0804 - val_loss: 0.0804\n",
            "Epoch 176/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0803 - val_loss: 0.0808\n",
            "Epoch 177/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0803 - val_loss: 0.0801\n",
            "Epoch 178/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0803 - val_loss: 0.0803\n",
            "Epoch 179/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0802 - val_loss: 0.0805\n",
            "Epoch 180/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0802 - val_loss: 0.0802\n",
            "Epoch 181/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0803 - val_loss: 0.0803\n",
            "Epoch 182/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0801 - val_loss: 0.0800\n",
            "Epoch 183/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0802 - val_loss: 0.0798\n",
            "Epoch 184/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0801 - val_loss: 0.0802\n",
            "Epoch 185/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0801 - val_loss: 0.0801\n",
            "Epoch 186/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0801 - val_loss: 0.0799\n",
            "Epoch 187/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0800 - val_loss: 0.0801\n",
            "Epoch 188/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0801 - val_loss: 0.0797\n",
            "Epoch 189/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0800 - val_loss: 0.0801\n",
            "Epoch 190/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0799 - val_loss: 0.0804\n",
            "Epoch 191/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0800 - val_loss: 0.0800\n",
            "Epoch 192/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0799 - val_loss: 0.0801\n",
            "Epoch 193/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0799 - val_loss: 0.0798\n",
            "Epoch 194/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0799 - val_loss: 0.0799\n",
            "Epoch 195/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0799 - val_loss: 0.0798\n",
            "Epoch 196/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0798 - val_loss: 0.0797\n",
            "Epoch 197/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0798 - val_loss: 0.0801\n",
            "Epoch 198/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0798 - val_loss: 0.0796\n",
            "Epoch 199/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0798 - val_loss: 0.0803\n",
            "Epoch 200/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0798 - val_loss: 0.0800\n",
            "Epoch 201/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0798 - val_loss: 0.0800\n",
            "Epoch 202/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0798 - val_loss: 0.0799\n",
            "Epoch 203/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0797 - val_loss: 0.0805\n",
            "Epoch 204/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0798 - val_loss: 0.0795\n",
            "Epoch 205/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0796 - val_loss: 0.0801\n",
            "Epoch 206/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0797 - val_loss: 0.0795\n",
            "Epoch 207/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0797 - val_loss: 0.0800\n",
            "Epoch 208/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0796 - val_loss: 0.0799\n",
            "Epoch 209/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0796 - val_loss: 0.0806\n",
            "Epoch 210/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0796 - val_loss: 0.0798\n",
            "Epoch 211/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0796 - val_loss: 0.0795\n",
            "Epoch 212/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0796 - val_loss: 0.0793\n",
            "Epoch 213/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0795 - val_loss: 0.0798\n",
            "Epoch 214/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0795 - val_loss: 0.0799\n",
            "Epoch 215/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0795 - val_loss: 0.0795\n",
            "Epoch 216/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0795 - val_loss: 0.0795\n",
            "Epoch 217/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0795 - val_loss: 0.0792\n",
            "Epoch 218/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0794 - val_loss: 0.0801\n",
            "Epoch 219/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0795 - val_loss: 0.0793\n",
            "Epoch 220/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0795 - val_loss: 0.0796\n",
            "Epoch 221/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0795 - val_loss: 0.0795\n",
            "Epoch 222/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0794 - val_loss: 0.0794\n",
            "Epoch 223/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0794 - val_loss: 0.0794\n",
            "Epoch 224/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0794 - val_loss: 0.0795\n",
            "Epoch 225/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0794 - val_loss: 0.0795\n",
            "Epoch 226/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0794 - val_loss: 0.0794\n",
            "Epoch 227/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0794 - val_loss: 0.0796\n",
            "Epoch 228/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0794 - val_loss: 0.0795\n",
            "Epoch 229/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0794 - val_loss: 0.0794\n",
            "Epoch 230/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0793 - val_loss: 0.0794\n",
            "Epoch 231/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0793 - val_loss: 0.0795\n",
            "Epoch 232/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0793 - val_loss: 0.0794\n",
            "Epoch 233/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0793 - val_loss: 0.0793\n",
            "Epoch 234/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0793 - val_loss: 0.0794\n",
            "Epoch 235/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0792 - val_loss: 0.0792\n",
            "Epoch 236/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0793 - val_loss: 0.0794\n",
            "Epoch 237/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0793 - val_loss: 0.0793\n",
            "Epoch 238/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0792 - val_loss: 0.0790\n",
            "Epoch 239/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0792 - val_loss: 0.0799\n",
            "Epoch 240/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0792 - val_loss: 0.0793\n",
            "Epoch 241/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0792 - val_loss: 0.0793\n",
            "Epoch 242/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0791 - val_loss: 0.0791\n",
            "Epoch 243/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0792 - val_loss: 0.0790\n",
            "Epoch 244/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0792 - val_loss: 0.0797\n",
            "Epoch 245/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0792 - val_loss: 0.0791\n",
            "Epoch 246/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0791 - val_loss: 0.0793\n",
            "Epoch 247/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0791 - val_loss: 0.0791\n",
            "Epoch 248/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0792 - val_loss: 0.0792\n",
            "Epoch 249/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0791 - val_loss: 0.0793\n",
            "Epoch 250/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0791 - val_loss: 0.0794\n",
            "Epoch 251/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0791 - val_loss: 0.0791\n",
            "Epoch 252/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0791 - val_loss: 0.0792\n",
            "Epoch 253/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0791 - val_loss: 0.0793\n",
            "Epoch 254/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0791 - val_loss: 0.0795\n",
            "Epoch 255/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0791 - val_loss: 0.0794\n",
            "Epoch 256/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0791 - val_loss: 0.0789\n",
            "Epoch 257/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0790 - val_loss: 0.0792\n",
            "Epoch 258/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0790 - val_loss: 0.0795\n",
            "Epoch 259/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0790 - val_loss: 0.0790\n",
            "Epoch 260/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0790 - val_loss: 0.0792\n",
            "Epoch 261/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0790 - val_loss: 0.0792\n",
            "Epoch 262/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0790 - val_loss: 0.0790\n",
            "Epoch 263/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0789 - val_loss: 0.0788\n",
            "Epoch 264/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0790 - val_loss: 0.0789\n",
            "Epoch 265/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0790 - val_loss: 0.0792\n",
            "Epoch 266/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0790 - val_loss: 0.0788\n",
            "Epoch 267/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0789 - val_loss: 0.0793\n",
            "Epoch 268/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0789 - val_loss: 0.0789\n",
            "Epoch 269/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0789 - val_loss: 0.0793\n",
            "Epoch 270/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0789 - val_loss: 0.0793\n",
            "Epoch 271/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0789 - val_loss: 0.0795\n",
            "Epoch 272/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0789 - val_loss: 0.0791\n",
            "Epoch 273/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0789 - val_loss: 0.0789\n",
            "Epoch 274/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0789 - val_loss: 0.0795\n",
            "Epoch 275/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0789 - val_loss: 0.0789\n",
            "Epoch 276/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0789 - val_loss: 0.0792\n",
            "Epoch 277/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0789 - val_loss: 0.0789\n",
            "Epoch 278/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0789 - val_loss: 0.0793\n",
            "Epoch 279/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0789 - val_loss: 0.0788\n",
            "Epoch 280/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0788 - val_loss: 0.0792\n",
            "Epoch 281/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0789 - val_loss: 0.0791\n",
            "Epoch 282/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0788 - val_loss: 0.0791\n",
            "Epoch 283/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0788 - val_loss: 0.0791\n",
            "Epoch 284/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0789 - val_loss: 0.0793\n",
            "Epoch 285/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0788 - val_loss: 0.0786\n",
            "Epoch 286/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0788 - val_loss: 0.0789\n",
            "Epoch 287/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0788 - val_loss: 0.0789\n",
            "Epoch 288/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0788 - val_loss: 0.0788\n",
            "Epoch 289/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0788 - val_loss: 0.0790\n",
            "Epoch 290/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0788 - val_loss: 0.0788\n",
            "Epoch 291/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0787 - val_loss: 0.0787\n",
            "Epoch 292/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0788 - val_loss: 0.0787\n",
            "Epoch 293/1000\n",
            "60000/60000 [==============================] - 1s 16us/sample - loss: 0.0787 - val_loss: 0.0794\n",
            "Epoch 294/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0787 - val_loss: 0.0788\n",
            "Epoch 295/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0787 - val_loss: 0.0790\n",
            "Epoch 296/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0787 - val_loss: 0.0789\n",
            "Epoch 297/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0787 - val_loss: 0.0790\n",
            "Epoch 298/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0787 - val_loss: 0.0789\n",
            "Epoch 299/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0787 - val_loss: 0.0788\n",
            "Epoch 300/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0787 - val_loss: 0.0789\n",
            "Epoch 301/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0787 - val_loss: 0.0789\n",
            "Epoch 302/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0787 - val_loss: 0.0787\n",
            "Epoch 303/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0786 - val_loss: 0.0787\n",
            "Epoch 304/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0787 - val_loss: 0.0789\n",
            "Epoch 305/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0787 - val_loss: 0.0787\n",
            "Epoch 306/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0787 - val_loss: 0.0788\n",
            "Epoch 307/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0786 - val_loss: 0.0787\n",
            "Epoch 308/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0787 - val_loss: 0.0789\n",
            "Epoch 309/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0786 - val_loss: 0.0792\n",
            "Epoch 310/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0786 - val_loss: 0.0786\n",
            "Epoch 311/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0787 - val_loss: 0.0789\n",
            "Epoch 312/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0786 - val_loss: 0.0787\n",
            "Epoch 313/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0786 - val_loss: 0.0788\n",
            "Epoch 314/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0786 - val_loss: 0.0787\n",
            "Epoch 315/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0786 - val_loss: 0.0788\n",
            "Epoch 316/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0786 - val_loss: 0.0786\n",
            "Epoch 317/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0785 - val_loss: 0.0788\n",
            "Epoch 318/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0786 - val_loss: 0.0785\n",
            "Epoch 319/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0786 - val_loss: 0.0787\n",
            "Epoch 320/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0786 - val_loss: 0.0786\n",
            "Epoch 321/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0786 - val_loss: 0.0789\n",
            "Epoch 322/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0786 - val_loss: 0.0788\n",
            "Epoch 323/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0785 - val_loss: 0.0785\n",
            "Epoch 324/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0786 - val_loss: 0.0786\n",
            "Epoch 325/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0786 - val_loss: 0.0790\n",
            "Epoch 326/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0785 - val_loss: 0.0786\n",
            "Epoch 327/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0786 - val_loss: 0.0788\n",
            "Epoch 328/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0785 - val_loss: 0.0793\n",
            "Epoch 329/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0785 - val_loss: 0.0787\n",
            "Epoch 330/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0785 - val_loss: 0.0788\n",
            "Epoch 331/1000\n",
            "60000/60000 [==============================] - 1s 17us/sample - loss: 0.0786 - val_loss: 0.0788\n",
            "Epoch 332/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0785 - val_loss: 0.0788\n",
            "Epoch 333/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0786 - val_loss: 0.0788\n",
            "Epoch 334/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0785 - val_loss: 0.0786\n",
            "Epoch 335/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0785 - val_loss: 0.0786\n",
            "Epoch 336/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0785 - val_loss: 0.0789\n",
            "Epoch 337/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0785 - val_loss: 0.0784\n",
            "Epoch 338/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0785 - val_loss: 0.0786\n",
            "Epoch 339/1000\n",
            "60000/60000 [==============================] - 1s 16us/sample - loss: 0.0785 - val_loss: 0.0785\n",
            "Epoch 340/1000\n",
            "60000/60000 [==============================] - 1s 16us/sample - loss: 0.0785 - val_loss: 0.0792\n",
            "Epoch 341/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0785 - val_loss: 0.0787\n",
            "Epoch 342/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0785 - val_loss: 0.0784\n",
            "Epoch 343/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0784 - val_loss: 0.0785\n",
            "Epoch 344/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0785 - val_loss: 0.0787\n",
            "Epoch 345/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0784 - val_loss: 0.0789\n",
            "Epoch 346/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0784 - val_loss: 0.0787\n",
            "Epoch 347/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0785 - val_loss: 0.0789\n",
            "Epoch 348/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0785 - val_loss: 0.0786\n",
            "Epoch 349/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0785 - val_loss: 0.0788\n",
            "Epoch 350/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0784 - val_loss: 0.0786\n",
            "Epoch 351/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0784 - val_loss: 0.0788\n",
            "Epoch 352/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0784 - val_loss: 0.0786\n",
            "Epoch 353/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0784 - val_loss: 0.0784\n",
            "Epoch 354/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0784 - val_loss: 0.0787\n",
            "Epoch 355/1000\n",
            "60000/60000 [==============================] - 1s 16us/sample - loss: 0.0785 - val_loss: 0.0785\n",
            "Epoch 356/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0784 - val_loss: 0.0786\n",
            "Epoch 357/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0784 - val_loss: 0.0785\n",
            "Epoch 358/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0784 - val_loss: 0.0788\n",
            "Epoch 359/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0784 - val_loss: 0.0784\n",
            "Epoch 360/1000\n",
            "60000/60000 [==============================] - 1s 16us/sample - loss: 0.0784 - val_loss: 0.0786\n",
            "Epoch 361/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0784 - val_loss: 0.0785\n",
            "Epoch 362/1000\n",
            "60000/60000 [==============================] - 1s 17us/sample - loss: 0.0784 - val_loss: 0.0785\n",
            "Epoch 363/1000\n",
            "60000/60000 [==============================] - 1s 17us/sample - loss: 0.0784 - val_loss: 0.0784\n",
            "Epoch 364/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0784 - val_loss: 0.0787\n",
            "Epoch 365/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0783 - val_loss: 0.0787\n",
            "Epoch 366/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0784 - val_loss: 0.0786\n",
            "Epoch 367/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0784 - val_loss: 0.0785\n",
            "Epoch 368/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0784 - val_loss: 0.0787\n",
            "Epoch 369/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0784 - val_loss: 0.0788\n",
            "Epoch 370/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0783 - val_loss: 0.0784\n",
            "Epoch 371/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0784 - val_loss: 0.0786\n",
            "Epoch 372/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0783 - val_loss: 0.0786\n",
            "Epoch 373/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0784 - val_loss: 0.0786\n",
            "Epoch 374/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0783 - val_loss: 0.0788\n",
            "Epoch 375/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0783 - val_loss: 0.0785\n",
            "Epoch 376/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0783 - val_loss: 0.0785\n",
            "Epoch 377/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0783 - val_loss: 0.0785\n",
            "Epoch 378/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0783 - val_loss: 0.0784\n",
            "Epoch 379/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0784 - val_loss: 0.0785\n",
            "Epoch 380/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0783 - val_loss: 0.0785\n",
            "Epoch 381/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0783 - val_loss: 0.0787\n",
            "Epoch 382/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0783 - val_loss: 0.0785\n",
            "Epoch 383/1000\n",
            "60000/60000 [==============================] - 1s 16us/sample - loss: 0.0783 - val_loss: 0.0788\n",
            "Epoch 384/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0783 - val_loss: 0.0786\n",
            "Epoch 385/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0783 - val_loss: 0.0784\n",
            "Epoch 386/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0783 - val_loss: 0.0785\n",
            "Epoch 387/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0783 - val_loss: 0.0786\n",
            "Epoch 388/1000\n",
            "60000/60000 [==============================] - 1s 16us/sample - loss: 0.0783 - val_loss: 0.0785\n",
            "Epoch 389/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0783 - val_loss: 0.0783\n",
            "Epoch 390/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0783 - val_loss: 0.0786\n",
            "Epoch 391/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0782 - val_loss: 0.0785\n",
            "Epoch 392/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0783 - val_loss: 0.0784\n",
            "Epoch 393/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0783 - val_loss: 0.0786\n",
            "Epoch 394/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0782 - val_loss: 0.0787\n",
            "Epoch 395/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0783 - val_loss: 0.0786\n",
            "Epoch 396/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0783 - val_loss: 0.0786\n",
            "Epoch 397/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0783 - val_loss: 0.0785\n",
            "Epoch 398/1000\n",
            "60000/60000 [==============================] - 1s 16us/sample - loss: 0.0782 - val_loss: 0.0785\n",
            "Epoch 399/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0782 - val_loss: 0.0786\n",
            "Epoch 400/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0782 - val_loss: 0.0787\n",
            "Epoch 401/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0782 - val_loss: 0.0782\n",
            "Epoch 402/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0782 - val_loss: 0.0783\n",
            "Epoch 403/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0782 - val_loss: 0.0788\n",
            "Epoch 404/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0782 - val_loss: 0.0786\n",
            "Epoch 405/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0782 - val_loss: 0.0787\n",
            "Epoch 406/1000\n",
            "60000/60000 [==============================] - 1s 16us/sample - loss: 0.0783 - val_loss: 0.0784\n",
            "Epoch 407/1000\n",
            "60000/60000 [==============================] - 1s 16us/sample - loss: 0.0782 - val_loss: 0.0786\n",
            "Epoch 408/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0782 - val_loss: 0.0784\n",
            "Epoch 409/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0782 - val_loss: 0.0787\n",
            "Epoch 410/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0782 - val_loss: 0.0783\n",
            "Epoch 411/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0782 - val_loss: 0.0783\n",
            "Epoch 412/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0782 - val_loss: 0.0785\n",
            "Epoch 413/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0782 - val_loss: 0.0784\n",
            "Epoch 414/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0782 - val_loss: 0.0784\n",
            "Epoch 415/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0782 - val_loss: 0.0783\n",
            "Epoch 416/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0782 - val_loss: 0.0787\n",
            "Epoch 417/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0782 - val_loss: 0.0784\n",
            "Epoch 418/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0782 - val_loss: 0.0784\n",
            "Epoch 419/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0782 - val_loss: 0.0781\n",
            "Epoch 420/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0782 - val_loss: 0.0785\n",
            "Epoch 421/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0781 - val_loss: 0.0784\n",
            "Epoch 422/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0782 - val_loss: 0.0787\n",
            "Epoch 423/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0782 - val_loss: 0.0784\n",
            "Epoch 424/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0781 - val_loss: 0.0785\n",
            "Epoch 425/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0782 - val_loss: 0.0784\n",
            "Epoch 426/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0782 - val_loss: 0.0782\n",
            "Epoch 427/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0781 - val_loss: 0.0784\n",
            "Epoch 428/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0782 - val_loss: 0.0784\n",
            "Epoch 429/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0781 - val_loss: 0.0785\n",
            "Epoch 430/1000\n",
            "60000/60000 [==============================] - 1s 17us/sample - loss: 0.0781 - val_loss: 0.0788\n",
            "Epoch 431/1000\n",
            "60000/60000 [==============================] - 1s 17us/sample - loss: 0.0782 - val_loss: 0.0783\n",
            "Epoch 432/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0781 - val_loss: 0.0787\n",
            "Epoch 433/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0782 - val_loss: 0.0782\n",
            "Epoch 434/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0781 - val_loss: 0.0785\n",
            "Epoch 435/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0782 - val_loss: 0.0784\n",
            "Epoch 436/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0781 - val_loss: 0.0784\n",
            "Epoch 437/1000\n",
            "60000/60000 [==============================] - 1s 16us/sample - loss: 0.0781 - val_loss: 0.0783\n",
            "Epoch 438/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0781 - val_loss: 0.0784\n",
            "Epoch 439/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0781 - val_loss: 0.0784\n",
            "Epoch 440/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0781 - val_loss: 0.0783\n",
            "Epoch 441/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0781 - val_loss: 0.0784\n",
            "Epoch 442/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0781 - val_loss: 0.0785\n",
            "Epoch 443/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0781 - val_loss: 0.0788\n",
            "Epoch 444/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0781 - val_loss: 0.0784\n",
            "Epoch 445/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0781 - val_loss: 0.0782\n",
            "Epoch 446/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0781 - val_loss: 0.0783\n",
            "Epoch 447/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0781 - val_loss: 0.0783\n",
            "Epoch 448/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0781 - val_loss: 0.0782\n",
            "Epoch 449/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0781 - val_loss: 0.0783\n",
            "Epoch 450/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0781 - val_loss: 0.0788\n",
            "Epoch 451/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0781 - val_loss: 0.0784\n",
            "Epoch 452/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0781 - val_loss: 0.0783\n",
            "Epoch 453/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0781 - val_loss: 0.0783\n",
            "Epoch 454/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0780 - val_loss: 0.0781\n",
            "Epoch 455/1000\n",
            "60000/60000 [==============================] - 1s 16us/sample - loss: 0.0780 - val_loss: 0.0781\n",
            "Epoch 456/1000\n",
            "60000/60000 [==============================] - 1s 16us/sample - loss: 0.0781 - val_loss: 0.0784\n",
            "Epoch 457/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0781 - val_loss: 0.0783\n",
            "Epoch 458/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0781 - val_loss: 0.0785\n",
            "Epoch 459/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0780 - val_loss: 0.0782\n",
            "Epoch 460/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0781 - val_loss: 0.0782\n",
            "Epoch 461/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0780 - val_loss: 0.0785\n",
            "Epoch 462/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0780 - val_loss: 0.0783\n",
            "Epoch 463/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0780 - val_loss: 0.0785\n",
            "Epoch 464/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0780 - val_loss: 0.0783\n",
            "Epoch 465/1000\n",
            "60000/60000 [==============================] - 1s 16us/sample - loss: 0.0780 - val_loss: 0.0784\n",
            "Epoch 466/1000\n",
            "60000/60000 [==============================] - 1s 16us/sample - loss: 0.0780 - val_loss: 0.0781\n",
            "Epoch 467/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0780 - val_loss: 0.0784\n",
            "Epoch 468/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0780 - val_loss: 0.0784\n",
            "Epoch 469/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0780 - val_loss: 0.0783\n",
            "Epoch 470/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0780 - val_loss: 0.0783\n",
            "Epoch 471/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0781 - val_loss: 0.0785\n",
            "Epoch 472/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0780 - val_loss: 0.0784\n",
            "Epoch 473/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0780 - val_loss: 0.0783\n",
            "Epoch 474/1000\n",
            "60000/60000 [==============================] - 1s 17us/sample - loss: 0.0780 - val_loss: 0.0782\n",
            "Epoch 475/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0780 - val_loss: 0.0784\n",
            "Epoch 476/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0780 - val_loss: 0.0785\n",
            "Epoch 477/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0780 - val_loss: 0.0787\n",
            "Epoch 478/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0780 - val_loss: 0.0783\n",
            "Epoch 479/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0780 - val_loss: 0.0782\n",
            "Epoch 480/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0780 - val_loss: 0.0782\n",
            "Epoch 481/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0780 - val_loss: 0.0781\n",
            "Epoch 482/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0780 - val_loss: 0.0780\n",
            "Epoch 483/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0780 - val_loss: 0.0789\n",
            "Epoch 484/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0780 - val_loss: 0.0780\n",
            "Epoch 485/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0780 - val_loss: 0.0780\n",
            "Epoch 486/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0779 - val_loss: 0.0787\n",
            "Epoch 487/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0780 - val_loss: 0.0780\n",
            "Epoch 488/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0780 - val_loss: 0.0785\n",
            "Epoch 489/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0779 - val_loss: 0.0781\n",
            "Epoch 490/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0780 - val_loss: 0.0782\n",
            "Epoch 491/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0780 - val_loss: 0.0784\n",
            "Epoch 492/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0780 - val_loss: 0.0782\n",
            "Epoch 493/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0779 - val_loss: 0.0785\n",
            "Epoch 494/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0779 - val_loss: 0.0784\n",
            "Epoch 495/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0779 - val_loss: 0.0784\n",
            "Epoch 496/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0779 - val_loss: 0.0784\n",
            "Epoch 497/1000\n",
            "60000/60000 [==============================] - 1s 16us/sample - loss: 0.0779 - val_loss: 0.0784\n",
            "Epoch 498/1000\n",
            "60000/60000 [==============================] - 1s 17us/sample - loss: 0.0779 - val_loss: 0.0785\n",
            "Epoch 499/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0780 - val_loss: 0.0784\n",
            "Epoch 500/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0779 - val_loss: 0.0783\n",
            "Epoch 501/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0779 - val_loss: 0.0783\n",
            "Epoch 502/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0780 - val_loss: 0.0789\n",
            "Epoch 503/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0779 - val_loss: 0.0783\n",
            "Epoch 504/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0779 - val_loss: 0.0783\n",
            "Epoch 505/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0779 - val_loss: 0.0781\n",
            "Epoch 506/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0779 - val_loss: 0.0785\n",
            "Epoch 507/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0779 - val_loss: 0.0783\n",
            "Epoch 508/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0779 - val_loss: 0.0785\n",
            "Epoch 509/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0779 - val_loss: 0.0781\n",
            "Epoch 510/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0779 - val_loss: 0.0784\n",
            "Epoch 511/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0779 - val_loss: 0.0786\n",
            "Epoch 512/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0779 - val_loss: 0.0781\n",
            "Epoch 513/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0779 - val_loss: 0.0783\n",
            "Epoch 514/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0779 - val_loss: 0.0781\n",
            "Epoch 515/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0779 - val_loss: 0.0781\n",
            "Epoch 516/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0779 - val_loss: 0.0780\n",
            "Epoch 517/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0778 - val_loss: 0.0782\n",
            "Epoch 518/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0779 - val_loss: 0.0782\n",
            "Epoch 519/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0779 - val_loss: 0.0783\n",
            "Epoch 520/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0779 - val_loss: 0.0781\n",
            "Epoch 521/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0779 - val_loss: 0.0782\n",
            "Epoch 522/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0779 - val_loss: 0.0783\n",
            "Epoch 523/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0779 - val_loss: 0.0782\n",
            "Epoch 524/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0778 - val_loss: 0.0782\n",
            "Epoch 525/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0778 - val_loss: 0.0783\n",
            "Epoch 526/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0779 - val_loss: 0.0781\n",
            "Epoch 527/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0779 - val_loss: 0.0780\n",
            "Epoch 528/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0778 - val_loss: 0.0782\n",
            "Epoch 529/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0778 - val_loss: 0.0783\n",
            "Epoch 530/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0779 - val_loss: 0.0781\n",
            "Epoch 531/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0778 - val_loss: 0.0783\n",
            "Epoch 532/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0778 - val_loss: 0.0783\n",
            "Epoch 533/1000\n",
            "60000/60000 [==============================] - 1s 16us/sample - loss: 0.0779 - val_loss: 0.0785\n",
            "Epoch 534/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0778 - val_loss: 0.0782\n",
            "Epoch 535/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0779 - val_loss: 0.0783\n",
            "Epoch 536/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0778 - val_loss: 0.0782\n",
            "Epoch 537/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0778 - val_loss: 0.0782\n",
            "Epoch 538/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0779 - val_loss: 0.0782\n",
            "Epoch 539/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0778 - val_loss: 0.0781\n",
            "Epoch 540/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0778 - val_loss: 0.0784\n",
            "Epoch 541/1000\n",
            "60000/60000 [==============================] - 1s 16us/sample - loss: 0.0778 - val_loss: 0.0783\n",
            "Epoch 542/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0778 - val_loss: 0.0789\n",
            "Epoch 543/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0778 - val_loss: 0.0786\n",
            "Epoch 544/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0778 - val_loss: 0.0783\n",
            "Epoch 545/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0778 - val_loss: 0.0780\n",
            "Epoch 546/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0778 - val_loss: 0.0782\n",
            "Epoch 547/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0778 - val_loss: 0.0783\n",
            "Epoch 548/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0778 - val_loss: 0.0783\n",
            "Epoch 549/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0778 - val_loss: 0.0781\n",
            "Epoch 550/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0778 - val_loss: 0.0781\n",
            "Epoch 551/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0778 - val_loss: 0.0780\n",
            "Epoch 552/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0778 - val_loss: 0.0781\n",
            "Epoch 553/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0778 - val_loss: 0.0783\n",
            "Epoch 554/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0778 - val_loss: 0.0783\n",
            "Epoch 555/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0778 - val_loss: 0.0782\n",
            "Epoch 556/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0778 - val_loss: 0.0780\n",
            "Epoch 557/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0778 - val_loss: 0.0784\n",
            "Epoch 558/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0778 - val_loss: 0.0781\n",
            "Epoch 559/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0777 - val_loss: 0.0782\n",
            "Epoch 560/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0778 - val_loss: 0.0782\n",
            "Epoch 561/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0778 - val_loss: 0.0784\n",
            "Epoch 562/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0778 - val_loss: 0.0780\n",
            "Epoch 563/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0778 - val_loss: 0.0779\n",
            "Epoch 564/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0778 - val_loss: 0.0781\n",
            "Epoch 565/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0778 - val_loss: 0.0787\n",
            "Epoch 566/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0778 - val_loss: 0.0782\n",
            "Epoch 567/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0778 - val_loss: 0.0779\n",
            "Epoch 568/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0777 - val_loss: 0.0785\n",
            "Epoch 569/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0777 - val_loss: 0.0782\n",
            "Epoch 570/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0778 - val_loss: 0.0781\n",
            "Epoch 571/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0778 - val_loss: 0.0781\n",
            "Epoch 572/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0777 - val_loss: 0.0780\n",
            "Epoch 573/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0778 - val_loss: 0.0779\n",
            "Epoch 574/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0777 - val_loss: 0.0778\n",
            "Epoch 575/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0777 - val_loss: 0.0781\n",
            "Epoch 576/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0777 - val_loss: 0.0780\n",
            "Epoch 577/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0777 - val_loss: 0.0783\n",
            "Epoch 578/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0778 - val_loss: 0.0780\n",
            "Epoch 579/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0777 - val_loss: 0.0779\n",
            "Epoch 580/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0777 - val_loss: 0.0781\n",
            "Epoch 581/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0778 - val_loss: 0.0782\n",
            "Epoch 582/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0777 - val_loss: 0.0782\n",
            "Epoch 583/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0777 - val_loss: 0.0780\n",
            "Epoch 584/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0777 - val_loss: 0.0783\n",
            "Epoch 585/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0777 - val_loss: 0.0780\n",
            "Epoch 586/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0777 - val_loss: 0.0780\n",
            "Epoch 587/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0777 - val_loss: 0.0780\n",
            "Epoch 588/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0777 - val_loss: 0.0787\n",
            "Epoch 589/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0777 - val_loss: 0.0783\n",
            "Epoch 590/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0777 - val_loss: 0.0780\n",
            "Epoch 591/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0777 - val_loss: 0.0782\n",
            "Epoch 592/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0777 - val_loss: 0.0784\n",
            "Epoch 593/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0777 - val_loss: 0.0784\n",
            "Epoch 594/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0777 - val_loss: 0.0780\n",
            "Epoch 595/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0777 - val_loss: 0.0783\n",
            "Epoch 596/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0777 - val_loss: 0.0783\n",
            "Epoch 597/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0777 - val_loss: 0.0785\n",
            "Epoch 598/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0777 - val_loss: 0.0780\n",
            "Epoch 599/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0777 - val_loss: 0.0779\n",
            "Epoch 600/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0777 - val_loss: 0.0781\n",
            "Epoch 601/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0777 - val_loss: 0.0782\n",
            "Epoch 602/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0777 - val_loss: 0.0781\n",
            "Epoch 603/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0777 - val_loss: 0.0780\n",
            "Epoch 604/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0777 - val_loss: 0.0783\n",
            "Epoch 605/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0777 - val_loss: 0.0779\n",
            "Epoch 606/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0777 - val_loss: 0.0779\n",
            "Epoch 607/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0776 - val_loss: 0.0781\n",
            "Epoch 608/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0777 - val_loss: 0.0780\n",
            "Epoch 609/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0776 - val_loss: 0.0780\n",
            "Epoch 610/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0777 - val_loss: 0.0783\n",
            "Epoch 611/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0776 - val_loss: 0.0780\n",
            "Epoch 612/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0777 - val_loss: 0.0782\n",
            "Epoch 613/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0776 - val_loss: 0.0783\n",
            "Epoch 614/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0777 - val_loss: 0.0785\n",
            "Epoch 615/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0776 - val_loss: 0.0785\n",
            "Epoch 616/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0777 - val_loss: 0.0784\n",
            "Epoch 617/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0777 - val_loss: 0.0780\n",
            "Epoch 618/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0776 - val_loss: 0.0781\n",
            "Epoch 619/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0776 - val_loss: 0.0782\n",
            "Epoch 620/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0776 - val_loss: 0.0780\n",
            "Epoch 621/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0777 - val_loss: 0.0781\n",
            "Epoch 622/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0776 - val_loss: 0.0782\n",
            "Epoch 623/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0776 - val_loss: 0.0778\n",
            "Epoch 624/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0776 - val_loss: 0.0779\n",
            "Epoch 625/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0776 - val_loss: 0.0779\n",
            "Epoch 626/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0776 - val_loss: 0.0780\n",
            "Epoch 627/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0776 - val_loss: 0.0783\n",
            "Epoch 628/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0777 - val_loss: 0.0780\n",
            "Epoch 629/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0776 - val_loss: 0.0781\n",
            "Epoch 630/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0776 - val_loss: 0.0780\n",
            "Epoch 631/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0776 - val_loss: 0.0779\n",
            "Epoch 632/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0777 - val_loss: 0.0779\n",
            "Epoch 633/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0776 - val_loss: 0.0779\n",
            "Epoch 634/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0776 - val_loss: 0.0783\n",
            "Epoch 635/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0776 - val_loss: 0.0778\n",
            "Epoch 636/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0776 - val_loss: 0.0780\n",
            "Epoch 637/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0776 - val_loss: 0.0782\n",
            "Epoch 638/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0776 - val_loss: 0.0783\n",
            "Epoch 639/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0776 - val_loss: 0.0780\n",
            "Epoch 640/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0776 - val_loss: 0.0781\n",
            "Epoch 641/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0776 - val_loss: 0.0782\n",
            "Epoch 642/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0776 - val_loss: 0.0784\n",
            "Epoch 643/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0776 - val_loss: 0.0779\n",
            "Epoch 644/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0776 - val_loss: 0.0781\n",
            "Epoch 645/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0776 - val_loss: 0.0781\n",
            "Epoch 646/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0776 - val_loss: 0.0780\n",
            "Epoch 647/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0776 - val_loss: 0.0781\n",
            "Epoch 648/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0776 - val_loss: 0.0779\n",
            "Epoch 649/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0776 - val_loss: 0.0780\n",
            "Epoch 650/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0775 - val_loss: 0.0781\n",
            "Epoch 651/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0776 - val_loss: 0.0780\n",
            "Epoch 652/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0776 - val_loss: 0.0784\n",
            "Epoch 653/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0776 - val_loss: 0.0782\n",
            "Epoch 654/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0776 - val_loss: 0.0780\n",
            "Epoch 655/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0776 - val_loss: 0.0779\n",
            "Epoch 656/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0776 - val_loss: 0.0781\n",
            "Epoch 657/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0776 - val_loss: 0.0780\n",
            "Epoch 658/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0776 - val_loss: 0.0778\n",
            "Epoch 659/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0775 - val_loss: 0.0780\n",
            "Epoch 660/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0776 - val_loss: 0.0785\n",
            "Epoch 661/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0776 - val_loss: 0.0779\n",
            "Epoch 662/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0776 - val_loss: 0.0779\n",
            "Epoch 663/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0776 - val_loss: 0.0782\n",
            "Epoch 664/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0775 - val_loss: 0.0779\n",
            "Epoch 665/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0776 - val_loss: 0.0778\n",
            "Epoch 666/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0775 - val_loss: 0.0781\n",
            "Epoch 667/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0776 - val_loss: 0.0783\n",
            "Epoch 668/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0775 - val_loss: 0.0779\n",
            "Epoch 669/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0775 - val_loss: 0.0780\n",
            "Epoch 670/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0776 - val_loss: 0.0780\n",
            "Epoch 671/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0775 - val_loss: 0.0778\n",
            "Epoch 672/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0776 - val_loss: 0.0780\n",
            "Epoch 673/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0776 - val_loss: 0.0783\n",
            "Epoch 674/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0775 - val_loss: 0.0778\n",
            "Epoch 675/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0776 - val_loss: 0.0779\n",
            "Epoch 676/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0775 - val_loss: 0.0786\n",
            "Epoch 677/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0775 - val_loss: 0.0778\n",
            "Epoch 678/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0775 - val_loss: 0.0778\n",
            "Epoch 679/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0775 - val_loss: 0.0780\n",
            "Epoch 680/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0775 - val_loss: 0.0779\n",
            "Epoch 681/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0775 - val_loss: 0.0783\n",
            "Epoch 682/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0776 - val_loss: 0.0781\n",
            "Epoch 683/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0775 - val_loss: 0.0782\n",
            "Epoch 684/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0775 - val_loss: 0.0781\n",
            "Epoch 685/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0775 - val_loss: 0.0780\n",
            "Epoch 686/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0776 - val_loss: 0.0779\n",
            "Epoch 687/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0775 - val_loss: 0.0781\n",
            "Epoch 688/1000\n",
            "60000/60000 [==============================] - 1s 16us/sample - loss: 0.0775 - val_loss: 0.0780\n",
            "Epoch 689/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0775 - val_loss: 0.0781\n",
            "Epoch 690/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0775 - val_loss: 0.0779\n",
            "Epoch 691/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0775 - val_loss: 0.0778\n",
            "Epoch 692/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0775 - val_loss: 0.0778\n",
            "Epoch 693/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0775 - val_loss: 0.0778\n",
            "Epoch 694/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0775 - val_loss: 0.0781\n",
            "Epoch 695/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0775 - val_loss: 0.0781\n",
            "Epoch 696/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0775 - val_loss: 0.0781\n",
            "Epoch 697/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0775 - val_loss: 0.0778\n",
            "Epoch 698/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0775 - val_loss: 0.0782\n",
            "Epoch 699/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0775 - val_loss: 0.0780\n",
            "Epoch 700/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0775 - val_loss: 0.0782\n",
            "Epoch 701/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0775 - val_loss: 0.0782\n",
            "Epoch 702/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0775 - val_loss: 0.0780\n",
            "Epoch 703/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0775 - val_loss: 0.0778\n",
            "Epoch 704/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0775 - val_loss: 0.0781\n",
            "Epoch 705/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0775 - val_loss: 0.0778\n",
            "Epoch 706/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0775 - val_loss: 0.0778\n",
            "Epoch 707/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0775 - val_loss: 0.0777\n",
            "Epoch 708/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0775 - val_loss: 0.0779\n",
            "Epoch 709/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0775 - val_loss: 0.0779\n",
            "Epoch 710/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0775 - val_loss: 0.0779\n",
            "Epoch 711/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0774 - val_loss: 0.0782\n",
            "Epoch 712/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0775 - val_loss: 0.0781\n",
            "Epoch 713/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0774 - val_loss: 0.0779\n",
            "Epoch 714/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0775 - val_loss: 0.0779\n",
            "Epoch 715/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0775 - val_loss: 0.0781\n",
            "Epoch 716/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0774 - val_loss: 0.0779\n",
            "Epoch 717/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0775 - val_loss: 0.0779\n",
            "Epoch 718/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0775 - val_loss: 0.0782\n",
            "Epoch 719/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0775 - val_loss: 0.0779\n",
            "Epoch 720/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0774 - val_loss: 0.0780\n",
            "Epoch 721/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0775 - val_loss: 0.0780\n",
            "Epoch 722/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0774 - val_loss: 0.0783\n",
            "Epoch 723/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0774 - val_loss: 0.0779\n",
            "Epoch 724/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0775 - val_loss: 0.0778\n",
            "Epoch 725/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0775 - val_loss: 0.0780\n",
            "Epoch 726/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0775 - val_loss: 0.0778\n",
            "Epoch 727/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0775 - val_loss: 0.0778\n",
            "Epoch 728/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0774 - val_loss: 0.0780\n",
            "Epoch 729/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0775 - val_loss: 0.0779\n",
            "Epoch 730/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0775 - val_loss: 0.0779\n",
            "Epoch 731/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0774 - val_loss: 0.0779\n",
            "Epoch 732/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0774 - val_loss: 0.0779\n",
            "Epoch 733/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0774 - val_loss: 0.0783\n",
            "Epoch 734/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0774 - val_loss: 0.0781\n",
            "Epoch 735/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0774 - val_loss: 0.0780\n",
            "Epoch 736/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0775 - val_loss: 0.0778\n",
            "Epoch 737/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0774 - val_loss: 0.0779\n",
            "Epoch 738/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0775 - val_loss: 0.0778\n",
            "Epoch 739/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0774 - val_loss: 0.0779\n",
            "Epoch 740/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0775 - val_loss: 0.0777\n",
            "Epoch 741/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0774 - val_loss: 0.0780\n",
            "Epoch 742/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0774 - val_loss: 0.0784\n",
            "Epoch 743/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0775 - val_loss: 0.0779\n",
            "Epoch 744/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0774 - val_loss: 0.0777\n",
            "Epoch 745/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0774 - val_loss: 0.0778\n",
            "Epoch 746/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0774 - val_loss: 0.0780\n",
            "Epoch 747/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0774 - val_loss: 0.0779\n",
            "Epoch 748/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0775 - val_loss: 0.0779\n",
            "Epoch 749/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0774 - val_loss: 0.0780\n",
            "Epoch 750/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0774 - val_loss: 0.0778\n",
            "Epoch 751/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0774 - val_loss: 0.0781\n",
            "Epoch 752/1000\n",
            "60000/60000 [==============================] - 1s 16us/sample - loss: 0.0775 - val_loss: 0.0779\n",
            "Epoch 753/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0774 - val_loss: 0.0780\n",
            "Epoch 754/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0775 - val_loss: 0.0782\n",
            "Epoch 755/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0774 - val_loss: 0.0778\n",
            "Epoch 756/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0774 - val_loss: 0.0778\n",
            "Epoch 757/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0774 - val_loss: 0.0780\n",
            "Epoch 758/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0774 - val_loss: 0.0777\n",
            "Epoch 759/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0774 - val_loss: 0.0780\n",
            "Epoch 760/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0774 - val_loss: 0.0778\n",
            "Epoch 761/1000\n",
            "60000/60000 [==============================] - 1s 16us/sample - loss: 0.0774 - val_loss: 0.0778\n",
            "Epoch 762/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0774 - val_loss: 0.0779\n",
            "Epoch 763/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0774 - val_loss: 0.0780\n",
            "Epoch 764/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0774 - val_loss: 0.0776\n",
            "Epoch 765/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0774 - val_loss: 0.0780\n",
            "Epoch 766/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0774 - val_loss: 0.0777\n",
            "Epoch 767/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0774 - val_loss: 0.0780\n",
            "Epoch 768/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0774 - val_loss: 0.0779\n",
            "Epoch 769/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0774 - val_loss: 0.0777\n",
            "Epoch 770/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0774 - val_loss: 0.0779\n",
            "Epoch 771/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0774 - val_loss: 0.0777\n",
            "Epoch 772/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0774 - val_loss: 0.0777\n",
            "Epoch 773/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0774 - val_loss: 0.0779\n",
            "Epoch 774/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0774 - val_loss: 0.0778\n",
            "Epoch 775/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0774 - val_loss: 0.0779\n",
            "Epoch 776/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0774 - val_loss: 0.0779\n",
            "Epoch 777/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0774 - val_loss: 0.0779\n",
            "Epoch 778/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0773 - val_loss: 0.0779\n",
            "Epoch 779/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0774 - val_loss: 0.0779\n",
            "Epoch 780/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0774 - val_loss: 0.0779\n",
            "Epoch 781/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0774 - val_loss: 0.0780\n",
            "Epoch 782/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0774 - val_loss: 0.0778\n",
            "Epoch 783/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0773 - val_loss: 0.0778\n",
            "Epoch 784/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0774 - val_loss: 0.0776\n",
            "Epoch 785/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0774 - val_loss: 0.0780\n",
            "Epoch 786/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0774 - val_loss: 0.0780\n",
            "Epoch 787/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0773 - val_loss: 0.0780\n",
            "Epoch 788/1000\n",
            "60000/60000 [==============================] - 1s 16us/sample - loss: 0.0774 - val_loss: 0.0779\n",
            "Epoch 789/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0774 - val_loss: 0.0778\n",
            "Epoch 790/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0774 - val_loss: 0.0780\n",
            "Epoch 791/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0773 - val_loss: 0.0781\n",
            "Epoch 792/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0774 - val_loss: 0.0778\n",
            "Epoch 793/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0774 - val_loss: 0.0778\n",
            "Epoch 794/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0774 - val_loss: 0.0778\n",
            "Epoch 795/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0773 - val_loss: 0.0778\n",
            "Epoch 796/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0773 - val_loss: 0.0782\n",
            "Epoch 797/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0774 - val_loss: 0.0780\n",
            "Epoch 798/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0774 - val_loss: 0.0778\n",
            "Epoch 799/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0773 - val_loss: 0.0778\n",
            "Epoch 800/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0774 - val_loss: 0.0777\n",
            "Epoch 801/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0773 - val_loss: 0.0780\n",
            "Epoch 802/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0774 - val_loss: 0.0779\n",
            "Epoch 803/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0774 - val_loss: 0.0780\n",
            "Epoch 804/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0774 - val_loss: 0.0780\n",
            "Epoch 805/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0773 - val_loss: 0.0777\n",
            "Epoch 806/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0773 - val_loss: 0.0778\n",
            "Epoch 807/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0774 - val_loss: 0.0778\n",
            "Epoch 808/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0773 - val_loss: 0.0778\n",
            "Epoch 809/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0774 - val_loss: 0.0779\n",
            "Epoch 810/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0773 - val_loss: 0.0783\n",
            "Epoch 811/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0773 - val_loss: 0.0782\n",
            "Epoch 812/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0773 - val_loss: 0.0781\n",
            "Epoch 813/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0773 - val_loss: 0.0778\n",
            "Epoch 814/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0773 - val_loss: 0.0781\n",
            "Epoch 815/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0773 - val_loss: 0.0778\n",
            "Epoch 816/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0773 - val_loss: 0.0777\n",
            "Epoch 817/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0774 - val_loss: 0.0776\n",
            "Epoch 818/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0773 - val_loss: 0.0779\n",
            "Epoch 819/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0773 - val_loss: 0.0778\n",
            "Epoch 820/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0773 - val_loss: 0.0778\n",
            "Epoch 821/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0773 - val_loss: 0.0778\n",
            "Epoch 822/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0773 - val_loss: 0.0782\n",
            "Epoch 823/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0773 - val_loss: 0.0779\n",
            "Epoch 824/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0774 - val_loss: 0.0777\n",
            "Epoch 825/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0773 - val_loss: 0.0779\n",
            "Epoch 826/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0773 - val_loss: 0.0777\n",
            "Epoch 827/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0773 - val_loss: 0.0779\n",
            "Epoch 828/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0773 - val_loss: 0.0777\n",
            "Epoch 829/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0773 - val_loss: 0.0777\n",
            "Epoch 830/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0773 - val_loss: 0.0776\n",
            "Epoch 831/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0773 - val_loss: 0.0777\n",
            "Epoch 832/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0773 - val_loss: 0.0778\n",
            "Epoch 833/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0773 - val_loss: 0.0777\n",
            "Epoch 834/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0773 - val_loss: 0.0778\n",
            "Epoch 835/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0773 - val_loss: 0.0778\n",
            "Epoch 836/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0773 - val_loss: 0.0779\n",
            "Epoch 837/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0773 - val_loss: 0.0777\n",
            "Epoch 838/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0773 - val_loss: 0.0780\n",
            "Epoch 839/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0773 - val_loss: 0.0776\n",
            "Epoch 840/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0779\n",
            "Epoch 841/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0773 - val_loss: 0.0779\n",
            "Epoch 842/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0773 - val_loss: 0.0778\n",
            "Epoch 843/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0773 - val_loss: 0.0777\n",
            "Epoch 844/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0773 - val_loss: 0.0779\n",
            "Epoch 845/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0773 - val_loss: 0.0781\n",
            "Epoch 846/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0773 - val_loss: 0.0782\n",
            "Epoch 847/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0773 - val_loss: 0.0776\n",
            "Epoch 848/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0773 - val_loss: 0.0778\n",
            "Epoch 849/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0773 - val_loss: 0.0782\n",
            "Epoch 850/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0773 - val_loss: 0.0780\n",
            "Epoch 851/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0777\n",
            "Epoch 852/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0773 - val_loss: 0.0776\n",
            "Epoch 853/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0773 - val_loss: 0.0777\n",
            "Epoch 854/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0773 - val_loss: 0.0778\n",
            "Epoch 855/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0773 - val_loss: 0.0781\n",
            "Epoch 856/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0773 - val_loss: 0.0777\n",
            "Epoch 857/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0773 - val_loss: 0.0778\n",
            "Epoch 858/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0773 - val_loss: 0.0778\n",
            "Epoch 859/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0773 - val_loss: 0.0777\n",
            "Epoch 860/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0773 - val_loss: 0.0783\n",
            "Epoch 861/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0772 - val_loss: 0.0776\n",
            "Epoch 862/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0773 - val_loss: 0.0779\n",
            "Epoch 863/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0773 - val_loss: 0.0780\n",
            "Epoch 864/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0773 - val_loss: 0.0776\n",
            "Epoch 865/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0773 - val_loss: 0.0778\n",
            "Epoch 866/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0773 - val_loss: 0.0779\n",
            "Epoch 867/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0778\n",
            "Epoch 868/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0772 - val_loss: 0.0780\n",
            "Epoch 869/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0773 - val_loss: 0.0779\n",
            "Epoch 870/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0778\n",
            "Epoch 871/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0778\n",
            "Epoch 872/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0773 - val_loss: 0.0777\n",
            "Epoch 873/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0779\n",
            "Epoch 874/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0773 - val_loss: 0.0781\n",
            "Epoch 875/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0773 - val_loss: 0.0776\n",
            "Epoch 876/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0776\n",
            "Epoch 877/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0773 - val_loss: 0.0777\n",
            "Epoch 878/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0773 - val_loss: 0.0777\n",
            "Epoch 879/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0772 - val_loss: 0.0776\n",
            "Epoch 880/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0777\n",
            "Epoch 881/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0773 - val_loss: 0.0778\n",
            "Epoch 882/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0776\n",
            "Epoch 883/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0772 - val_loss: 0.0778\n",
            "Epoch 884/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0777\n",
            "Epoch 885/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0773 - val_loss: 0.0777\n",
            "Epoch 886/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0781\n",
            "Epoch 887/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0773 - val_loss: 0.0778\n",
            "Epoch 888/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0772 - val_loss: 0.0778\n",
            "Epoch 889/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0772 - val_loss: 0.0778\n",
            "Epoch 890/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0776\n",
            "Epoch 891/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0773 - val_loss: 0.0777\n",
            "Epoch 892/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0777\n",
            "Epoch 893/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0772 - val_loss: 0.0780\n",
            "Epoch 894/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0780\n",
            "Epoch 895/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0773 - val_loss: 0.0776\n",
            "Epoch 896/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0781\n",
            "Epoch 897/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0772 - val_loss: 0.0778\n",
            "Epoch 898/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0778\n",
            "Epoch 899/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0773 - val_loss: 0.0779\n",
            "Epoch 900/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0775\n",
            "Epoch 901/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0778\n",
            "Epoch 902/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0776\n",
            "Epoch 903/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0777\n",
            "Epoch 904/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0773 - val_loss: 0.0779\n",
            "Epoch 905/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0772 - val_loss: 0.0776\n",
            "Epoch 906/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0777\n",
            "Epoch 907/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0772 - val_loss: 0.0776\n",
            "Epoch 908/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0772 - val_loss: 0.0779\n",
            "Epoch 909/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0772 - val_loss: 0.0777\n",
            "Epoch 910/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0781\n",
            "Epoch 911/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0777\n",
            "Epoch 912/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0773 - val_loss: 0.0776\n",
            "Epoch 913/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0772 - val_loss: 0.0777\n",
            "Epoch 914/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0776\n",
            "Epoch 915/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0778\n",
            "Epoch 916/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0777\n",
            "Epoch 917/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0772 - val_loss: 0.0777\n",
            "Epoch 918/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0778\n",
            "Epoch 919/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0777\n",
            "Epoch 920/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0772 - val_loss: 0.0776\n",
            "Epoch 921/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0778\n",
            "Epoch 922/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0773 - val_loss: 0.0778\n",
            "Epoch 923/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0777\n",
            "Epoch 924/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0778\n",
            "Epoch 925/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0778\n",
            "Epoch 926/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0772 - val_loss: 0.0779\n",
            "Epoch 927/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0780\n",
            "Epoch 928/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0779\n",
            "Epoch 929/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0776\n",
            "Epoch 930/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0778\n",
            "Epoch 931/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0772 - val_loss: 0.0779\n",
            "Epoch 932/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0772 - val_loss: 0.0779\n",
            "Epoch 933/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0772 - val_loss: 0.0778\n",
            "Epoch 934/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0772 - val_loss: 0.0779\n",
            "Epoch 935/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0772 - val_loss: 0.0779\n",
            "Epoch 936/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0777\n",
            "Epoch 937/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0777\n",
            "Epoch 938/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0776\n",
            "Epoch 939/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0771 - val_loss: 0.0778\n",
            "Epoch 940/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0781\n",
            "Epoch 941/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0777\n",
            "Epoch 942/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0775\n",
            "Epoch 943/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0779\n",
            "Epoch 944/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0777\n",
            "Epoch 945/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0772 - val_loss: 0.0780\n",
            "Epoch 946/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0772 - val_loss: 0.0778\n",
            "Epoch 947/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0776\n",
            "Epoch 948/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0776\n",
            "Epoch 949/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0779\n",
            "Epoch 950/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0778\n",
            "Epoch 951/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0777\n",
            "Epoch 952/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0780\n",
            "Epoch 953/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0772 - val_loss: 0.0784\n",
            "Epoch 954/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0781\n",
            "Epoch 955/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0772 - val_loss: 0.0778\n",
            "Epoch 956/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0772 - val_loss: 0.0781\n",
            "Epoch 957/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0772 - val_loss: 0.0778\n",
            "Epoch 958/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0771 - val_loss: 0.0776\n",
            "Epoch 959/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0782\n",
            "Epoch 960/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0775\n",
            "Epoch 961/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0780\n",
            "Epoch 962/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0772 - val_loss: 0.0780\n",
            "Epoch 963/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0772 - val_loss: 0.0776\n",
            "Epoch 964/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0779\n",
            "Epoch 965/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0779\n",
            "Epoch 966/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0781\n",
            "Epoch 967/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0777\n",
            "Epoch 968/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0776\n",
            "Epoch 969/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0771 - val_loss: 0.0777\n",
            "Epoch 970/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0778\n",
            "Epoch 971/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0772 - val_loss: 0.0780\n",
            "Epoch 972/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0772 - val_loss: 0.0777\n",
            "Epoch 973/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0772 - val_loss: 0.0778\n",
            "Epoch 974/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0779\n",
            "Epoch 975/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0778\n",
            "Epoch 976/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0771 - val_loss: 0.0776\n",
            "Epoch 977/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0779\n",
            "Epoch 978/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0777\n",
            "Epoch 979/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0772 - val_loss: 0.0782\n",
            "Epoch 980/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0775\n",
            "Epoch 981/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0771 - val_loss: 0.0779\n",
            "Epoch 982/1000\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0771 - val_loss: 0.0776\n",
            "Epoch 983/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0772 - val_loss: 0.0777\n",
            "Epoch 984/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0771 - val_loss: 0.0775\n",
            "Epoch 985/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0772 - val_loss: 0.0778\n",
            "Epoch 986/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0772 - val_loss: 0.0779\n",
            "Epoch 987/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0771 - val_loss: 0.0777\n",
            "Epoch 988/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0775\n",
            "Epoch 989/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0771 - val_loss: 0.0775\n",
            "Epoch 990/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0771 - val_loss: 0.0775\n",
            "Epoch 991/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0771 - val_loss: 0.0777\n",
            "Epoch 992/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0772 - val_loss: 0.0777\n",
            "Epoch 993/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0778\n",
            "Epoch 994/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0776\n",
            "Epoch 995/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0771 - val_loss: 0.0775\n",
            "Epoch 996/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0772 - val_loss: 0.0775\n",
            "Epoch 997/1000\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0771 - val_loss: 0.0777\n",
            "Epoch 998/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0771 - val_loss: 0.0778\n",
            "Epoch 999/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0772 - val_loss: 0.0777\n",
            "Epoch 1000/1000\n",
            "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0771 - val_loss: 0.0776\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f2f54852438>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciOQ7abZsfHn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoded_imgs = autoencoder.predict(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQowP5nic62h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "outputId": "51a4ab70-4f63-4c26-86ae-0d1ef797ae7f"
      },
      "source": [
        "# use Matplotlib (don't ask)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "n = 10  # how many digits we will display\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "    # display original\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(x_test[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # display reconstruction\n",
        "    ax = plt.subplot(2, n, i + 1 + n)\n",
        "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABG0AAADnCAYAAACkCqtqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 1440x288 with 20 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "toc-hr-collapsed": true,
        "id": "xPi_FN5Kc62t",
        "colab_type": "text"
      },
      "source": [
        "### Convolutional autoencoder\n",
        "\n",
        "> Since our inputs are images, it makes sense to use convolutional neural networks (convnets) as encoders and decoders. In practical settings, autoencoders applied to images are always convolutional autoencoders --they simply perform much better.\n",
        "\n",
        "> Let's implement one. The encoder will consist in a stack of Conv2D and MaxPooling2D layers (max pooling being used for spatial down-sampling), while the decoder will consist in a stack of Conv2D and UpSampling2D layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-IN6-PKc62w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
        "from tensorflow.keras.models import Model\n",
        "# Create Model\n",
        "input_img = Input(shape=(28,28,1))\n",
        "x = Conv2D(16, (3,3), activation='relu', padding='same')(input_img)\n",
        "x = MaxPooling2D((2,2), padding='same')(x)\n",
        "x = Conv2D(8, (3,3), activation='relu', padding='same')(x)\n",
        "x = MaxPooling2D((2,2), padding='same')(x)\n",
        "x = Conv2D(8, (3,3), activation='relu', padding='same')(x)\n",
        "encoded = MaxPooling2D((2,2), padding='same')(x)\n",
        "# ^ this is the representation. The shape (4,4,8) => 128 dimensional representation\n",
        "x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "x = Conv2D(16, (3, 3), activation='relu')(x)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "decoded = Conv2D(1, (3,3), activation='sigmoid', padding='same')(x)\n",
        "autoencoder = Model(input_img, decoded)\n",
        "autoencoder.compile(optimizer='nadam', loss='binary_crossentropy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2REe5NJc621",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "import numpy as np\n",
        "\n",
        "(x_train, _), (x_test, _) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))  # adapt this if using `channels_first` image data format\n",
        "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))  # adapt this if using `channels_first` image data format"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHBtc-sUc628",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8789114e-6f64-4ec7-cb88-bbda1bc3e59b"
      },
      "source": [
        "# wandb.init(project=\"mnist_autoencoder\", entity=\"ds5\")\n",
        "\n",
        "autoencoder.fit(x_train, x_train,\n",
        "                epochs=100,\n",
        "                batch_size=256,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test, x_test),\n",
        "                verbose=True)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/100\n",
            "60000/60000 [==============================] - 2s 36us/sample - loss: 0.1218 - val_loss: 0.1208\n",
            "Epoch 2/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.1198 - val_loss: 0.1190\n",
            "Epoch 3/100\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 0.1184 - val_loss: 0.1211\n",
            "Epoch 4/100\n",
            "60000/60000 [==============================] - 2s 36us/sample - loss: 0.1169 - val_loss: 0.1172\n",
            "Epoch 5/100\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 0.1155 - val_loss: 0.1124\n",
            "Epoch 6/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.1145 - val_loss: 0.1122\n",
            "Epoch 7/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.1135 - val_loss: 0.1149\n",
            "Epoch 8/100\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 0.1123 - val_loss: 0.1094\n",
            "Epoch 9/100\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 0.1116 - val_loss: 0.1109\n",
            "Epoch 10/100\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 0.1106 - val_loss: 0.1080\n",
            "Epoch 11/100\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 0.1099 - val_loss: 0.1076\n",
            "Epoch 12/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.1092 - val_loss: 0.1105\n",
            "Epoch 13/100\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 0.1083 - val_loss: 0.1067\n",
            "Epoch 14/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.1078 - val_loss: 0.1052\n",
            "Epoch 15/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.1070 - val_loss: 0.1047\n",
            "Epoch 16/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.1062 - val_loss: 0.1043\n",
            "Epoch 17/100\n",
            "60000/60000 [==============================] - 2s 34us/sample - loss: 0.1056 - val_loss: 0.1032\n",
            "Epoch 18/100\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 0.1050 - val_loss: 0.1043\n",
            "Epoch 19/100\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 0.1045 - val_loss: 0.1023\n",
            "Epoch 20/100\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 0.1040 - val_loss: 0.1041\n",
            "Epoch 21/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.1035 - val_loss: 0.1028\n",
            "Epoch 22/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.1031 - val_loss: 0.1018\n",
            "Epoch 23/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.1026 - val_loss: 0.1004\n",
            "Epoch 24/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.1023 - val_loss: 0.1007\n",
            "Epoch 25/100\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 0.1018 - val_loss: 0.1007\n",
            "Epoch 26/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.1014 - val_loss: 0.0998\n",
            "Epoch 27/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.1010 - val_loss: 0.0991\n",
            "Epoch 28/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.1009 - val_loss: 0.1012\n",
            "Epoch 29/100\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 0.1005 - val_loss: 0.0986\n",
            "Epoch 30/100\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 0.1001 - val_loss: 0.0996\n",
            "Epoch 31/100\n",
            "60000/60000 [==============================] - 2s 34us/sample - loss: 0.0999 - val_loss: 0.0982\n",
            "Epoch 32/100\n",
            "60000/60000 [==============================] - 2s 37us/sample - loss: 0.0997 - val_loss: 0.0996\n",
            "Epoch 33/100\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0993 - val_loss: 0.0975\n",
            "Epoch 34/100\n",
            "60000/60000 [==============================] - 2s 39us/sample - loss: 0.0991 - val_loss: 0.0973\n",
            "Epoch 35/100\n",
            "60000/60000 [==============================] - 2s 34us/sample - loss: 0.0989 - val_loss: 0.0974\n",
            "Epoch 36/100\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0986 - val_loss: 0.0968\n",
            "Epoch 37/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0985 - val_loss: 0.0967\n",
            "Epoch 38/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0981 - val_loss: 0.0964\n",
            "Epoch 39/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0980 - val_loss: 0.0968\n",
            "Epoch 40/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0977 - val_loss: 0.0963\n",
            "Epoch 41/100\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0977 - val_loss: 0.0968\n",
            "Epoch 42/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0974 - val_loss: 0.0970\n",
            "Epoch 43/100\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0972 - val_loss: 0.0961\n",
            "Epoch 44/100\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0971 - val_loss: 0.0980\n",
            "Epoch 45/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0969 - val_loss: 0.0966\n",
            "Epoch 46/100\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0968 - val_loss: 0.0951\n",
            "Epoch 47/100\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0965 - val_loss: 0.0952\n",
            "Epoch 48/100\n",
            "60000/60000 [==============================] - 2s 34us/sample - loss: 0.0965 - val_loss: 0.0961\n",
            "Epoch 49/100\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0962 - val_loss: 0.0955\n",
            "Epoch 50/100\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0961 - val_loss: 0.0946\n",
            "Epoch 51/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0960 - val_loss: 0.0947\n",
            "Epoch 52/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0957 - val_loss: 0.0947\n",
            "Epoch 53/100\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0958 - val_loss: 0.0948\n",
            "Epoch 54/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0954 - val_loss: 0.0940\n",
            "Epoch 55/100\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0955 - val_loss: 0.0939\n",
            "Epoch 56/100\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0952 - val_loss: 0.0938\n",
            "Epoch 57/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0952 - val_loss: 0.0938\n",
            "Epoch 58/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0951 - val_loss: 0.0940\n",
            "Epoch 59/100\n",
            "60000/60000 [==============================] - 2s 34us/sample - loss: 0.0949 - val_loss: 0.0957\n",
            "Epoch 60/100\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0949 - val_loss: 0.0935\n",
            "Epoch 61/100\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0946 - val_loss: 0.0946\n",
            "Epoch 62/100\n",
            "60000/60000 [==============================] - 2s 36us/sample - loss: 0.0946 - val_loss: 0.0930\n",
            "Epoch 63/100\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0944 - val_loss: 0.0949\n",
            "Epoch 64/100\n",
            "60000/60000 [==============================] - 2s 39us/sample - loss: 0.0943 - val_loss: 0.0930\n",
            "Epoch 65/100\n",
            "60000/60000 [==============================] - 2s 34us/sample - loss: 0.0943 - val_loss: 0.0928\n",
            "Epoch 66/100\n",
            "60000/60000 [==============================] - 2s 34us/sample - loss: 0.0941 - val_loss: 0.0927\n",
            "Epoch 67/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0941 - val_loss: 0.0928\n",
            "Epoch 68/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0940 - val_loss: 0.0924\n",
            "Epoch 69/100\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0938 - val_loss: 0.0926\n",
            "Epoch 70/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0937 - val_loss: 0.0962\n",
            "Epoch 71/100\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0936 - val_loss: 0.0925\n",
            "Epoch 72/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0936 - val_loss: 0.0923\n",
            "Epoch 73/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0935 - val_loss: 0.0921\n",
            "Epoch 74/100\n",
            "60000/60000 [==============================] - 2s 34us/sample - loss: 0.0934 - val_loss: 0.0930\n",
            "Epoch 75/100\n",
            "60000/60000 [==============================] - 2s 34us/sample - loss: 0.0933 - val_loss: 0.0933\n",
            "Epoch 76/100\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0932 - val_loss: 0.0918\n",
            "Epoch 77/100\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0931 - val_loss: 0.0917\n",
            "Epoch 78/100\n",
            "60000/60000 [==============================] - 2s 35us/sample - loss: 0.0931 - val_loss: 0.0924\n",
            "Epoch 79/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0930 - val_loss: 0.0914\n",
            "Epoch 80/100\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0929 - val_loss: 0.0922\n",
            "Epoch 81/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0928 - val_loss: 0.0915\n",
            "Epoch 82/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0927 - val_loss: 0.0915\n",
            "Epoch 83/100\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0926 - val_loss: 0.0914\n",
            "Epoch 84/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0926 - val_loss: 0.0911\n",
            "Epoch 85/100\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0924 - val_loss: 0.0910\n",
            "Epoch 86/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0925 - val_loss: 0.0910\n",
            "Epoch 87/100\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0924 - val_loss: 0.0915\n",
            "Epoch 88/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0923 - val_loss: 0.0911\n",
            "Epoch 89/100\n",
            "60000/60000 [==============================] - 2s 34us/sample - loss: 0.0922 - val_loss: 0.0907\n",
            "Epoch 90/100\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0922 - val_loss: 0.0906\n",
            "Epoch 91/100\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0921 - val_loss: 0.0914\n",
            "Epoch 92/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0920 - val_loss: 0.0909\n",
            "Epoch 93/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0920 - val_loss: 0.0921\n",
            "Epoch 94/100\n",
            "60000/60000 [==============================] - 2s 35us/sample - loss: 0.0919 - val_loss: 0.0912\n",
            "Epoch 95/100\n",
            "60000/60000 [==============================] - 2s 36us/sample - loss: 0.0918 - val_loss: 0.0905\n",
            "Epoch 96/100\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0918 - val_loss: 0.0915\n",
            "Epoch 97/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0917 - val_loss: 0.0908\n",
            "Epoch 98/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0916 - val_loss: 0.0902\n",
            "Epoch 99/100\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0915 - val_loss: 0.0905\n",
            "Epoch 100/100\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0915 - val_loss: 0.0911\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f2f27c5f0f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGG3VLuoc63A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "outputId": "f62e8858-3768-4e19-9371-02fd1c825651"
      },
      "source": [
        "decoded_imgs = autoencoder.predict(x_test)\n",
        "\n",
        "n = 10\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "    # display original\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(x_test[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # display reconstruction\n",
        "    ax = plt.subplot(2, n, i + n + 1)\n",
        "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABG0AAADnCAYAAACkCqtqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGwAAAAAAgAjioQ0AAAAAAEAE/T99TatAoHTrbAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1440x288 with 20 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAD2wUf7c63E",
        "colab_type": "text"
      },
      "source": [
        "#### Visualization of the Representations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3f06e7rc63K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "outputId": "7ce9774a-c186-4414-d2cc-2a561c9eccfa"
      },
      "source": [
        "encoder = Model(input_img, encoded)\n",
        "encoded_imgs = encoder.predict(x_train)\n",
        "n = 10\n",
        "plt.figure(figsize=(20, 8))\n",
        "for i in range(n):\n",
        "    ax = plt.subplot(1, n, i + 1)\n",
        "    plt.imshow(encoded_imgs[i].reshape(4, 4 * 8).T)\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABEUAAAHECAYAAADfz5RiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 1440x576 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9q1dTJic63T",
        "colab_type": "text"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "You will train an autoencoder at some point in the near future. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KdMOfhjc63V",
        "colab_type": "text"
      },
      "source": [
        "# Information Retrieval with Autoencoders (Learn)\n",
        "<a id=\"p3\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ClYWTw7c63Y",
        "colab_type": "text"
      },
      "source": [
        "## Overview\n",
        "\n",
        "A common usecase for autoencoders is for reverse image search. Let's try to draw an image and see what's most similiar in our dataset. \n",
        "\n",
        "To accomplish this we will need to slice our autoendoer in half to extract our reduced features. :) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEonxfr9c63b",
        "colab_type": "text"
      },
      "source": [
        "## Follow Along"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9ofj9vnc63d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = Model(input_img, encoded)\n",
        "encoded_imgs = encoder.predict(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xns1nvosc63g",
        "colab_type": "code",
        "outputId": "54e6bfd0-b6b2-4603-93b8-108c3fdf7994",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "encoded_imgs[0].reshape(128,1).T"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.4790851 , 2.3232322 , 1.087991  , 0.9226434 , 1.6101459 ,\n",
              "        2.2363017 , 1.7657552 , 2.3495488 , 0.        , 2.8328328 ,\n",
              "        1.8055375 , 3.3572028 , 3.4000893 , 3.432051  , 4.6047115 ,\n",
              "        3.241147  , 0.        , 2.3656414 , 1.5640965 , 2.9645853 ,\n",
              "        3.667851  , 3.084609  , 4.518644  , 3.3771532 , 0.        ,\n",
              "        1.6485674 , 0.17617878, 1.044943  , 1.6217088 , 1.1776809 ,\n",
              "        2.9426231 , 1.4739957 , 1.1625355 , 1.2991534 , 0.8546624 ,\n",
              "        1.8235604 , 2.9661636 , 2.6701665 , 1.2851604 , 3.1148674 ,\n",
              "        0.4240024 , 3.448675  , 0.20827171, 5.1491485 , 4.5644727 ,\n",
              "        3.9846869 , 5.278815  , 3.3978376 , 0.        , 3.1698608 ,\n",
              "        0.        , 3.8498943 , 4.4716063 , 3.121283  , 5.731592  ,\n",
              "        2.3804243 , 0.18434393, 2.2308931 , 0.        , 1.0999466 ,\n",
              "        1.4097198 , 1.2093432 , 2.1723142 , 0.931146  , 0.15338063,\n",
              "        2.2272007 , 1.6636353 , 2.5554733 , 2.077355  , 2.2723439 ,\n",
              "        2.7362065 , 3.2313194 , 0.        , 3.9330132 , 1.9940858 ,\n",
              "        4.168784  , 4.4598427 , 4.055987  , 4.7946897 , 3.6937566 ,\n",
              "        0.24039316, 3.9162617 , 0.        , 4.1102595 , 4.6011796 ,\n",
              "        3.5312166 , 5.051401  , 2.3485897 , 0.70206803, 1.303743  ,\n",
              "        0.        , 1.0155858 , 0.6872325 , 1.1781101 , 1.4719286 ,\n",
              "        0.68852144, 0.46757293, 1.7307214 , 0.        , 2.4287705 ,\n",
              "        3.0110612 , 1.9790478 , 1.7672589 , 2.574007  , 0.52342343,\n",
              "        2.2049198 , 0.        , 3.3068228 , 2.9355245 , 1.7935961 ,\n",
              "        2.5332823 , 1.7298414 , 1.4197683 , 1.3806779 , 0.        ,\n",
              "        1.7505124 , 1.2616323 , 0.605127  , 1.6629655 , 0.7087393 ,\n",
              "        1.2678204 , 0.5962485 , 0.        , 0.42957258, 0.5235811 ,\n",
              "        0.4149214 , 0.92243606, 0.51311743]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O40mSTC-0z6A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = encoded_imgs.reshape((encoded_imgs.shape[0],128))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYGRTsvY1r8a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8147f87b-4381-40c6-edbb-b4a8bcd4a401"
      },
      "source": [
        "train.shape"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 128)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oR8xTBExc63k",
        "colab_type": "code",
        "outputId": "7ef4b5be-6b0a-4391-8bb1-a91b174da1ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "nn = NearestNeighbors(n_neighbors=10, algorithm='ball_tree')\n",
        "nn.fit(train)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NearestNeighbors(algorithm='ball_tree', leaf_size=30, metric='minkowski',\n",
              "                 metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
              "                 radius=1.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4s_OaZGPc63u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "1d2da67a-d0a1-4d05-b523-1cbf243f8053"
      },
      "source": [
        "test = encoder.predict(x_test)\n",
        "test = test.reshape(test.shape[0], 128)\n",
        "\n",
        "nn.kneighbors([test[0]])"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[1.33353514, 1.90090051, 1.99074035, 2.08060219, 2.16404304,\n",
              "         2.18149654, 2.20976844, 2.27837042, 2.2832953 , 2.31402112]]),\n",
              " array([[53843, 47003,  4130, 14505, 29762, 40368, 50255, 14563, 53783,\n",
              "         30502]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjazRWXt2eNJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "35782ec4-1f7f-4514-e47a-7b7eb491e24a"
      },
      "source": [
        "search_image = test[0]\n",
        "\n",
        "plt.imshow(x_test[0].reshape(28,28))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f2f25afc908>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCcIOJEHYgSQIO5DEvwEvYRv57rmVLgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKfVD6qI2tQG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "48b10b24-6eab-4966-94ee-28370e016ef1"
      },
      "source": [
        "best_response = train[53843]\n",
        "\n",
        "plt.imshow(x_train[53843].reshape(28,28))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f2f259f52e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAM30lEQVR4nO3dX4gV993H8c8nVm9U0DwhItak1SQE\nyYUtIgFLaAhKKojRQKgXxUDoFq0PEXpRsReaXIWHp5aHXBS2+VNbWqXBJpFQ8tRIQRpIySbYxCTW\n+AQXV9a1TS5MQ9DEfp+LnYRtsmd2nZlz5qzf9wuWc858z8x8OcnHmTN/zs8RIQDXvuvabgBAbxB2\nIAnCDiRB2IEkCDuQxFd6uTLbHPoHuiwiPNn0Wlt22/fa/pvt07Z31VkWgO5y1fPstmdJOiVpraQR\nSa9K2hIRb5fMw5Yd6LJubNlXSzodEe9FxGVJByVtrLE8AF1UJ+xLJJ2d8HqkmPZvbA/YHrI9VGNd\nAGrq+gG6iBiUNCixGw+0qc6W/ZykpRNef7WYBqAP1Qn7q5Jutf1123MkfVfS4WbaAtC0yrvxEfGp\n7R2S/lfSLElPRcRbjXUGoFGVT71VWhnf2YGu68pFNQBmDsIOJEHYgSQIO5AEYQeSIOxAEoQdSIKw\nA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiC\nsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEpXHZ5ck22ckfSjpiqRPI2JVE00BaF6tsBfujoh/NLAc\nAF3EbjyQRN2wh6Q/2n7N9sBkb7A9YHvI9lDNdQGowRFRfWZ7SUScs32jpCOS/jMijpW8v/rKAExL\nRHiy6bW27BFxrni8IOlZSavrLA9A91QOu+25tud/9lzSOkknmmoMQLPqHI1fJOlZ258t57cR8WIj\nXQFoXK3v7Fe9Mr6zA13Xle/sAGYOwg4kQdiBJAg7kARhB5Jo4kaYGeHGG28srZ88ebK0vnDhwo61\nRx55pHTes2fPltZPnz5dWh8aKr/S+KOPPiqtz1TLli0rrS9durS0XvbfdGxsrFJPMxlbdiAJwg4k\nQdiBJAg7kARhB5Ig7EAShB1IgrveCmvWrCmt7927t2PtrrvuKp139uzZVVr63MjISGn9k08+qbX8\nflV2bYMkLViwoLT+3HPPdaxt3ry5Uk8zAXe9AckRdiAJwg4kQdiBJAg7kARhB5Ig7EASae5nn8rL\nL79cWl+7dm3H2p133lk678aNG0vrmzZtKq3fdtttpXVMbt68eW230FfYsgNJEHYgCcIOJEHYgSQI\nO5AEYQeSIOxAEtzP3gdmzZpVWi+GxU5ncHCwtP7ggw+W1l966aWOtXXr1lVpaUaofD+77adsX7B9\nYsK0620fsf1u8Vj+KwMAWjed3fhfSrr3C9N2SToaEbdKOlq8BtDHpgx7RByT9MEXJm+UtL94vl/S\nfQ33BaBhVa+NXxQRo8Xz85IWdXqj7QFJAxXXA6AhtW+EiYgoO/AWEYOSBiUO0AFtqnrqbcz2Ykkq\nHi801xKAbqga9sOSthbPt0p6vpl2AHTLlLvxtg9I+rakG2yPSNoj6TFJv7P9kKRhSQ90s8lr3ZUr\nV9puoRXLly8vrd9///2l9amuETlw4MBV93QtmzLsEbGlQ+mehnsB0EVcLgskQdiBJAg7kARhB5Ig\n7EAS/JQ0WrN9+/bS+vz580vrp06dKq0//fTTV93TtYwtO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4k\nwXl2dNWcOXM61jZs2FBr2c8880yt+bNhyw4kQdiBJAg7kARhB5Ig7EAShB1IgrADSXCeHV21Z8+e\njrVbbrmldN7R0dHS+hNPPFGpp6zYsgNJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEpxnRy0333xzaX3b\ntm2Vl/3444+X1oeHhysvO6Mpt+y2n7J9wfaJCdP22j5n+3jxt767bQKoazq78b+UdO8k038WESuL\nvz802xaApk0Z9og4JumDHvQCoIvqHKDbYfuNYjd/Yac32R6wPWR7qMa6ANRUNew/l7Rc0kpJo5J+\n2umNETEYEasiYlXFdQFoQKWwR8RYRFyJiH9J+oWk1c22BaBplcJue/GEl5sknej0XgD9wRFR/gb7\ngKRvS7pB0pikPcXrlZJC0hlJP4iI8puPx5dVvjLMOEePHi2t33333R1rFy9eLJ339ttvL62fP3++\ntJ5VRHiy6VNeVBMRWyaZ/GTtjgD0FJfLAkkQdiAJwg4kQdiBJAg7kAS3uKLUsmXLSuurV1e/nmrX\nrl2ldU6tNYstO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kwXl2lNqxY0dpfe7cuaX1S5cuday98MIL\nlXpCNWzZgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJzrMnd9NNN5XWt2/fXmv5O3fu7FgbGRmptWxc\nHbbsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AE59mvcdddV/7v+e7du0vrc+bMKa2///77pfUXX3yx\ntI7emXLLbnup7T/Zftv2W7YfLqZfb/uI7XeLx4XdbxdAVdPZjf9U0o8iYoWkOyX90PYKSbskHY2I\nWyUdLV4D6FNThj0iRiPi9eL5h5LekbRE0kZJ+4u37Zd0X7eaBFDfVX1nt/01Sd+Q9BdJiyJitCid\nl7SowzwDkgaqtwigCdM+Gm97nqRDknZGxMWJtYgISTHZfBExGBGrImJVrU4B1DKtsNuerfGg/yYi\nfl9MHrO9uKgvlnShOy0CaMKUu/G2LelJSe9ExL4JpcOStkp6rHh8visdopb169eX1gcG6n3D2rZt\nW2l9eHi41vLRnOl8Z18j6XuS3rR9vJi2W+Mh/53thyQNS3qgOy0CaMKUYY+IP0tyh/I9zbYDoFu4\nXBZIgrADSRB2IAnCDiRB2IEkPH7xW49WZvduZYnccccdHWvHjh0rnXfBggWl9VdeeaW0fs895Sdk\nPv7449I6mhcRk549Y8sOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0nwU9IzwPhPCnS2YcOGjrWpzqNf\nvny5tD7VkM2cR5852LIDSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBLczz4DbNq0qbR+6NChyss+efJk\naX3FihWVl412cD87kBxhB5Ig7EAShB1IgrADSRB2IAnCDiQxnfHZl0r6laRFkkLSYET8j+29kr4v\n6e/FW3dHxB+61WhmmzdvrjzvpUuXSuuPPvpo5WVjZpnOj1d8KulHEfG67fmSXrN9pKj9LCL+u3vt\nAWjKdMZnH5U0Wjz/0PY7kpZ0uzEAzbqq7+y2vybpG5L+UkzaYfsN20/ZXthhngHbQ7aHanUKoJZp\nh932PEmHJO2MiIuSfi5puaSVGt/y/3Sy+SJiMCJWRcSqBvoFUNG0wm57tsaD/puI+L0kRcRYRFyJ\niH9J+oWk1d1rE0BdU4bd4z9t+qSkdyJi34Tpiye8bZOkE823B6Ap0zkav0bS9yS9aft4MW23pC22\nV2r8dNwZST/oSofQvHnzKs+7b9++0vrBgwcrLxszy3SOxv9Z0mT3x3JOHZhBuIIOSIKwA0kQdiAJ\nwg4kQdiBJAg7kAQ/JQ1cY/gpaSA5wg4kQdiBJAg7kARhB5Ig7EAShB1IYjr3szfpH5KGJ7y+oZjW\nj/q1t37tS6K3qprs7eZOhZ5eVPOlldtD/frbdP3aW7/2JdFbVb3qjd14IAnCDiTRdtgHW15/mX7t\nrV/7kuitqp701up3dgC90/aWHUCPEHYgiVbCbvte23+zfdr2rjZ66MT2Gdtv2j7e9vh0xRh6F2yf\nmDDtettHbL9bPE46xl5Lve21fa747I7bXt9Sb0tt/8n227bfsv1wMb3Vz66kr558bj3/zm57lqRT\nktZKGpH0qqQtEfF2TxvpwPYZSasiovULMGzfJemfkn4VEXcU0/5L0gcR8VjxD+XCiPhxn/S2V9I/\n2x7GuxitaPHEYcYl3SfpQbX42ZX09YB68Lm1sWVfLel0RLwXEZclHZS0sYU++l5EHJP0wRcmb5S0\nv3i+X+P/s/Rch976QkSMRsTrxfMPJX02zHirn11JXz3RRtiXSDo74fWI+mu895D0R9uv2R5ou5lJ\nLIqI0eL5eUmL2mxmElMO491LXxhmvG8+uyrDn9fFAbov+1ZEfFPSdyT9sNhd7Usx/h2sn86dTmsY\n716ZZJjxz7X52VUd/ryuNsJ+TtLSCa+/WkzrCxFxrni8IOlZ9d9Q1GOfjaBbPF5ouZ/P9dMw3pMN\nM64++OzaHP68jbC/KulW21+3PUfSdyUdbqGPL7E9tzhwIttzJa1T/w1FfVjS1uL5VknPt9jLv+mX\nYbw7DTOulj+71oc/j4ie/0lar/Ej8v8n6Sdt9NChr2WS/lr8vdV2b5IOaHy37hONH9t4SNJ/SDoq\n6V1JL0m6vo96+7WkNyW9ofFgLW6pt29pfBf9DUnHi7/1bX92JX315HPjclkgCQ7QAUkQdiAJwg4k\nQdiBJAg7kARhB5Ig7EAS/w8qJPZXgmiF9QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ut0guH25c630",
        "colab_type": "text"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "You should already be familiar with KNN and similarity queries, so the key component of this section is know what to 'slice' from your autoencoder (the encoder) to extract features from your data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23g6fbfnc632",
        "colab_type": "text"
      },
      "source": [
        "# Review\n",
        "\n",
        "* <a href=\"#p1\">Part 1</a>: Describe the componenets of an autoencoder\n",
        "    - Enocder\n",
        "    - Decoder\n",
        "* <a href=\"#p2\">Part 2</a>: Train an autoencoder\n",
        "    - Can do in Keras Easily\n",
        "    - Can use a variety of architectures\n",
        "    - Architectures must follow hourglass shape\n",
        "* <a href=\"#p3\">Part 3</a>: Apply an autoenocder to a basic information retrieval problem\n",
        "    - Extract just the encoder to use for various tasks\n",
        "    - AE ares good for dimensionality reduction, reverse image search, and may more things. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dduwkAqFc633",
        "colab_type": "text"
      },
      "source": [
        "# Sources\n",
        "\n",
        "__References__\n",
        "- [Building Autoencoders in Keras](https://blog.keras.io/building-autoencoders-in-keras.html)\n",
        "- [Deep Learning Cookbook](http://shop.oreilly.com/product/0636920097471.do)\n",
        "\n",
        "__Additional Material__"
      ]
    }
  ]
}