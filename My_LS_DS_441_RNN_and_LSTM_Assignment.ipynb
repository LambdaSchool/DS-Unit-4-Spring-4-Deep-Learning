{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "My LS_DS_441_RNN_and_LSTM_Assignment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wel51x/DS-Unit-4-Sprint-4-Deep-Learning/blob/master/My_LS_DS_441_RNN_and_LSTM_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_IizNKWLomoA"
      },
      "source": [
        "# Lambda School Data Science - Recurrent Neural Networks and LSTM\n",
        "\n",
        "> \"Yesterday's just a memory - tomorrow is never what it's supposed to be.\" -- Bob Dylan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAbnJeJ8j0s3",
        "colab_type": "text"
      },
      "source": [
        "####have down-version numpy to get \"RNN/LSTM Sentiment Classification with Keras\" to work in colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yBeveIrj7x4",
        "colab_type": "code",
        "outputId": "baabc43b-261d-4015-dc55-35a25425a6fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "source": [
        "!pip install numpy==1.16.2\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting numpy==1.16.2\n",
            "  Using cached https://files.pythonhosted.org/packages/35/d5/4f8410ac303e690144f0a0603c4b8fd3b986feb2749c435f7cdbb288f17e/numpy-1.16.2-cp36-cp36m-manylinux1_x86_64.whl\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy\n",
            "  Found existing installation: numpy 1.16.3\n",
            "    Uninstalling numpy-1.16.3:\n",
            "      Successfully uninstalled numpy-1.16.3\n",
            "Successfully installed numpy-1.16.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "os-szg47dgwf"
      },
      "source": [
        "### Forecasting\n",
        "\n",
        "Forecasting - at it's simplest, it just means \"predict the future\":"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0lfZdD_cp1t5"
      },
      "source": [
        "# Assignment\n",
        "\n",
        "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
        "\n",
        "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
        "\n",
        "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
        "\n",
        "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
        "\n",
        "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
        "\n",
        "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pqhutb1j9sb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports\n",
        "from random import random\n",
        "import numpy as np\n",
        "import requests"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ltj1je1fp5rO",
        "colab": {}
      },
      "source": [
        "# TODO - Words, words, mere words, no matter from the heart.\n",
        "# Grab first ten\n",
        "r = requests.get('http://www.gutenberg.org/files/100/100-0.txt', verify=True)\n",
        "x = r.text.find('From')\n",
        "y = r.text.find('thine or thee.')\n",
        "article_text = r.text[x : y+14]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6X5y_IKixSD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "75ff32c6-195f-4345-cf5b-f54d1687b7d0"
      },
      "source": [
        "chars = list(set(article_text)) # split and remove duplicate characters. convert to list.\n",
        "\n",
        "num_chars = len(chars) # the number of unique characters\n",
        "txt_data_size = len(article_text)\n",
        "\n",
        "print(\"unique characters : \", num_chars)\n",
        "print(\"txt_data_size : \", txt_data_size)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unique characters :  68\n",
            "txt_data_size :  6561\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRCADQermdIJ",
        "colab_type": "text"
      },
      "source": [
        "#### one hot encode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCkYGITxjK9v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "033b0b3d-b8e5-4137-8c74-f14b5e52ccb5"
      },
      "source": [
        "char_to_int = dict((c, i) for i, c in enumerate(chars)) # \"enumerate\" retruns index and value. Convert it to dictionary\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "print(char_to_int)\n",
        "print(\"----------------------------------------------------\")\n",
        "print(int_to_char)\n",
        "print(\"----------------------------------------------------\")\n",
        "# integer encode input data\n",
        "integer_encoded = [char_to_int[i] for i in article_text] # \"integer_encoded\" is a list which has a sequence converted from an original data to integers.\n",
        "print(integer_encoded)\n",
        "print(\"----------------------------------------------------\")\n",
        "print(\"data length : \", len(integer_encoded))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'C': 0, 'j': 1, 'U': 2, 'l': 3, '\\n': 4, 'G': 5, 'H': 6, 'u': 7, 'A': 8, ',': 9, 'S': 10, 'B': 11, 'b': 12, 'f': 13, 'y': 14, 'c': 15, 't': 16, 'o': 17, 'v': 18, ':': 19, '?': 20, '8': 21, 'w': 22, 'a': 23, 'T': 24, 'p': 25, '4': 26, 'R': 27, '6': 28, 'h': 29, 'D': 30, 'O': 31, 'x': 32, 'L': 33, ')': 34, '2': 35, 'k': 36, '\\r': 37, '1': 38, 's': 39, 'z': 40, ' ': 41, '7': 42, 'm': 43, '’': 44, '9': 45, 'e': 46, '3': 47, '0': 48, 'M': 49, ';': 50, 'i': 51, 'Y': 52, 'q': 53, 'P': 54, '5': 55, 'F': 56, '.': 57, '(': 58, 'g': 59, '‘': 60, 'I': 61, 'W': 62, 'N': 63, 'n': 64, 'r': 65, 'd': 66, '-': 67}\n",
            "----------------------------------------------------\n",
            "{0: 'C', 1: 'j', 2: 'U', 3: 'l', 4: '\\n', 5: 'G', 6: 'H', 7: 'u', 8: 'A', 9: ',', 10: 'S', 11: 'B', 12: 'b', 13: 'f', 14: 'y', 15: 'c', 16: 't', 17: 'o', 18: 'v', 19: ':', 20: '?', 21: '8', 22: 'w', 23: 'a', 24: 'T', 25: 'p', 26: '4', 27: 'R', 28: '6', 29: 'h', 30: 'D', 31: 'O', 32: 'x', 33: 'L', 34: ')', 35: '2', 36: 'k', 37: '\\r', 38: '1', 39: 's', 40: 'z', 41: ' ', 42: '7', 43: 'm', 44: '’', 45: '9', 46: 'e', 47: '3', 48: '0', 49: 'M', 50: ';', 51: 'i', 52: 'Y', 53: 'q', 54: 'P', 55: '5', 56: 'F', 57: '.', 58: '(', 59: 'g', 60: '‘', 61: 'I', 62: 'W', 63: 'N', 64: 'n', 65: 'r', 66: 'd', 67: '-'}\n",
            "----------------------------------------------------\n",
            "[56, 65, 17, 43, 41, 13, 23, 51, 65, 46, 39, 16, 41, 15, 65, 46, 23, 16, 7, 65, 46, 39, 41, 22, 46, 41, 66, 46, 39, 51, 65, 46, 41, 51, 64, 15, 65, 46, 23, 39, 46, 9, 37, 4, 24, 29, 23, 16, 41, 16, 29, 46, 65, 46, 12, 14, 41, 12, 46, 23, 7, 16, 14, 44, 39, 41, 65, 17, 39, 46, 41, 43, 51, 59, 29, 16, 41, 64, 46, 18, 46, 65, 41, 66, 51, 46, 9, 37, 4, 11, 7, 16, 41, 23, 39, 41, 16, 29, 46, 41, 65, 51, 25, 46, 65, 41, 39, 29, 17, 7, 3, 66, 41, 12, 14, 41, 16, 51, 43, 46, 41, 66, 46, 15, 46, 23, 39, 46, 9, 37, 4, 6, 51, 39, 41, 16, 46, 64, 66, 46, 65, 41, 29, 46, 51, 65, 41, 43, 51, 59, 29, 16, 41, 12, 46, 23, 65, 41, 29, 51, 39, 41, 43, 46, 43, 17, 65, 14, 19, 37, 4, 11, 7, 16, 41, 16, 29, 17, 7, 41, 15, 17, 64, 16, 65, 23, 15, 16, 46, 66, 41, 16, 17, 41, 16, 29, 51, 64, 46, 41, 17, 22, 64, 41, 12, 65, 51, 59, 29, 16, 41, 46, 14, 46, 39, 9, 37, 4, 56, 46, 46, 66, 44, 39, 16, 41, 16, 29, 14, 41, 3, 51, 59, 29, 16, 44, 39, 41, 13, 3, 23, 43, 46, 41, 22, 51, 16, 29, 41, 39, 46, 3, 13, 67, 39, 7, 12, 39, 16, 23, 64, 16, 51, 23, 3, 41, 13, 7, 46, 3, 9, 37, 4, 49, 23, 36, 51, 64, 59, 41, 23, 41, 13, 23, 43, 51, 64, 46, 41, 22, 29, 46, 65, 46, 41, 23, 12, 7, 64, 66, 23, 64, 15, 46, 41, 3, 51, 46, 39, 9, 37, 4, 24, 29, 14, 41, 39, 46, 3, 13, 41, 16, 29, 14, 41, 13, 17, 46, 9, 41, 16, 17, 41, 16, 29, 14, 41, 39, 22, 46, 46, 16, 41, 39, 46, 3, 13, 41, 16, 17, 17, 41, 15, 65, 7, 46, 3, 19, 37, 4, 24, 29, 17, 7, 41, 16, 29, 23, 16, 41, 23, 65, 16, 41, 64, 17, 22, 41, 16, 29, 46, 41, 22, 17, 65, 3, 66, 44, 39, 41, 13, 65, 46, 39, 29, 41, 17, 65, 64, 23, 43, 46, 64, 16, 9, 37, 4, 8, 64, 66, 41, 17, 64, 3, 14, 41, 29, 46, 65, 23, 3, 66, 41, 16, 17, 41, 16, 29, 46, 41, 59, 23, 7, 66, 14, 41, 39, 25, 65, 51, 64, 59, 9, 37, 4, 62, 51, 16, 29, 51, 64, 41, 16, 29, 51, 64, 46, 41, 17, 22, 64, 41, 12, 7, 66, 41, 12, 7, 65, 51, 46, 39, 16, 41, 16, 29, 14, 41, 15, 17, 64, 16, 46, 64, 16, 9, 37, 4, 8, 64, 66, 9, 41, 16, 46, 64, 66, 46, 65, 41, 15, 29, 7, 65, 3, 9, 41, 43, 23, 36, 44, 39, 16, 41, 22, 23, 39, 16, 46, 41, 51, 64, 41, 64, 51, 59, 59, 23, 65, 66, 51, 64, 59, 19, 37, 4, 41, 41, 54, 51, 16, 14, 41, 16, 29, 46, 41, 22, 17, 65, 3, 66, 9, 41, 17, 65, 41, 46, 3, 39, 46, 41, 16, 29, 51, 39, 41, 59, 3, 7, 16, 16, 17, 64, 41, 12, 46, 9, 37, 4, 41, 41, 24, 17, 41, 46, 23, 16, 41, 16, 29, 46, 41, 22, 17, 65, 3, 66, 44, 39, 41, 66, 7, 46, 9, 41, 12, 14, 41, 16, 29, 46, 41, 59, 65, 23, 18, 46, 41, 23, 64, 66, 41, 16, 29, 46, 46, 57, 37, 4, 37, 4, 37, 4, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 35, 37, 4, 37, 4, 62, 29, 46, 64, 41, 13, 17, 65, 16, 14, 41, 22, 51, 64, 16, 46, 65, 39, 41, 39, 29, 23, 3, 3, 41, 12, 46, 39, 51, 46, 59, 46, 41, 16, 29, 14, 41, 12, 65, 17, 22, 9, 37, 4, 8, 64, 66, 41, 66, 51, 59, 41, 66, 46, 46, 25, 41, 16, 65, 46, 64, 15, 29, 46, 39, 41, 51, 64, 41, 16, 29, 14, 41, 12, 46, 23, 7, 16, 14, 44, 39, 41, 13, 51, 46, 3, 66, 9, 37, 4, 24, 29, 14, 41, 14, 17, 7, 16, 29, 44, 39, 41, 25, 65, 17, 7, 66, 41, 3, 51, 18, 46, 65, 14, 41, 39, 17, 41, 59, 23, 40, 46, 66, 41, 17, 64, 41, 64, 17, 22, 9, 37, 4, 62, 51, 3, 3, 41, 12, 46, 41, 23, 41, 16, 23, 16, 16, 46, 65, 46, 66, 41, 22, 46, 46, 66, 41, 17, 13, 41, 39, 43, 23, 3, 3, 41, 22, 17, 65, 16, 29, 41, 29, 46, 3, 66, 19, 37, 4, 24, 29, 46, 64, 41, 12, 46, 51, 64, 59, 41, 23, 39, 36, 46, 66, 9, 41, 22, 29, 46, 65, 46, 41, 23, 3, 3, 41, 16, 29, 14, 41, 12, 46, 23, 7, 16, 14, 41, 3, 51, 46, 39, 9, 37, 4, 62, 29, 46, 65, 46, 41, 23, 3, 3, 41, 16, 29, 46, 41, 16, 65, 46, 23, 39, 7, 65, 46, 41, 17, 13, 41, 16, 29, 14, 41, 3, 7, 39, 16, 14, 41, 66, 23, 14, 39, 50, 37, 4, 24, 17, 41, 39, 23, 14, 9, 41, 22, 51, 16, 29, 51, 64, 41, 16, 29, 51, 64, 46, 41, 17, 22, 64, 41, 66, 46, 46, 25, 41, 39, 7, 64, 36, 46, 64, 41, 46, 14, 46, 39, 9, 37, 4, 62, 46, 65, 46, 41, 23, 64, 41, 23, 3, 3, 67, 46, 23, 16, 51, 64, 59, 41, 39, 29, 23, 43, 46, 9, 41, 23, 64, 66, 41, 16, 29, 65, 51, 13, 16, 3, 46, 39, 39, 41, 25, 65, 23, 51, 39, 46, 57, 37, 4, 6, 17, 22, 41, 43, 7, 15, 29, 41, 43, 17, 65, 46, 41, 25, 65, 23, 51, 39, 46, 41, 66, 46, 39, 46, 65, 18, 44, 66, 41, 16, 29, 14, 41, 12, 46, 23, 7, 16, 14, 44, 39, 41, 7, 39, 46, 9, 37, 4, 61, 13, 41, 16, 29, 17, 7, 41, 15, 17, 7, 3, 66, 39, 16, 41, 23, 64, 39, 22, 46, 65, 41, 60, 24, 29, 51, 39, 41, 13, 23, 51, 65, 41, 15, 29, 51, 3, 66, 41, 17, 13, 41, 43, 51, 64, 46, 37, 4, 10, 29, 23, 3, 3, 41, 39, 7, 43, 41, 43, 14, 41, 15, 17, 7, 64, 16, 9, 41, 23, 64, 66, 41, 43, 23, 36, 46, 41, 43, 14, 41, 17, 3, 66, 41, 46, 32, 15, 7, 39, 46, 9, 44, 37, 4, 54, 65, 17, 18, 51, 64, 59, 41, 29, 51, 39, 41, 12, 46, 23, 7, 16, 14, 41, 12, 14, 41, 39, 7, 15, 15, 46, 39, 39, 51, 17, 64, 41, 16, 29, 51, 64, 46, 57, 37, 4, 41, 41, 24, 29, 51, 39, 41, 22, 46, 65, 46, 41, 16, 17, 41, 12, 46, 41, 64, 46, 22, 41, 43, 23, 66, 46, 41, 22, 29, 46, 64, 41, 16, 29, 17, 7, 41, 23, 65, 16, 41, 17, 3, 66, 9, 37, 4, 41, 41, 8, 64, 66, 41, 39, 46, 46, 41, 16, 29, 14, 41, 12, 3, 17, 17, 66, 41, 22, 23, 65, 43, 41, 22, 29, 46, 64, 41, 16, 29, 17, 7, 41, 13, 46, 46, 3, 44, 39, 16, 41, 51, 16, 41, 15, 17, 3, 66, 57, 37, 4, 37, 4, 37, 4, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 47, 37, 4, 37, 4, 33, 17, 17, 36, 41, 51, 64, 41, 16, 29, 14, 41, 59, 3, 23, 39, 39, 41, 23, 64, 66, 41, 16, 46, 3, 3, 41, 16, 29, 46, 41, 13, 23, 15, 46, 41, 16, 29, 17, 7, 41, 18, 51, 46, 22, 46, 39, 16, 9, 37, 4, 63, 17, 22, 41, 51, 39, 41, 16, 29, 46, 41, 16, 51, 43, 46, 41, 16, 29, 23, 16, 41, 13, 23, 15, 46, 41, 39, 29, 17, 7, 3, 66, 41, 13, 17, 65, 43, 41, 23, 64, 17, 16, 29, 46, 65, 9, 37, 4, 62, 29, 17, 39, 46, 41, 13, 65, 46, 39, 29, 41, 65, 46, 25, 23, 51, 65, 41, 51, 13, 41, 64, 17, 22, 41, 16, 29, 17, 7, 41, 64, 17, 16, 41, 65, 46, 64, 46, 22, 46, 39, 16, 9, 37, 4, 24, 29, 17, 7, 41, 66, 17, 39, 16, 41, 12, 46, 59, 7, 51, 3, 46, 41, 16, 29, 46, 41, 22, 17, 65, 3, 66, 9, 41, 7, 64, 12, 3, 46, 39, 39, 41, 39, 17, 43, 46, 41, 43, 17, 16, 29, 46, 65, 57, 37, 4, 56, 17, 65, 41, 22, 29, 46, 65, 46, 41, 51, 39, 41, 39, 29, 46, 41, 39, 17, 41, 13, 23, 51, 65, 41, 22, 29, 17, 39, 46, 41, 7, 64, 46, 23, 65, 46, 66, 41, 22, 17, 43, 12, 37, 4, 30, 51, 39, 66, 23, 51, 64, 39, 41, 16, 29, 46, 41, 16, 51, 3, 3, 23, 59, 46, 41, 17, 13, 41, 16, 29, 14, 41, 29, 7, 39, 12, 23, 64, 66, 65, 14, 20, 37, 4, 31, 65, 41, 22, 29, 17, 41, 51, 39, 41, 29, 46, 41, 39, 17, 41, 13, 17, 64, 66, 41, 22, 51, 3, 3, 41, 12, 46, 41, 16, 29, 46, 41, 16, 17, 43, 12, 37, 4, 31, 13, 41, 29, 51, 39, 41, 39, 46, 3, 13, 67, 3, 17, 18, 46, 41, 16, 17, 41, 39, 16, 17, 25, 41, 25, 17, 39, 16, 46, 65, 51, 16, 14, 20, 37, 4, 24, 29, 17, 7, 41, 23, 65, 16, 41, 16, 29, 14, 41, 43, 17, 16, 29, 46, 65, 44, 39, 41, 59, 3, 23, 39, 39, 41, 23, 64, 66, 41, 39, 29, 46, 41, 51, 64, 41, 16, 29, 46, 46, 37, 4, 0, 23, 3, 3, 39, 41, 12, 23, 15, 36, 41, 16, 29, 46, 41, 3, 17, 18, 46, 3, 14, 41, 8, 25, 65, 51, 3, 41, 17, 13, 41, 29, 46, 65, 41, 25, 65, 51, 43, 46, 9, 37, 4, 10, 17, 41, 16, 29, 17, 7, 41, 16, 29, 65, 17, 7, 59, 29, 41, 22, 51, 64, 66, 17, 22, 39, 41, 17, 13, 41, 16, 29, 51, 64, 46, 41, 23, 59, 46, 41, 39, 29, 23, 3, 16, 41, 39, 46, 46, 9, 37, 4, 30, 46, 39, 25, 51, 16, 46, 41, 17, 13, 41, 22, 65, 51, 64, 36, 3, 46, 39, 41, 16, 29, 51, 39, 41, 16, 29, 14, 41, 59, 17, 3, 66, 46, 64, 41, 16, 51, 43, 46, 57, 37, 4, 41, 41, 11, 7, 16, 41, 51, 13, 41, 16, 29, 17, 7, 41, 3, 51, 18, 46, 41, 65, 46, 43, 46, 43, 12, 46, 65, 46, 66, 41, 64, 17, 16, 41, 16, 17, 41, 12, 46, 9, 37, 4, 41, 41, 30, 51, 46, 41, 39, 51, 64, 59, 3, 46, 41, 23, 64, 66, 41, 16, 29, 51, 64, 46, 41, 51, 43, 23, 59, 46, 41, 66, 51, 46, 39, 41, 22, 51, 16, 29, 41, 16, 29, 46, 46, 57, 37, 4, 37, 4, 37, 4, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 26, 37, 4, 37, 4, 2, 64, 16, 29, 65, 51, 13, 16, 14, 41, 3, 17, 18, 46, 3, 51, 64, 46, 39, 39, 41, 22, 29, 14, 41, 66, 17, 39, 16, 41, 16, 29, 17, 7, 41, 39, 25, 46, 64, 66, 9, 37, 4, 2, 25, 17, 64, 41, 16, 29, 14, 41, 39, 46, 3, 13, 41, 16, 29, 14, 41, 12, 46, 23, 7, 16, 14, 44, 39, 41, 3, 46, 59, 23, 15, 14, 20, 37, 4, 63, 23, 16, 7, 65, 46, 44, 39, 41, 12, 46, 53, 7, 46, 39, 16, 41, 59, 51, 18, 46, 39, 41, 64, 17, 16, 29, 51, 64, 59, 41, 12, 7, 16, 41, 66, 17, 16, 29, 41, 3, 46, 64, 66, 9, 37, 4, 8, 64, 66, 41, 12, 46, 51, 64, 59, 41, 13, 65, 23, 64, 36, 41, 39, 29, 46, 41, 3, 46, 64, 66, 39, 41, 16, 17, 41, 16, 29, 17, 39, 46, 41, 23, 65, 46, 41, 13, 65, 46, 46, 19, 37, 4, 24, 29, 46, 64, 41, 12, 46, 23, 7, 16, 46, 17, 7, 39, 41, 64, 51, 59, 59, 23, 65, 66, 41, 22, 29, 14, 41, 66, 17, 39, 16, 41, 16, 29, 17, 7, 41, 23, 12, 7, 39, 46, 9, 37, 4, 24, 29, 46, 41, 12, 17, 7, 64, 16, 46, 17, 7, 39, 41, 3, 23, 65, 59, 46, 39, 39, 41, 59, 51, 18, 46, 64, 41, 16, 29, 46, 46, 41, 16, 17, 41, 59, 51, 18, 46, 20, 37, 4, 54, 65, 17, 13, 51, 16, 3, 46, 39, 39, 41, 7, 39, 7, 65, 46, 65, 41, 22, 29, 14, 41, 66, 17, 39, 16, 41, 16, 29, 17, 7, 41, 7, 39, 46, 37, 4, 10, 17, 41, 59, 65, 46, 23, 16, 41, 23, 41, 39, 7, 43, 41, 17, 13, 41, 39, 7, 43, 39, 41, 14, 46, 16, 41, 15, 23, 64, 39, 16, 41, 64, 17, 16, 41, 3, 51, 18, 46, 20, 37, 4, 56, 17, 65, 41, 29, 23, 18, 51, 64, 59, 41, 16, 65, 23, 13, 13, 51, 15, 41, 22, 51, 16, 29, 41, 16, 29, 14, 41, 39, 46, 3, 13, 41, 23, 3, 17, 64, 46, 9, 37, 4, 24, 29, 17, 7, 41, 17, 13, 41, 16, 29, 14, 41, 39, 46, 3, 13, 41, 16, 29, 14, 41, 39, 22, 46, 46, 16, 41, 39, 46, 3, 13, 41, 66, 17, 39, 16, 41, 66, 46, 15, 46, 51, 18, 46, 9, 37, 4, 24, 29, 46, 64, 41, 29, 17, 22, 41, 22, 29, 46, 64, 41, 64, 23, 16, 7, 65, 46, 41, 15, 23, 3, 3, 39, 41, 16, 29, 46, 46, 41, 16, 17, 41, 12, 46, 41, 59, 17, 64, 46, 9, 37, 4, 62, 29, 23, 16, 41, 23, 15, 15, 46, 25, 16, 23, 12, 3, 46, 41, 23, 7, 66, 51, 16, 41, 15, 23, 64, 39, 16, 41, 16, 29, 17, 7, 41, 3, 46, 23, 18, 46, 20, 37, 4, 41, 41, 24, 29, 14, 41, 7, 64, 7, 39, 46, 66, 41, 12, 46, 23, 7, 16, 14, 41, 43, 7, 39, 16, 41, 12, 46, 41, 16, 17, 43, 12, 46, 66, 41, 22, 51, 16, 29, 41, 16, 29, 46, 46, 9, 37, 4, 41, 41, 62, 29, 51, 15, 29, 41, 7, 39, 46, 66, 41, 3, 51, 18, 46, 39, 41, 16, 29, 44, 41, 46, 32, 46, 15, 7, 16, 17, 65, 41, 16, 17, 41, 12, 46, 57, 37, 4, 37, 4, 37, 4, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 55, 37, 4, 37, 4, 24, 29, 17, 39, 46, 41, 29, 17, 7, 65, 39, 41, 16, 29, 23, 16, 41, 22, 51, 16, 29, 41, 59, 46, 64, 16, 3, 46, 41, 22, 17, 65, 36, 41, 66, 51, 66, 41, 13, 65, 23, 43, 46, 37, 4, 24, 29, 46, 41, 3, 17, 18, 46, 3, 14, 41, 59, 23, 40, 46, 41, 22, 29, 46, 65, 46, 41, 46, 18, 46, 65, 14, 41, 46, 14, 46, 41, 66, 17, 16, 29, 41, 66, 22, 46, 3, 3, 37, 4, 62, 51, 3, 3, 41, 25, 3, 23, 14, 41, 16, 29, 46, 41, 16, 14, 65, 23, 64, 16, 39, 41, 16, 17, 41, 16, 29, 46, 41, 18, 46, 65, 14, 41, 39, 23, 43, 46, 9, 37, 4, 8, 64, 66, 41, 16, 29, 23, 16, 41, 7, 64, 13, 23, 51, 65, 41, 22, 29, 51, 15, 29, 41, 13, 23, 51, 65, 3, 14, 41, 66, 17, 16, 29, 41, 46, 32, 15, 46, 3, 19, 37, 4, 56, 17, 65, 41, 64, 46, 18, 46, 65, 67, 65, 46, 39, 16, 51, 64, 59, 41, 16, 51, 43, 46, 41, 3, 46, 23, 66, 39, 41, 39, 7, 43, 43, 46, 65, 41, 17, 64, 37, 4, 24, 17, 41, 29, 51, 66, 46, 17, 7, 39, 41, 22, 51, 64, 16, 46, 65, 41, 23, 64, 66, 41, 15, 17, 64, 13, 17, 7, 64, 66, 39, 41, 29, 51, 43, 41, 16, 29, 46, 65, 46, 9, 37, 4, 10, 23, 25, 41, 15, 29, 46, 15, 36, 46, 66, 41, 22, 51, 16, 29, 41, 13, 65, 17, 39, 16, 41, 23, 64, 66, 41, 3, 7, 39, 16, 14, 41, 3, 46, 23, 18, 46, 39, 41, 53, 7, 51, 16, 46, 41, 59, 17, 64, 46, 9, 37, 4, 11, 46, 23, 7, 16, 14, 41, 17, 44, 46, 65, 67, 39, 64, 17, 22, 46, 66, 41, 23, 64, 66, 41, 12, 23, 65, 46, 64, 46, 39, 39, 41, 46, 18, 46, 65, 14, 41, 22, 29, 46, 65, 46, 19, 37, 4, 24, 29, 46, 64, 41, 22, 46, 65, 46, 41, 64, 17, 16, 41, 39, 7, 43, 43, 46, 65, 44, 39, 41, 66, 51, 39, 16, 51, 3, 3, 23, 16, 51, 17, 64, 41, 3, 46, 13, 16, 37, 4, 8, 41, 3, 51, 53, 7, 51, 66, 41, 25, 65, 51, 39, 17, 64, 46, 65, 41, 25, 46, 64, 16, 41, 51, 64, 41, 22, 23, 3, 3, 39, 41, 17, 13, 41, 59, 3, 23, 39, 39, 9, 37, 4, 11, 46, 23, 7, 16, 14, 44, 39, 41, 46, 13, 13, 46, 15, 16, 41, 22, 51, 16, 29, 41, 12, 46, 23, 7, 16, 14, 41, 22, 46, 65, 46, 41, 12, 46, 65, 46, 13, 16, 9, 37, 4, 63, 17, 65, 41, 51, 16, 41, 64, 17, 65, 41, 64, 17, 41, 65, 46, 43, 46, 43, 12, 65, 23, 64, 15, 46, 41, 22, 29, 23, 16, 41, 51, 16, 41, 22, 23, 39, 57, 37, 4, 41, 41, 11, 7, 16, 41, 13, 3, 17, 22, 46, 65, 39, 41, 66, 51, 39, 16, 51, 3, 3, 46, 66, 41, 16, 29, 17, 7, 59, 29, 41, 16, 29, 46, 14, 41, 22, 51, 16, 29, 41, 22, 51, 64, 16, 46, 65, 41, 43, 46, 46, 16, 9, 37, 4, 41, 41, 33, 46, 46, 39, 46, 41, 12, 7, 16, 41, 16, 29, 46, 51, 65, 41, 39, 29, 17, 22, 9, 41, 16, 29, 46, 51, 65, 41, 39, 7, 12, 39, 16, 23, 64, 15, 46, 41, 39, 16, 51, 3, 3, 41, 3, 51, 18, 46, 39, 41, 39, 22, 46, 46, 16, 57, 37, 4, 37, 4, 37, 4, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 28, 37, 4, 37, 4, 24, 29, 46, 64, 41, 3, 46, 16, 41, 64, 17, 16, 41, 22, 51, 64, 16, 46, 65, 44, 39, 41, 65, 23, 59, 59, 46, 66, 41, 29, 23, 64, 66, 41, 66, 46, 13, 23, 15, 46, 9, 37, 4, 61, 64, 41, 16, 29, 46, 46, 41, 16, 29, 14, 41, 39, 7, 43, 43, 46, 65, 41, 46, 65, 46, 41, 16, 29, 17, 7, 41, 12, 46, 41, 66, 51, 39, 16, 51, 3, 3, 46, 66, 19, 37, 4, 49, 23, 36, 46, 41, 39, 22, 46, 46, 16, 41, 39, 17, 43, 46, 41, 18, 51, 23, 3, 50, 41, 16, 65, 46, 23, 39, 7, 65, 46, 41, 16, 29, 17, 7, 41, 39, 17, 43, 46, 41, 25, 3, 23, 15, 46, 9, 37, 4, 62, 51, 16, 29, 41, 12, 46, 23, 7, 16, 14, 44, 39, 41, 16, 65, 46, 23, 39, 7, 65, 46, 41, 46, 65, 46, 41, 51, 16, 41, 12, 46, 41, 39, 46, 3, 13, 67, 36, 51, 3, 3, 46, 66, 19, 37, 4, 24, 29, 23, 16, 41, 7, 39, 46, 41, 51, 39, 41, 64, 17, 16, 41, 13, 17, 65, 12, 51, 66, 66, 46, 64, 41, 7, 39, 7, 65, 14, 9, 37, 4, 62, 29, 51, 15, 29, 41, 29, 23, 25, 25, 51, 46, 39, 41, 16, 29, 17, 39, 46, 41, 16, 29, 23, 16, 41, 25, 23, 14, 41, 16, 29, 46, 41, 22, 51, 3, 3, 51, 64, 59, 41, 3, 17, 23, 64, 50, 37, 4, 24, 29, 23, 16, 44, 39, 41, 13, 17, 65, 41, 16, 29, 14, 41, 39, 46, 3, 13, 41, 16, 17, 41, 12, 65, 46, 46, 66, 41, 23, 64, 17, 16, 29, 46, 65, 41, 16, 29, 46, 46, 9, 37, 4, 31, 65, 41, 16, 46, 64, 41, 16, 51, 43, 46, 39, 41, 29, 23, 25, 25, 51, 46, 65, 41, 12, 46, 41, 51, 16, 41, 16, 46, 64, 41, 13, 17, 65, 41, 17, 64, 46, 9, 37, 4, 24, 46, 64, 41, 16, 51, 43, 46, 39, 41, 16, 29, 14, 41, 39, 46, 3, 13, 41, 22, 46, 65, 46, 41, 29, 23, 25, 25, 51, 46, 65, 41, 16, 29, 23, 64, 41, 16, 29, 17, 7, 41, 23, 65, 16, 9, 37, 4, 61, 13, 41, 16, 46, 64, 41, 17, 13, 41, 16, 29, 51, 64, 46, 41, 16, 46, 64, 41, 16, 51, 43, 46, 39, 41, 65, 46, 13, 51, 59, 7, 65, 46, 66, 41, 16, 29, 46, 46, 19, 37, 4, 24, 29, 46, 64, 41, 22, 29, 23, 16, 41, 15, 17, 7, 3, 66, 41, 66, 46, 23, 16, 29, 41, 66, 17, 41, 51, 13, 41, 16, 29, 17, 7, 41, 39, 29, 17, 7, 3, 66, 39, 16, 41, 66, 46, 25, 23, 65, 16, 9, 37, 4, 33, 46, 23, 18, 51, 64, 59, 41, 16, 29, 46, 46, 41, 3, 51, 18, 51, 64, 59, 41, 51, 64, 41, 25, 17, 39, 16, 46, 65, 51, 16, 14, 20, 37, 4, 41, 41, 11, 46, 41, 64, 17, 16, 41, 39, 46, 3, 13, 67, 22, 51, 3, 3, 46, 66, 41, 13, 17, 65, 41, 16, 29, 17, 7, 41, 23, 65, 16, 41, 43, 7, 15, 29, 41, 16, 17, 17, 41, 13, 23, 51, 65, 9, 37, 4, 41, 41, 24, 17, 41, 12, 46, 41, 66, 46, 23, 16, 29, 44, 39, 41, 15, 17, 64, 53, 7, 46, 39, 16, 41, 23, 64, 66, 41, 43, 23, 36, 46, 41, 22, 17, 65, 43, 39, 41, 16, 29, 51, 64, 46, 41, 29, 46, 51, 65, 57, 37, 4, 37, 4, 37, 4, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 42, 37, 4, 37, 4, 33, 17, 41, 51, 64, 41, 16, 29, 46, 41, 17, 65, 51, 46, 64, 16, 41, 22, 29, 46, 64, 41, 16, 29, 46, 41, 59, 65, 23, 15, 51, 17, 7, 39, 41, 3, 51, 59, 29, 16, 37, 4, 33, 51, 13, 16, 39, 41, 7, 25, 41, 29, 51, 39, 41, 12, 7, 65, 64, 51, 64, 59, 41, 29, 46, 23, 66, 9, 41, 46, 23, 15, 29, 41, 7, 64, 66, 46, 65, 41, 46, 14, 46, 37, 4, 30, 17, 16, 29, 41, 29, 17, 43, 23, 59, 46, 41, 16, 17, 41, 29, 51, 39, 41, 64, 46, 22, 67, 23, 25, 25, 46, 23, 65, 51, 64, 59, 41, 39, 51, 59, 29, 16, 9, 37, 4, 10, 46, 65, 18, 51, 64, 59, 41, 22, 51, 16, 29, 41, 3, 17, 17, 36, 39, 41, 29, 51, 39, 41, 39, 23, 15, 65, 46, 66, 41, 43, 23, 1, 46, 39, 16, 14, 9, 37, 4, 8, 64, 66, 41, 29, 23, 18, 51, 64, 59, 41, 15, 3, 51, 43, 12, 46, 66, 41, 16, 29, 46, 41, 39, 16, 46, 46, 25, 67, 7, 25, 41, 29, 46, 23, 18, 46, 64, 3, 14, 41, 29, 51, 3, 3, 9, 37, 4, 27, 46, 39, 46, 43, 12, 3, 51, 64, 59, 41, 39, 16, 65, 17, 64, 59, 41, 14, 17, 7, 16, 29, 41, 51, 64, 41, 29, 51, 39, 41, 43, 51, 66, 66, 3, 46, 41, 23, 59, 46, 9, 37, 4, 52, 46, 16, 41, 43, 17, 65, 16, 23, 3, 41, 3, 17, 17, 36, 39, 41, 23, 66, 17, 65, 46, 41, 29, 51, 39, 41, 12, 46, 23, 7, 16, 14, 41, 39, 16, 51, 3, 3, 9, 37, 4, 8, 16, 16, 46, 64, 66, 51, 64, 59, 41, 17, 64, 41, 29, 51, 39, 41, 59, 17, 3, 66, 46, 64, 41, 25, 51, 3, 59, 65, 51, 43, 23, 59, 46, 19, 37, 4, 11, 7, 16, 41, 22, 29, 46, 64, 41, 13, 65, 17, 43, 41, 29, 51, 59, 29, 43, 17, 39, 16, 41, 25, 51, 16, 15, 29, 41, 22, 51, 16, 29, 41, 22, 46, 23, 65, 14, 41, 15, 23, 65, 9, 37, 4, 33, 51, 36, 46, 41, 13, 46, 46, 12, 3, 46, 41, 23, 59, 46, 41, 29, 46, 41, 65, 46, 46, 3, 46, 16, 29, 41, 13, 65, 17, 43, 41, 16, 29, 46, 41, 66, 23, 14, 9, 37, 4, 24, 29, 46, 41, 46, 14, 46, 39, 41, 58, 13, 17, 65, 46, 41, 66, 7, 16, 46, 17, 7, 39, 34, 41, 64, 17, 22, 41, 15, 17, 64, 18, 46, 65, 16, 46, 66, 41, 23, 65, 46, 37, 4, 56, 65, 17, 43, 41, 29, 51, 39, 41, 3, 17, 22, 41, 16, 65, 23, 15, 16, 41, 23, 64, 66, 41, 3, 17, 17, 36, 41, 23, 64, 17, 16, 29, 46, 65, 41, 22, 23, 14, 19, 37, 4, 41, 41, 10, 17, 41, 16, 29, 17, 7, 9, 41, 16, 29, 14, 41, 39, 46, 3, 13, 41, 17, 7, 16, 67, 59, 17, 51, 64, 59, 41, 51, 64, 41, 16, 29, 14, 41, 64, 17, 17, 64, 19, 37, 4, 41, 41, 2, 64, 3, 17, 17, 36, 46, 66, 41, 17, 64, 41, 66, 51, 46, 39, 16, 41, 7, 64, 3, 46, 39, 39, 41, 16, 29, 17, 7, 41, 59, 46, 16, 41, 23, 41, 39, 17, 64, 57, 37, 4, 37, 4, 37, 4, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 21, 37, 4, 37, 4, 49, 7, 39, 51, 15, 41, 16, 17, 41, 29, 46, 23, 65, 9, 41, 22, 29, 14, 41, 29, 46, 23, 65, 44, 39, 16, 41, 16, 29, 17, 7, 41, 43, 7, 39, 51, 15, 41, 39, 23, 66, 3, 14, 20, 37, 4, 10, 22, 46, 46, 16, 39, 41, 22, 51, 16, 29, 41, 39, 22, 46, 46, 16, 39, 41, 22, 23, 65, 41, 64, 17, 16, 9, 41, 1, 17, 14, 41, 66, 46, 3, 51, 59, 29, 16, 39, 41, 51, 64, 41, 1, 17, 14, 19, 37, 4, 62, 29, 14, 41, 3, 17, 18, 44, 39, 16, 41, 16, 29, 17, 7, 41, 16, 29, 23, 16, 41, 22, 29, 51, 15, 29, 41, 16, 29, 17, 7, 41, 65, 46, 15, 46, 51, 18, 44, 39, 16, 41, 64, 17, 16, 41, 59, 3, 23, 66, 3, 14, 9, 37, 4, 31, 65, 41, 46, 3, 39, 46, 41, 65, 46, 15, 46, 51, 18, 44, 39, 16, 41, 22, 51, 16, 29, 41, 25, 3, 46, 23, 39, 7, 65, 46, 41, 16, 29, 51, 64, 46, 41, 23, 64, 64, 17, 14, 20, 37, 4, 61, 13, 41, 16, 29, 46, 41, 16, 65, 7, 46, 41, 15, 17, 64, 15, 17, 65, 66, 41, 17, 13, 41, 22, 46, 3, 3, 67, 16, 7, 64, 46, 66, 41, 39, 17, 7, 64, 66, 39, 9, 37, 4, 11, 14, 41, 7, 64, 51, 17, 64, 39, 41, 43, 23, 65, 65, 51, 46, 66, 41, 66, 17, 41, 17, 13, 13, 46, 64, 66, 41, 16, 29, 51, 64, 46, 41, 46, 23, 65, 9, 37, 4, 24, 29, 46, 14, 41, 66, 17, 41, 12, 7, 16, 41, 39, 22, 46, 46, 16, 3, 14, 41, 15, 29, 51, 66, 46, 41, 16, 29, 46, 46, 9, 41, 22, 29, 17, 41, 15, 17, 64, 13, 17, 7, 64, 66, 39, 37, 4, 61, 64, 41, 39, 51, 64, 59, 3, 46, 64, 46, 39, 39, 41, 16, 29, 46, 41, 25, 23, 65, 16, 39, 41, 16, 29, 23, 16, 41, 16, 29, 17, 7, 41, 39, 29, 17, 7, 3, 66, 39, 16, 41, 12, 46, 23, 65, 19, 37, 4, 49, 23, 65, 36, 41, 29, 17, 22, 41, 17, 64, 46, 41, 39, 16, 65, 51, 64, 59, 41, 39, 22, 46, 46, 16, 41, 29, 7, 39, 12, 23, 64, 66, 41, 16, 17, 41, 23, 64, 17, 16, 29, 46, 65, 9, 37, 4, 10, 16, 65, 51, 36, 46, 39, 41, 46, 23, 15, 29, 41, 51, 64, 41, 46, 23, 15, 29, 41, 12, 14, 41, 43, 7, 16, 7, 23, 3, 41, 17, 65, 66, 46, 65, 51, 64, 59, 50, 37, 4, 27, 46, 39, 46, 43, 12, 3, 51, 64, 59, 41, 39, 51, 65, 46, 9, 41, 23, 64, 66, 41, 15, 29, 51, 3, 66, 9, 41, 23, 64, 66, 41, 29, 23, 25, 25, 14, 41, 43, 17, 16, 29, 46, 65, 9, 37, 4, 62, 29, 17, 41, 23, 3, 3, 41, 51, 64, 41, 17, 64, 46, 9, 41, 17, 64, 46, 41, 25, 3, 46, 23, 39, 51, 64, 59, 41, 64, 17, 16, 46, 41, 66, 17, 41, 39, 51, 64, 59, 19, 37, 4, 41, 41, 62, 29, 17, 39, 46, 41, 39, 25, 46, 46, 15, 29, 3, 46, 39, 39, 41, 39, 17, 64, 59, 41, 12, 46, 51, 64, 59, 41, 43, 23, 64, 14, 9, 41, 39, 46, 46, 43, 51, 64, 59, 41, 17, 64, 46, 9, 37, 4, 41, 41, 10, 51, 64, 59, 39, 41, 16, 29, 51, 39, 41, 16, 17, 41, 16, 29, 46, 46, 9, 41, 60, 24, 29, 17, 7, 41, 39, 51, 64, 59, 3, 46, 41, 22, 51, 3, 16, 41, 25, 65, 17, 18, 46, 41, 64, 17, 64, 46, 44, 57, 37, 4, 37, 4, 37, 4, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 45, 37, 4, 37, 4, 61, 39, 41, 51, 16, 41, 13, 17, 65, 41, 13, 46, 23, 65, 41, 16, 17, 41, 22, 46, 16, 41, 23, 41, 22, 51, 66, 17, 22, 44, 39, 41, 46, 14, 46, 9, 37, 4, 24, 29, 23, 16, 41, 16, 29, 17, 7, 41, 15, 17, 64, 39, 7, 43, 44, 39, 16, 41, 16, 29, 14, 41, 39, 46, 3, 13, 41, 51, 64, 41, 39, 51, 64, 59, 3, 46, 41, 3, 51, 13, 46, 20, 37, 4, 8, 29, 9, 41, 51, 13, 41, 16, 29, 17, 7, 41, 51, 39, 39, 7, 46, 3, 46, 39, 39, 41, 39, 29, 23, 3, 16, 41, 29, 23, 25, 41, 16, 17, 41, 66, 51, 46, 9, 37, 4, 24, 29, 46, 41, 22, 17, 65, 3, 66, 41, 22, 51, 3, 3, 41, 22, 23, 51, 3, 41, 16, 29, 46, 46, 41, 3, 51, 36, 46, 41, 23, 41, 43, 23, 36, 46, 3, 46, 39, 39, 41, 22, 51, 13, 46, 9, 37, 4, 24, 29, 46, 41, 22, 17, 65, 3, 66, 41, 22, 51, 3, 3, 41, 12, 46, 41, 16, 29, 14, 41, 22, 51, 66, 17, 22, 41, 23, 64, 66, 41, 39, 16, 51, 3, 3, 41, 22, 46, 46, 25, 9, 37, 4, 24, 29, 23, 16, 41, 16, 29, 17, 7, 41, 64, 17, 41, 13, 17, 65, 43, 41, 17, 13, 41, 16, 29, 46, 46, 41, 29, 23, 39, 16, 41, 3, 46, 13, 16, 41, 12, 46, 29, 51, 64, 66, 9, 37, 4, 62, 29, 46, 64, 41, 46, 18, 46, 65, 14, 41, 25, 65, 51, 18, 23, 16, 46, 41, 22, 51, 66, 17, 22, 41, 22, 46, 3, 3, 41, 43, 23, 14, 41, 36, 46, 46, 25, 9, 37, 4, 11, 14, 41, 15, 29, 51, 3, 66, 65, 46, 64, 44, 39, 41, 46, 14, 46, 39, 9, 41, 29, 46, 65, 41, 29, 7, 39, 12, 23, 64, 66, 44, 39, 41, 39, 29, 23, 25, 46, 41, 51, 64, 41, 43, 51, 64, 66, 19, 37, 4, 33, 17, 17, 36, 41, 22, 29, 23, 16, 41, 23, 64, 41, 7, 64, 16, 29, 65, 51, 13, 16, 41, 51, 64, 41, 16, 29, 46, 41, 22, 17, 65, 3, 66, 41, 66, 17, 16, 29, 41, 39, 25, 46, 64, 66, 37, 4, 10, 29, 51, 13, 16, 39, 41, 12, 7, 16, 41, 29, 51, 39, 41, 25, 3, 23, 15, 46, 9, 41, 13, 17, 65, 41, 39, 16, 51, 3, 3, 41, 16, 29, 46, 41, 22, 17, 65, 3, 66, 41, 46, 64, 1, 17, 14, 39, 41, 51, 16, 50, 37, 4, 11, 7, 16, 41, 12, 46, 23, 7, 16, 14, 44, 39, 41, 22, 23, 39, 16, 46, 41, 29, 23, 16, 29, 41, 51, 64, 41, 16, 29, 46, 41, 22, 17, 65, 3, 66, 41, 23, 64, 41, 46, 64, 66, 9, 37, 4, 8, 64, 66, 41, 36, 46, 25, 16, 41, 7, 64, 7, 39, 46, 66, 41, 16, 29, 46, 41, 7, 39, 46, 65, 41, 39, 17, 41, 66, 46, 39, 16, 65, 17, 14, 39, 41, 51, 16, 19, 37, 4, 41, 41, 63, 17, 41, 3, 17, 18, 46, 41, 16, 17, 22, 23, 65, 66, 41, 17, 16, 29, 46, 65, 39, 41, 51, 64, 41, 16, 29, 23, 16, 41, 12, 17, 39, 17, 43, 41, 39, 51, 16, 39, 37, 4, 41, 41, 24, 29, 23, 16, 41, 17, 64, 41, 29, 51, 43, 39, 46, 3, 13, 41, 39, 7, 15, 29, 41, 43, 7, 65, 66, 44, 65, 17, 7, 39, 41, 39, 29, 23, 43, 46, 41, 15, 17, 43, 43, 51, 16, 39, 57, 37, 4, 37, 4, 37, 4, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 38, 48, 37, 4, 37, 4, 56, 17, 65, 41, 39, 29, 23, 43, 46, 41, 66, 46, 64, 14, 41, 16, 29, 23, 16, 41, 16, 29, 17, 7, 41, 12, 46, 23, 65, 44, 39, 16, 41, 3, 17, 18, 46, 41, 16, 17, 41, 23, 64, 14, 37, 4, 62, 29, 17, 41, 13, 17, 65, 41, 16, 29, 14, 41, 39, 46, 3, 13, 41, 23, 65, 16, 41, 39, 17, 41, 7, 64, 25, 65, 17, 18, 51, 66, 46, 64, 16, 57, 37, 4, 5, 65, 23, 64, 16, 41, 51, 13, 41, 16, 29, 17, 7, 41, 22, 51, 3, 16, 9, 41, 16, 29, 17, 7, 41, 23, 65, 16, 41, 12, 46, 3, 17, 18, 46, 66, 41, 17, 13, 41, 43, 23, 64, 14, 9, 37, 4, 11, 7, 16, 41, 16, 29, 23, 16, 41, 16, 29, 17, 7, 41, 64, 17, 64, 46, 41, 3, 17, 18, 44, 39, 16, 41, 51, 39, 41, 43, 17, 39, 16, 41, 46, 18, 51, 66, 46, 64, 16, 19, 37, 4, 56, 17, 65, 41, 16, 29, 17, 7, 41, 23, 65, 16, 41, 39, 17, 41, 25, 17, 39, 39, 46, 39, 39, 46, 66, 41, 22, 51, 16, 29, 41, 43, 7, 65, 66, 44, 65, 17, 7, 39, 41, 29, 23, 16, 46, 9, 37, 4, 24, 29, 23, 16, 41, 44, 59, 23, 51, 64, 39, 16, 41, 16, 29, 14, 41, 39, 46, 3, 13, 41, 16, 29, 17, 7, 41, 39, 16, 51, 15, 36, 44, 39, 16, 41, 64, 17, 16, 41, 16, 17, 41, 15, 17, 64, 39, 25, 51, 65, 46, 9, 37, 4, 10, 46, 46, 36, 51, 64, 59, 41, 16, 29, 23, 16, 41, 12, 46, 23, 7, 16, 46, 17, 7, 39, 41, 65, 17, 17, 13, 41, 16, 17, 41, 65, 7, 51, 64, 23, 16, 46, 37, 4, 62, 29, 51, 15, 29, 41, 16, 17, 41, 65, 46, 25, 23, 51, 65, 41, 39, 29, 17, 7, 3, 66, 41, 12, 46, 41, 16, 29, 14, 41, 15, 29, 51, 46, 13, 41, 66, 46, 39, 51, 65, 46, 19, 37, 4, 31, 41, 15, 29, 23, 64, 59, 46, 41, 16, 29, 14, 41, 16, 29, 17, 7, 59, 29, 16, 9, 41, 16, 29, 23, 16, 41, 61, 41, 43, 23, 14, 41, 15, 29, 23, 64, 59, 46, 41, 43, 14, 41, 43, 51, 64, 66, 9, 37, 4, 10, 29, 23, 3, 3, 41, 29, 23, 16, 46, 41, 12, 46, 41, 13, 23, 51, 65, 46, 65, 41, 3, 17, 66, 59, 46, 66, 41, 16, 29, 23, 64, 41, 59, 46, 64, 16, 3, 46, 41, 3, 17, 18, 46, 20, 37, 4, 11, 46, 41, 23, 39, 41, 16, 29, 14, 41, 25, 65, 46, 39, 46, 64, 15, 46, 41, 51, 39, 41, 59, 65, 23, 15, 51, 17, 7, 39, 41, 23, 64, 66, 41, 36, 51, 64, 66, 9, 37, 4, 31, 65, 41, 16, 17, 41, 16, 29, 14, 41, 39, 46, 3, 13, 41, 23, 16, 41, 3, 46, 23, 39, 16, 41, 36, 51, 64, 66, 67, 29, 46, 23, 65, 16, 46, 66, 41, 25, 65, 17, 18, 46, 9, 37, 4, 41, 41, 49, 23, 36, 46, 41, 16, 29, 46, 46, 41, 23, 64, 17, 16, 29, 46, 65, 41, 39, 46, 3, 13, 41, 13, 17, 65, 41, 3, 17, 18, 46, 41, 17, 13, 41, 43, 46, 9, 37, 4, 41, 41, 24, 29, 23, 16, 41, 12, 46, 23, 7, 16, 14, 41, 39, 16, 51, 3, 3, 41, 43, 23, 14, 41, 3, 51, 18, 46, 41, 51, 64, 41, 16, 29, 51, 64, 46, 41, 17, 65, 41, 16, 29, 46, 46, 57]\n",
            "----------------------------------------------------\n",
            "data length :  6561\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RysEhSqmYLX",
        "colab_type": "text"
      },
      "source": [
        "#### hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubprmX-hjRt2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "iteration = 500\n",
        "sequence_length = 40\n",
        "batch_size = round((txt_data_size /sequence_length)+0.5) # = math.ceil\n",
        "hidden_size = 128  # size of hidden layer of neurons.  \n",
        "learning_rate = 1e-1\n",
        "\n",
        "# model parameters\n",
        "\n",
        "W_xh = np.random.randn(hidden_size, num_chars)*0.01     # weight input -> hidden. \n",
        "W_hh = np.random.randn(hidden_size, hidden_size)*0.01   # weight hidden -> hidden\n",
        "W_hy = np.random.randn(num_chars, hidden_size)*0.01     # weight hidden -> output\n",
        "\n",
        "b_h = np.zeros((hidden_size, 1)) # hidden bias\n",
        "b_y = np.zeros((num_chars, 1)) # output bias\n",
        "\n",
        "h_prev = np.zeros((hidden_size,1)) # h_(t-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjvJhIDqmS1E",
        "colab_type": "text"
      },
      "source": [
        "#### Forward propagation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGDdTnVVkElU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forwardprop(inputs, targets, h_prev):\n",
        "        \n",
        "    # Since the RNN receives the sequence, the weights are not updated during one sequence.\n",
        "    xs, hs, ys, ps = {}, {}, {}, {} # dictionary\n",
        "    hs[-1] = np.copy(h_prev) # Copy previous hidden state vector to -1 key value.\n",
        "    loss = 0 # loss initialization\n",
        "    \n",
        "    for t in range(len(inputs)): # t is a \"time step\" and is used as a key(dic).  \n",
        "        \n",
        "        xs[t] = np.zeros((num_chars,1)) \n",
        "        xs[t][inputs[t]] = 1\n",
        "        hs[t] = np.tanh(np.dot(W_xh, xs[t]) + np.dot(W_hh, hs[t-1]) + b_h) # hidden state. \n",
        "        ys[t] = np.dot(W_hy, hs[t]) + b_y # unnormalized log probabilities for next chars\n",
        "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars. \n",
        "        # Softmax. -> The sum of probabilities is 1 even without the exp() function, but all of the elements are positive through the exp() function.\n",
        " \n",
        "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss). Efficient and simple code\n",
        "\n",
        "#         y_class = np.zeros((num_chars, 1)) \n",
        "#         y_class[targets[t]] =1\n",
        "#         loss += np.sum(y_class*(-np.log(ps[t]))) # softmax (cross-entropy loss)        \n",
        "\n",
        "    return loss, ps, hs, xs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgbdPTQHmONx",
        "colab_type": "text"
      },
      "source": [
        "#### Backward propagation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1l4L4Taludi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def backprop(ps, inputs, hs, xs):\n",
        "\n",
        "    dWxh, dWhh, dWhy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy) # make all zero matrices.\n",
        "    dbh, dby = np.zeros_like(b_h), np.zeros_like(b_y)\n",
        "    dhnext = np.zeros_like(hs[0]) # (hidden_size,1) \n",
        "\n",
        "    # reversed\n",
        "    for t in reversed(range(len(inputs))):\n",
        "        dy = np.copy(ps[t]) # shape (num_chars,1).  \"dy\" means \"dloss/dy\"\n",
        "        dy[targets[t]] -= 1 # backprop into y. After taking the soft max in the input vector, subtract 1 from the value of the element corresponding to the correct label.\n",
        "        dWhy += np.dot(dy, hs[t].T)\n",
        "        dby += dy \n",
        "        dh = np.dot(W_hy.T, dy) + dhnext # backprop into h. \n",
        "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity #tanh'(x) = 1-tanh^2(x)\n",
        "        dbh += dhraw\n",
        "        dWxh += np.dot(dhraw, xs[t].T)\n",
        "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
        "        dhnext = np.dot(W_hh.T, dhraw)\n",
        "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]: \n",
        "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients.  \n",
        "    \n",
        "    return dWxh, dWhh, dWhy, dbh, dby"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1ae-K8GmJqc",
        "colab_type": "text"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCykTWesmDfX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "f621b338-f95b-4ea2-e8e1-f4faefdea4b6"
      },
      "source": [
        "%%time\n",
        "\n",
        "data_pointer = 0\n",
        "\n",
        "# memory variables for Adagrad\n",
        "mWxh, mWhh, mWhy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy)\n",
        "mbh, mby = np.zeros_like(b_h), np.zeros_like(b_y) \n",
        "\n",
        "for i in range(iteration+1):\n",
        "    h_prev = np.zeros((hidden_size,1)) # reset RNN memory\n",
        "    data_pointer = 0 # go from start of data\n",
        "    \n",
        "    for b in range(batch_size):\n",
        "        \n",
        "        inputs = [char_to_int[ch] for ch in article_text[data_pointer:data_pointer+sequence_length]]\n",
        "        targets = [char_to_int[ch] for ch in article_text[data_pointer+1:data_pointer+sequence_length+1]] # t+1        \n",
        "            \n",
        "        if (data_pointer+sequence_length+1 >= len(article_text) and b == batch_size-1): # processing of the last part of the input data. \n",
        "#             targets.append(char_to_int[txt_data[0]])   # When the data doesn't fit, add the first char to the back.\n",
        "            targets.append(char_to_int[\" \"])   # When the data doesn't fit, add space(\" \") to the back.\n",
        "\n",
        "\n",
        "        # forward\n",
        "        loss, ps, hs, xs = forwardprop(inputs, targets, h_prev)\n",
        "#         print(loss)\n",
        "    \n",
        "        # backward\n",
        "        dWxh, dWhh, dWhy, dbh, dby = backprop(ps, inputs, hs, xs) \n",
        "        \n",
        "        \n",
        "    # perform parameter update with Adagrad\n",
        "        for param, dparam, mem in zip([W_xh, W_hh, W_hy, b_h, b_y], \n",
        "                                    [dWxh, dWhh, dWhy, dbh, dby], \n",
        "                                    [mWxh, mWhh, mWhy, mbh, mby]):\n",
        "            mem += dparam * dparam # elementwise\n",
        "            param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update      \n",
        "    \n",
        "        data_pointer += sequence_length # move data pointer\n",
        "        \n",
        "    if i % 25 == 0:\n",
        "        print ('iter %d, loss: %f' % (i, loss)) # print progress"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter 0, loss: 1.013929\n",
            "iter 25, loss: 0.020648\n",
            "iter 50, loss: 0.006868\n",
            "iter 75, loss: 0.007810\n",
            "iter 100, loss: 0.006554\n",
            "iter 125, loss: 0.006616\n",
            "iter 150, loss: 0.004147\n",
            "iter 175, loss: 0.003310\n",
            "iter 200, loss: 0.002658\n",
            "iter 225, loss: 0.002035\n",
            "iter 250, loss: 0.001868\n",
            "iter 275, loss: 0.001897\n",
            "iter 300, loss: 0.001902\n",
            "iter 325, loss: 0.001976\n",
            "iter 350, loss: 0.001875\n",
            "iter 375, loss: 0.001738\n",
            "iter 400, loss: 0.001727\n",
            "iter 425, loss: 0.002082\n",
            "iter 450, loss: 0.002615\n",
            "iter 475, loss: 0.002942\n",
            "iter 500, loss: 0.002798\n",
            "CPU times: user 9min 50s, sys: 6min 5s, total: 15min 56s\n",
            "Wall time: 8min 4s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vo6x-LMroI2",
        "colab_type": "text"
      },
      "source": [
        "#### Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwAV1xxcrQKu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(test_char, length):\n",
        "    x = np.zeros((num_chars, 1)) \n",
        "    x[char_to_int[test_char]] = 1\n",
        "    ixes = []\n",
        "    h = np.zeros((hidden_size,1))\n",
        "\n",
        "    for t in range(length):\n",
        "        h = np.tanh(np.dot(W_xh, x) + np.dot(W_hh, h) + b_h) \n",
        "        y = np.dot(W_hy, h) + b_y\n",
        "        p = np.exp(y) / np.sum(np.exp(y)) \n",
        "        ix = np.random.choice(range(num_chars), p=p.ravel()) # ravel -> rank0\n",
        "        # \"ix\" is a list of indexes selected according to the soft max probability.\n",
        "        x = np.zeros((num_chars, 1)) # init\n",
        "        x[ix] = 1 \n",
        "        ixes.append(ix) # list\n",
        "    txt = test_char + ''.join(int_to_char[i] for i in ixes)\n",
        "    print ('----\\n %s \\n----' % (txt, ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLP6HY4vrquv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "4418e6f1-1dda-430e-c89a-a80ff8198274"
      },
      "source": [
        "predict('S', 500)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----\n",
            " Se ouch withire anlost thou thae conglenesss in pleas’s thy dor thou thou used if frere times,\r\n",
            "That coms hid self and dondd’st rears, way pef-loir anur (fots ar that not lise:\r\n",
            "  To hile welf thou so nf he chere foud prey:\r\n",
            "  Samest ent thou thom\r\n",
            "Thath tind thou bece doth llor deoved wimm ‘Th plle spllf stilg conother mat sthinced Sar mear, in that in that of not.\r\n",
            "\r\n",
            "\r\n",
            "The inine the will on rlove?\r\n",
            "  That cone when’s:\r\n",
            "Lied,\r\n",
            "  Thap so dooked buciigg trakiglambcige on thou ridouts cof cor shaf  \n",
            "----\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LU-bA4-r4vp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "2bc33d73-f6a5-49be-e009-1a9841d880d7"
      },
      "source": [
        "predict('C', 750)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----\n",
            " Cail wime praild harice hipppiveeats or there hash cherideld with wilf thy on thou not spof bud the each doth ho fond ity \r\n",
            "Ret,\r\n",
            "So the gazlovang still thic not iwculo the worpcery to thy sone shat anus preredine unthry uns and,\r\n",
            "Ift aft,\r\n",
            "Busurst to thide be is thane thou no dus tookchiss why kinde,\r\n",
            "The still thet condd,\r\n",
            "Sheausinother.\r\n",
            "  Who non.\r\n",
            "\r\n",
            "Muse,\r\n",
            "Thy so faill the eots datimuchise for shald to in that not be maplats in thou the rakmthine preage dor chou thou with co loy frat now buce stiets thou cong in thy lof musbllhise sond confourm’st no for rotfouglaving be my lr chil’s nd thee,\r\n",
            "Thos what anojoy time his thy that thou not buy ming nlor dust,\r\n",
            "  Lealt whath wilt:\r\n",
            "Thif thou nots be do whe sorld k’s in welf kind,\r\n",
            "  Nust de \n",
            "----\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zE4a4O7Bp5x1"
      },
      "source": [
        "# Resources and Stretch Goals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uT3UV3gap9H6"
      },
      "source": [
        "## Stretch goals:\n",
        "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
        "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
        "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
        "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
        "- Run on bigger, better data\n",
        "\n",
        "## Resources:\n",
        "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
        "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
        "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
        "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
        "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
      ]
    }
  ]
}