{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_441_RNN_and_LSTM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/valogonor/DS-Unit-4-Sprint-4-Deep-Learning/blob/master/LS_DS_441_RNN_and_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "_IizNKWLomoA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lambda School Data Science - Recurrent Neural Networks and LSTM\n",
        "\n",
        "> \"Yesterday's just a memory - tomorrow is never what it's supposed to be.\" -- Bob Dylan"
      ]
    },
    {
      "metadata": {
        "id": "0EZdBzC6pvV9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lecture\n",
        "\n",
        "Wish you could save [Time In A Bottle](https://www.youtube.com/watch?v=AnWWj6xOleY)? With statistics you can do the next best thing - understand how data varies over time (or any sequential order), and use the order/time dimension predictively.\n",
        "\n",
        "A sequence is just any enumerated collection - order counts, and repetition is allowed. Python lists are a good elemental example - `[1, 2, 2, -1]` is a valid list, and is different from `[1, 2, -1, 2]`. The data structures we tend to use (e.g. NumPy arrays) are often built on this fundamental structure.\n",
        "\n",
        "A time series is data where you have not just the order but some actual continuous marker for where they lie \"in time\" - this could be a date, a timestamp, [Unix time](https://en.wikipedia.org/wiki/Unix_time), or something else. All time series are also sequences, and for some techniques you may just consider their order and not \"how far apart\" the entries are (if you have particularly consistent data collected at regular intervals it may not matter)."
      ]
    },
    {
      "metadata": {
        "id": "5_m0hJ4uCzHz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Time series with plain old regression\n",
        "\n",
        "Recurrences are fancy, and we'll get to those later - let's start with something simple. Regression can handle time series just fine if you just set them up correctly - let's try some made-up stock data. And to make it, let's use a few list comprehensions!"
      ]
    },
    {
      "metadata": {
        "id": "GkJUFfsgnqr_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from random import random\n",
        "days = np.array((range(28)))\n",
        "stock_quotes = np.array([random() + day * random() for day in days])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y-ORgKGNBOcb",
        "colab_type": "code",
        "outputId": "37234704-6c6c-4c1f-df84-b3390d47dfc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "cell_type": "code",
      "source": [
        "stock_quotes"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.5570214 ,  0.63729266,  2.11216783,  1.8482017 ,  4.49965607,\n",
              "        4.86804939,  0.61081719,  2.52704309,  3.14905101,  2.89319363,\n",
              "        8.47486423,  5.16450863,  0.97510568,  8.56253833,  3.1871747 ,\n",
              "       12.21421089,  0.842354  , 14.3682239 , 15.34740816, 10.1928061 ,\n",
              "        3.28111408, 15.58624796, 21.08730582,  1.55959257,  7.68790922,\n",
              "       23.41839703, 22.57507693, 22.45177185])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "X3lR2wGvBx3a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's take a look with a scatter plot:"
      ]
    },
    {
      "metadata": {
        "id": "pVUTC2tmBSIq",
        "colab_type": "code",
        "outputId": "972fc268-c34f-40f7-e4bc-27081d3bd680",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "cell_type": "code",
      "source": [
        "from matplotlib.pyplot import scatter\n",
        "scatter(days, stock_quotes);"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAD5NJREFUeJzt3V+IXOd5x/HfL4oKi1NYGy9C3rpV\nGozARNQKQ9piE1zSRE5urOjC1BetCgHlwoYEgqidm5hCkaiatL0oAbc2USF1CUSRTWOqGCfgBkKa\nkeVW/oPqEGzqtSxtcEUcWKitPL3Y2Wi13d2ZM3Nmznmf8/2A2dkzs57n6LC/PfOc97yvI0IAgPK9\nr+kCAAD1INABIAkCHQCSINABIAkCHQCSINABIAkCHQCSINABIAkCHQCSeP8s3+zmm2+OPXv2zPIt\nAaB4Z8+e/VlELAx73UwDfc+ePer3+7N8SwAonu3XR3kdLRcASIJAB4AkCHQASIJAB4AkCHQASGKm\no1wAoI1On1vSiTMX9OaVFd0yP6ejB/bq4P7FpsuqjEAH0Gmnzy3p4VPntfLuVUnS0pUVPXzqvCQV\nF+q0XAB02okzF34V5mtW3r2qE2cuNFTR+Ah0AJ325pWVStvbjEAH0Gm3zM9V2t5mBDqATjt6YK/m\ndu64btvczh06emBvQxWNj4uiADpt7cIno1wAIIGD+xeLDPCNCHQAqKDNY9YJdAAYUdUx67MOfy6K\nAsCIqoxZXwv/pSsrCl0L/9PnlqZWH4EOACOqMma9iRuWCHQAGFGVMetN3LBEoAPAiKqMWW/ihiUC\nHQBGdHD/oo4d2qfF+TlZ0uL8nI4d2rfphc4mblhilAsAVDDqmPUmblgi0AFgSmZ9wxItFwBIgkAH\ngCQIdABIgh46gMa0eV6UEhHoABqRaS3PtqDlAqARmdbybAsCHUAjMq3l2RYEOoBGZFrLsy0IdACN\nyLSWZ1twURRAIzKt5dkWBDqAxmRZy7MtaLkAQBJDA932rba/b/tl2y/Z/vxg+022n7H96uDrjdMv\nFwCwlVHO0N+T9MWIuF3S70l6wPbtkh6S9GxE3Cbp2cH3AICGDA30iLgYEc8PHr8j6RVJi5LulXRy\n8LKTkg5Oq0gAwHCVeui290jaL+lHknZFxMXBU29J2lVrZQCASkYOdNsfkPQtSV+IiJ+vfy4iQlJs\n8XNHbPdt95eXlycqFgCwtZEC3fZOrYb5NyLi1GDzJdu7B8/vlnR5s5+NiEcjohcRvYWFhTpqBgBs\nYpRRLpb0mKRXIuKr6556StLhwePDkp6svzwAwKhGubHoTkl/LOm87RcG274k6bikb9r+rKTXJd03\nnRIBAKMYGugR8QNJ3uLpj9dbDgBgXNwpCgBJEOgAkASBDgBJEOgAkASBDgBJEOgAkASBDgBJEOgA\nkASBDgBJEOgAkASBDgBJEOgAkASBDgBJEOgAkASBDgBJEOgAkASBDgBJEOgAkASBDgBJEOgAkASB\nDgBJEOgAkASBDgBJEOgAkASBDgBJEOgAkASBDgBJEOgAkASBDgBJEOgAkMT7my4AQC6nzy3pxJkL\nevPKim6Zn9PRA3t1cP9i02V1AoEOoDanzy3p4VPntfLuVUnS0pUVPXzqvCQR6jNAoANJtOHM+MSZ\nC78K8zUr717ViTMXCPQZINCBBNpyZvzmlZVK21GvoRdFbT9u+7LtF9dte8T2ku0XBv99erplAtjO\ndmfGs3TL/Fyl7ajXKKNcvi7pnk22/3VE3DH47+l6ywJQRVvOjI8e2Ku5nTuu2za3c4eOHtg70zq6\namigR8Rzkt6eQS0AxtSWM+OD+xd17NA+Lc7PyZIW5+d07NA++uczMkkP/UHbfyKpL+mLEfE/NdUE\noKKjB/Ze10OXmjszPrh/kQBvyLg3Fn1N0ock3SHpoqSvbPVC20ds9233l5eXx3w7ANvhzBiS5IgY\n/iJ7j6R/iYgPV3luo16vF/1+v3KRANBlts9GRG/Y68Y6Q7e9e923n5H04lavBQDMxtAeuu0nJN0t\n6Wbbb0j6sqS7bd8hKSS9JulzU6wR6Kw23CyEcgwN9Ii4f5PNj02hFgDrtOVmIZSD2RaBlmrLzUIo\nB4EOtFRbbhZCOQh0oKXacrMQykGgAy3FbfSoitkWgZZau/DJKBeMikAHWozb6FEFLRcASIJAB4Ak\nCHQASIJAB4AkCHQASIJAB4AkCHQASIJAB4AkCHQASIJAB4AkuPUfmBCrCqEtCHRgAqwqhDah5QJM\ngFWF0CYEOjABVhVCmxDowARYVQhtQqADE2BVIbQJF0WBCbCqENqEQAcmxKpCaAtaLgCQBIEOAEkQ\n6ACQBIEOAEkQ6ACQBIEOAEkQ6ACQBIEOAEkQ6ACQBIEOAElw6z/QQayylNPQM3Tbj9u+bPvFddtu\nsv2M7VcHX2+cbpkA6rK2ytLSlRWFrq2ydPrcUtOlYUKjtFy+LumeDdsekvRsRNwm6dnB9wAKwCpL\neQ0N9Ih4TtLbGzbfK+nk4PFJSQdrrgvAlLDKUl7jXhTdFREXB4/fkrRrqxfaPmK7b7u/vLw85tsB\nqAurLOU18SiXiAhJsc3zj0ZELyJ6CwsLk74dgAmxylJe445yuWR7d0RctL1b0uU6iwIwPayylNe4\ngf6UpMOSjg++PllbRQCmjlWWchpl2OITkn4oaa/tN2x/VqtB/gnbr0r6w8H3AIAGDT1Dj4j7t3jq\n4zXXAgCYALf+A0ASBDoAJEGgA0ASBDoAJMFsi5i6tszs15Y6gGkh0DFVazP7rU0GtTazn6SZhmlb\n6gCmiZYLpqotM/u1pQ5gmgh0TFVbZvZrSx3ANBHomKq2zOzXljqAaSLQMVVtmdmvLXUA08RFUUxV\nW2b2a0sdwDR5dTrz2ej1etHv92f2fgC6K9MwVdtnI6I37HWcoQNIp6vDVOmhA0inq8NUCXQA6XR1\nmCqBDiCdrg5TJdABpNPVYapcFAWQTleHqRLoAFLq4kLYtFwAIAkCHQCSINABIAkCHQCSINABIAkC\nHQCSINABIAkCHQCSINABIAkCHQCSINABIAkCHQCSINABIAkCHQCSYPpctEqmldqBWZso0G2/Jukd\nSVclvRcRvTqKQjd1daV2oC51tFz+ICLuIMwxqa6u1A7UhZZLC9BmWNXVldqBukx6hh6Svmv7rO0j\nm73A9hHbfdv95eXlCd8un7U2w9KVFYWutRlOn1tqurSZ6+pK7UBdJg30uyLiI5I+JekB2x/b+IKI\neDQiehHRW1hYmPDt8qHNcE1XV2oH6jJRyyUilgZfL9v+tqSPSnqujsJKVqWFQpvhmq6u1A7UZexA\nt32DpPdFxDuDx5+U9Oe1VVaoqiM1bpmf09Im4d3VNkMXV2oH6jJJy2WXpB/Y/g9J/y7pOxHxr/WU\nVa6qLRTaDADqMvYZekT8VNLv1FhLClVbKLQZANSFYYs1G6eFQpsBQB2Yy6VmtFAANIUz9JrRQgHQ\nFAJ9CmihAGgCLRcASIJAB4AkaLkAGIoJ5MpAoEMSv7Ab8e9xDfPUl4OWC5jxcQP+Pa7HBHLlINDB\nL+wG/HtcjwnkykHLBWP9wmZuSRBg12MCuXJwho7KC0tkb0mw0Mb1uPu5HAQ6Kv/CZm9JEGDXO7h/\nUccO7dPi/JwsaXF+TscO7UvziSwTWi6JjdoWqTpdQfaWBNM3/H/c/VwGAj2pqkPNqvzCdqGnSoCh\nRLRckppmW4SWBNBOnT5DZ6TGeGhJAO3U2UDPfvfbtNsitCSA9ulsy4WRGgCy6ewZetWWRGntGdoi\nQPd0NtCrtCRKbc/QFgG6pbMtlyotieztGaAEp88t6c7j39MHH/qO7jz+vTR3Jteps2foVVoS2W+k\nAdqu1E/Js9bZQJdGb0l04UYaoM22+5RMoF/T2ZZLFYwYAZrFp+TREOgjYHIioFnMgDmaTrdcqmDE\nSPuUNpQU4zt6YO91PXSJT8mbIdBRpJIvkvGHqDruqxgNgY4ilXqRrOQ/RE3jU/Jw6QI9+9lP9v0b\nVakXyUr9Q4QytD7QqwRY9rOf7PtXRalDSUv9Q4QytHqUS9W1K7Pf0Zl9/6oodSgpozUwTa0O9KoB\nlv3sJ/v+VVHqUNJS/xChDK1uuVQNsFI/ho8q+/5VVeJFMkZrYJomCnTb90j6W0k7JP1DRByvpaqB\nqgGWfaxq9v3rihL/EKEMY7dcbO+Q9HeSPiXpdkn32769rsKk6h9PS/0YPqrs+wdgMo6I8X7Q/n1J\nj0TEgcH3D0tSRBzb6md6vV70+/1K78MwPQBdZ/tsRPSGvW6SlsuipP9e9/0bkn53gv/fpvh4CgCj\nmfooF9tHbPdt95eXl6f9dgDQWZME+pKkW9d9/xuDbdeJiEcjohcRvYWFhQneDgCwnUkC/ceSbrP9\nQdu/JumPJD1VT1kAgKrG7qFHxHu2H5R0RqvDFh+PiJdqqwwAUMlE49Aj4mlJT9dUCwBgAmMPWxzr\nzexlSa+P+eM3S/pZjeW0UfZ9ZP/Kl30f27p/vxURQy9CzjTQJ2G7P8o4zJJl30f2r3zZ97H0/Wv1\n5FwAgNER6ACQREmB/mjTBcxA9n1k/8qXfR+L3r9ieugAgO2VdIYOANhGEYFu+x7bF2z/xPZDTddT\nN9uv2T5v+wXb1aajbCnbj9u+bPvFddtusv2M7VcHX29sssZJbLF/j9heGhzHF2x/uskaJ2H7Vtvf\nt/2y7Zdsf36wPcUx3Gb/ij6GrW+5DOZd/y9Jn9DqjI4/lnR/RLzcaGE1sv2apF5EtHH861hsf0zS\nLyT9Y0R8eLDtLyW9HRHHB3+Yb4yIP2uyznFtsX+PSPpFRPxVk7XVwfZuSbsj4nnbvy7prKSDkv5U\nCY7hNvt3nwo+hiWcoX9U0k8i4qcR8b+S/lnSvQ3XhCEi4jlJb2/YfK+kk4PHJ7X6C1SkLfYvjYi4\nGBHPDx6/I+kVrU6ZneIYbrN/RSsh0Debd734f/gNQtJ3bZ+1faTpYqZoV0RcHDx+S9KuJouZkgdt\n/+egJVNkO2Ij23sk7Zf0IyU8hhv2Tyr4GJYQ6F1wV0R8RKvL+T0w+DifWqz2+trd76vua5I+JOkO\nSRclfaXZciZn+wOSviXpCxHx8/XPZTiGm+xf0cewhEAfad71kkXE0uDrZUnf1mqbKaNLg97lWg/z\ncsP11CoiLkXE1Yj4paS/V+HH0fZOrYbdNyLi1GBzmmO42f6VfgxLCPTU867bvmFwUUa2b5D0SUkv\nbv9TxXpK0uHB48OSnmywltqtBd3AZ1TwcbRtSY9JeiUivrruqRTHcKv9K/0Ytn6UiyQNhg79ja7N\nu/4XDZdUG9u/rdWzcml1OuN/yrB/tp+QdLdWZ6+7JOnLkk5L+qak39TqrJv3RUSRFxa32L+7tfpR\nPSS9Julz6/rNRbF9l6R/k3Re0i8Hm7+k1T5z8cdwm/27XwUfwyICHQAwXAktFwDACAh0AEiCQAeA\nJAh0AEiCQAeAJAh0AEiCQAeAJAh0AEji/wB9BzzaU1gIswAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "berjkI5HZd49",
        "colab_type": "code",
        "outputId": "5b699cab-473b-4a67-cbcf-36f4e5bf8109",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "days.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "hgD4q-T_B0jd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Looks pretty linear, let's try a simple OLS regression.\n",
        "\n",
        "First, these need to be NumPy arrays:"
      ]
    },
    {
      "metadata": {
        "id": "A3Q0MrnUBXAl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "days = days.reshape(-1, 1)  # X needs to be column vectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LTZWJmcOZmp4",
        "colab_type": "code",
        "outputId": "c5afde89-646e-40db-82d8-0274d9e0ca31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "days.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "vqr0SHOnB5yR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now let's use good old `scikit-learn` and linear regression:"
      ]
    },
    {
      "metadata": {
        "id": "PqyHxgFvBYl5",
        "colab_type": "code",
        "outputId": "63891b8b-f6fd-439d-d8d6-3431a4f4820d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "ols_stocks = LinearRegression()\n",
        "ols_stocks.fit(days, stock_quotes)\n",
        "ols_stocks.score(days, stock_quotes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5311207522596724"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "KlU0mr-KB_Yk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "That seems to work pretty well, but real stocks don't work like this.\n",
        "\n",
        "Let's make *slightly* more realistic data that depends on more than just time:"
      ]
    },
    {
      "metadata": {
        "id": "-FV1Emb2BuLz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Not everything is best as a comprehension\n",
        "stock_data = np.empty([len(days), 4])\n",
        "for day in days:\n",
        "    asset = random()\n",
        "    liability = random()\n",
        "    quote = random() + ((day * random()) + (20 * asset) - (15 * liability))\n",
        "    quote = max(quote, 0.01)  # Want positive quotes\n",
        "    stock_data[day] = np.array([quote, day, asset, liability])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6Qe2zzN1CESe",
        "colab_type": "code",
        "outputId": "f0e5f289-aa2d-4b41-818b-6c9ade071f71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 526
        }
      },
      "cell_type": "code",
      "source": [
        "stock_data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.00000000e-02, 0.00000000e+00, 4.21660894e-01, 8.55157754e-01],\n",
              "       [2.91489360e+00, 1.00000000e+00, 8.64141425e-01, 9.83792812e-01],\n",
              "       [2.32583983e+00, 2.00000000e+00, 1.68945928e-01, 1.49609604e-01],\n",
              "       [1.05528652e+01, 3.00000000e+00, 7.25044246e-01, 3.56775893e-01],\n",
              "       [1.02888261e+01, 4.00000000e+00, 9.77410675e-01, 8.43416655e-01],\n",
              "       [1.16005573e+01, 5.00000000e+00, 6.81890380e-01, 4.67442116e-01],\n",
              "       [1.00083329e+01, 6.00000000e+00, 8.88767839e-01, 6.42901900e-01],\n",
              "       [6.19267530e-01, 7.00000000e+00, 4.30956377e-01, 8.32382876e-01],\n",
              "       [3.63290439e+00, 8.00000000e+00, 5.58830102e-01, 9.78720046e-01],\n",
              "       [1.49211442e+01, 9.00000000e+00, 7.04396311e-01, 1.00458805e-01],\n",
              "       [1.00000000e-02, 1.00000000e+01, 1.11113784e-01, 8.60859654e-01],\n",
              "       [1.00000000e-02, 1.10000000e+01, 2.30403702e-01, 7.09090022e-01],\n",
              "       [2.10509559e+01, 1.20000000e+01, 9.79883608e-01, 6.48423266e-01],\n",
              "       [1.32654385e+01, 1.30000000e+01, 6.48152704e-01, 4.44903242e-01],\n",
              "       [1.63830476e+01, 1.40000000e+01, 6.78212172e-01, 1.12235713e-01],\n",
              "       [1.00000000e-02, 1.50000000e+01, 4.25528477e-01, 6.77517642e-01],\n",
              "       [1.85055856e+01, 1.60000000e+01, 5.21229309e-01, 3.06071336e-01],\n",
              "       [1.57419207e+01, 1.70000000e+01, 7.80953963e-01, 5.60492967e-01],\n",
              "       [1.09577638e+01, 1.80000000e+01, 4.08319546e-01, 9.27874026e-01],\n",
              "       [1.00000000e-02, 1.90000000e+01, 2.28267123e-01, 9.16381665e-01],\n",
              "       [1.11863878e+01, 2.00000000e+01, 2.30810684e-01, 6.89804752e-01],\n",
              "       [3.36888221e+01, 2.10000000e+01, 7.38211977e-01, 5.40348028e-02],\n",
              "       [2.67302001e+01, 2.20000000e+01, 9.55923625e-01, 2.04499046e-02],\n",
              "       [1.27888528e+01, 2.30000000e+01, 8.02809551e-02, 5.27198202e-01],\n",
              "       [1.59139117e+01, 2.40000000e+01, 4.15699132e-01, 6.87124856e-01],\n",
              "       [2.03906171e+01, 2.50000000e+01, 6.48787763e-01, 3.34046906e-01],\n",
              "       [1.00000000e-02, 2.60000000e+01, 1.78449290e-02, 3.40974010e-01],\n",
              "       [2.63284369e+00, 2.70000000e+01, 4.38712671e-01, 5.09712600e-01]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "BzYy4Pb2CLCh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's look again:"
      ]
    },
    {
      "metadata": {
        "id": "qdBcScz4CIXr",
        "colab_type": "code",
        "outputId": "2f259743-bb38-4990-c78b-b87bf846f10f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "cell_type": "code",
      "source": [
        "stock_quotes = stock_data[:,0]\n",
        "scatter(days, stock_quotes);"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAEs5JREFUeJzt3W+MHHd9x/HPp9ejrCDSJcrVsi9x\nnVJ0PIiFr9parYyqNAguRUg5rCpqHtCgojqVGgkkdIqdJwS1yG4NpE8qJKOkuBL/IjCXCGgPCwel\nPAmccyZOYq6k1FGzNvEhOJFIJ+pcvn2wc4lt3d3O7s7s7vz2/ZJOtzc7m/3OTfy5337nNzOOCAEA\nqu+3+l0AAKAYBDoAJIJAB4BEEOgAkAgCHQASQaADQCIIdABIBIEOAIloGei232r7h7Z/bPs525/K\nln/R9v/YPpN97Sm/XADAZn47xzq/kXR7RLxqe1TSD2z/e/bcbER8Pe+b3XjjjbFr164OygSA4XX6\n9OlfRMR4q/VaBno0rw3wavbjaPbV0fUCdu3apYWFhU5eCgBDy/aLedbL1UO3PWL7jKRLkk5GxFPZ\nU5+2/Yzth2z/Toe1AgAKkCvQI2ItIvZIuknSXtu3Sjok6V2S/kjSDZLu3+i1tg/YXrC9sLy8XFDZ\nAIBrtTXLJSJWJD0h6Y6IuBhNv5H0r5L2bvKaYxFRj4j6+HjLFhAAoEN5ZrmM2x7LHtckvU/ST2xv\nz5ZZ0oykZ8ssFACwtTyzXLZLOm57RM0/AI9GxLdsn7I9LsmSzkj62xLrBAC0kGeWyzOSpjZYfnsp\nFQGotLnFho7OL+nCyqp2jNU0Oz2pmamJfpc1FPKM0AEgl7nFhg6dOKvVy2uSpMbKqg6dOCtJhHoP\ncOo/gMIcnV96I8zXrV5e09H5pT5VNFwIdACFubCy2tZyFItAB1CYHWO1tpajWAQ6gMLMTk+qNjpy\n1bLa6Ihmpyf7VNFw4aAogMKsH/hklkt/EOgACjUzNUGA9wktFwBIBIEOAIkg0AEgEQQ6ACSCQAeA\nRBDoAJAIAh0AEkGgA0AiCHQASASBDgCJINABIBEEOgAkgkAHgES0DHTbb7X9Q9s/tv2c7U9ly2+x\n/ZTtF2x/zfZbyi8XALCZPCP030i6PSLeLWmPpDts/7Gkf5T0UET8gaRfSfpoeWUCAFppGejR9Gr2\n42j2FZJul/T1bPlxSTOlVAgAyCVXD932iO0zki5JOinpvyWtRMRr2SovSeKK9gDQR7kCPSLWImKP\npJsk7ZX0rrxvYPuA7QXbC8vLyx2WCQBopa1ZLhGxIukJSX8iacz2+i3sbpLU2OQ1xyKiHhH18fHx\nrooFAGwuzyyXcdtj2eOapPdJOqdmsP9Ftto9kh4rq0gAQGt5bhK9XdJx2yNq/gF4NCK+Zft5SV+1\n/Q+SFiU9XGKdAIAWWgZ6RDwjaWqD5T9Ts58OABgAnCkKAIkg0AEgEQQ6ACSCQAeARBDoAJAIAh0A\nEkGgA0AiCHQASASBDgCJINABIBEEOgAkIs/FuQCgFHOLDR2dX9KFlVXtGKtpdnpSM1PcK6dTBDqA\nvphbbOjQibNavbwmSWqsrOrQibOSRKh3iJYLgL44Or/0RpivW728pqPzS32qqPoIdAB9cWFlta3l\naI1AB9AXO8ZqbS1HawQ6gL6YnZ5UbXTkqmW10RHNTk/2qaLq46AogL5YP/DJLJfiEOgA+mZmaoIA\nLxAtFwBIBIEOAIloGei2b7b9hO3nbT9n+2PZ8gdtN2yfyb4+UH65AIDN5OmhvybpExHxtO3rJJ22\nfTJ77qGI+Ex55QEA8moZ6BFxUdLF7PErts9J4igGAAyYtnrotndJmpL0VLboPtvP2H7E9vWbvOaA\n7QXbC8vLy10VCwDYXO5At/12Sd+Q9PGI+LWkz0t6h6Q9ao7gP7vR6yLiWETUI6I+Pj5eQMkAgI3k\nCnTbo2qG+Zci4oQkRcTLEbEWEa9L+oKkveWVCQBoJc8sF0t6WNK5iPjcFcu3X7HahyQ9W3x5AIC8\n8sxy2Sfpw5LO2j6TLXtA0t2290gKSecl3VtKhQCAXPLMcvmBJG/w1HeKLwcA0CnOFAWARBDoAJAI\nAh0AEkGgA0AiCHQASASBDgCJINABIBEEOgAkgkAHgEQQ6ACQCAIdABJBoANAIgh0AEgEgQ4Aichz\nPXRgIM0tNnR0fkkXVla1Y6ym2elJzUxx/3IMLwIdlTS32NChE2e1enlNktRYWdWhE2cliVDH0KLl\ngko6Or/0RpivW728pqPzS32qCOg/Ah2VdGFlta3lwDCg5YJK2jFWU2OD8N4xVutDNai6VI7HMEJH\nJc1OT6o2OnLVstroiGanJ/tUEapq/XhMY2VVoTePx8wtNvpdWttaBrrtm20/Yft528/Z/li2/Abb\nJ23/NPt+ffnlAk0zUxM6vH+3JsZqsqSJsZoO799dyVEV+iul4zF5Wi6vSfpERDxt+zpJp22flPQR\nSd+LiCO2D0o6KOn+8koFrjYzNUGAo2spHY9pOUKPiIsR8XT2+BVJ5yRNSLpT0vFsteOSZsoqEgDK\nstlxlyoej2mrh257l6QpSU9J2hYRF7Onfi5pW6GVAUAPpHQ8JvcsF9tvl/QNSR+PiF/bfuO5iAjb\nscnrDkg6IEk7d+7srloAKNh62y6FWS6O2DCHr17JHpX0LUnzEfG5bNmSpNsi4qLt7ZK+HxFb/kmr\n1+uxsLBQQNkAMDxsn46Ieqv18sxysaSHJZ1bD/PM45LuyR7fI+mxTgoFABQjT8tln6QPSzpr+0y2\n7AFJRyQ9avujkl6UdFc5JQIA8mgZ6BHxA0ne5On3FlsOAKBTnPoP9Fgqp5lj8BDoQA9x2V+UiWu5\nAD2U0mnmGDwEOtBDKZ1mjsFDoAM9lNJp5hg8BDrQQymdZo7Bw0FRoIdSOs0cg4dAB3qMy/6iLLRc\nACARBDoAJIJAB4BEEOgAkAgCHQASwSwXYANcQAtVRKAD1+ACWqgqWi7ANbiAFqqKETpwDS6glYZh\nbJsR6MA1dozV1NggvPtxAa1hDKUiDGvbjJYLcI1BuYDWeig1VlYVejOU5hYbPa2jioa1bUagA9eY\nmZrQ4f27NTFWkyVNjNV0eP/uno/shjWUijCsbbOWLRfbj0j6oKRLEXFrtuxBSX8jaTlb7YGI+E5Z\nRQK9NggX0BrWUCrCILXNeinPCP2Lku7YYPlDEbEn+yLMh8zcYkP7jpzSLQe/rX1HTtEGKAE3w+jc\noLTNeq1loEfEk5J+2YNaUBH0dntjWEOpCIPSNuu1bma53Gf7ryQtSPpERPyqoJow4Lbq7ab+D6aX\nuBlGdwahbdZrnQb65yX9vaTIvn9W0l9vtKLtA5IOSNLOnTs7fDsMEnq7vTOMoYTOdTTLJSJejoi1\niHhd0hck7d1i3WMRUY+I+vj4eKd1YoDQ2wUGU0eBbnv7FT9+SNKzxZSDKqC3CwymPNMWvyLpNkk3\n2n5J0icl3WZ7j5otl/OS7i2xRgwYervAYHJE9OzN6vV6LCws9Oz9ACAFtk9HRL3VepwpCgCJINAB\nIBFcbRFDgysXInUEOobCsF5OFcOFlguGAlcuxDBghI6hwNmt1UfLrDVG6BgKnN1abVwQLh8CHUOB\ns1urjZZZPrRcMBQ4u7XaaJnlQ6BjaHDlwuoa1jsQtYuWC4CBR8ssH0boAAYeLbN8CPQBwHQsoDVa\nZq0R6H3GGYwAikKg9xn350QV8CmyGgj0PmM6FgYdnyKrg1kufcYZjCjK3GJD+46c0i0Hv619R04V\ndhYlJ/VUB4HeZ0zHQhHKPDWeT5HVQaDnVNboZ2ZqQof379bEWE2WNDFW0+H9u/koi7aUOYrmU2R1\n0EPPoeweItOx0K0yR9Gz05NX/f8v8SlyUDFCz6Hd0U9Zo3lgM2WOovkUWR0tR+i2H5H0QUmXIuLW\nbNkNkr4maZek85LuiohflVdmf7Uz+il7NM/0MWyk7FE0nyKrIc8I/YuS7rhm2UFJ34uId0r6XvZz\nstoZ/ZTZy+Sa0NgMo2hIOUboEfGk7V3XLL5T0m3Z4+OSvi/p/gLrGijtjH7K7GVyEhK2wiganfbQ\nt0XExezxzyVt22xF2wdsL9heWF5e7vDt+qud0U+ZvUymjwHYStezXCIibMcWzx+TdEyS6vX6pusN\nuryjnzJ7mVwTGsBWOh2hv2x7uyRl3y8VV1K1ldnL5CQkAFvpdIT+uKR7JB3Jvj9WWEUJKKuXyTWh\nAWwlz7TFr6h5APRG2y9J+qSaQf6o7Y9KelHSXWUWiTdx4AvAZvLMcrl7k6feW3AtAIAucKYoACSC\nQAeARBDoAJAIAh0AEkGgA0AiCHQASASBDgCJINABIBEEOgAkgkAHgEQQ6ACQCAIdABJBoANAIgh0\nAEgEgQ4AiSDQASARBDoAJIJAB4BEEOgAkAgCHQAS0fIm0VuxfV7SK5LWJL0WEfUiigIAtK+rQM/8\nWUT8ooD/DgCgC7RcACAR3QZ6SPqu7dO2DxRREACgM922XN4TEQ3bvyvppO2fRMSTV66QBf0BSdq5\nc2eXbwcA2ExXI/SIaGTfL0n6pqS9G6xzLCLqEVEfHx/v5u0AAFvoONBtv832deuPJb1f0rNFFQYA\naE83LZdtkr5pe/2/8+WI+I9CqgIAtK3jQI+In0l6d4G1AAC6UMQ8dCRgbrGho/NLurCyqh1jNc1O\nT2pmaqLfZQFoA4EOzS02dOjEWa1eXpMkNVZWdejEWUki1IEK4cQi6Oj80hthvm718pqOzi/1qSIA\nnSDQoQsrq20tBzCYkmu50Atu346xmhobhPeOsVofqgHQqaRG6Ou94MbKqkJv9oLnFhv9Lm2gzU5P\nqjY6ctWy2uiIZqcn+1QRkIa5xYb2HTmlWw5+W/uOnCo9i5IKdHrBnZmZmtDh/bs1MVaTJU2M1XR4\n/24+2QBd6McAM6mWS7u9YNozb5qZmhjabQfKsNUAs6x/a0mN0Dfr+W60nPYMgDL1Y7JBUoHeTi+Y\n9gyAMrUzwCxKUoHeTi+YqXoAytSPyQZJ9dCl/L1gpuoBKNN6DvXyOF1ygZ7X7PTkVae7S0zVA1Cs\nXk82GNpA78dfTwAo09AGusRUPQBpSeqgKAAMMwIdABIx1C2X1HEmLDBcCPREcdMKYPjQckkUZ8IC\nw6erQLd9h+0l2y/YPlhUUegeZ8ICw6fjlovtEUn/Iul9kl6S9CPbj0fE80UVJ9EH7tQgnQnbzj6s\n4v6m5uEyyL+7bkboeyW9EBE/i4j/k/RVSXcWU1YTV0Ts3KDctKKdfVjF/U3Nw2XQf3fdBPqEpP+9\n4ueXsmWFoQ/cuUG5aUU7+7CK+5uah8ug/+5Kn+Vi+4CkA5K0c+fOtl5LH7g7g3AmbDv7sIr7m5qH\ny6D/7roZoTck3XzFzzdly64SEccioh4R9fHx8bbeoB/XE0ax2tmHVdzf1DxcBv13102g/0jSO23f\nYvstkv5S0uPFlNU0KH1gdK6dfVjF/U3Nw2XQf3cdt1wi4jXb90malzQi6ZGIeK6wysQVEVPQzj6s\n4v6m5uEy6L87R0TP3qxer8fCwkLP3g8AUmD7dETUW63HmaIAkAgCHQASQaADQCIIdABIBIEOAIno\n6SwX28uSXuzw5TdK+kWB5Qyi1LeR7au+1LdxULfv9yKi5ZmZPQ30btheyDNtp8pS30a2r/pS38aq\nbx8tFwBIBIEOAImoUqAf63cBPZD6NrJ91Zf6NlZ6+yrTQwcAbK1KI3QAwBYqEeip34za9nnbZ22f\nsZ3E1ctsP2L7ku1nr1h2g+2Ttn+afb++nzV2Y5Pte9B2I9uPZ2x/oJ81dsP2zbafsP287edsfyxb\nnsQ+3GL7Kr0PB77lkt2M+r90xc2oJd1d9M2o+8n2eUn1iBjE+a8dsf2nkl6V9G8RcWu27J8k/TIi\njmR/mK+PiPv7WWenNtm+ByW9GhGf6WdtRbC9XdL2iHja9nWSTkuakfQRJbAPt9i+u1ThfViFEXrp\nN6NG8SLiSUm/vGbxnZKOZ4+Pq/kPqJI22b5kRMTFiHg6e/yKpHNq3jM4iX24xfZVWhUCvfSbUQ+A\nkPRd26eze7CmaltEXMwe/1zStn4WU5L7bD+TtWQq2Y64lu1dkqYkPaUE9+E12ydVeB9WIdCHwXsi\n4g8l/bmkv8s+zictmr2+we73te/zkt4haY+ki5I+299yumf77ZK+IenjEfHrK59LYR9usH2V3odV\nCPRcN6OusohoZN8vSfqmmm2mFL2c9S7Xe5iX+lxPoSLi5YhYi4jXJX1BFd+PtkfVDLsvRcSJbHEy\n+3Cj7av6PqxCoJd+M+p+sv227KCMbL9N0vslPbv1qyrrcUn3ZI/vkfRYH2sp3HrQZT6kCu9H25b0\nsKRzEfG5K55KYh9utn1V34cDP8tFkrKpQ/+sN29G/ek+l1QY27+v5qhcat60+8spbJ/tr0i6Tc2r\n170s6ZOS5iQ9KmmnmlfdvCsiKnlgcZPtu03Nj+oh6byke6/oN1eK7fdI+k9JZyW9ni1+QM0+c+X3\n4Rbbd7cqvA8rEegAgNaq0HIBAORAoANAIgh0AEgEgQ4AiSDQASARBDoAJIJAB4BEEOgAkIj/B6sO\nQFFG9usuAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "SBXb7dieCO5h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "How does our old model do?"
      ]
    },
    {
      "metadata": {
        "id": "7gAxCgy1COnX",
        "colab_type": "code",
        "outputId": "23750b70-1d6b-413c-bbf9-54e62094e517",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "days = np.array(days).reshape(-1, 1)\n",
        "ols_stocks.fit(days, stock_quotes)\n",
        "ols_stocks.score(days, stock_quotes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.12086018181701486"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "Lc3pYUJYbMkk",
        "colab_type": "code",
        "outputId": "718bad1e-55c6-47c5-92ae-d59a7212725a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "stock_data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "3E94vTFUCax_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Not bad, but can we do better?"
      ]
    },
    {
      "metadata": {
        "id": "mCR5GImZCbGz",
        "colab_type": "code",
        "outputId": "2242a1ee-8174-41fb-c8f4-997eb3ccdc6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "ols_stocks.fit(stock_data[:,1:], stock_quotes)\n",
        "ols_stocks.score(stock_data[:,1:], stock_quotes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7254294484477495"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "1Qk-jlBCCiKB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Yep - unsurprisingly, the other covariates (assets and liabilities) have info.\n",
        "\n",
        "But, they do worse without the day data."
      ]
    },
    {
      "metadata": {
        "id": "dDcZl7I5Cf5D",
        "colab_type": "code",
        "outputId": "40872f9d-4aeb-4c0f-bce4-4e1b6c4fe9d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "ols_stocks.fit(stock_data[:,2:], stock_quotes)\n",
        "ols_stocks.score(stock_data[:,2:], stock_quotes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5857214854273096"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "pnLXlrK8ENjb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Time series jargon\n",
        "\n",
        "There's a lot of semi-standard language and tricks to talk about this sort of data. [NIST](https://www.itl.nist.gov/div898/handbook/pmc/section4/pmc4.htm) has an excellent guidebook, but here are some highlights:"
      ]
    },
    {
      "metadata": {
        "id": "yWUyhnTbcq55",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Moving average\n",
        "\n",
        "Moving average aka rolling average aka running average.\n",
        "\n",
        "Convert a series of data to a series of averages of continguous subsets:"
      ]
    },
    {
      "metadata": {
        "id": "47bHhBSCcvw-",
        "colab_type": "code",
        "outputId": "adb880b6-bc6e-40d4-e608-f0fe3faa3d7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 526
        }
      },
      "cell_type": "code",
      "source": [
        "stock_quotes_rolling = [sum(stock_quotes[i:i+3]) / 3\n",
        "                        for i in range(len(stock_quotes - 2))]\n",
        "stock_quotes_rolling"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.750244476832771,\n",
              " 5.26453288501194,\n",
              " 7.722510394977088,\n",
              " 10.814082884231274,\n",
              " 10.632572107130253,\n",
              " 7.409385907247409,\n",
              " 4.753501604998836,\n",
              " 6.3911053762421615,\n",
              " 6.188016199533099,\n",
              " 4.980381402321473,\n",
              " 7.02365195832862,\n",
              " 11.442131450762254,\n",
              " 16.899813970510923,\n",
              " 9.886162012182302,\n",
              " 11.632877729047621,\n",
              " 11.419168779688258,\n",
              " 15.068423374297486,\n",
              " 8.903228164998533,\n",
              " 7.3847171921961,\n",
              " 14.961736626602912,\n",
              " 23.868470002739162,\n",
              " 24.402625015220124,\n",
              " 18.477654880302165,\n",
              " 16.364460521616223,\n",
              " 12.104842911548387,\n",
              " 7.677820248789314,\n",
              " 0.8809478980056694,\n",
              " 0.8776145646723362]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "36XvbGhoc186",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Pandas has nice series related functions:"
      ]
    },
    {
      "metadata": {
        "id": "nTNatxtycys_",
        "colab_type": "code",
        "outputId": "612f83b9-9ebc-45d9-b1bd-4ee98bcee467",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 927
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(stock_quotes)\n",
        "df.rolling(3).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.750244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5.264533</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7.722510</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>10.814083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>10.632572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7.409386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>4.753502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>6.391105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>6.188016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>4.980381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>7.023652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>11.442131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>16.899814</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>9.886162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>11.632878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>11.419169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>15.068423</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>8.903228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>7.384717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>14.961737</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>23.868470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>24.402625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>18.477655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>16.364461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>12.104843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>7.677820</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            0\n",
              "0         NaN\n",
              "1         NaN\n",
              "2    1.750244\n",
              "3    5.264533\n",
              "4    7.722510\n",
              "5   10.814083\n",
              "6   10.632572\n",
              "7    7.409386\n",
              "8    4.753502\n",
              "9    6.391105\n",
              "10   6.188016\n",
              "11   4.980381\n",
              "12   7.023652\n",
              "13  11.442131\n",
              "14  16.899814\n",
              "15   9.886162\n",
              "16  11.632878\n",
              "17  11.419169\n",
              "18  15.068423\n",
              "19   8.903228\n",
              "20   7.384717\n",
              "21  14.961737\n",
              "22  23.868470\n",
              "23  24.402625\n",
              "24  18.477655\n",
              "25  16.364461\n",
              "26  12.104843\n",
              "27   7.677820"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "os-szg47dgwf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Forecasting\n",
        "\n",
        "Forecasting - at it's simplest, it just means \"predict the future\":"
      ]
    },
    {
      "metadata": {
        "id": "D_qtt6irdj0x",
        "colab_type": "code",
        "outputId": "58cb4eda-843a-43aa-ce10-403962bf3f1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "ols_stocks.fit(stock_data[:,1:], stock_quotes)\n",
        "ols_stocks.predict([[29, 0.5, 0.5]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([17.21298615])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "fjnQY0trdnHp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "One way to predict if you just have the series data is to use the prior observation. This can be pretty good (if you had to pick one feature to model the temperature for tomorrow, the temperature today is a good choice)."
      ]
    },
    {
      "metadata": {
        "id": "bzC4DV9Hdupp",
        "colab_type": "code",
        "outputId": "9f7dbb99-1969-47c3-c911-caa30c03e6f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "temperature = np.array([30 + random() * day\n",
        "                        for day in np.array(range(365)).reshape(-1, 1)])\n",
        "temperature_next = temperature[1:].reshape(-1, 1)\n",
        "temperature_ols = LinearRegression()\n",
        "temperature_ols.fit(temperature[:-1], temperature_next)\n",
        "temperature_ols.score(temperature[:-1], temperature_next)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.20144138389976884"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "RFdssXQbdxbE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "But you can often make it better by considering more than one prior observation."
      ]
    },
    {
      "metadata": {
        "id": "pVfUqD2YdxxZ",
        "colab_type": "code",
        "outputId": "e1cb048f-53de-47ae-b08d-091b10a50aa2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "temperature_next_next = temperature[2:].reshape(-1, 1)\n",
        "temperature_two_past = np.concatenate([temperature[:-2], temperature_next[:-1]],\n",
        "                                      axis=1)\n",
        "temperature_ols.fit(temperature_two_past, temperature_next_next)\n",
        "temperature_ols.score(temperature_two_past, temperature_next_next)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.25478095944753787"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "c9QltBdmd7TV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Exponential smoothing\n",
        "\n",
        "Exponential smoothing means using exponentially decreasing past weights to predict the future.\n",
        "\n",
        "You could roll your own, but let's use Pandas."
      ]
    },
    {
      "metadata": {
        "id": "hvMNqunOeC_B",
        "colab_type": "code",
        "outputId": "43c3d249-9536-4c0f-cc04-2d4772a54df6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1992
        }
      },
      "cell_type": "code",
      "source": [
        "temperature_df = pd.DataFrame(temperature)\n",
        "temperature_df.ewm(halflife=7).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>30.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>30.341456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>30.261324</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>30.186050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>30.494095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>30.936964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>31.792322</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>32.625315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>32.741700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>33.124500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>33.717854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>34.217478</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>35.180314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>35.581916</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>36.364137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>37.284915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>37.362599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>37.783659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>37.737816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>38.057442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>38.886868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>39.891625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>40.316552</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>41.422235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>40.749923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>42.098465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>42.050932</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>41.058548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>41.816922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>43.177626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>335</th>\n",
              "      <td>198.299964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>336</th>\n",
              "      <td>203.867517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>337</th>\n",
              "      <td>211.728885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>338</th>\n",
              "      <td>197.050620</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>339</th>\n",
              "      <td>209.782167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>340</th>\n",
              "      <td>215.218735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>341</th>\n",
              "      <td>219.512309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>342</th>\n",
              "      <td>230.381966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>343</th>\n",
              "      <td>226.143000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>344</th>\n",
              "      <td>217.041042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>345</th>\n",
              "      <td>200.560001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>346</th>\n",
              "      <td>205.209241</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>347</th>\n",
              "      <td>212.521839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>348</th>\n",
              "      <td>216.367915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>349</th>\n",
              "      <td>220.638188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>350</th>\n",
              "      <td>226.284107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>351</th>\n",
              "      <td>240.327234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>352</th>\n",
              "      <td>226.078525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>353</th>\n",
              "      <td>220.573083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>354</th>\n",
              "      <td>211.533325</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>355</th>\n",
              "      <td>209.140212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>356</th>\n",
              "      <td>210.264441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>357</th>\n",
              "      <td>204.384390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>358</th>\n",
              "      <td>193.589388</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>359</th>\n",
              "      <td>208.486529</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>360</th>\n",
              "      <td>199.978412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>361</th>\n",
              "      <td>193.213642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>362</th>\n",
              "      <td>198.417640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>363</th>\n",
              "      <td>209.217211</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>364</th>\n",
              "      <td>226.255833</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>365 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              0\n",
              "0     30.000000\n",
              "1     30.341456\n",
              "2     30.261324\n",
              "3     30.186050\n",
              "4     30.494095\n",
              "5     30.936964\n",
              "6     31.792322\n",
              "7     32.625315\n",
              "8     32.741700\n",
              "9     33.124500\n",
              "10    33.717854\n",
              "11    34.217478\n",
              "12    35.180314\n",
              "13    35.581916\n",
              "14    36.364137\n",
              "15    37.284915\n",
              "16    37.362599\n",
              "17    37.783659\n",
              "18    37.737816\n",
              "19    38.057442\n",
              "20    38.886868\n",
              "21    39.891625\n",
              "22    40.316552\n",
              "23    41.422235\n",
              "24    40.749923\n",
              "25    42.098465\n",
              "26    42.050932\n",
              "27    41.058548\n",
              "28    41.816922\n",
              "29    43.177626\n",
              "..          ...\n",
              "335  198.299964\n",
              "336  203.867517\n",
              "337  211.728885\n",
              "338  197.050620\n",
              "339  209.782167\n",
              "340  215.218735\n",
              "341  219.512309\n",
              "342  230.381966\n",
              "343  226.143000\n",
              "344  217.041042\n",
              "345  200.560001\n",
              "346  205.209241\n",
              "347  212.521839\n",
              "348  216.367915\n",
              "349  220.638188\n",
              "350  226.284107\n",
              "351  240.327234\n",
              "352  226.078525\n",
              "353  220.573083\n",
              "354  211.533325\n",
              "355  209.140212\n",
              "356  210.264441\n",
              "357  204.384390\n",
              "358  193.589388\n",
              "359  208.486529\n",
              "360  199.978412\n",
              "361  193.213642\n",
              "362  198.417640\n",
              "363  209.217211\n",
              "364  226.255833\n",
              "\n",
              "[365 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "gBEjBZVbeH6R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Halflife is among the parameters we can play with:"
      ]
    },
    {
      "metadata": {
        "id": "HjZgMwYkeODN",
        "colab_type": "code",
        "outputId": "b5d084bb-4e7b-4ed4-b7b4-7511d9be4144",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "cell_type": "code",
      "source": [
        "sse_1 = ((temperature_df - temperature_df.ewm(halflife=7).mean())**2).sum()\n",
        "sse_2 = ((temperature_df - temperature_df.ewm(halflife=3).mean())**2).sum()\n",
        "print(sse_1)\n",
        "print(sse_2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    1.171995e+06\n",
            "dtype: float64\n",
            "0    951423.99921\n",
            "dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "s39bj4g9eQ9Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Note - the first error being higher doesn't mean it's necessarily *worse*. It's *smoother* as expected, and if that's what we care about - great!"
      ]
    },
    {
      "metadata": {
        "id": "OcPMn8o4eYP1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Seasonality\n",
        "\n",
        "Seasonality - \"day of week\"-effects, and more. In a lot of real world data, certain time periods are systemically different, e.g. holidays for retailers, weekends for restaurants, seasons for weather.\n",
        "\n",
        "Let's try to make some seasonal data - a store that sells more later in a week:"
      ]
    },
    {
      "metadata": {
        "id": "h0qPMWCreheL",
        "colab_type": "code",
        "outputId": "f61a94c8-462a-4be1-8e0a-83860de28cc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "cell_type": "code",
      "source": [
        "sales = np.array([random() + (day % 7) * random() for day in days])\n",
        "scatter(days, sales)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7f721ba10ba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "LEADkcMzelxY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "How does linear regression do at fitting this?"
      ]
    },
    {
      "metadata": {
        "id": "EV5kt69GenV3",
        "colab_type": "code",
        "outputId": "a2126794-d237-4130-84a4-5ee507d108ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "sales_ols = LinearRegression()\n",
        "sales_ols.fit(days, sales)\n",
        "sales_ols.score(days, sales)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.11783836419607518"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "7shN1eBMep9Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "That's not great - and the fix depends on the domain. Here, we know it'd be best to actually use \"day of week\" as a feature."
      ]
    },
    {
      "metadata": {
        "id": "Qo9eFlHIeqtA",
        "colab_type": "code",
        "outputId": "cc707d87-9af2-42ee-acd0-58750f478f78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "day_of_week = days % 7\n",
        "sales_ols.fit(day_of_week, sales)\n",
        "sales_ols.score(day_of_week, sales)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4095877701097166"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "9ooJIfIMex2G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Note that it's also important to have representative data across whatever seasonal feature(s) you use - don't predict retailers based only on Christmas, as that won't generalize well."
      ]
    },
    {
      "metadata": {
        "id": "44QZgrPUe3-Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Recurrent Neural Networks\n",
        "\n",
        "There's plenty more to \"traditional\" time series, but the latest and greatest technique for sequence data is recurrent neural networks. A recurrence relation in math is an equation that uses recursion to define a sequence - a famous example is the Fibonacci numbers:\n",
        "\n",
        "$F_n = F_{n-1} + F_{n-2}$\n",
        "\n",
        "For formal math you also need a base case $F_0=1, F_1=1$, and then the rest builds from there. But for neural networks what we're really talking about are loops:\n",
        "\n",
        "![Recurrent neural network](https://upload.wikimedia.org/wikipedia/commons/b/b5/Recurrent_neural_network_unfold.svg)\n",
        "\n",
        "The hidden layers have edges (output) going back to their own input - this loop means that for any time `t` the training is at least partly based on the output from time `t-1`. The entire network is being represented on the left, and you can unfold the network explicitly to see how it behaves at any given `t`.\n",
        "\n",
        "Different units can have this \"loop\", but a particularly successful one is the long short-term memory unit (LSTM):\n",
        "\n",
        "![Long short-term memory unit](https://upload.wikimedia.org/wikipedia/commons/thumb/6/63/Long_Short-Term_Memory.svg/1024px-Long_Short-Term_Memory.svg.png)\n",
        "\n",
        "There's a lot going on here - in a nutshell, the calculus still works out and backpropagation can still be implemented. The advantage (ane namesake) of LSTM is that it can generally put more weight on recent (short-term) events while not completely losing older (long-term) information.\n",
        "\n",
        "After enough iterations, a typical neural network will start calculating prior gradients that are so small they effectively become zero - this is the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem), and is what RNN with LSTM addresses. Pay special attention to the $c_t$ parameters and how they pass through the unit to get an intuition for how this problem is solved.\n",
        "\n",
        "So why are these cool? One particularly compelling application is actually not time series but language modeling - language is inherently ordered data (letters/words go one after another, and the order *matters*). [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) is a famous and worth reading blog post on this topic.\n",
        "\n",
        "For our purposes, let's use TensorFlow and Keras to train RNNs with natural language. Resources:\n",
        "\n",
        "- https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py\n",
        "- https://keras.io/layers/recurrent/#lstm\n",
        "- http://adventuresinmachinelearning.com/keras-lstm-tutorial/\n",
        "\n",
        "Note that `tensorflow.contrib` [also has an implementation of RNN/LSTM](https://www.tensorflow.org/tutorials/sequences/recurrent)."
      ]
    },
    {
      "metadata": {
        "id": "eWrQllf8WEd-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### RNN/LSTM Sentiment Classification with Keras"
      ]
    },
    {
      "metadata": {
        "id": "Ti23G0gRe3kr",
        "colab_type": "code",
        "outputId": "e152db92-8a84-4cc2-ff57-db552c5f8570",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1426
        }
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "#Trains an LSTM model on the IMDB sentiment classification task.\n",
        "The dataset is actually too small for LSTM to be of any advantage\n",
        "compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
        "**Notes**\n",
        "- RNNs are tricky. Choice of batch size is important,\n",
        "choice of loss and optimizer is critical, etc.\n",
        "Some configurations won't converge.\n",
        "- LSTM loss decrease patterns during training can be quite different\n",
        "from what you see with CNNs/MLPs/etc.\n",
        "'''\n",
        "from __future__ import print_function\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding\n",
        "from keras.layers import LSTM\n",
        "from keras.datasets import imdb\n",
        "\n",
        "max_features = 20000\n",
        "# cut texts after this number of words (among top max_features most common words)\n",
        "maxlen = 80\n",
        "batch_size = 32\n",
        "\n",
        "print('Loading data...')\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')\n",
        "\n",
        "print('Pad sequences (samples x time)')\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "\n",
        "print('Build model...')\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 128))\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# try using different optimizers and different optimizer configs\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print('Train...')\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=15,\n",
        "          validation_data=(x_test, y_test))\n",
        "score, acc = model.evaluate(x_test, y_test,\n",
        "                            batch_size=batch_size)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "25000 train sequences\n",
            "25000 test sequences\n",
            "Pad sequences (samples x time)\n",
            "x_train shape: (25000, 80)\n",
            "x_test shape: (25000, 80)\n",
            "Build model...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Train...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/15\n",
            "25000/25000 [==============================] - 153s 6ms/step - loss: 0.4661 - acc: 0.7810 - val_loss: 0.3857 - val_acc: 0.8325\n",
            "Epoch 2/15\n",
            "25000/25000 [==============================] - 147s 6ms/step - loss: 0.3033 - acc: 0.8750 - val_loss: 0.3751 - val_acc: 0.8356\n",
            "Epoch 3/15\n",
            "25000/25000 [==============================] - 150s 6ms/step - loss: 0.2223 - acc: 0.9106 - val_loss: 0.4112 - val_acc: 0.8222\n",
            "Epoch 4/15\n",
            "22816/25000 [==========================>...] - ETA: 11s - loss: 0.1526 - acc: 0.9422"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-c26ac6abc1af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m           validation_data=(x_test, y_test))\n\u001b[0m\u001b[1;32m     52\u001b[0m score, acc = model.evaluate(x_test, y_test,\n\u001b[1;32m     53\u001b[0m                             batch_size=batch_size)\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "7pETWPIe362y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### RNN Text generation with NumPy\n",
        "\n",
        "What else can we do with RNN? Since we're analyzing the *sequence*, we can do more than classify - we can *generate* text. We'll pull some news stories using [newspaper](https://github.com/codelucas/newspaper/)."
      ]
    },
    {
      "metadata": {
        "id": "fz1m55G5WSrQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Initialization"
      ]
    },
    {
      "metadata": {
        "id": "ahlHBeoZCaLX",
        "colab_type": "code",
        "outputId": "ecf47ed3-c9de-42b0-c3cb-8c592410d587",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install newspaper3k"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: newspaper3k in /usr/local/lib/python3.6/dist-packages (0.2.8)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (4.1.1)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (4.6.3)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.18.4)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.5.3)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (3.13)\n",
            "Requirement already satisfied: tldextract>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.2.1)\n",
            "Requirement already satisfied: jieba3k>=0.35.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (0.35.1)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (4.2.6)\n",
            "Requirement already satisfied: feedparser>=5.2.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (5.2.1)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (3.2.5)\n",
            "Requirement already satisfied: tinysegmenter==0.3 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (0.3)\n",
            "Requirement already satisfied: cssselect>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (1.0.3)\n",
            "Requirement already satisfied: feedfinder2>=0.0.4 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (0.0.4)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow>=3.3.0->newspaper3k) (0.46)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (1.22)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (2019.3.9)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.5.3->newspaper3k) (1.11.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from tldextract>=2.0.1->newspaper3k) (40.9.0)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.6/dist-packages (from tldextract>=2.0.1->newspaper3k) (1.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fTPlziljCiNJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import newspaper"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i1R4UR8nBAj3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ap = newspaper.build('https://www.apnews.com/', memoize_articles=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bk9JF2zaCxoO",
        "colab_type": "code",
        "outputId": "e8916bb4-ac9f-4746-8c08-493a6e2c5d75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "len(ap.articles)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "77"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "metadata": {
        "id": "Vc6JgAIJDF4E",
        "colab_type": "code",
        "outputId": "ec1c42f5-2eb9-48ec-cb9e-04e3099bc509",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "article_text = ''\n",
        "\n",
        "for article in ap.articles[:1]:\n",
        "    try:\n",
        "        article.download()\n",
        "        article.parse()\n",
        "        article_text += '\\n\\n' + article.text\n",
        "    except:\n",
        "        print('Failed: ' + article.url)\n",
        "\n",
        "article_text = article_text.split('\\n\\n')[1]\n",
        "print(article_text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Close Get email notifications on {{subject}} daily!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "805DXB1GJYTr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "url = 'https://www.apnews.com/a292ff1cd8934579b60fac464c9d5e5c'\n",
        "article = newspaper.Article(url)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1xjmcJ0TI5cN",
        "colab_type": "code",
        "outputId": "7ca6a3e6-e623-4f8d-8237-b9acb457e59a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "cell_type": "code",
      "source": [
        "article_text = ''\n",
        "\n",
        "try:\n",
        "    article.download()\n",
        "    article.parse()\n",
        "    article_text += '\\n\\n' + article.text\n",
        "except:\n",
        "    print('Failed: ' + article.url)\n",
        "\n",
        "article_text = article_text.split('\\n\\n')[1]\n",
        "print(article_text)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This image made from video aired on \"Jeopardy!\" on Tuesday, April 9. 2019, and provided by Jeopardy Productions, Inc. shows James Holzhauer. The 34-year-old professional sports gambler from Las Vegas won more than $110,000 on \"Jeopardy!\" on Tuesday, breaking the record for single-day cash winnings. (Jeopardy Productions, Inc. via AP)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rsMBBMcv_nRM",
        "colab_type": "code",
        "outputId": "81286e95-5767-45a7-fc81-771add7f4a48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "# Based on \"The Unreasonable Effectiveness of RNN\" implementation\n",
        "import numpy as np\n",
        "\n",
        "chars = list(set(article_text)) # split and remove duplicate characters. convert to list.\n",
        "\n",
        "num_chars = len(chars) # the number of unique characters\n",
        "txt_data_size = len(article_text)\n",
        "\n",
        "print(\"unique characters : \", num_chars)\n",
        "print(\"txt_data_size : \", txt_data_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unique characters :  46\n",
            "txt_data_size :  335\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aQygqc_CAWRA",
        "colab_type": "code",
        "outputId": "ec8807f8-a5d7-48e8-bd15-116e236f264f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "cell_type": "code",
      "source": [
        "# one hot encode\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars)) # \"enumerate\" returns index and value. Convert it to dictionary\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "print(char_to_int)\n",
        "print(\"----------------------------------------------------\")\n",
        "print(int_to_char)\n",
        "print(\"----------------------------------------------------\")\n",
        "# integer encode input data\n",
        "integer_encoded = [char_to_int[i] for i in article_text] # \"integer_encoded\" is a list which has a sequence converted from an original data to integers.\n",
        "print(integer_encoded)\n",
        "print(\"----------------------------------------------------\")\n",
        "print(\"data length : \", len(integer_encoded))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'h': 0, 'T': 1, 'I': 2, '\"': 3, '-': 4, ')': 5, 'a': 6, 'd': 7, 'J': 8, '.': 9, 'p': 10, 't': 11, 'm': 12, 's': 13, '!': 14, 'k': 15, 'i': 16, 'z': 17, '(': 18, '$': 19, 'n': 20, 'c': 21, 'e': 22, 'A': 23, 'b': 24, 'V': 25, '4': 26, '9': 27, '3': 28, '2': 29, 'P': 30, 'f': 31, 'v': 32, 'l': 33, '0': 34, 'H': 35, 'L': 36, 'u': 37, 'w': 38, 'o': 39, ',': 40, 'g': 41, '1': 42, ' ': 43, 'y': 44, 'r': 45}\n",
            "----------------------------------------------------\n",
            "{0: 'h', 1: 'T', 2: 'I', 3: '\"', 4: '-', 5: ')', 6: 'a', 7: 'd', 8: 'J', 9: '.', 10: 'p', 11: 't', 12: 'm', 13: 's', 14: '!', 15: 'k', 16: 'i', 17: 'z', 18: '(', 19: '$', 20: 'n', 21: 'c', 22: 'e', 23: 'A', 24: 'b', 25: 'V', 26: '4', 27: '9', 28: '3', 29: '2', 30: 'P', 31: 'f', 32: 'v', 33: 'l', 34: '0', 35: 'H', 36: 'L', 37: 'u', 38: 'w', 39: 'o', 40: ',', 41: 'g', 42: '1', 43: ' ', 44: 'y', 45: 'r'}\n",
            "----------------------------------------------------\n",
            "[1, 0, 16, 13, 43, 16, 12, 6, 41, 22, 43, 12, 6, 7, 22, 43, 31, 45, 39, 12, 43, 32, 16, 7, 22, 39, 43, 6, 16, 45, 22, 7, 43, 39, 20, 43, 3, 8, 22, 39, 10, 6, 45, 7, 44, 14, 3, 43, 39, 20, 43, 1, 37, 22, 13, 7, 6, 44, 40, 43, 23, 10, 45, 16, 33, 43, 27, 9, 43, 29, 34, 42, 27, 40, 43, 6, 20, 7, 43, 10, 45, 39, 32, 16, 7, 22, 7, 43, 24, 44, 43, 8, 22, 39, 10, 6, 45, 7, 44, 43, 30, 45, 39, 7, 37, 21, 11, 16, 39, 20, 13, 40, 43, 2, 20, 21, 9, 43, 13, 0, 39, 38, 13, 43, 8, 6, 12, 22, 13, 43, 35, 39, 33, 17, 0, 6, 37, 22, 45, 9, 43, 1, 0, 22, 43, 28, 26, 4, 44, 22, 6, 45, 4, 39, 33, 7, 43, 10, 45, 39, 31, 22, 13, 13, 16, 39, 20, 6, 33, 43, 13, 10, 39, 45, 11, 13, 43, 41, 6, 12, 24, 33, 22, 45, 43, 31, 45, 39, 12, 43, 36, 6, 13, 43, 25, 22, 41, 6, 13, 43, 38, 39, 20, 43, 12, 39, 45, 22, 43, 11, 0, 6, 20, 43, 19, 42, 42, 34, 40, 34, 34, 34, 43, 39, 20, 43, 3, 8, 22, 39, 10, 6, 45, 7, 44, 14, 3, 43, 39, 20, 43, 1, 37, 22, 13, 7, 6, 44, 40, 43, 24, 45, 22, 6, 15, 16, 20, 41, 43, 11, 0, 22, 43, 45, 22, 21, 39, 45, 7, 43, 31, 39, 45, 43, 13, 16, 20, 41, 33, 22, 4, 7, 6, 44, 43, 21, 6, 13, 0, 43, 38, 16, 20, 20, 16, 20, 41, 13, 9, 43, 18, 8, 22, 39, 10, 6, 45, 7, 44, 43, 30, 45, 39, 7, 37, 21, 11, 16, 39, 20, 13, 40, 43, 2, 20, 21, 9, 43, 32, 16, 6, 43, 23, 30, 5]\n",
            "----------------------------------------------------\n",
            "data length :  335\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bcpMSWDHFowT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "\n",
        "iteration = 1000\n",
        "sequence_length = 40\n",
        "batch_size = round((txt_data_size /sequence_length)+0.5) # = math.ceil\n",
        "hidden_size = 500  # size of hidden layer of neurons.  \n",
        "learning_rate = 1e-1\n",
        "\n",
        "\n",
        "# model parameters\n",
        "\n",
        "W_xh = np.random.randn(hidden_size, num_chars)*0.01     # weight input -> hidden. \n",
        "W_hh = np.random.randn(hidden_size, hidden_size)*0.01   # weight hidden -> hidden\n",
        "W_hy = np.random.randn(num_chars, hidden_size)*0.01     # weight hidden -> output\n",
        "\n",
        "b_h = np.zeros((hidden_size, 1)) # hidden bias\n",
        "b_y = np.zeros((num_chars, 1)) # output bias\n",
        "\n",
        "h_prev = np.zeros((hidden_size,1)) # h_(t-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bkqoN86qWaI4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Forward propagation"
      ]
    },
    {
      "metadata": {
        "id": "imfg_Ew0WdDL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def forwardprop(inputs, targets, h_prev):\n",
        "        \n",
        "    # Since the RNN receives the sequence, the weights are not updated during one sequence.\n",
        "    xs, hs, ys, ps = {}, {}, {}, {} # dictionary\n",
        "    hs[-1] = np.copy(h_prev) # Copy previous hidden state vector to -1 key value.\n",
        "    loss = 0 # loss initialization\n",
        "    \n",
        "    for t in range(len(inputs)): # t is a \"time step\" and is used as a key(dic).  \n",
        "        \n",
        "        xs[t] = np.zeros((num_chars,1)) \n",
        "        xs[t][inputs[t]] = 1\n",
        "        hs[t] = np.tanh(np.dot(W_xh, xs[t]) + np.dot(W_hh, hs[t-1]) + b_h) # hidden state. \n",
        "        ys[t] = np.dot(W_hy, hs[t]) + b_y # unnormalized log probabilities for next chars\n",
        "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars. \n",
        "        # Softmax. -> The sum of probabilities is 1 even without the exp() function, but all of the elements are positive through the exp() function.\n",
        " \n",
        "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss). Efficient and simple code\n",
        "\n",
        "#         y_class = np.zeros((num_chars, 1)) \n",
        "#         y_class[targets[t]] =1\n",
        "#         loss += np.sum(y_class*(-np.log(ps[t]))) # softmax (cross-entropy loss)        \n",
        "\n",
        "    return loss, ps, hs, xs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zm6qwNiqWdMe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Backward propagation"
      ]
    },
    {
      "metadata": {
        "id": "81qBiz_xWenI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def backprop(ps, inputs, hs, xs):\n",
        "\n",
        "    dWxh, dWhh, dWhy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy) # make all zero matrices.\n",
        "    dbh, dby = np.zeros_like(b_h), np.zeros_like(b_y)\n",
        "    dhnext = np.zeros_like(hs[0]) # (hidden_size,1) \n",
        "\n",
        "    # reversed\n",
        "    for t in reversed(range(len(inputs))):\n",
        "        dy = np.copy(ps[t]) # shape (num_chars,1).  \"dy\" means \"dloss/dy\"\n",
        "        dy[targets[t]] -= 1 # backprop into y. After taking the soft max in the input vector, subtract 1 from the value of the element corresponding to the correct label.\n",
        "        dWhy += np.dot(dy, hs[t].T)\n",
        "        dby += dy \n",
        "        dh = np.dot(W_hy.T, dy) + dhnext # backprop into h. \n",
        "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity #tanh'(x) = 1-tanh^2(x)\n",
        "        dbh += dhraw\n",
        "        dWxh += np.dot(dhraw, xs[t].T)\n",
        "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
        "        dhnext = np.dot(W_hh.T, dhraw)\n",
        "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]: \n",
        "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients.  \n",
        "    \n",
        "    return dWxh, dWhh, dWhy, dbh, dby"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r8sBvcdbWfhi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Training"
      ]
    },
    {
      "metadata": {
        "id": "iA4RM70LWgO_",
        "colab_type": "code",
        "outputId": "7ac6f924-ac92-4340-b694-4ad5f8cce731",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "data_pointer = 0\n",
        "\n",
        "# memory variables for Adagrad\n",
        "mWxh, mWhh, mWhy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy)\n",
        "mbh, mby = np.zeros_like(b_h), np.zeros_like(b_y) \n",
        "\n",
        "for i in range(iteration):\n",
        "    h_prev = np.zeros((hidden_size,1)) # reset RNN memory\n",
        "    data_pointer = 0 # go from start of data\n",
        "    \n",
        "    for b in range(batch_size):\n",
        "        \n",
        "        inputs = [char_to_int[ch] for ch in article_text[data_pointer:data_pointer+sequence_length]]\n",
        "        targets = [char_to_int[ch] for ch in article_text[data_pointer+1:data_pointer+sequence_length+1]] # t+1        \n",
        "            \n",
        "        if (data_pointer+sequence_length+1 >= len(article_text) and b == batch_size-1): # processing of the last part of the input data. \n",
        "#             targets.append(char_to_int[txt_data[0]])   # When the data doesn't fit, add the first char to the back.\n",
        "            targets.append(char_to_int[\" \"])   # When the data doesn't fit, add space(\" \") to the back.\n",
        "\n",
        "\n",
        "        # forward\n",
        "        loss, ps, hs, xs = forwardprop(inputs, targets, h_prev)\n",
        "#         print(loss)\n",
        "    \n",
        "        # backward\n",
        "        dWxh, dWhh, dWhy, dbh, dby = backprop(ps, inputs, hs, xs) \n",
        "        \n",
        "        \n",
        "    # perform parameter update with Adagrad\n",
        "        for param, dparam, mem in zip([W_xh, W_hh, W_hy, b_h, b_y], \n",
        "                                    [dWxh, dWhh, dWhy, dbh, dby], \n",
        "                                    [mWxh, mWhh, mWhy, mbh, mby]):\n",
        "            mem += dparam * dparam # elementwise\n",
        "            param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update      \n",
        "    \n",
        "        data_pointer += sequence_length # move data pointer\n",
        "        \n",
        "    if i % 100 == 0:\n",
        "        print ('iter %d, loss: %f' % (i, loss)) # print progress"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter 0, loss: 352.259156\n",
            "iter 100, loss: 48.365061\n",
            "iter 200, loss: 43.312493\n",
            "iter 300, loss: 39.026001\n",
            "iter 400, loss: 33.363448\n",
            "iter 500, loss: 28.105412\n",
            "iter 600, loss: 21.002214\n",
            "iter 700, loss: 13.006238\n",
            "iter 800, loss: 9.396751\n",
            "iter 900, loss: 11.171581\n",
            "CPU times: user 10min 29s, sys: 6min 17s, total: 16min 46s\n",
            "Wall time: 8min 28s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tjh8Ip68WgYV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Prediction"
      ]
    },
    {
      "metadata": {
        "id": "HDCxDNPG68Hx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predict(test_char, length):\n",
        "    x = np.zeros((num_chars, 1)) \n",
        "    x[char_to_int[test_char]] = 1\n",
        "    ixes = []\n",
        "    h = np.zeros((hidden_size,1))\n",
        "\n",
        "    for t in range(length):\n",
        "        h = np.tanh(np.dot(W_xh, x) + np.dot(W_hh, h) + b_h) \n",
        "        y = np.dot(W_hy, h) + b_y\n",
        "        p = np.exp(y) / np.sum(np.exp(y)) \n",
        "        ix = np.random.choice(range(num_chars), p=p.ravel()) # ravel -> rank0\n",
        "        # \"ix\" is a list of indexes selected according to the soft max probability.\n",
        "        x = np.zeros((num_chars, 1)) # init\n",
        "        x[ix] = 1 \n",
        "        ixes.append(ix) # list\n",
        "    txt = test_char + ''.join(int_to_char[i] for i in ixes)\n",
        "    print ('----\\n %s \\n----' % (txt, ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nGVhl-Gxh6N6",
        "colab_type": "code",
        "outputId": "d28b62bd-6c59-43cc-da27-1a6c03c8d724",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "predict('c', 50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----\n",
            " c.drddd , sf \"JeIdrJeon froi wi vial wi r,, Prdy, H \n",
            "----\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xPsz-oefL1kP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Well... that's *vaguely* language-looking. Can you do better?"
      ]
    },
    {
      "metadata": {
        "id": "0lfZdD_cp1t5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Assignment\n",
        "\n",
        "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
        "\n",
        "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
        "\n",
        "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
        "\n",
        "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
        "\n",
        "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
        "\n",
        "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
      ]
    },
    {
      "metadata": {
        "id": "Ltj1je1fp5rO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import numpy as np\n",
        "r = requests.get('https://www.gutenberg.org/files/100/100-0.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uxlmZvCOaqCf",
        "colab_type": "code",
        "outputId": "39234bff-4eb6-4479-8e71-f18d18c4b83a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "cell_type": "code",
      "source": [
        "# After some Project Gutenberg preamble, the works begin around character 1500\n",
        "complete_works = r.text[3000:]\n",
        "# First I'll use a smaller subset of the complete works\n",
        "article_text = complete_works[:5002]\n",
        "article_text"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'    1\\r\\n\\r\\nFrom fairest creatures we desire increase,\\r\\nThat thereby beauty’s rose might never die,\\r\\nBut as the riper should by time decease,\\r\\nHis tender heir might bear his memory:\\r\\nBut thou contracted to thine own bright eyes,\\r\\nFeed’st thy light’s flame with self-substantial fuel,\\r\\nMaking a famine where abundance lies,\\r\\nThy self thy foe, to thy sweet self too cruel:\\r\\nThou that art now the world’s fresh ornament,\\r\\nAnd only herald to the gaudy spring,\\r\\nWithin thine own bud buriest thy content,\\r\\nAnd, tender churl, mak’st waste in niggarding:\\r\\n  Pity the world, or else this glutton be,\\r\\n  To eat the world’s due, by the grave and thee.\\r\\n\\r\\n\\r\\n                    2\\r\\n\\r\\nWhen forty winters shall besiege thy brow,\\r\\nAnd dig deep trenches in thy beauty’s field,\\r\\nThy youth’s proud livery so gazed on now,\\r\\nWill be a tattered weed of small worth held:\\r\\nThen being asked, where all thy beauty lies,\\r\\nWhere all the treasure of thy lusty days;\\r\\nTo say, within thine own deep sunken eyes,\\r\\nWere an all-eating shame, and thriftless praise.\\r\\nHow much more praise deserv’d thy beauty’s use,\\r\\nIf thou couldst answer ‘This fair child of mine\\r\\nShall sum my count, and make my old excuse,’\\r\\nProving his beauty by succession thine.\\r\\n  This were to be new made when thou art old,\\r\\n  And see thy blood warm when thou feel’st it cold.\\r\\n\\r\\n\\r\\n                    3\\r\\n\\r\\nLook in thy glass and tell the face thou viewest,\\r\\nNow is the time that face should form another,\\r\\nWhose fresh repair if now thou not renewest,\\r\\nThou dost beguile the world, unbless some mother.\\r\\nFor where is she so fair whose uneared womb\\r\\nDisdains the tillage of thy husbandry?\\r\\nOr who is he so fond will be the tomb\\r\\nOf his self-love to stop posterity?\\r\\nThou art thy mother’s glass and she in thee\\r\\nCalls back the lovely April of her prime,\\r\\nSo thou through windows of thine age shalt see,\\r\\nDespite of wrinkles this thy golden time.\\r\\n  But if thou live remembered not to be,\\r\\n  Die single and thine image dies with thee.\\r\\n\\r\\n\\r\\n                    4\\r\\n\\r\\nUnthrifty loveliness why dost thou spend,\\r\\nUpon thy self thy beauty’s legacy?\\r\\nNature’s bequest gives nothing but doth lend,\\r\\nAnd being frank she lends to those are free:\\r\\nThen beauteous niggard why dost thou abuse,\\r\\nThe bounteous largess given thee to give?\\r\\nProfitless usurer why dost thou use\\r\\nSo great a sum of sums yet canst not live?\\r\\nFor having traffic with thy self alone,\\r\\nThou of thy self thy sweet self dost deceive,\\r\\nThen how when nature calls thee to be gone,\\r\\nWhat acceptable audit canst thou leave?\\r\\n  Thy unused beauty must be tombed with thee,\\r\\n  Which used lives th’ executor to be.\\r\\n\\r\\n\\r\\n                    5\\r\\n\\r\\nThose hours that with gentle work did frame\\r\\nThe lovely gaze where every eye doth dwell\\r\\nWill play the tyrants to the very same,\\r\\nAnd that unfair which fairly doth excel:\\r\\nFor never-resting time leads summer on\\r\\nTo hideous winter and confounds him there,\\r\\nSap checked with frost and lusty leaves quite gone,\\r\\nBeauty o’er-snowed and bareness every where:\\r\\nThen were not summer’s distillation left\\r\\nA liquid prisoner pent in walls of glass,\\r\\nBeauty’s effect with beauty were bereft,\\r\\nNor it nor no remembrance what it was.\\r\\n  But flowers distilled though they with winter meet,\\r\\n  Leese but their show, their substance still lives sweet.\\r\\n\\r\\n\\r\\n                    6\\r\\n\\r\\nThen let not winter’s ragged hand deface,\\r\\nIn thee thy summer ere thou be distilled:\\r\\nMake sweet some vial; treasure thou some place,\\r\\nWith beauty’s treasure ere it be self-killed:\\r\\nThat use is not forbidden usury,\\r\\nWhich happies those that pay the willing loan;\\r\\nThat’s for thy self to breed another thee,\\r\\nOr ten times happier be it ten for one,\\r\\nTen times thy self were happier than thou art,\\r\\nIf ten of thine ten times refigured thee:\\r\\nThen what could death do if thou shouldst depart,\\r\\nLeaving thee living in posterity?\\r\\n  Be not self-willed for thou art much too fair,\\r\\n  To be death’s conquest and make worms thine heir.\\r\\n\\r\\n\\r\\n                    7\\r\\n\\r\\nLo in the orient when the gracious light\\r\\nLifts up his burning head, each under eye\\r\\nDoth homage to his new-appearing sight,\\r\\nServing with looks his sacred majesty,\\r\\nAnd having climbed the steep-up heavenly hill,\\r\\nResembling strong youth in his middle age,\\r\\nYet mortal looks adore his beauty still,\\r\\nAttending on his golden pilgrimage:\\r\\nBut when from highmost pitch with weary car,\\r\\nLike feeble age he reeleth from the day,\\r\\nThe eyes (fore duteous) now converted are\\r\\nFrom his low tract and look another way:\\r\\n  So thou, thy self out-going in thy noon:\\r\\n  Unlooked on diest unless thou get a son.\\r\\n\\r\\n\\r\\n                    8\\r\\n\\r\\nMusic to hear, why hear’st thou music sadly?\\r\\nSweets with sweets war not, joy delights in joy:\\r\\nWhy lov’st thou that which thou receiv’st not gladly,\\r\\nOr else receiv’st with pleasure thine annoy?\\r\\nIf the true concord of well-tuned sounds,\\r\\nBy unions married do offend thine ear,\\r\\nThey do but sweetly chide thee, who confounds\\r\\nIn singleness the parts that thou shouldst bear:\\r\\nMark how one string sweet husband to another,'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "metadata": {
        "id": "us4z2bI3arWD",
        "colab_type": "code",
        "outputId": "65afbf8e-d1f6-4985-c5b7-ea3f14e3d446",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "cell_type": "code",
      "source": [
        "chars = list(set(article_text)) # split and remove duplicate characters. convert to list.\n",
        "num_chars = len(chars) # the number of unique characters\n",
        "txt_data_size = len(article_text)\n",
        "\n",
        "print(\"unique characters : \", num_chars)\n",
        "print(\"txt_data_size : \", txt_data_size)\n",
        "print('All characters: \\n', [x for x in chars])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unique characters :  65\n",
            "txt_data_size :  5002\n",
            "All characters: \n",
            " ['h', 'T', 'I', 'j', 'y', '-', ')', 'C', 'a', 'd', '1', 'D', '.', 'M', 't', 'p', 'm', 'Y', 's', 'q', 'S', 'k', 'i', 'B', 'z', '‘', '5', '(', 'n', '8', ';', 'N', '7', 'e', 'R', 'x', 'A', ':', '6', 'O', '4', 'U', '3', '2', 'P', 'f', 'v', 'l', '?', 'W', 'F', 'H', 'L', '\\r', 'u', 'w', '’', 'o', ',', 'g', '\\n', 'c', ' ', 'b', 'r']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RsrmucouemyN",
        "colab_type": "code",
        "outputId": "8bdb7dfc-2a73-4115-c35a-b568205e63a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "cell_type": "code",
      "source": [
        "# integer-encode\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars)) # \"enumerate\" retruns index and value. Convert it to dictionary\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "print(char_to_int)\n",
        "print(\"----------------------------------------------------\")\n",
        "print(int_to_char)\n",
        "print(\"----------------------------------------------------\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'h': 0, 'T': 1, 'I': 2, 'j': 3, 'y': 4, '-': 5, ')': 6, 'C': 7, 'a': 8, 'd': 9, '1': 10, 'D': 11, '.': 12, 'M': 13, 't': 14, 'p': 15, 'm': 16, 'Y': 17, 's': 18, 'q': 19, 'S': 20, 'k': 21, 'i': 22, 'B': 23, 'z': 24, '‘': 25, '5': 26, '(': 27, 'n': 28, '8': 29, ';': 30, 'N': 31, '7': 32, 'e': 33, 'R': 34, 'x': 35, 'A': 36, ':': 37, '6': 38, 'O': 39, '4': 40, 'U': 41, '3': 42, '2': 43, 'P': 44, 'f': 45, 'v': 46, 'l': 47, '?': 48, 'W': 49, 'F': 50, 'H': 51, 'L': 52, '\\r': 53, 'u': 54, 'w': 55, '’': 56, 'o': 57, ',': 58, 'g': 59, '\\n': 60, 'c': 61, ' ': 62, 'b': 63, 'r': 64}\n",
            "----------------------------------------------------\n",
            "{0: 'h', 1: 'T', 2: 'I', 3: 'j', 4: 'y', 5: '-', 6: ')', 7: 'C', 8: 'a', 9: 'd', 10: '1', 11: 'D', 12: '.', 13: 'M', 14: 't', 15: 'p', 16: 'm', 17: 'Y', 18: 's', 19: 'q', 20: 'S', 21: 'k', 22: 'i', 23: 'B', 24: 'z', 25: '‘', 26: '5', 27: '(', 28: 'n', 29: '8', 30: ';', 31: 'N', 32: '7', 33: 'e', 34: 'R', 35: 'x', 36: 'A', 37: ':', 38: '6', 39: 'O', 40: '4', 41: 'U', 42: '3', 43: '2', 44: 'P', 45: 'f', 46: 'v', 47: 'l', 48: '?', 49: 'W', 50: 'F', 51: 'H', 52: 'L', 53: '\\r', 54: 'u', 55: 'w', 56: '’', 57: 'o', 58: ',', 59: 'g', 60: '\\n', 61: 'c', 62: ' ', 63: 'b', 64: 'r'}\n",
            "----------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "djtND_oHg7ep",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "\n",
        "iteration = 1000\n",
        "sequence_length = 30\n",
        "batch_size = round((txt_data_size /sequence_length)+0.5) # = math.ceil\n",
        "hidden_size = 100  # size of hidden layer of neurons.  \n",
        "learning_rate = 1e-1\n",
        "\n",
        "\n",
        "# model parameters\n",
        "\n",
        "W_xh = np.random.randn(hidden_size, num_chars)*0.01     # weight input -> hidden. \n",
        "W_hh = np.random.randn(hidden_size, hidden_size)*0.01   # weight hidden -> hidden\n",
        "W_hy = np.random.randn(num_chars, hidden_size)*0.01     # weight hidden -> output\n",
        "\n",
        "b_h = np.zeros((hidden_size, 1)) # hidden bias\n",
        "b_y = np.zeros((num_chars, 1)) # output bias\n",
        "\n",
        "h_prev = np.zeros((hidden_size,1)) # h_(t-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0UitHj-ehCdG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def forwardprop(inputs, targets, h_prev):\n",
        "        \n",
        "    # Since the RNN receives the sequence, the weights are not updated during one sequence.\n",
        "    xs, hs, ys, ps = {}, {}, {}, {} # dictionary\n",
        "    hs[-1] = np.copy(h_prev) # Copy previous hidden state vector to -1 key value.\n",
        "    loss = 0 # loss initialization\n",
        "    \n",
        "    for t in range(len(inputs)): # t is a \"time step\" and is used as a key(dic).  \n",
        "        \n",
        "        xs[t] = np.zeros((num_chars,1)) \n",
        "        xs[t][inputs[t]] = 1\n",
        "        hs[t] = np.tanh(np.dot(W_xh, xs[t]) + np.dot(W_hh, hs[t-1]) + b_h) # hidden state. \n",
        "        ys[t] = np.dot(W_hy, hs[t]) + b_y # unnormalized log probabilities for next chars\n",
        "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars. \n",
        "        # Softmax. -> The sum of probabilities is 1 even without the exp() function, but all of the elements are positive through the exp() function.\n",
        " \n",
        "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss). Efficient and simple code\n",
        "\n",
        "#         y_class = np.zeros((num_chars, 1)) \n",
        "#         y_class[targets[t]] =1\n",
        "#         loss += np.sum(y_class*(-np.log(ps[t]))) # softmax (cross-entropy loss)        \n",
        "\n",
        "    return loss, ps, hs, xs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AjW3K06_hJJQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def backprop(ps, inputs, hs, xs):\n",
        "\n",
        "    dWxh, dWhh, dWhy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy) # make all zero matrices.\n",
        "    dbh, dby = np.zeros_like(b_h), np.zeros_like(b_y)\n",
        "    dhnext = np.zeros_like(hs[0]) # (hidden_size,1) \n",
        "\n",
        "    # reversed\n",
        "    for t in reversed(range(len(inputs))):\n",
        "        dy = np.copy(ps[t]) # shape (num_chars,1).  \"dy\" means \"dloss/dy\"\n",
        "        dy[targets[t]] -= 1 # backprop into y. After taking the soft max in the input vector, subtract 1 from the value of the element corresponding to the correct label.\n",
        "        dWhy += np.dot(dy, hs[t].T)\n",
        "        dby += dy \n",
        "        dh = np.dot(W_hy.T, dy) + dhnext # backprop into h. \n",
        "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity #tanh'(x) = 1-tanh^2(x)\n",
        "        dbh += dhraw\n",
        "        dWxh += np.dot(dhraw, xs[t].T)\n",
        "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
        "        dhnext = np.dot(W_hh.T, dhraw)\n",
        "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]: \n",
        "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients.  \n",
        "    \n",
        "    return dWxh, dWhh, dWhy, dbh, dby"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_8kk7zbPhPbO",
        "colab_type": "code",
        "outputId": "f455fe47-1b3a-47e5-d376-87e0b00fb4e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1870
        }
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "data_pointer = 0\n",
        "\n",
        "# memory variables for Adagrad\n",
        "mWxh, mWhh, mWhy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy)\n",
        "mbh, mby = np.zeros_like(b_h), np.zeros_like(b_y) \n",
        "\n",
        "for i in range(iteration):\n",
        "    h_prev = np.zeros((hidden_size,1)) # reset RNN memory\n",
        "    data_pointer = 0 # go from start of data\n",
        "    \n",
        "    for b in range(batch_size):\n",
        "        \n",
        "        inputs = [char_to_int[ch] for ch in article_text[data_pointer:data_pointer+sequence_length]]\n",
        "        targets = [char_to_int[ch] for ch in article_text[data_pointer+1:data_pointer+sequence_length+1]] # t+1        \n",
        "            \n",
        "        if (data_pointer+sequence_length+1 >= len(article_text) and b == batch_size-1): # processing of the last part of the input data. \n",
        "#             targets.append(char_to_int[txt_data[0]])   # When the data doesn't fit, add the first char to the back.\n",
        "            targets.append(char_to_int[\" \"])   # When the data doesn't fit, add space(\" \") to the back.\n",
        "\n",
        "\n",
        "        # forward\n",
        "        loss, ps, hs, xs = forwardprop(inputs, targets, h_prev)\n",
        "#         print(loss)\n",
        "    \n",
        "        # backward\n",
        "        dWxh, dWhh, dWhy, dbh, dby = backprop(ps, inputs, hs, xs) \n",
        "        \n",
        "        \n",
        "    # perform parameter update with Adagrad\n",
        "        for param, dparam, mem in zip([W_xh, W_hh, W_hy, b_h, b_y], \n",
        "                                    [dWxh, dWhh, dWhy, dbh, dby], \n",
        "                                    [mWxh, mWhh, mWhy, mbh, mby]):\n",
        "            mem += dparam * dparam # elementwise\n",
        "            param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update      \n",
        "    \n",
        "        data_pointer += sequence_length # move data pointer\n",
        "        \n",
        "    if i % 10 == 0:\n",
        "        print ('iter %d, loss: %f' % (i, loss)) # print progress"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter 0, loss: 74.492733\n",
            "iter 10, loss: 50.946351\n",
            "iter 20, loss: 49.838280\n",
            "iter 30, loss: 48.899516\n",
            "iter 40, loss: 46.798355\n",
            "iter 50, loss: 44.879880\n",
            "iter 60, loss: 43.656898\n",
            "iter 70, loss: 41.486141\n",
            "iter 80, loss: 40.303347\n",
            "iter 90, loss: 39.007932\n",
            "iter 100, loss: 36.628398\n",
            "iter 110, loss: 34.933445\n",
            "iter 120, loss: 33.316448\n",
            "iter 130, loss: 32.117863\n",
            "iter 140, loss: 30.893944\n",
            "iter 150, loss: 29.020065\n",
            "iter 160, loss: 29.227221\n",
            "iter 170, loss: 30.677962\n",
            "iter 180, loss: 28.219979\n",
            "iter 190, loss: 27.994171\n",
            "iter 200, loss: 26.970684\n",
            "iter 210, loss: 28.137430\n",
            "iter 220, loss: 29.149861\n",
            "iter 230, loss: 27.336565\n",
            "iter 240, loss: 27.243725\n",
            "iter 250, loss: 28.876122\n",
            "iter 260, loss: 26.108546\n",
            "iter 270, loss: 27.006014\n",
            "iter 280, loss: 28.527787\n",
            "iter 290, loss: 26.292602\n",
            "iter 300, loss: 28.571185\n",
            "iter 310, loss: 26.737177\n",
            "iter 320, loss: 25.924647\n",
            "iter 330, loss: 25.792840\n",
            "iter 340, loss: 26.287769\n",
            "iter 350, loss: 26.636698\n",
            "iter 360, loss: 28.248297\n",
            "iter 370, loss: 24.402417\n",
            "iter 380, loss: 24.826754\n",
            "iter 390, loss: 24.881701\n",
            "iter 400, loss: 23.730005\n",
            "iter 410, loss: 24.044497\n",
            "iter 420, loss: 22.506448\n",
            "iter 430, loss: 23.320152\n",
            "iter 440, loss: 22.837706\n",
            "iter 450, loss: 22.679964\n",
            "iter 460, loss: 22.930072\n",
            "iter 470, loss: 21.419656\n",
            "iter 480, loss: 21.614832\n",
            "iter 490, loss: 21.762670\n",
            "iter 500, loss: 22.394776\n",
            "iter 510, loss: 21.043627\n",
            "iter 520, loss: 21.040438\n",
            "iter 530, loss: 20.667121\n",
            "iter 540, loss: 21.934300\n",
            "iter 550, loss: 21.579805\n",
            "iter 560, loss: 21.261932\n",
            "iter 570, loss: 20.274287\n",
            "iter 580, loss: 20.199463\n",
            "iter 590, loss: 20.362432\n",
            "iter 600, loss: 21.168069\n",
            "iter 610, loss: 19.608435\n",
            "iter 620, loss: 19.699882\n",
            "iter 630, loss: 18.883415\n",
            "iter 640, loss: 19.057677\n",
            "iter 650, loss: 18.937508\n",
            "iter 660, loss: 18.318092\n",
            "iter 670, loss: 18.684037\n",
            "iter 680, loss: 18.556454\n",
            "iter 690, loss: 17.886292\n",
            "iter 700, loss: 17.975337\n",
            "iter 710, loss: 17.095061\n",
            "iter 720, loss: 16.835954\n",
            "iter 730, loss: 17.088904\n",
            "iter 740, loss: 17.415120\n",
            "iter 750, loss: 17.994176\n",
            "iter 760, loss: 16.680258\n",
            "iter 770, loss: 16.115673\n",
            "iter 780, loss: 16.795009\n",
            "iter 790, loss: 16.076110\n",
            "iter 800, loss: 17.252829\n",
            "iter 810, loss: 16.169412\n",
            "iter 820, loss: 16.483235\n",
            "iter 830, loss: 15.826084\n",
            "iter 840, loss: 16.507327\n",
            "iter 850, loss: 16.047187\n",
            "iter 860, loss: 15.252352\n",
            "iter 870, loss: 16.146171\n",
            "iter 880, loss: 15.849550\n",
            "iter 890, loss: 15.639069\n",
            "iter 900, loss: 16.207782\n",
            "iter 910, loss: 14.767573\n",
            "iter 920, loss: 15.811447\n",
            "iter 930, loss: 15.477362\n",
            "iter 940, loss: 14.137331\n",
            "iter 950, loss: 15.287364\n",
            "iter 960, loss: 14.389838\n",
            "iter 970, loss: 14.770606\n",
            "iter 980, loss: 14.532606\n",
            "iter 990, loss: 14.738387\n",
            "CPU times: user 22min 47s, sys: 14min 18s, total: 37min 5s\n",
            "Wall time: 18min 47s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VogNTFAjl0D3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predict(test_char, length):\n",
        "    x = np.zeros((num_chars, 1)) \n",
        "    x[char_to_int[test_char]] = 1\n",
        "    ixes = []\n",
        "    h = np.zeros((hidden_size,1))\n",
        "\n",
        "    for t in range(length):\n",
        "        h = np.tanh(np.dot(W_xh, x) + np.dot(W_hh, h) + b_h) \n",
        "        y = np.dot(W_hy, h) + b_y\n",
        "        p = np.exp(y) / np.sum(np.exp(y)) \n",
        "        ix = np.random.choice(range(num_chars), p=p.ravel()) # ravel -> rank0\n",
        "        # \"ix\" is a list of indexes selected according to the soft max probability.\n",
        "        x = np.zeros((num_chars, 1)) # init\n",
        "        x[ix] = 1 \n",
        "        ixes.append(ix) # list\n",
        "    txt = test_char + ''.join(int_to_char[i] for i in ixes)\n",
        "    print ('----\\n %s \\n----' % (txt, ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nPLbqB9VmAy0",
        "colab_type": "code",
        "outputId": "f9c5e8be-e263-4239-ca25-bf6f535907b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "cell_type": "code",
      "source": [
        "predict('C', 1000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----\n",
            " Cabe, doo tere. Buinctring sumirl agd do glorl ande hid try bur ablind on fitrent yad heove, Thadieleng, Ff thou cone myer secemselande his wh frelf thou be douty this galded weavingwerumusseless avithy beauty. Sweet anchise the hes ont ans thee. 2 Whing owes gally? Leattle to the calf Ust cotherivlencard frace shecy of sme ine look anchieg willeds tor hend, Or here: But nongh chee: Then beingagghen the tipin thy beauty’s hes noo gutr fliflams kim If muthich mapr’st telp ato ty dellenguer fails thise his nom foendang outt thow thou ust ond on mumoms Atre formbres shise ghee. 4 Uxvere ande hou ty deets shese outver’s nowe sees and look in thy swert oner-buthere hiner in self ofot lie fleressmy, thou rive? Fo the, Weblene The ereasunos ongrith will-rise fove galggabuney deat’s dis wind thin faap-iotred ongring hea tst gie, Thy loon: That why ofespran the treaseak and suster has ond the vertce feell the leld-agliag suml, Witheded when vely? Or thahill ont goodiee thine ow in joy with so th \n",
            "----\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zE4a4O7Bp5x1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Resources and Stretch Goals"
      ]
    },
    {
      "metadata": {
        "id": "uT3UV3gap9H6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Stretch goals:\n",
        "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
        "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
        "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
        "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
        "- Run on bigger, better data\n",
        "\n",
        "## Resources:\n",
        "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
        "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
        "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
        "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
        "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
      ]
    }
  ]
}