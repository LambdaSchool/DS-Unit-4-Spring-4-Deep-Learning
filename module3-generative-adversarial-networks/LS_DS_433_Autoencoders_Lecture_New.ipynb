{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 3 Lesson 3*\n",
    "# Autoencoders\n",
    "\n",
    "__Problem:__ Is it possible to automatically represent an image as a fixed-sized vector even if it isnâ€™t labeled?\n",
    "\n",
    "__Solution:__ Use an autoencoder\n",
    "\n",
    "Why do we need to represent an image as a fixed-sized vector do you ask? \n",
    "\n",
    "* __Information Retrieval__\n",
    "    - [Reverse Image Search](https://en.wikipedia.org/wiki/Reverse_image_search)\n",
    "    - [Recommendation Systems - Content Based Filtering](https://en.wikipedia.org/wiki/Recommender_system#Content-based_filtering)\n",
    "* __Dimensionality Reduction__\n",
    "    - [Feature Extraction](https://www.kaggle.com/c/vsb-power-line-fault-detection/discussion/78285)\n",
    "    - [Manifold Learning](https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction)\n",
    "\n",
    "We've already seen *representation learning* when we talked about word embedding modelings during our NLP week. Today we're going to achieve a similiar goal on images using *autoencoders*. An autoencoder is a neural network that is trained to attempt to copy its input to its output. Usually they are restricted in ways that allow them to copy only approximately. The model often learns useful properties of the data, because it is forced to prioritize which aspecs of the input should be copied. The properties of autoencoders have made them an important part of modern generative modeling approaches. Consider autoencoders a special case of feed-forward networks (the kind we've been studying); backpropagation and gradient descent still work. \n",
    "\n",
    "## Learning Objectives\n",
    "*At the end of the lecture you should be to*:\n",
    "* <a href=\"#p1\">Part 1</a>: Describe the componenets of an autoencoder\n",
    "* <a href=\"#p2\">Part 2</a>: Train an autoencoder\n",
    "* <a href=\"#p3\">Part 3</a>: Apply an autoenocder to a basic information retrieval problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"p1\"></a>\n",
    "\n",
    "## Autoencoder Architecture\n",
    "\n",
    "The *encoder* compresses the input data and the *decoder* does the reverse to produce the uncompressed version of the data to create a reconstruction of the input as accurately as possible:\n",
    "\n",
    "<img src='https://miro.medium.com/max/1400/1*44eDEuZBEsmG_TCAKRI3Kw@2x.png' width=800/>\n",
    "\n",
    "The learning process gis described simply as minimizing a loss function: \n",
    "$ L(x, g(f(x))) $\n",
    "\n",
    "- $L$ is a loss function penalizing $g(f(x))$ for being dissimiliar from $x$ (such as mean squared error)\n",
    "- $f$ is the encoder function\n",
    "- $g$ is the decoder function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"p2\"></a>\n",
    "## Training an Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compress input to a tiny vector 128x128, 4x4\n",
    "#loss function calculated by original inputs as true output\n",
    "#use original image itself as label\n",
    "#decoder reconstitutes compression to generate output\n",
    "\n",
    "#learning process minimizing our loss - mse, \n",
    "#loss of autoencoder is inputs x compared to encoder, and decoder of x\n",
    "\n",
    "#decoder of encoder of x\n",
    "#autoencoder architecture - using input as label, design of network is like hourglass, compressing everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Reshape, Concatenate, Flatten, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
    "from tensorflow.keras.losses import binary_crossentropy, kullback_leibler_divergence\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import get_file\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from struct import unpack\n",
    "import json\n",
    "import glob\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from io import BytesIO\n",
    "import PIL\n",
    "from PIL import ImageDraw\n",
    "\n",
    "from IPython.display import clear_output, Image, display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/quickdraw_dataset/full/binary/cat.bin\n",
      "19578880/19571324 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "BASE_PATH = 'https://storage.googleapis.com/quickdraw_dataset/full/binary/'\n",
    "path = get_file('cat', BASE_PATH + 'cat.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A drawing is a list of strokes, each made up of a series of x and y coordinates. The x and y coordinates are stored separately, so we need to zip them into a list to feed into the ImageDraw object we just created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lilyx\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((104721, 32, 32, 1), (18481, 32, 32, 1))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_icons(path, train_size=0.85):#zip into an image, easier to work with\n",
    "    x = []\n",
    "    with open(path, 'rb') as f:#open path\n",
    "        while True:\n",
    "            img = PIL.Image.new('L', (32, 32), 'white')#create image representation 32x32\n",
    "            draw = ImageDraw.Draw(img)#on image, draw strokes\n",
    "            header = f.read(15)\n",
    "            if len(header) != 15:#unique to dataset\n",
    "                break\n",
    "            strokes, = unpack('H', f.read(2))\n",
    "            for i in range(strokes):\n",
    "                n_points, = unpack('H', f.read(2))#unpack strokes\n",
    "                fmt = str(n_points) + 'B'\n",
    "                read_scaled = lambda: (p // 8 for \n",
    "                                       p in unpack(fmt, f.read(n_points)))\n",
    "                points = [*zip(read_scaled(), read_scaled())]\n",
    "                draw.line(points, fill=0, width=2)\n",
    "            img = img_to_array(img)\n",
    "            x.append(img)#add to image and save to x, which is a list\n",
    "    x = np.asarray(x) / 255\n",
    "    return train_test_split(x, train_size=train_size)\n",
    "\n",
    "\n",
    "x_train, x_test = load_icons(path)\n",
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        ...,\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        ...,\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        ...,\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        ...,\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]#most of image are white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-00ad597f417c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mautoencoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_autoencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mautoencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mcreate_autoencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-00ad597f417c>\u001b[0m in \u001b[0;36mcreate_autoencoder\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcreate_autoencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mautoencoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_autoencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mautoencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcreate_autoencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "... last 1 frames repeated, from the frame below ...\n",
      "\u001b[1;32m<ipython-input-12-00ad597f417c>\u001b[0m in \u001b[0;36mcreate_autoencoder\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcreate_autoencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mautoencoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_autoencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mautoencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcreate_autoencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRecursionError\u001b[0m: maximum recursion depth exceeded"
     ]
    }
   ],
   "source": [
    "def create_autoencoder():\n",
    "    autoencoder = create_autoencoder()\n",
    "    autoencoder.summary()\n",
    "create_autoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_autoencoder():#specify model as function\n",
    "    input_img = Input(shape=(32, 32, 1))#1 represents degree of whiteness\n",
    "\n",
    "    channels = 2#represent dimensions of the data x,y\n",
    "    x = input_img\n",
    "    #encoder\n",
    "    for i in range(4):#looking at image from each of the perspectives\n",
    "#         [[1,1,1,1],\n",
    "#          [1,1,1,1],\n",
    "#          [1,1,1,1],\n",
    "#          [1,1,1,1]]\n",
    "        channels *= 2\n",
    "        left = Conv2D(channels, (3, 3), activation='relu', padding='same')(x)#passing x as an input of convolution layer\n",
    "        right = Conv2D(channels, (2, 2), activation='relu', padding='same')(x)#taking input passing it down\n",
    "        conc = Concatenate()([left, right])\n",
    "        x = MaxPooling2D((2, 2), padding='same')(conc)#pooling layer, taking an average, dimensionality reduction technique\n",
    "        #max takes the maxiumum from convolutional pool\n",
    "\n",
    "    x = Dense(channels)(x)#output we'd want from our model, to complete training we need to reconstitute\n",
    "    #decoder\n",
    "    for i in range(4):\n",
    "        x = Conv2D(channels, (3, 3), activation='relu', padding='same')(x)#we upsample\n",
    "        x = UpSampling2D((2, 2))(x)\n",
    "        channels //= 2\n",
    "    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "    autoencoder = Model(input_img, decoded)\n",
    "    autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')#algorithmn for updating weights\n",
    "    return autoencoder\n",
    "\n",
    "autoencoder = create_autoencoder()\n",
    "autoencoder.summary()\n",
    "\n",
    "#encode\n",
    "    #we get 32 sets of 4x4 weights\n",
    "    #take image, slice across 4 perspectives\n",
    "    #convolve that\n",
    "    #do on left and right sides\n",
    "    #concatenate togeter with 32x32x8\n",
    "    #pull information together\n",
    "    #do again convolve 16x16x8\n",
    "    #convolve\n",
    "    #condense\n",
    "    #repeat\n",
    "    #get 2x2x64, get 2x2x32\n",
    "#decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=100,\n",
    "                batch_size=128,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test),\n",
    "                callbacks=[TensorBoard(log_dir='/tmp/autoencoder')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = 25\n",
    "idx = np.random.randint(x_test.shape[0], size=cols)\n",
    "sample = x_test[idx]\n",
    "decoded_imgs = autoencoder.predict(sample)\n",
    "decoded_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict sample and decode\n",
    "def decode_img(tile, factor=1.0):\n",
    "    tile = tile.reshape(tile.shape[:-1])\n",
    "    tile = np.clip(tile * 255, 0, 255)\n",
    "    return PIL.Image.fromarray(tile)\n",
    "    \n",
    "\n",
    "overview = PIL.Image.new('RGB', (cols * 32, 64 + 20), (128, 128, 128))#create palette shape\n",
    "for idx in range(cols):#iterate from our dataset\n",
    "    overview.paste(decode_img(sample[idx]), (idx * 32, 5))\n",
    "    overview.paste(decode_img(decoded_imgs[idx]), (idx * 32, 42))\n",
    "f = BytesIO()\n",
    "overview.save(f, 'png')#pull original image, and image decoded\n",
    "display(Image(data=f.getvalue()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#improvement: sample from true distribution\n",
    "#take average from existing images\n",
    "#create fixed-vector representation with 64 values.\n",
    "\n",
    "#z log represents our log variance of cat images\n",
    "\n",
    "\n",
    "#reverse image search\n",
    "#content based recommendation systems like pandora - extract features from music"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"p3\"></a>\n",
    "## Part 3: Information Retrieval with Autoencoders\n",
    "\n",
    "Let's slice our autoendoer in half to extract our reduced features. :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#we flatten model to 128\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "layer_name = 'dense'\n",
    "\n",
    "intermediate_layer_model = Model(inputs=autoencoder.input,#take autoencoder inputs\n",
    "                                 outputs=autoencoder.get_layer(layer_name).output)#get the layer and output from that\n",
    "\n",
    "intermediate_output = intermediate_layer_model.predict(x_train)#call the predict methodHey are you following along the lecture? Do you have the same code as he does in his notebook? I don't think I have variational autoencoders function that he is on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop Over Each Input Observation\n",
    "intermediate_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_output[0].T\n",
    "#gets 4x4\n",
    "#for recommendation flatten down "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do k nearest neighbor search on top of our outputs\n",
    "#vector searches \n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "nn = NearestNeighbors(n_neighbors=10, algorithm='ball_tree')\n",
    "nn.fit(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can in half\n",
    "#extract call method for new model to get intermediate output\n",
    "#take intermediate output and feed it into\n",
    "\n",
    "#prepare for lecture tomorrow - read open.ai papers\n",
    "\n",
    "#first time to apply this technique is for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gan 2 networks mashed together into one \n",
    "\n",
    "#input z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"p3\"></a>\n",
    "Part 4: Information Retrieval with Autoencoders\n",
    "\n",
    "Generator - \n",
    "Discriminator - generates samples - decides what is real or fake, loss shoots back to generator to discriminator modeled behind prisoner's delima \n",
    "\n",
    "train generator first, train discriminator second\n",
    "create gan input, get x, get x discriminator, return gan network\n",
    "\n",
    "load mnist, create batch size, instaniate generator, go through epoch\n",
    "\n",
    "overfitting: autoencoder output is exactly like autoencoder input - penalty can be added\n",
    "\n",
    "inputs ar random noise \n",
    "\n",
    "create labels\n",
    "train discriminator, train generator\n",
    "plot images, retrain whole thing\n",
    "plot images every 20 epochs, \n",
    "\n",
    "gans give better results for images\n",
    "autoencoders compress information\n",
    "autoencoders can be for generative purposes\n",
    "gans include an adversarial component\n",
    "\n",
    "using inputs as labels in autoencoders - we use just one network\n",
    "in gan there are two networks competing against each other \n",
    "generator in gan is using noise \n",
    "gans are hard to achieve good results\n",
    "\n",
    "loss is determined by what we know about what is real or fake labels are internal - unsupervised approach\n",
    "\n",
    "\n",
    "image classification - is something or not\n",
    "     subdomain of image classification: object detection - bounding box - around object and attach probability\n",
    "     \n",
    "keras, pytorch - involves customization\n",
    "whereas aws or gcp automl - could involve consultant when there is less infrastructure\n",
    "\n",
    "importances are techniques and applications to real problems"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "U4-S1-NLP (Python 3.6)",
   "language": "python",
   "name": "u4-s1-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
