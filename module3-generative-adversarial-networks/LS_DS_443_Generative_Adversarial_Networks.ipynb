{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_IizNKWLomoA"
   },
   "source": [
    "# Lambda School Data Science - Generative Adversarial Networks\n",
    "\n",
    "![This Person Does Not Exist](https://thispersondoesnotexist.com/image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0EZdBzC6pvV9"
   },
   "source": [
    "# Lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W0hA8noPn94y"
   },
   "source": [
    "## A plot made for Hollywood\n",
    "\n",
    "Meet Gene:\n",
    "\n",
    "![Gene Wilder](https://upload.wikimedia.org/wikipedia/commons/thumb/9/9e/Gene_Wilder_1970.JPG/379px-Gene_Wilder_1970.JPG)\n",
    "\n",
    "Gene is the clever sort, and occasionally a bit mischievous. His goal is to **generate** things that trick Val:\n",
    "\n",
    "![Val Kilmer](https://upload.wikimedia.org/wikipedia/commons/2/22/Val_Kilmer_Cannes.jpg)\n",
    "\n",
    "Val has a simple job, but he takes it seriously - he wants to **validate** the authenticity of things that tricky Gene passes his way. Overall, Gene is a forger, and Val is a detective.\n",
    "\n",
    "And as with all great mystery dramas, Gene and Val are essentially playing a sort of game. They compete, and in doing so drive each other to improve.\n",
    "\n",
    "Considered as a GAN, Gene is the generator, and Val is the discriminator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Lg1r7f3lfCw"
   },
   "source": [
    "## *Two* neural networks - adversaries!\n",
    "\n",
    "![Spy vs. Spy](https://upload.wikimedia.org/wikipedia/commons/thumb/9/99/Comikaze_Expo_2011_-_Spy_vs_Spy_%286325381362%29.jpg/360px-Comikaze_Expo_2011_-_Spy_vs_Spy_%286325381362%29.jpg)\n",
    "\n",
    "Generative Adversarial Networks is an approach to unsupervised learning, based on the insight that we can train *two* networks simultaneously and pit them against each other.\n",
    "\n",
    "- The discriminator is trained with real - but unlabeled - data, and has the goal of identifying whether or not some new item belongs in it.\n",
    "- The generator starts from noise (it doesn't see the data at all!), and tries to generate output to fool the discriminator (and gets to update based on feedback).\n",
    "\n",
    "GANs can be considered a zero-sum game, in the [game theory](https://en.wikipedia.org/wiki/Game_theory) sense. Game theory is a common approach to modeling strategic competitive behavior between rational decision makers, and is heavily used in economics as well as computer science.\n",
    "\n",
    "If you've also heard the hype about [reinforcement learning](https://en.wikipedia.org/wiki/Reinforcement_learning), one way to understand it is:\n",
    "\n",
    "```\n",
    "Reinforcement Learning : GAN :: Decision Theory : Game Theory\n",
    "```\n",
    "\n",
    "That is, Reinforcement Learning is more closely analogous to [decision theory](https://en.wikipedia.org/wiki/Decision_theory), a relative to the field of game theory, featuring the behavior of an \"agent\" against \"nature\" (the environment). The agent is strategic and rational, but the environment simply is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "poUB58kaP7J5"
   },
   "source": [
    "## A Foray into Game Theory\n",
    "\n",
    "What is a \"zero sum\" game? It is a model of the interaction of two strategic agents, in a situation where, for one to gain, the other must lose, and vice-versa.\n",
    "\n",
    "A famous example is the [Prisoner's Dilemma](https://en.wikipedia.org/wiki/Prisoner's_dilemma). The typical story behind this game is something like this:\n",
    "\n",
    "> Two criminals who committed a crime together are caught by the authorities. There is enough evidence to put them each away for 3 years, but the police interrogate them separately, and offer each of them a deal - \"Tell us what the other criminal did, and we'll go lighter on you.\"\n",
    "\n",
    "> The deal is tempting - the person who takes it shaves 2 years off their sentence. But, it adds 5 years to the sentence of the other person. So if both talk, they both get 3 - 2 + 5 = 6 years, twice as much if they both don't. But if one talks and the other doesn't, the talker gets 1 year and the non-talker gets 8!\n",
    "\n",
    "> The result is, individually, they both prefer defecting (talking with the police) regardless of what the other person does. But, they'd both be better off if they could somehow trust one another to not talk to the police.\n",
    "\n",
    "Mathematically, we consider this outcome a *Nash equilibrium* - a stable situation where neither player would want to unilaterally change strategy. But, it's one where a *pareto superior* outcome exists (an outcome that both players would prefer to what they have now).\n",
    "\n",
    "An illustration (with different numbers) of the Prisoner's Dilemma:\n",
    "\n",
    "![Prisoner's Dilemmat](https://upload.wikimedia.org/wikipedia/commons/thumb/6/65/Dilema_do_Prisioneiro.png/480px-Dilema_do_Prisioneiro.png)\n",
    "\n",
    "More generally, these could be referred to as \"constant sum\" games - \"zero sum\" implies that for any player to get ahead, the other must inevitably end up behind. The above illustration could be of a game where people are \"splitting loot\", and so everybody *gets* something - it's just that some get more than others. The utility can be normalized so it sums to zero, or any other constant.\n",
    "\n",
    "Game Theory is one of the core tools used in social science and other areas to model and explain behavior. The main path to overcome \"dilemmas\" is *iteration* - through repetition, players can have a reputation, and value that reputation more than the outcome in any single round. For example, think of the lengths some restaurants take to ensure positive reviews.\n",
    "\n",
    "*Exercise* - think of at least two scenarios that could be explained with Prisoner's Dilemma, and of one other scenario that you think could also be modeled as some sort of strategic game between agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5z5Z0pnYPwSf"
   },
   "source": [
    "## Minimum Viable GAN\n",
    "\n",
    "Courtesy of Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "penxpauRuyWt"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm  # performance timing\n",
    "\n",
    "# Building on Keras\n",
    "from keras.layers import Input\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.optimizers import Adam\n",
    "from keras import initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2qQOHKrRu-rN"
   },
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "random_dim = 100\n",
    " \n",
    "def load_minst_data():\n",
    "    # load the data - we'll use Fashion MNIST, for a change of pace\n",
    "    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "    # normalize our inputs to be in the range[-1, 1] \n",
    "    x_train = (x_train.astype(np.float32) - 127.5)/127.5\n",
    "    # convert x_train with a shape of (60000, 28, 28) to (60000, 784) so we have\n",
    "    # 784 columns per row\n",
    "    x_train = x_train.reshape(60000, 784)\n",
    "    return (x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_FDog5HivCwV"
   },
   "outputs": [],
   "source": [
    "def get_discriminator(optimizer):\n",
    "    discriminator = Sequential()\n",
    "    discriminator.add(Dense(\n",
    "        1024, input_dim=784,\n",
    "        kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "    discriminator.add(Dropout(0.3))\n",
    " \n",
    "    discriminator.add(Dense(512))\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "    discriminator.add(Dropout(0.3))\n",
    " \n",
    "    discriminator.add(Dense(256))\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "    discriminator.add(Dropout(0.3))\n",
    " \n",
    "    discriminator.add(Dense(1, activation='sigmoid'))\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    return discriminator\n",
    "\n",
    "def get_generator(optimizer):\n",
    "    generator = Sequential()\n",
    "    generator.add(Dense(\n",
    "        256, input_dim=random_dim,\n",
    "        kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
    "    generator.add(LeakyReLU(0.2))\n",
    " \n",
    "    generator.add(Dense(512))\n",
    "    generator.add(LeakyReLU(0.2))\n",
    " \n",
    "    generator.add(Dense(1024))\n",
    "    generator.add(LeakyReLU(0.2))\n",
    " \n",
    "    generator.add(Dense(784, activation='tanh'))\n",
    "    generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    return generator\n",
    "\n",
    "def get_gan_network(discriminator, random_dim, generator, optimizer):\n",
    "    # We initially set trainable to False since we only want to train either the \n",
    "    # generator or discriminator at a time\n",
    "    discriminator.trainable = False\n",
    "    # gan input (noise) will be 100-dimensional vectors\n",
    "    gan_input = Input(shape=(random_dim,))\n",
    "    # the output of the generator (an image)\n",
    "    x = generator(gan_input)\n",
    "    # get the output of the discriminator (probability if the image is real/not)\n",
    "    gan_output = discriminator(x)\n",
    "    gan = Model(inputs=gan_input, outputs=gan_output)\n",
    "    gan.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    return gan\n",
    "\n",
    "def plot_generated_images(epoch, generator, examples=100, dim=(10, 10),\n",
    "                          figsize=(10, 10)):\n",
    "    noise = np.random.normal(0, 1, size=[examples, random_dim])\n",
    "    generated_images = generator.predict(noise)\n",
    "    generated_images = generated_images.reshape(examples, 28, 28)\n",
    " \n",
    "    plt.figure(figsize=figsize)\n",
    "    for i in range(generated_images.shape[0]):\n",
    "        plt.subplot(dim[0], dim[1], i+1)\n",
    "        plt.imshow(generated_images[i], interpolation='nearest', cmap='gray_r')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('gan_generated_image_epoch_%d.png' % epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3681
    },
    "colab_type": "code",
    "id": "YKsazCE-vFLy",
    "outputId": "dd5b57f9-e4c7-496d-c79e-9cc21a66aa0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 5us/step\n",
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "19136512/26421880 [====================>.........] - ETA: 12s"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-1d1ee8618a8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mplot_generated_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-1d1ee8618a8d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, batch_size)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Get the training and testing data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_minst_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Split the training data into batches of size 128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbatch_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-d321f938c8b5>\u001b[0m in \u001b[0;36mload_minst_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_minst_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# load the data - we'll use Fashion MNIST, for a change of pace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfashion_mnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m# normalize our inputs to be in the range[-1, 1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m127.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m127.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/datasets/fashion_mnist.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m         paths.append(get_file(fname,\n\u001b[1;32m     28\u001b[0m                               \u001b[0morigin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                               cache_subdir=dirname))\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlbpath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget_file\u001b[0;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m                 \u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_progress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m                 \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    445\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train(epochs=1, batch_size=128):\n",
    "    # Get the training and testing data\n",
    "    x_train, y_train, x_test, y_test = load_minst_data()\n",
    "    # Split the training data into batches of size 128\n",
    "    batch_count = x_train.shape[0] // batch_size\n",
    " \n",
    "    # Build our GAN netowrk\n",
    "    adam = Adam(lr=0.0002, beta_1=0.5)\n",
    "    generator = get_generator(adam)\n",
    "    discriminator = get_discriminator(adam)\n",
    "    gan = get_gan_network(discriminator, random_dim, generator, adam)\n",
    " \n",
    "    for e in range(1, epochs+1):\n",
    "        print ('-'*15, 'Epoch %d' % e, '-'*15)\n",
    "        for _ in tqdm(range(batch_count)):\n",
    "            # Get a random set of input noise and images\n",
    "            noise = np.random.normal(0, 1, size=[batch_size, random_dim])\n",
    "            image_batch = x_train[np.random.randint(0, x_train.shape[0],\n",
    "                                                    size=batch_size)]\n",
    " \n",
    "            # Generate fake MNIST images\n",
    "            generated_images = generator.predict(noise)\n",
    "            X = np.concatenate([image_batch, generated_images])\n",
    " \n",
    "            # Labels for generated and real data\n",
    "            y_dis = np.zeros(2*batch_size)\n",
    "            # One-sided label smoothing\n",
    "            y_dis[:batch_size] = 0.9\n",
    " \n",
    "            # Train discriminator\n",
    "            discriminator.trainable = True\n",
    "            discriminator.train_on_batch(X, y_dis)\n",
    " \n",
    "            # Train generator\n",
    "            noise = np.random.normal(0, 1, size=[batch_size, random_dim])\n",
    "            y_gen = np.ones(batch_size)\n",
    "            discriminator.trainable = False\n",
    "            gan.train_on_batch(noise, y_gen)\n",
    " \n",
    "        if e == 1 or e % 20 == 0:\n",
    "            plot_generated_images(e, generator)\n",
    " \n",
    "train(40, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jHwENTrvL5pP"
   },
   "source": [
    "Pretty decent results, even after not too many iterations.\n",
    "\n",
    "We can do even better, with pretrained StyleGAN!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R8XpLKVincLu"
   },
   "source": [
    "## StyleGAN - A Style-Based Generator Architecture for Generative Adversarial Networks\n",
    "\n",
    "Original paper: https://arxiv.org/abs/1812.04948\n",
    "\n",
    "Source code: https://github.com/NVlabs/stylegan\n",
    "\n",
    "Many applications:\n",
    "- https://thispersondoesnotexist.com\n",
    "- https://thiscatdoesnotexist.com\n",
    "- https://thisairbnbdoesnotexist.com\n",
    "- https://stackroboflow.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "e1FaXXDoi5Z2",
    "outputId": "9ce51778-43f5-4617-8093-e3afaf2337eb"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/NVlabs/stylegan\n",
    "%cd stylegan/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1580
    },
    "colab_type": "code",
    "id": "GkJUFfsgnqr_",
    "outputId": "559f89d5-b07c-4966-f326-485410039faa"
   },
   "outputs": [],
   "source": [
    "# From stylegan/pretrained_example.py\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import dnnlib\n",
    "import dnnlib.tflib as tflib\n",
    "import config\n",
    "\n",
    "def main():\n",
    "    # Initialize TensorFlow.\n",
    "    tflib.init_tf()\n",
    "\n",
    "    # Load pre-trained network.\n",
    "    url = 'https://drive.google.com/uc?id=1MEGjdvVpUsu1jB4zrXZN7Y4kBBOzizDQ' # karras2019stylegan-ffhq-1024x1024.pkl\n",
    "    with dnnlib.util.open_url(url, cache_dir=config.cache_dir) as f:\n",
    "        _G, _D, Gs = pickle.load(f)\n",
    "        # _G = Instantaneous snapshot of the generator. Mainly useful for resuming a previous training run.\n",
    "        # _D = Instantaneous snapshot of the discriminator. Mainly useful for resuming a previous training run.\n",
    "        # Gs = Long-term average of the generator. Yields higher-quality results than the instantaneous snapshot.\n",
    "\n",
    "    # Print network details.\n",
    "    Gs.print_layers()\n",
    "\n",
    "    # Pick latent vector.\n",
    "    rnd = np.random.RandomState(5)\n",
    "    latents = rnd.randn(1, Gs.input_shape[1])\n",
    "\n",
    "    # Generate image.\n",
    "    fmt = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True)\n",
    "    images = Gs.run(latents, None, truncation_psi=0.7, randomize_noise=True, output_transform=fmt)\n",
    "\n",
    "    # Save image.\n",
    "    os.makedirs(config.result_dir, exist_ok=True)\n",
    "    png_filename = os.path.join(config.result_dir, 'example.png')\n",
    "    PIL.Image.fromarray(images[0], 'RGB').save(png_filename)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1041
    },
    "colab_type": "code",
    "id": "7rqXQzb6N1jF",
    "outputId": "0a5b7fda-861e-4d6d-c692-c4c00643e957"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='results/example.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0lfZdD_cp1t5"
   },
   "source": [
    "# Assignment - ⭐ EmojiGAN ⭐\n",
    "\n",
    "Using the provided \"minimum viable GAN\" code, train a pair of networks to generate emoji. To get you started, here's some emoji data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "colab_type": "code",
    "id": "Ltj1je1fp5rO",
    "outputId": "98ced068-b9a4-442c-9659-d2a36f9c6791"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji_data_python in /anaconda3/lib/python3.7/site-packages (1.1.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji_data_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 15285
    },
    "colab_type": "code",
    "id": "U6pPH5jkak29",
    "outputId": "4598d5ff-ab8d-4470-f104-a0430ff2a55d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-04-10 14:22:42--  https://github.com/LambdaSchool/DS-Unit-4-Sprint-4-Deep-Learning/raw/master/module3-generative-adversarial-networks/emoji.zip\n",
      "Resolving github.com (github.com)... 192.30.255.113, 192.30.255.112\n",
      "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/LambdaSchool/DS-Unit-4-Sprint-4-Deep-Learning/master/module3-generative-adversarial-networks/emoji.zip [following]\n",
      "--2019-04-10 14:22:42--  https://raw.githubusercontent.com/LambdaSchool/DS-Unit-4-Sprint-4-Deep-Learning/master/module3-generative-adversarial-networks/emoji.zip\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.188.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.188.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3407395 (3.2M) [application/zip]\n",
      "Saving to: ‘emoji.zip’\n",
      "\n",
      "emoji.zip           100%[===================>]   3.25M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2019-04-10 14:22:43 (30.4 MB/s) - ‘emoji.zip’ saved [3407395/3407395]\n",
      "\n",
      "Archive:  emoji.zip\n",
      " extracting: emoji/00a9.png          \n",
      " extracting: emoji/00ae.png          \n",
      " extracting: emoji/1f004.png         \n",
      " extracting: emoji/1f0cf.png         \n",
      " extracting: emoji/1f170.png         \n",
      " extracting: emoji/1f171.png         \n",
      " extracting: emoji/1f17e.png         \n",
      " extracting: emoji/1f17f.png         \n",
      " extracting: emoji/1f18e.png         \n",
      " extracting: emoji/1f191.png         \n",
      " extracting: emoji/1f192.png         \n",
      " extracting: emoji/1f193.png         \n",
      " extracting: emoji/1f194.png         \n",
      " extracting: emoji/1f195.png         \n",
      " extracting: emoji/1f196.png         \n",
      " extracting: emoji/1f197.png         \n",
      " extracting: emoji/1f198.png         \n",
      " extracting: emoji/1f199.png         \n",
      " extracting: emoji/1f19a.png         \n",
      " extracting: emoji/1f1e6.png         \n",
      " extracting: emoji/1f1e7.png         \n",
      " extracting: emoji/1f1e8-1f1f3.png   \n",
      " extracting: emoji/1f1e8.png         \n",
      " extracting: emoji/1f1e9-1f1ea.png   \n",
      " extracting: emoji/1f1e9.png         \n",
      " extracting: emoji/1f1ea-1f1f8.png   \n",
      " extracting: emoji/1f1ea.png         \n",
      " extracting: emoji/1f1eb-1f1f7.png   \n",
      " extracting: emoji/1f1eb.png         \n",
      " extracting: emoji/1f1ec-1f1e7.png   \n",
      " extracting: emoji/1f1ec.png         \n",
      " extracting: emoji/1f1ed.png         \n",
      " extracting: emoji/1f1ee-1f1f9.png   \n",
      " extracting: emoji/1f1ee.png         \n",
      " extracting: emoji/1f1ef-1f1f5.png   \n",
      " extracting: emoji/1f1ef.png         \n",
      " extracting: emoji/1f1f0-1f1f7.png   \n",
      " extracting: emoji/1f1f0.png         \n",
      " extracting: emoji/1f1f1.png         \n",
      " extracting: emoji/1f1f2.png         \n",
      " extracting: emoji/1f1f3.png         \n",
      " extracting: emoji/1f1f4.png         \n",
      " extracting: emoji/1f1f5.png         \n",
      " extracting: emoji/1f1f6.png         \n",
      " extracting: emoji/1f1f7-1f1fa.png   \n",
      " extracting: emoji/1f1f7.png         \n",
      " extracting: emoji/1f1f8.png         \n",
      " extracting: emoji/1f1f9.png         \n",
      " extracting: emoji/1f1fa-1f1f8.png   \n",
      " extracting: emoji/1f1fa.png         \n",
      " extracting: emoji/1f1fb.png         \n",
      " extracting: emoji/1f1fc.png         \n",
      " extracting: emoji/1f1fd.png         \n",
      " extracting: emoji/1f1fe.png         \n",
      " extracting: emoji/1f1ff.png         \n",
      " extracting: emoji/1f201.png         \n",
      " extracting: emoji/1f202.png         \n",
      " extracting: emoji/1f21a.png         \n",
      " extracting: emoji/1f22f.png         \n",
      " extracting: emoji/1f232.png         \n",
      " extracting: emoji/1f233.png         \n",
      " extracting: emoji/1f234.png         \n",
      " extracting: emoji/1f235.png         \n",
      " extracting: emoji/1f236.png         \n",
      " extracting: emoji/1f237.png         \n",
      " extracting: emoji/1f238.png         \n",
      " extracting: emoji/1f239.png         \n",
      " extracting: emoji/1f23a.png         \n",
      " extracting: emoji/1f250.png         \n",
      " extracting: emoji/1f251.png         \n",
      " extracting: emoji/1f300.png         \n",
      " extracting: emoji/1f301.png         \n",
      " extracting: emoji/1f302.png         \n",
      " extracting: emoji/1f303.png         \n",
      " extracting: emoji/1f304.png         \n",
      " extracting: emoji/1f305.png         \n",
      " extracting: emoji/1f306.png         \n",
      " extracting: emoji/1f307.png         \n",
      " extracting: emoji/1f308.png         \n",
      " extracting: emoji/1f309.png         \n",
      " extracting: emoji/1f30a.png         \n",
      " extracting: emoji/1f30b.png         \n",
      " extracting: emoji/1f30c.png         \n",
      " extracting: emoji/1f30d.png         \n",
      " extracting: emoji/1f30e.png         \n",
      " extracting: emoji/1f30f.png         \n",
      " extracting: emoji/1f310.png         \n",
      " extracting: emoji/1f311.png         \n",
      " extracting: emoji/1f312.png         \n",
      " extracting: emoji/1f313.png         \n",
      " extracting: emoji/1f314.png         \n",
      " extracting: emoji/1f315.png         \n",
      " extracting: emoji/1f316.png         \n",
      " extracting: emoji/1f317.png         \n",
      " extracting: emoji/1f318.png         \n",
      " extracting: emoji/1f319.png         \n",
      " extracting: emoji/1f31a.png         \n",
      " extracting: emoji/1f31b.png         \n",
      " extracting: emoji/1f31c.png         \n",
      " extracting: emoji/1f31d.png         \n",
      " extracting: emoji/1f31e.png         \n",
      " extracting: emoji/1f31f.png         \n",
      " extracting: emoji/1f320.png         \n",
      " extracting: emoji/1f330.png         \n",
      " extracting: emoji/1f331.png         \n",
      " extracting: emoji/1f332.png         \n",
      " extracting: emoji/1f333.png         \n",
      " extracting: emoji/1f334.png         \n",
      " extracting: emoji/1f335.png         \n",
      " extracting: emoji/1f337.png         \n",
      " extracting: emoji/1f338.png         \n",
      " extracting: emoji/1f339.png         \n",
      " extracting: emoji/1f33a.png         \n",
      " extracting: emoji/1f33b.png         \n",
      " extracting: emoji/1f33c.png         \n",
      " extracting: emoji/1f33d.png         \n",
      " extracting: emoji/1f33e.png         \n",
      " extracting: emoji/1f33f.png         \n",
      " extracting: emoji/1f340.png         \n",
      " extracting: emoji/1f341.png         \n",
      " extracting: emoji/1f342.png         \n",
      " extracting: emoji/1f343.png         \n",
      " extracting: emoji/1f344.png         \n",
      " extracting: emoji/1f345.png         \n",
      " extracting: emoji/1f346.png         \n",
      " extracting: emoji/1f347.png         \n",
      " extracting: emoji/1f348.png         \n",
      " extracting: emoji/1f349.png         \n",
      " extracting: emoji/1f34a.png         \n",
      " extracting: emoji/1f34b.png         \n",
      " extracting: emoji/1f34c.png         \n",
      " extracting: emoji/1f34d.png         \n",
      " extracting: emoji/1f34e.png         \n",
      " extracting: emoji/1f34f.png         \n",
      " extracting: emoji/1f350.png         \n",
      " extracting: emoji/1f351.png         \n",
      " extracting: emoji/1f352.png         \n",
      " extracting: emoji/1f353.png         \n",
      " extracting: emoji/1f354.png         \n",
      " extracting: emoji/1f355.png         \n",
      " extracting: emoji/1f356.png         \n",
      " extracting: emoji/1f357.png         \n",
      " extracting: emoji/1f358.png         \n",
      " extracting: emoji/1f359.png         \n",
      " extracting: emoji/1f35a.png         \n",
      " extracting: emoji/1f35b.png         \n",
      " extracting: emoji/1f35c.png         \n",
      " extracting: emoji/1f35d.png         \n",
      " extracting: emoji/1f35e.png         \n",
      " extracting: emoji/1f35f.png         \n",
      " extracting: emoji/1f360.png         \n",
      " extracting: emoji/1f361.png         \n",
      " extracting: emoji/1f362.png         \n",
      " extracting: emoji/1f363.png         \n",
      " extracting: emoji/1f364.png         \n",
      " extracting: emoji/1f365.png         \n",
      " extracting: emoji/1f366.png         \n",
      " extracting: emoji/1f367.png         \n",
      " extracting: emoji/1f368.png         \n",
      " extracting: emoji/1f369.png         \n",
      " extracting: emoji/1f36a.png         \n",
      " extracting: emoji/1f36b.png         \n",
      " extracting: emoji/1f36c.png         \n",
      " extracting: emoji/1f36d.png         \n",
      " extracting: emoji/1f36e.png         \n",
      " extracting: emoji/1f36f.png         \n",
      " extracting: emoji/1f370.png         \n",
      " extracting: emoji/1f371.png         \n",
      " extracting: emoji/1f372.png         \n",
      " extracting: emoji/1f373.png         \n",
      " extracting: emoji/1f374.png         \n",
      " extracting: emoji/1f375.png         \n",
      " extracting: emoji/1f376.png         \n",
      " extracting: emoji/1f377.png         \n",
      " extracting: emoji/1f378.png         \n",
      " extracting: emoji/1f379.png         \n",
      " extracting: emoji/1f37a.png         \n",
      " extracting: emoji/1f37b.png         \n",
      " extracting: emoji/1f37c.png         \n",
      " extracting: emoji/1f380.png         \n",
      " extracting: emoji/1f381.png         \n",
      " extracting: emoji/1f382.png         \n",
      " extracting: emoji/1f383.png         \n",
      " extracting: emoji/1f384.png         \n",
      " extracting: emoji/1f385.png         \n",
      " extracting: emoji/1f386.png         \n",
      " extracting: emoji/1f387.png         \n",
      " extracting: emoji/1f388.png         \n",
      " extracting: emoji/1f389.png         \n",
      " extracting: emoji/1f38a.png         \n",
      " extracting: emoji/1f38b.png         \n",
      " extracting: emoji/1f38c.png         \n",
      " extracting: emoji/1f38d.png         \n",
      " extracting: emoji/1f38e.png         \n",
      " extracting: emoji/1f38f.png         \n",
      " extracting: emoji/1f390.png         \n",
      " extracting: emoji/1f391.png         \n",
      " extracting: emoji/1f392.png         \n",
      " extracting: emoji/1f393.png         \n",
      " extracting: emoji/1f3a0.png         \n",
      " extracting: emoji/1f3a1.png         \n",
      " extracting: emoji/1f3a2.png         \n",
      " extracting: emoji/1f3a3.png         \n",
      " extracting: emoji/1f3a4.png         \n",
      " extracting: emoji/1f3a5.png         \n",
      " extracting: emoji/1f3a6.png         \n",
      " extracting: emoji/1f3a7.png         \n",
      " extracting: emoji/1f3a8.png         \n",
      " extracting: emoji/1f3a9.png         \n",
      " extracting: emoji/1f3aa.png         \n",
      " extracting: emoji/1f3ab.png         \n",
      " extracting: emoji/1f3ac.png         \n",
      " extracting: emoji/1f3ad.png         \n",
      " extracting: emoji/1f3ae.png         \n",
      " extracting: emoji/1f3af.png         \n",
      " extracting: emoji/1f3b0.png         \n",
      " extracting: emoji/1f3b1.png         \n",
      " extracting: emoji/1f3b2.png         \n",
      " extracting: emoji/1f3b3.png         \n",
      " extracting: emoji/1f3b4.png         \n",
      " extracting: emoji/1f3b5.png         \n",
      " extracting: emoji/1f3b6.png         \n",
      " extracting: emoji/1f3b7.png         \n",
      " extracting: emoji/1f3b8.png         \n",
      " extracting: emoji/1f3b9.png         \n",
      " extracting: emoji/1f3ba.png         \n",
      " extracting: emoji/1f3bb.png         \n",
      " extracting: emoji/1f3bc.png         \n",
      " extracting: emoji/1f3bd.png         \n",
      " extracting: emoji/1f3be.png         \n",
      " extracting: emoji/1f3bf.png         \n",
      " extracting: emoji/1f3c0.png         \n",
      " extracting: emoji/1f3c1.png         \n",
      " extracting: emoji/1f3c2.png         \n",
      " extracting: emoji/1f3c3.png         \n",
      " extracting: emoji/1f3c4.png         \n",
      " extracting: emoji/1f3c6.png         \n",
      " extracting: emoji/1f3c7.png         \n",
      " extracting: emoji/1f3c8.png         \n",
      " extracting: emoji/1f3c9.png         \n",
      " extracting: emoji/1f3ca.png         \n",
      " extracting: emoji/1f3e0.png         \n",
      " extracting: emoji/1f3e1.png         \n",
      " extracting: emoji/1f3e2.png         \n",
      " extracting: emoji/1f3e3.png         \n",
      " extracting: emoji/1f3e4.png         \n",
      " extracting: emoji/1f3e5.png         \n",
      " extracting: emoji/1f3e6.png         \n",
      " extracting: emoji/1f3e7.png         \n",
      " extracting: emoji/1f3e8.png         \n",
      " extracting: emoji/1f3e9.png         \n",
      " extracting: emoji/1f3ea.png         \n",
      " extracting: emoji/1f3eb.png         \n",
      " extracting: emoji/1f3ec.png         \n",
      " extracting: emoji/1f3ed.png         \n",
      " extracting: emoji/1f3ee.png         \n",
      " extracting: emoji/1f3ef.png         \n",
      " extracting: emoji/1f3f0.png         \n",
      " extracting: emoji/1f400.png         \n",
      " extracting: emoji/1f401.png         \n",
      " extracting: emoji/1f402.png         \n",
      " extracting: emoji/1f403.png         \n",
      " extracting: emoji/1f404.png         \n",
      " extracting: emoji/1f405.png         \n",
      " extracting: emoji/1f406.png         \n",
      " extracting: emoji/1f407.png         \n",
      " extracting: emoji/1f408.png         \n",
      " extracting: emoji/1f409.png         \n",
      " extracting: emoji/1f40a.png         \n",
      " extracting: emoji/1f40b.png         \n",
      " extracting: emoji/1f40c.png         \n",
      " extracting: emoji/1f40d.png         \n",
      " extracting: emoji/1f40e.png         \n",
      " extracting: emoji/1f40f.png         \n",
      " extracting: emoji/1f410.png         \n",
      " extracting: emoji/1f411.png         \n",
      " extracting: emoji/1f412.png         \n",
      " extracting: emoji/1f413.png         \n",
      " extracting: emoji/1f414.png         \n",
      " extracting: emoji/1f415.png         \n",
      " extracting: emoji/1f416.png         \n",
      " extracting: emoji/1f417.png         \n",
      " extracting: emoji/1f418.png         \n",
      " extracting: emoji/1f419.png         \n",
      " extracting: emoji/1f41a.png         \n",
      " extracting: emoji/1f41b.png         \n",
      " extracting: emoji/1f41c.png         \n",
      " extracting: emoji/1f41d.png         \n",
      " extracting: emoji/1f41e.png         \n",
      " extracting: emoji/1f41f.png         \n",
      " extracting: emoji/1f420.png         \n",
      " extracting: emoji/1f421.png         \n",
      " extracting: emoji/1f422.png         \n",
      " extracting: emoji/1f423.png         \n",
      " extracting: emoji/1f424.png         \n",
      " extracting: emoji/1f425.png         \n",
      " extracting: emoji/1f426.png         \n",
      " extracting: emoji/1f427.png         \n",
      " extracting: emoji/1f428.png         \n",
      " extracting: emoji/1f429.png         \n",
      " extracting: emoji/1f42a.png         \n",
      " extracting: emoji/1f42b.png         \n",
      " extracting: emoji/1f42c.png         \n",
      " extracting: emoji/1f42d.png         \n",
      " extracting: emoji/1f42e.png         \n",
      " extracting: emoji/1f42f.png         \n",
      " extracting: emoji/1f430.png         \n",
      " extracting: emoji/1f431.png         \n",
      " extracting: emoji/1f432.png         \n",
      " extracting: emoji/1f433.png         \n",
      " extracting: emoji/1f434.png         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " extracting: emoji/1f435.png         \n",
      " extracting: emoji/1f436.png         \n",
      " extracting: emoji/1f437.png         \n",
      " extracting: emoji/1f438.png         \n",
      " extracting: emoji/1f439.png         \n",
      " extracting: emoji/1f43a.png         \n",
      " extracting: emoji/1f43b.png         \n",
      " extracting: emoji/1f43c.png         \n",
      " extracting: emoji/1f43d.png         \n",
      " extracting: emoji/1f43e.png         \n",
      " extracting: emoji/1f440.png         \n",
      " extracting: emoji/1f442.png         \n",
      " extracting: emoji/1f443.png         \n",
      " extracting: emoji/1f444.png         \n",
      " extracting: emoji/1f445.png         \n",
      " extracting: emoji/1f446.png         \n",
      " extracting: emoji/1f447.png         \n",
      " extracting: emoji/1f448.png         \n",
      " extracting: emoji/1f449.png         \n",
      " extracting: emoji/1f44a.png         \n",
      " extracting: emoji/1f44b.png         \n",
      " extracting: emoji/1f44c.png         \n",
      " extracting: emoji/1f44d.png         \n",
      " extracting: emoji/1f44e.png         \n",
      " extracting: emoji/1f44f.png         \n",
      " extracting: emoji/1f450.png         \n",
      " extracting: emoji/1f451.png         \n",
      " extracting: emoji/1f452.png         \n",
      " extracting: emoji/1f453.png         \n",
      " extracting: emoji/1f454.png         \n",
      " extracting: emoji/1f455.png         \n",
      " extracting: emoji/1f456.png         \n",
      " extracting: emoji/1f457.png         \n",
      " extracting: emoji/1f458.png         \n",
      " extracting: emoji/1f459.png         \n",
      " extracting: emoji/1f45a.png         \n",
      " extracting: emoji/1f45b.png         \n",
      " extracting: emoji/1f45c.png         \n",
      " extracting: emoji/1f45d.png         \n",
      " extracting: emoji/1f45e.png         \n",
      " extracting: emoji/1f45f.png         \n",
      " extracting: emoji/1f460.png         \n",
      " extracting: emoji/1f461.png         \n",
      " extracting: emoji/1f462.png         \n",
      " extracting: emoji/1f463.png         \n",
      " extracting: emoji/1f464.png         \n",
      " extracting: emoji/1f465.png         \n",
      " extracting: emoji/1f466.png         \n",
      " extracting: emoji/1f467.png         \n",
      " extracting: emoji/1f468.png         \n",
      " extracting: emoji/1f469.png         \n",
      " extracting: emoji/1f46a.png         \n",
      " extracting: emoji/1f46b.png         \n",
      " extracting: emoji/1f46c.png         \n",
      " extracting: emoji/1f46d.png         \n",
      " extracting: emoji/1f46e.png         \n",
      " extracting: emoji/1f46f.png         \n",
      " extracting: emoji/1f470.png         \n",
      " extracting: emoji/1f471.png         \n",
      " extracting: emoji/1f472.png         \n",
      " extracting: emoji/1f473.png         \n",
      " extracting: emoji/1f474.png         \n",
      " extracting: emoji/1f475.png         \n",
      " extracting: emoji/1f476.png         \n",
      " extracting: emoji/1f477.png         \n",
      " extracting: emoji/1f478.png         \n",
      " extracting: emoji/1f479.png         \n",
      " extracting: emoji/1f47a.png         \n",
      " extracting: emoji/1f47b.png         \n",
      " extracting: emoji/1f47c.png         \n",
      " extracting: emoji/1f47d.png         \n",
      " extracting: emoji/1f47e.png         \n",
      " extracting: emoji/1f47f.png         \n",
      " extracting: emoji/1f480.png         \n",
      " extracting: emoji/1f481.png         \n",
      " extracting: emoji/1f482.png         \n",
      " extracting: emoji/1f483.png         \n",
      " extracting: emoji/1f484.png         \n",
      " extracting: emoji/1f485.png         \n",
      " extracting: emoji/1f486.png         \n",
      " extracting: emoji/1f487.png         \n",
      " extracting: emoji/1f488.png         \n",
      " extracting: emoji/1f489.png         \n",
      " extracting: emoji/1f48a.png         \n",
      " extracting: emoji/1f48b.png         \n",
      " extracting: emoji/1f48c.png         \n",
      " extracting: emoji/1f48d.png         \n",
      " extracting: emoji/1f48e.png         \n",
      " extracting: emoji/1f48f.png         \n",
      " extracting: emoji/1f490.png         \n",
      " extracting: emoji/1f491.png         \n",
      " extracting: emoji/1f492.png         \n",
      " extracting: emoji/1f493.png         \n",
      " extracting: emoji/1f494.png         \n",
      " extracting: emoji/1f495.png         \n",
      " extracting: emoji/1f496.png         \n",
      " extracting: emoji/1f497.png         \n",
      " extracting: emoji/1f498.png         \n",
      " extracting: emoji/1f499.png         \n",
      " extracting: emoji/1f49a.png         \n",
      " extracting: emoji/1f49b.png         \n",
      " extracting: emoji/1f49c.png         \n",
      " extracting: emoji/1f49d.png         \n",
      " extracting: emoji/1f49e.png         \n",
      " extracting: emoji/1f49f.png         \n",
      " extracting: emoji/1f4a0.png         \n",
      " extracting: emoji/1f4a1.png         \n",
      " extracting: emoji/1f4a2.png         \n",
      " extracting: emoji/1f4a3.png         \n",
      " extracting: emoji/1f4a4.png         \n",
      " extracting: emoji/1f4a5.png         \n",
      " extracting: emoji/1f4a6.png         \n",
      " extracting: emoji/1f4a7.png         \n",
      " extracting: emoji/1f4a8.png         \n",
      " extracting: emoji/1f4a9.png         \n",
      " extracting: emoji/1f4aa.png         \n",
      " extracting: emoji/1f4ab.png         \n",
      " extracting: emoji/1f4ac.png         \n",
      " extracting: emoji/1f4ad.png         \n",
      " extracting: emoji/1f4ae.png         \n",
      " extracting: emoji/1f4af.png         \n",
      " extracting: emoji/1f4b0.png         \n",
      " extracting: emoji/1f4b1.png         \n",
      " extracting: emoji/1f4b2.png         \n",
      " extracting: emoji/1f4b3.png         \n",
      " extracting: emoji/1f4b4.png         \n",
      " extracting: emoji/1f4b5.png         \n",
      " extracting: emoji/1f4b6.png         \n",
      " extracting: emoji/1f4b7.png         \n",
      " extracting: emoji/1f4b8.png         \n",
      " extracting: emoji/1f4b9.png         \n",
      " extracting: emoji/1f4ba.png         \n",
      " extracting: emoji/1f4bb.png         \n",
      " extracting: emoji/1f4bc.png         \n",
      " extracting: emoji/1f4bd.png         \n",
      " extracting: emoji/1f4be.png         \n",
      " extracting: emoji/1f4bf.png         \n",
      " extracting: emoji/1f4c0.png         \n",
      " extracting: emoji/1f4c1.png         \n",
      " extracting: emoji/1f4c2.png         \n",
      " extracting: emoji/1f4c3.png         \n",
      " extracting: emoji/1f4c4.png         \n",
      " extracting: emoji/1f4c5.png         \n",
      " extracting: emoji/1f4c6.png         \n",
      " extracting: emoji/1f4c7.png         \n",
      " extracting: emoji/1f4c8.png         \n",
      " extracting: emoji/1f4c9.png         \n",
      " extracting: emoji/1f4ca.png         \n",
      " extracting: emoji/1f4cb.png         \n",
      " extracting: emoji/1f4cc.png         \n",
      " extracting: emoji/1f4cd.png         \n",
      " extracting: emoji/1f4ce.png         \n",
      " extracting: emoji/1f4cf.png         \n",
      " extracting: emoji/1f4d0.png         \n",
      " extracting: emoji/1f4d1.png         \n",
      " extracting: emoji/1f4d2.png         \n",
      " extracting: emoji/1f4d3.png         \n",
      " extracting: emoji/1f4d4.png         \n",
      " extracting: emoji/1f4d5.png         \n",
      " extracting: emoji/1f4d6.png         \n",
      " extracting: emoji/1f4d7.png         \n",
      " extracting: emoji/1f4d8.png         \n",
      " extracting: emoji/1f4d9.png         \n",
      " extracting: emoji/1f4da.png         \n",
      " extracting: emoji/1f4db.png         \n",
      " extracting: emoji/1f4dc.png         \n",
      " extracting: emoji/1f4dd.png         \n",
      " extracting: emoji/1f4de.png         \n",
      " extracting: emoji/1f4df.png         \n",
      " extracting: emoji/1f4e0.png         \n",
      " extracting: emoji/1f4e1.png         \n",
      " extracting: emoji/1f4e2.png         \n",
      " extracting: emoji/1f4e3.png         \n",
      " extracting: emoji/1f4e4.png         \n",
      " extracting: emoji/1f4e5.png         \n",
      " extracting: emoji/1f4e6.png         \n",
      " extracting: emoji/1f4e7.png         \n",
      " extracting: emoji/1f4e8.png         \n",
      " extracting: emoji/1f4e9.png         \n",
      " extracting: emoji/1f4ea.png         \n",
      " extracting: emoji/1f4eb.png         \n",
      " extracting: emoji/1f4ec.png         \n",
      " extracting: emoji/1f4ed.png         \n",
      " extracting: emoji/1f4ee.png         \n",
      " extracting: emoji/1f4ef.png         \n",
      " extracting: emoji/1f4f0.png         \n",
      " extracting: emoji/1f4f1.png         \n",
      " extracting: emoji/1f4f2.png         \n",
      " extracting: emoji/1f4f3.png         \n",
      " extracting: emoji/1f4f4.png         \n",
      " extracting: emoji/1f4f5.png         \n",
      " extracting: emoji/1f4f6.png         \n",
      " extracting: emoji/1f4f7.png         \n",
      " extracting: emoji/1f4f9.png         \n",
      " extracting: emoji/1f4fa.png         \n",
      " extracting: emoji/1f4fb.png         \n",
      " extracting: emoji/1f4fc.png         \n",
      " extracting: emoji/1f500.png         \n",
      " extracting: emoji/1f501.png         \n",
      " extracting: emoji/1f502.png         \n",
      " extracting: emoji/1f503.png         \n",
      " extracting: emoji/1f504.png         \n",
      " extracting: emoji/1f505.png         \n",
      " extracting: emoji/1f506.png         \n",
      " extracting: emoji/1f507.png         \n",
      " extracting: emoji/1f508.png         \n",
      " extracting: emoji/1f509.png         \n",
      " extracting: emoji/1f50a.png         \n",
      " extracting: emoji/1f50b.png         \n",
      " extracting: emoji/1f50c.png         \n",
      " extracting: emoji/1f50d.png         \n",
      " extracting: emoji/1f50e.png         \n",
      " extracting: emoji/1f50f.png         \n",
      " extracting: emoji/1f510.png         \n",
      " extracting: emoji/1f511.png         \n",
      " extracting: emoji/1f512.png         \n",
      " extracting: emoji/1f513.png         \n",
      " extracting: emoji/1f514.png         \n",
      " extracting: emoji/1f515.png         \n",
      " extracting: emoji/1f516.png         \n",
      " extracting: emoji/1f517.png         \n",
      " extracting: emoji/1f518.png         \n",
      " extracting: emoji/1f519.png         \n",
      " extracting: emoji/1f51a.png         \n",
      " extracting: emoji/1f51b.png         \n",
      " extracting: emoji/1f51c.png         \n",
      " extracting: emoji/1f51d.png         \n",
      " extracting: emoji/1f51e.png         \n",
      " extracting: emoji/1f51f.png         \n",
      " extracting: emoji/1f520.png         \n",
      " extracting: emoji/1f521.png         \n",
      " extracting: emoji/1f522.png         \n",
      " extracting: emoji/1f523.png         \n",
      " extracting: emoji/1f524.png         \n",
      " extracting: emoji/1f525.png         \n",
      " extracting: emoji/1f526.png         \n",
      " extracting: emoji/1f527.png         \n",
      " extracting: emoji/1f528.png         \n",
      " extracting: emoji/1f529.png         \n",
      " extracting: emoji/1f52a.png         \n",
      " extracting: emoji/1f52b.png         \n",
      " extracting: emoji/1f52c.png         \n",
      " extracting: emoji/1f52d.png         \n",
      " extracting: emoji/1f52e.png         \n",
      " extracting: emoji/1f52f.png         \n",
      " extracting: emoji/1f530.png         \n",
      " extracting: emoji/1f531.png         \n",
      " extracting: emoji/1f532.png         \n",
      " extracting: emoji/1f533.png         \n",
      " extracting: emoji/1f534.png         \n",
      " extracting: emoji/1f535.png         \n",
      " extracting: emoji/1f536.png         \n",
      " extracting: emoji/1f537.png         \n",
      " extracting: emoji/1f538.png         \n",
      " extracting: emoji/1f539.png         \n",
      " extracting: emoji/1f53a.png         \n",
      " extracting: emoji/1f53b.png         \n",
      " extracting: emoji/1f53c.png         \n",
      " extracting: emoji/1f53d.png         \n",
      " extracting: emoji/1f550.png         \n",
      " extracting: emoji/1f551.png         \n",
      " extracting: emoji/1f552.png         \n",
      " extracting: emoji/1f553.png         \n",
      " extracting: emoji/1f554.png         \n",
      " extracting: emoji/1f555.png         \n",
      " extracting: emoji/1f556.png         \n",
      " extracting: emoji/1f557.png         \n",
      " extracting: emoji/1f558.png         \n",
      " extracting: emoji/1f559.png         \n",
      " extracting: emoji/1f55a.png         \n",
      " extracting: emoji/1f55b.png         \n",
      " extracting: emoji/1f55c.png         \n",
      " extracting: emoji/1f55d.png         \n",
      " extracting: emoji/1f55e.png         \n",
      " extracting: emoji/1f55f.png         \n",
      " extracting: emoji/1f560.png         \n",
      " extracting: emoji/1f561.png         \n",
      " extracting: emoji/1f562.png         \n",
      " extracting: emoji/1f563.png         \n",
      " extracting: emoji/1f564.png         \n",
      " extracting: emoji/1f565.png         \n",
      " extracting: emoji/1f566.png         \n",
      " extracting: emoji/1f567.png         \n",
      " extracting: emoji/1f5fb.png         \n",
      " extracting: emoji/1f5fc.png         \n",
      " extracting: emoji/1f5fd.png         \n",
      " extracting: emoji/1f5fe.png         \n",
      " extracting: emoji/1f5ff.png         \n",
      " extracting: emoji/1f600.png         \n",
      " extracting: emoji/1f601.png         \n",
      " extracting: emoji/1f602.png         \n",
      " extracting: emoji/1f603.png         \n",
      " extracting: emoji/1f604.png         \n",
      " extracting: emoji/1f605.png         \n",
      " extracting: emoji/1f606.png         \n",
      " extracting: emoji/1f607.png         \n",
      " extracting: emoji/1f608.png         \n",
      " extracting: emoji/1f609.png         \n",
      " extracting: emoji/1f60a.png         \n",
      " extracting: emoji/1f60b.png         \n",
      " extracting: emoji/1f60c.png         \n",
      " extracting: emoji/1f60d.png         \n",
      " extracting: emoji/1f60e.png         \n",
      " extracting: emoji/1f60f.png         \n",
      " extracting: emoji/1f610.png         \n",
      " extracting: emoji/1f611.png         \n",
      " extracting: emoji/1f612.png         \n",
      " extracting: emoji/1f613.png         \n",
      " extracting: emoji/1f614.png         \n",
      " extracting: emoji/1f615.png         \n",
      " extracting: emoji/1f616.png         \n",
      " extracting: emoji/1f617.png         \n",
      " extracting: emoji/1f618.png         \n",
      " extracting: emoji/1f619.png         \n",
      " extracting: emoji/1f61a.png         \n",
      " extracting: emoji/1f61b.png         \n",
      " extracting: emoji/1f61c.png         \n",
      " extracting: emoji/1f61d.png         \n",
      " extracting: emoji/1f61e.png         \n",
      " extracting: emoji/1f61f.png         \n",
      " extracting: emoji/1f620.png         \n",
      " extracting: emoji/1f621.png         \n",
      " extracting: emoji/1f622.png         \n",
      " extracting: emoji/1f623.png         \n",
      " extracting: emoji/1f624.png         \n",
      " extracting: emoji/1f625.png         \n",
      " extracting: emoji/1f626.png         \n",
      " extracting: emoji/1f627.png         \n",
      " extracting: emoji/1f628.png         \n",
      " extracting: emoji/1f629.png         \n",
      " extracting: emoji/1f62a.png         \n",
      " extracting: emoji/1f62b.png         \n",
      " extracting: emoji/1f62c.png         \n",
      " extracting: emoji/1f62d.png         \n",
      " extracting: emoji/1f62e.png         \n",
      " extracting: emoji/1f62f.png         \n",
      " extracting: emoji/1f630.png         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " extracting: emoji/1f631.png         \n",
      " extracting: emoji/1f632.png         \n",
      " extracting: emoji/1f633.png         \n",
      " extracting: emoji/1f634.png         \n",
      " extracting: emoji/1f635.png         \n",
      " extracting: emoji/1f636.png         \n",
      " extracting: emoji/1f637.png         \n",
      " extracting: emoji/1f638.png         \n",
      " extracting: emoji/1f639.png         \n",
      " extracting: emoji/1f63a.png         \n",
      " extracting: emoji/1f63b.png         \n",
      " extracting: emoji/1f63c.png         \n",
      " extracting: emoji/1f63d.png         \n",
      " extracting: emoji/1f63e.png         \n",
      " extracting: emoji/1f63f.png         \n",
      " extracting: emoji/1f640.png         \n",
      " extracting: emoji/1f645.png         \n",
      " extracting: emoji/1f646.png         \n",
      " extracting: emoji/1f647.png         \n",
      " extracting: emoji/1f648.png         \n",
      " extracting: emoji/1f649.png         \n",
      " extracting: emoji/1f64a.png         \n",
      " extracting: emoji/1f64b.png         \n",
      " extracting: emoji/1f64c.png         \n",
      " extracting: emoji/1f64d.png         \n",
      " extracting: emoji/1f64e.png         \n",
      " extracting: emoji/1f64f.png         \n",
      " extracting: emoji/1f680.png         \n",
      " extracting: emoji/1f681.png         \n",
      " extracting: emoji/1f682.png         \n",
      " extracting: emoji/1f683.png         \n",
      " extracting: emoji/1f684.png         \n",
      " extracting: emoji/1f685.png         \n",
      " extracting: emoji/1f686.png         \n",
      " extracting: emoji/1f687.png         \n",
      " extracting: emoji/1f688.png         \n",
      " extracting: emoji/1f689.png         \n",
      " extracting: emoji/1f68a.png         \n",
      " extracting: emoji/1f68b.png         \n",
      " extracting: emoji/1f68c.png         \n",
      " extracting: emoji/1f68d.png         \n",
      " extracting: emoji/1f68e.png         \n",
      " extracting: emoji/1f68f.png         \n",
      " extracting: emoji/1f690.png         \n",
      " extracting: emoji/1f691.png         \n",
      " extracting: emoji/1f692.png         \n",
      " extracting: emoji/1f693.png         \n",
      " extracting: emoji/1f694.png         \n",
      " extracting: emoji/1f695.png         \n",
      " extracting: emoji/1f696.png         \n",
      " extracting: emoji/1f697.png         \n",
      " extracting: emoji/1f698.png         \n",
      " extracting: emoji/1f699.png         \n",
      " extracting: emoji/1f69a.png         \n",
      " extracting: emoji/1f69b.png         \n",
      " extracting: emoji/1f69c.png         \n",
      " extracting: emoji/1f69d.png         \n",
      " extracting: emoji/1f69e.png         \n",
      " extracting: emoji/1f69f.png         \n",
      " extracting: emoji/1f6a0.png         \n",
      " extracting: emoji/1f6a1.png         \n",
      " extracting: emoji/1f6a2.png         \n",
      " extracting: emoji/1f6a3.png         \n",
      " extracting: emoji/1f6a4.png         \n",
      " extracting: emoji/1f6a5.png         \n",
      " extracting: emoji/1f6a6.png         \n",
      " extracting: emoji/1f6a7.png         \n",
      " extracting: emoji/1f6a8.png         \n",
      " extracting: emoji/1f6a9.png         \n",
      " extracting: emoji/1f6aa.png         \n",
      " extracting: emoji/1f6ab.png         \n",
      " extracting: emoji/1f6ac.png         \n",
      " extracting: emoji/1f6ad.png         \n",
      " extracting: emoji/1f6ae.png         \n",
      " extracting: emoji/1f6af.png         \n",
      " extracting: emoji/1f6b0.png         \n",
      " extracting: emoji/1f6b1.png         \n",
      " extracting: emoji/1f6b2.png         \n",
      " extracting: emoji/1f6b3.png         \n",
      " extracting: emoji/1f6b4.png         \n",
      " extracting: emoji/1f6b5.png         \n",
      " extracting: emoji/1f6b6.png         \n",
      " extracting: emoji/1f6b7.png         \n",
      " extracting: emoji/1f6b8.png         \n",
      " extracting: emoji/1f6b9.png         \n",
      " extracting: emoji/1f6ba.png         \n",
      " extracting: emoji/1f6bb.png         \n",
      " extracting: emoji/1f6bc.png         \n",
      " extracting: emoji/1f6bd.png         \n",
      " extracting: emoji/1f6be.png         \n",
      " extracting: emoji/1f6bf.png         \n",
      " extracting: emoji/1f6c0.png         \n",
      " extracting: emoji/1f6c1.png         \n",
      " extracting: emoji/1f6c2.png         \n",
      " extracting: emoji/1f6c3.png         \n",
      " extracting: emoji/1f6c4.png         \n",
      " extracting: emoji/1f6c5.png         \n",
      " extracting: emoji/203c.png          \n",
      " extracting: emoji/2049.png          \n",
      " extracting: emoji/20e3.png          \n",
      " extracting: emoji/2122.png          \n",
      " extracting: emoji/2139.png          \n",
      " extracting: emoji/2194.png          \n",
      " extracting: emoji/2195.png          \n",
      " extracting: emoji/2196.png          \n",
      " extracting: emoji/2197.png          \n",
      " extracting: emoji/2198.png          \n",
      " extracting: emoji/2199.png          \n",
      " extracting: emoji/21a9.png          \n",
      " extracting: emoji/21aa.png          \n",
      " extracting: emoji/231a.png          \n",
      " extracting: emoji/231b.png          \n",
      " extracting: emoji/23e9.png          \n",
      " extracting: emoji/23ea.png          \n",
      " extracting: emoji/23eb.png          \n",
      " extracting: emoji/23ec.png          \n",
      " extracting: emoji/23f0.png          \n",
      " extracting: emoji/23f3.png          \n",
      " extracting: emoji/24c2.png          \n",
      " extracting: emoji/25aa.png          \n",
      " extracting: emoji/25ab.png          \n",
      " extracting: emoji/25b6.png          \n",
      " extracting: emoji/25c0.png          \n",
      " extracting: emoji/25fb.png          \n",
      " extracting: emoji/25fc.png          \n",
      " extracting: emoji/25fd.png          \n",
      " extracting: emoji/25fe.png          \n",
      " extracting: emoji/2600.png          \n",
      " extracting: emoji/2601.png          \n",
      " extracting: emoji/260e.png          \n",
      " extracting: emoji/2611.png          \n",
      " extracting: emoji/2614.png          \n",
      " extracting: emoji/2615.png          \n",
      " extracting: emoji/261d.png          \n",
      " extracting: emoji/263a.png          \n",
      " extracting: emoji/2648.png          \n",
      " extracting: emoji/2649.png          \n",
      " extracting: emoji/264a.png          \n",
      " extracting: emoji/264b.png          \n",
      " extracting: emoji/264c.png          \n",
      " extracting: emoji/264d.png          \n",
      " extracting: emoji/264e.png          \n",
      " extracting: emoji/264f.png          \n",
      " extracting: emoji/2650.png          \n",
      " extracting: emoji/2651.png          \n",
      " extracting: emoji/2652.png          \n",
      " extracting: emoji/2653.png          \n",
      " extracting: emoji/2660.png          \n",
      " extracting: emoji/2663.png          \n",
      " extracting: emoji/2665.png          \n",
      " extracting: emoji/2666.png          \n",
      " extracting: emoji/2668.png          \n",
      " extracting: emoji/267b.png          \n",
      " extracting: emoji/267f.png          \n",
      " extracting: emoji/2693.png          \n",
      " extracting: emoji/26a0.png          \n",
      " extracting: emoji/26a1.png          \n",
      " extracting: emoji/26aa.png          \n",
      " extracting: emoji/26ab.png          \n",
      " extracting: emoji/26bd.png          \n",
      " extracting: emoji/26be.png          \n",
      " extracting: emoji/26c4.png          \n",
      " extracting: emoji/26c5.png          \n",
      " extracting: emoji/26ce.png          \n",
      " extracting: emoji/26d4.png          \n",
      " extracting: emoji/26ea.png          \n",
      " extracting: emoji/26f2.png          \n",
      " extracting: emoji/26f3.png          \n",
      " extracting: emoji/26f5.png          \n",
      " extracting: emoji/26fa.png          \n",
      " extracting: emoji/26fd.png          \n",
      " extracting: emoji/2702.png          \n",
      " extracting: emoji/2705.png          \n",
      " extracting: emoji/2708.png          \n",
      " extracting: emoji/2709.png          \n",
      " extracting: emoji/270a.png          \n",
      " extracting: emoji/270b.png          \n",
      " extracting: emoji/270c.png          \n",
      " extracting: emoji/270f.png          \n",
      " extracting: emoji/2712.png          \n",
      " extracting: emoji/2714.png          \n",
      " extracting: emoji/2716.png          \n",
      " extracting: emoji/2728.png          \n",
      " extracting: emoji/2733.png          \n",
      " extracting: emoji/2734.png          \n",
      " extracting: emoji/2744.png          \n",
      " extracting: emoji/2747.png          \n",
      " extracting: emoji/274c.png          \n",
      " extracting: emoji/274e.png          \n",
      " extracting: emoji/2753.png          \n",
      " extracting: emoji/2754.png          \n",
      " extracting: emoji/2755.png          \n",
      " extracting: emoji/2757.png          \n",
      " extracting: emoji/2764.png          \n",
      " extracting: emoji/2795.png          \n",
      " extracting: emoji/2796.png          \n",
      " extracting: emoji/2797.png          \n",
      " extracting: emoji/27a1.png          \n",
      " extracting: emoji/27b0.png          \n",
      " extracting: emoji/27bf.png          \n",
      " extracting: emoji/2934.png          \n",
      " extracting: emoji/2935.png          \n",
      " extracting: emoji/2b05.png          \n",
      " extracting: emoji/2b06.png          \n",
      " extracting: emoji/2b07.png          \n",
      " extracting: emoji/2b1b.png          \n",
      " extracting: emoji/2b1c.png          \n",
      " extracting: emoji/2b50.png          \n",
      " extracting: emoji/2b55.png          \n",
      " extracting: emoji/3030.png          \n",
      " extracting: emoji/303d.png          \n",
      " extracting: emoji/3297.png          \n",
      " extracting: emoji/3299.png          \n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/LambdaSchool/DS-Unit-4-Sprint-4-Deep-Learning/raw/master/module3-generative-adversarial-networks/emoji.zip\n",
    "!unzip emoji.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "THt33z4SbBQ3"
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import color\n",
    "\n",
    "example_emoji = imageio.imread('emoji/1f683.png')\n",
    "grayscale_emoji = color.rgb2gray(example_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "p10_F1XEeRmc",
    "outputId": "c7126430-0e09-4880-b889-1a9292bda21e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_emoji.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "vE49epWUetuF",
    "outputId": "4fb62854-ce68-45f7-bcd2-d9886cc3a558"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(example_emoji);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vRrcs3ihiFXo",
    "outputId": "d0161cb5-a0cd-425e-e2e2-ef32dd78a49e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grayscale_emoji.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "VTytktY0iIyX",
    "outputId": "6951a4d9-f125-4332-8410-d7ed5a6c181f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(grayscale_emoji, cmap=\"gray\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AC_HfDXYhs-_"
   },
   "source": [
    "**Your goal** - *train a GAN that makes new emoji!*\n",
    "\n",
    "The good news - the data is naturally 28x28, which is the same size as the earlier example (resulting in an input layer size of $28 \\times 28=784$). It's big enough to kinda look like a thing, but small enough to be feasible to train with limited resources.\n",
    "\n",
    "The bad news - the emoji are 4 layer PNGs (RGBA), and grayscale conversion is inconsistent at best (the above looks pretty good, but experiment and you'll see). It's OK to convert to grayscale and train that way to start (since it'll pretty much drop in to the example code with minimal modification), but you may want to see if you can figure out handling all 4 layers of the input image (basically - growing the dimensionality of the data).\n",
    "\n",
    "The worse news - this dataset may not be large enough to get the same quality of results as MNIST. The resources/stretch goals section links to additional sources, so feel free to get creative (and practice your scraping/ingest skills) - but, it is suggested to do so only *after* working some with this as a starting point.\n",
    "\n",
    "*Hint* - the main challenge in getting an MVP running will just be loading and converting all the images. [os.listdir](https://docs.python.org/3.7/library/os.html#os.listdir) plus a loop, and refactoring the image processing code into a function, should go a long way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm  # performance timing\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Building on Keras\n",
    "from keras.layers import Input\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.optimizers import Adam\n",
    "from keras import initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[255, 255, 255, ..., 255, 255, 255],\n",
       "        [ 64, 192, 231, ...,  64, 192, 231],\n",
       "        [ 64, 192, 231, ...,  64, 192, 231],\n",
       "        ...,\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        [ 64, 192, 231, ...,  64, 192, 231]], dtype=uint8), None, None, None)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(101)\n",
    "data_dim = 3      # number of dimensions of data for each pixel\n",
    "random_dim = 147  # This value must be divisible by data_dim.\n",
    "                  # If not, we will split the last pixel's dimensions\n",
    "                  # between two sets of data that we feed to the network.\n",
    "                  # We should also be able to integer divide the number of values\n",
    "                  # in each emoji by this number. We found this number to work.\n",
    "\n",
    "def load_emoji_data():\n",
    "    emojis = []\n",
    "    for filename in os.listdir('./emoji'):\n",
    "        im = Image.open('./emoji/' + filename)\n",
    "        pixels = np.delete(np.array(im), data_dim, axis=2)\n",
    "        emojis.append(pixels)\n",
    "    return np.reshape(np.array(emojis), (861, 784 * data_dim)), None, None, None\n",
    "#     # load the data - we'll use Fashion MNIST, for a change of pace\n",
    "#     (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "#     # normalize our inputs to be in the range[-1, 1] \n",
    "#     x_train = (x_train.astype(np.float32) - 127.5)/127.5\n",
    "#     # convert x_train with a shape of (60000, 28, 28) to (60000, 784) so we have\n",
    "#     # 784 columns per row\n",
    "#     x_train = x_train.reshape(60000, 784)\n",
    "#     return (x_train, y_train, x_test, y_test)\n",
    "\n",
    "load_emoji_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discriminator(optimizer):\n",
    "    discriminator = Sequential()\n",
    "    discriminator.add(Dense(\n",
    "        1024, input_dim=784,\n",
    "        kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "    discriminator.add(Dropout(0.3))\n",
    " \n",
    "    discriminator.add(Dense(512))\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "    discriminator.add(Dropout(0.3))\n",
    " \n",
    "    discriminator.add(Dense(256))\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "    discriminator.add(Dropout(0.3))\n",
    " \n",
    "    discriminator.add(Dense(1, activation='sigmoid'))\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    return discriminator\n",
    "\n",
    "def get_generator(optimizer):\n",
    "    generator = Sequential()\n",
    "    generator.add(Dense(\n",
    "        256, input_dim=random_dim,\n",
    "        kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
    "    generator.add(LeakyReLU(0.2))\n",
    " \n",
    "    generator.add(Dense(512))\n",
    "    generator.add(LeakyReLU(0.2))\n",
    " \n",
    "    generator.add(Dense(1024))\n",
    "    generator.add(LeakyReLU(0.2))\n",
    " \n",
    "    generator.add(Dense(784, activation='tanh'))\n",
    "    generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    return generator\n",
    "\n",
    "def get_gan_network(discriminator, random_dim, generator, optimizer):\n",
    "    # We initially set trainable to False since we only want to train either the \n",
    "    # generator or discriminator at a time\n",
    "    discriminator.trainable = False\n",
    "    # gan input (noise) will be 100-dimensional vectors\n",
    "    gan_input = Input(shape=(random_dim,))\n",
    "    # the output of the generator (an image)\n",
    "    x = generator(gan_input)\n",
    "    # get the output of the discriminator (probability if the image is real/not)\n",
    "    gan_output = discriminator(x)\n",
    "    gan = Model(inputs=gan_input, outputs=gan_output)\n",
    "    gan.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    return gan\n",
    "\n",
    "def plot_generated_images(epoch, generator, examples=100, dim=(10, 10),\n",
    "                          figsize=(10, 10)):\n",
    "    noise = np.random.normal(0, 1, size=[examples, random_dim])\n",
    "    generated_images = generator.predict(noise)\n",
    "    generated_images = generated_images.reshape(examples, 28, 28)\n",
    " \n",
    "    plt.figure(figsize=figsize)\n",
    "    for i in range(generated_images.shape[0]):\n",
    "        plt.subplot(dim[0], dim[1], i+1)\n",
    "        plt.imshow(generated_images[i], interpolation='nearest', cmap='gray_r')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('gan_generated_image_epoch_%d.png' % epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/13 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 1 ---------------\n",
      "batch_size: 64 random_dim: 147\n",
      "image_batch.shape: (64, 2352)\n",
      "i * random_dim: 0\n",
      "(i + 1) * random_dim: 147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-bd70b480cbed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mplot_generated_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-61-bd70b480cbed>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, batch_size)\u001b[0m\n\u001b[1;32m     29\u001b[0m                 X = np.concatenate([image_batch[int(i * random_dim) : \\\n\u001b[1;32m     30\u001b[0m                                                 \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrandom_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                                                 generated_images]])\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0;31m# Labels for generated and real data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": [
    "def train(epochs=1, batch_size=128):\n",
    "    # Get the training and testing data\n",
    "    x_train, _, _, _ = load_emoji_data()\n",
    "    # Split the training data into batches of size 128\n",
    "    batch_count = x_train.shape[0] // batch_size\n",
    " \n",
    "    # Build our GAN netowrk\n",
    "    adam = Adam(lr=0.0002, beta_1=0.5)\n",
    "    generator = get_generator(adam)\n",
    "    discriminator = get_discriminator(adam)\n",
    "    gan = get_gan_network(discriminator, random_dim, generator, adam)\n",
    " \n",
    "    for e in range(1, epochs+1):\n",
    "        print ('-'*15, 'Epoch %d' % e, '-'*15)\n",
    "        for _ in tqdm(range(batch_count)):\n",
    "            for i in range(x_train.shape[0] // 16 - 1):\n",
    "                # Get a random set of input noise and images\n",
    "                noise = np.random.normal(0, 1, size=[batch_size, random_dim])\n",
    "                image_batch = x_train[np.random.randint(0, random_dim,\n",
    "                                                        size=batch_size)]\n",
    "\n",
    "                print('batch_size:', batch_size, 'random_dim:', random_dim)\n",
    "                print('image_batch.shape:', image_batch.shape)\n",
    "                print('i * random_dim:', i * random_dim)\n",
    "                print('(i + 1) * random_dim:', (i + 1) * random_dim)\n",
    "\n",
    "                # Generate fake MNIST images\n",
    "                generated_images = generator.predict(noise)\n",
    "                X = np.concatenate([image_batch[int(i * random_dim) : \\\n",
    "                                                int((i + 1) * random_dim), \\\n",
    "                                                generated_images]])\n",
    "\n",
    "                # Labels for generated and real data\n",
    "                y_dis = np.zeros(2*batch_size)\n",
    "                # One-sided label smoothing\n",
    "                y_dis[:batch_size] = 0.9\n",
    "\n",
    "                # Train discriminator\n",
    "                discriminator.trainable = True\n",
    "                discriminator.train_on_batch(X, y_dis)\n",
    "\n",
    "                # Train generator\n",
    "                noise = np.random.normal(0, 1, size=[batch_size, random_dim])\n",
    "                y_gen = np.ones(batch_size)\n",
    "                discriminator.trainable = False\n",
    "                gan.train_on_batch(noise, y_gen)\n",
    " \n",
    "        if e == 1 or e % 20 == 0:\n",
    "            plot_generated_images(e, generator)\n",
    " \n",
    "train(40, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zE4a4O7Bp5x1"
   },
   "source": [
    "# Resources and Stretch Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uT3UV3gap9H6"
   },
   "source": [
    "Stretch goals\n",
    "- [emoji-data](https://github.com/iamcal/emoji-data) - more, bigger, emoji\n",
    "- [Slackmojis](https://slackmojis.com) - even more - many of them animated, which would be a significant additional challenge (probably not something for a day)\n",
    "\n",
    "Resources\n",
    "- [StyleGAN Explained](https://towardsdatascience.com/explained-a-style-based-generator-architecture-for-gans-generating-and-tuning-realistic-6cb2be0f431) - blog post describing GANs and StyleGAN in particular\n",
    "- [Implementing GANs in TensorFlow](https://blog.paperspace.com/implementing-gans-in-tensorflow/) - blog post showing TF implementation of a simple GAN\n",
    "- [Training GANs using Google Colaboratory](https://towardsdatascience.com/training-gans-using-google-colaboratory-f91d4e6f61fe) - an approach using Torch and GPU instances\n",
    "- [Gym](https://gym.openai.com) - a toolkit for reinforcement learning, another innovative ML approach\n",
    "- [deep emoji generative adversarial network](https://github.com/anoff/deep-emoji-gan) - yes, the idea of an emoji GAN has been done - so check out this extended analysis of the results\n",
    "- [DeepMoji](http://deepmoji.mit.edu) - not a GAN, but a cool application of deep learning to emoji"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_443_Generative_Adversarial_Networks.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
