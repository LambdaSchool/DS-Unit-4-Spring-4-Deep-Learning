{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_Unit_4_Sprint_Challenge_4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShreyasJothish/DS-Unit-4-Sprint-4-Deep-Learning/blob/master/LS_DS_Unit_4_Sprint_Challenge_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "xhU3R-8dzk5z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lambda School Data Science Unit 4 Sprint Challenge 4\n",
        "\n",
        "## RNNs, CNNs, AutoML, and more...\n",
        "\n",
        "In this sprint challenge, you'll explore some of the cutting edge of Data Science.\n",
        "\n",
        "*Caution* - these approaches can be pretty heavy computationally. All problems were designed so that you should be able to achieve results within at most 5-10 minutes of runtime on Colab or a comparable environment. If something is running longer, doublecheck your approach!"
      ]
    },
    {
      "metadata": {
        "id": "-5UwGRnJOmD4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 1 - RNNs\n",
        "\n",
        "Use an RNN to fit a simple classification model on tweets to distinguish from tweets from Austen Allred and tweets from Weird Al Yankovic.\n",
        "\n",
        "Following is code to scrape the needed data (no API auth needed, uses [twitterscraper](https://github.com/taspinar/twitterscraper)):"
      ]
    },
    {
      "metadata": {
        "id": "3if1yTMUoG3U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install twitterscraper"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DS-9ksWjoJit",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from twitterscraper import query_tweets\n",
        "\n",
        "# Stretch Goal - Data for Austen is very less so fetching more tweets\n",
        "austen_tweets = query_tweets('from:austen', 3000)\n",
        "len(austen_tweets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fLKqFh8DovaN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "austen_tweets[0].text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MRQeIIf1orCS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "al_tweets = query_tweets('from:AlYankovic', 1000)\n",
        "len(al_tweets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_dB7I87ty8f1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "al_tweets[0].text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0mrcjEu_zRl4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "len(austen_tweets + al_tweets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WYCVJX6ep8iO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Your tasks:\n",
        "\n",
        "- Encode the characters to a sequence of integers for the model\n",
        "- Get the data into the appropriate shape/format, including labels and a train/test split\n",
        "- Use Keras to fit a predictive model, classifying tweets as being from Austen versus Weird Al\n",
        "- Report your overall score and accuracy\n",
        "\n",
        "For reference, the [Keras IMDB sentiment classification example](https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py) will be useful, as well the RNN code we used in class.\n",
        "\n",
        "*Note* - focus on getting a running model, not on maxing accuracy with extreme data size or epoch numbers. Only revisit and push accuracy if you get everything else done!"
      ]
    },
    {
      "metadata": {
        "id": "VLFzopIHND7a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Generic imports\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_QVSlFEAqWJM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO - your code!\n",
        "\n",
        "tweets_len = []\n",
        "\n",
        "for i in range(len(austen_tweets)):\n",
        "  tweet_len = len(austen_tweets[i].text)\n",
        "  tweets_len.append(tweet_len)\n",
        "\n",
        "print('Austen tweet lengths:')\n",
        "print(f'max: {max(tweets_len)}, min: {min(tweets_len)}')\n",
        "\n",
        "tweets_len = []\n",
        "\n",
        "for i in range(len(al_tweets)):\n",
        "  tweet_len = len(al_tweets[i].text)    \n",
        "  tweets_len.append(tweet_len)\n",
        "  \n",
        "print('AlYankovic tweet lengths:')\n",
        "print(f'max: {max(tweets_len)}, min: {min(tweets_len)}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MGIReaC4H5y4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Encode the characters to a sequence of integers for the model\n",
        "def convert_to_ascii(text):\n",
        "  ascii_list = [ord(char) for char in text]\n",
        "  return ascii_list\n",
        "\n",
        "def remove_url_from_text(text):\n",
        "  return re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
        "  \n",
        "# output = convert_to_ascii('hello')\n",
        "# print(output)\n",
        "\n",
        "austen_tweets_updated = []\n",
        "\n",
        "for i in range(len(austen_tweets)):\n",
        "  text_without_url = remove_url_from_text(austen_tweets[i].text)\n",
        "  updated_text = convert_to_ascii(text_without_url)\n",
        "  austen_tweets_updated.append(updated_text)\n",
        "  \n",
        "al_tweets_updated = []\n",
        "\n",
        "for i in range(len(al_tweets)):\n",
        "  text_without_url = remove_url_from_text(al_tweets[i].text)\n",
        "  updated_text = convert_to_ascii(text_without_url)\n",
        "  al_tweets_updated.append(updated_text)\n",
        "  \n",
        "\n",
        "print('Initial tweet counts')\n",
        "print(len(austen_tweets))\n",
        "print(len(al_tweets))\n",
        "\n",
        "print('Updated tweet counts')\n",
        "print(len(austen_tweets_updated))\n",
        "print(len(al_tweets_updated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kqymgWkJLRKT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Get the data into the appropriate shape/format, including labels \n",
        "# and a train/test split\n",
        "\n",
        "combined_updated_tweets = austen_tweets_updated + al_tweets_updated\n",
        "print('Combined tweet counts')\n",
        "print(len(combined_updated_tweets))\n",
        "print(combined_updated_tweets)\n",
        "\n",
        "X = np.array(combined_updated_tweets)\n",
        "# print(X)\n",
        "\n",
        "austen_tweets_label = np.ones((len(austen_tweets_updated),), dtype=np.int)\n",
        "al_tweets_lable = np.zeros((len(al_tweets_updated),), dtype=np.int)\n",
        "\n",
        "y = np.concatenate((austen_tweets_label,al_tweets_lable), axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qcHKiO-5QgZc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Use Keras to fit a predictive model, classifying tweets as being \n",
        "# from Austen versus Weird Al\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from __future__ import print_function\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding\n",
        "from keras.layers import LSTM\n",
        "from keras.datasets import imdb\n",
        "\n",
        "max_features = 20000\n",
        "# cut texts after this number of words (among top max_features most common words)\n",
        "maxlen = 370\n",
        "batch_size = 32\n",
        "\n",
        "print('Loading data...')\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# print(x_train)\n",
        "# print(y_train)\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')\n",
        "\n",
        "print('Pad sequences (samples x time)')\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "\n",
        "print('Build model...')\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 128))\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# try using different optimizers and different optimizer configs\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print('Train...')\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=15,\n",
        "          validation_data=(x_test, y_test))\n",
        "score, acc = model.evaluate(x_test, y_test,\n",
        "                            batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qjR7uXERsQS0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Report your overall score and accuracy\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lPn6c0x21gu1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Conclusion - RNN runs, and gives pretty decent improvement over a naive \"It's Al!\" model. To *really* improve the model, more playing with parameters, and just getting more data (particularly Austen tweets), would help. Also - RNN may well not be the best approach here, but it is at least a valid one."
      ]
    },
    {
      "metadata": {
        "id": "yz0LCZd_O4IG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 2- CNNs\n",
        "\n",
        "Time to play \"find the frog!\" Use Keras and ResNet50 to detect which of the following images contain frogs:"
      ]
    },
    {
      "metadata": {
        "id": "whIqEWR236Af",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install google_images_download"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EKnnnM8k38sN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google_images_download import google_images_download\n",
        "\n",
        "response = google_images_download.googleimagesdownload()\n",
        "arguments = {\"keywords\": \"animalpond\", \"limit\": 20, \"print_urls\": True}\n",
        "absolute_image_paths = response.download(arguments)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "si5YfNqS50QU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "At time of writing at least a few do, but since the Internet changes - it is possible your 5 won't. You can easily verify yourself, and (once you have working code) increase the number of images you pull to be more sure of getting a frog. Your goal is to validly run ResNet50 on the input images - don't worry about tuning or improving the model.\n",
        "\n",
        "*Hint* - ResNet 50 doesn't just return \"frog\". The three labels it has for frogs are: `bullfrog, tree frog, tailed frog`\n",
        "\n",
        "*Stretch goal* - also check for fish."
      ]
    },
    {
      "metadata": {
        "id": "FaT07ddW3nHz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO - your code!\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
        "\n",
        "def process_img_path(img_path):\n",
        "  return image.load_img(img_path, target_size=(224, 224))\n",
        "\n",
        "def img_contains(img, findstr='frog'):\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "  x = preprocess_input(x)\n",
        "  model = ResNet50(weights='imagenet')\n",
        "  features = model.predict(x)\n",
        "  results = decode_predictions(features, top=3)[0]\n",
        "  #for entry in results:\n",
        "  #  print(f'Found {entry[1]} with prediction score {entry[2]}')\n",
        "  \n",
        "  for entry in results:\n",
        "    entry_key = entry[1]\n",
        "    if entry_key.find(findstr) != -1:\n",
        "      return entry[2]\n",
        "  \n",
        "  return 0.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tuE7mZr0YGXw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Check the download path\n",
        "absolute_image_paths"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SIpFkEZXXtZu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# For each image check the presense of frog or fish\n",
        "\n",
        "image_path_list = absolute_image_paths['animalpond']\n",
        "for i, image_path in enumerate(image_path_list):\n",
        "  print(image_path)\n",
        "  processed_image = process_img_path(image_path)\n",
        "  results = img_contains(processed_image, 'frog')\n",
        "  print(f'Prediction for frog in the picture is {results}\\n')\n",
        "  results = img_contains(processed_image, 'fish')\n",
        "  print(f'Prediction for fish in the picture is {results}\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XEuhvSu7O5Rf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 3 - AutoML\n",
        "\n",
        "Use [TPOT](https://github.com/EpistasisLab/tpot) to fit a predictive model for the King County housing data, with `price` as the target output variable."
      ]
    },
    {
      "metadata": {
        "id": "pz0e_7Ve60pM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install tpot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GflK25wp7jnf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/ryanleeallred/datasets/master/kc_house_data.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7G5-Gqyg9A-V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!head kc_house_data.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KynXZjOY8hBL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As with previous questions, your goal is to run TPOT and successfully run and report error at the end.  Also, in the interest of time, feel free to choose small `generation=1` and `population_size=10` parameters so your pipeline runs efficiently and you are able to iterate and test.\n",
        "\n",
        "*Hint* - you'll have to drop and/or type coerce at least a few variables to get things working. It's fine to err on the side of dropping to get things running, as long as you still get a valid model with reasonable predictive power."
      ]
    },
    {
      "metadata": {
        "id": "BOREO8VJO7MZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO - your code!\n",
        "import pandas as pd\n",
        "from tpot import TPOTRegressor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aaAwXpuIdaml",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('kc_house_data.csv')\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GKfj5ZPlo3lZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Feature Engineering\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df['year'] = df['date'].dt.year\n",
        "df['month'] = df['date'].dt.month\n",
        "df['day_of_month'] = df['date'].dt.day\n",
        "df['day_of_week'] = df['date'].dt.weekday"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kJzO_2ywdj1C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b5J5kbAOdqf2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = df.drop(columns=['price','date'])\n",
        "y = df['price']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IJh2egPUd3WN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, train_size=0.75, test_size=0.25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "veR2Tl17d7Pm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "tpot = TPOTRegressor(generations=1, population_size=10, verbosity=2)\n",
        "tpot.fit(X_train, y_train)\n",
        "print(tpot.score(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q-Rtlq8leKAo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_test_predict = tpot.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v4Kw09UpeOdX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "MSE = mean_squared_error(y_test, y_test_predict)\n",
        "RMSE = (np.sqrt(MSE))\n",
        "\n",
        "print('MSE is {}'.format(MSE))\n",
        "print('RMSE is {}'.format(RMSE))\n",
        "\n",
        "R2 = r2_score(y_test, y_test_predict)\n",
        "\n",
        "print('R^2 is {}'.format(R2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "626zYgjkO7Vq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 4 - More..."
      ]
    },
    {
      "metadata": {
        "id": "__lDWfcUO8oo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Answer the following questions, with a target audience of a fellow Data Scientist:\n",
        "\n",
        "- What do you consider your strongest area, as a Data Scientist?\n",
        "\n",
        "**Answer** \n",
        "\n",
        "I like experimenting and iterating to improve the overall output of a Data Science problem. This is a key to any Data Scientist to try new approaches and interpret the results. This is needed because not all problems within Data Science is the same and for each problem we need a different approach of finding the solutions.\n",
        "\n",
        "Technically I have good experience in programming based on my previous experience in the Software field and currently I love programming in python.\n",
        "\n",
        "Also another key requirement for Data Scientist is to working with cross functional teams. I have worked and even managed teams as part of my previos work experience.\n",
        "\n",
        "Finally, my I like up skilling myself regularly on new technologies within the domain. This is again a key to be successful Data Scientist.\n",
        "\n",
        "\n",
        "- What area of Data Science would you most like to learn more about, and why?\n",
        "\n",
        "**Answer**\n",
        "\n",
        "Given an oppurtunity I want to democratise Data Science and learn how to make it reach the masses. Personally I would love to learn more about use of Data Science in the field of automation. Hard problem which I like to learn and work is related to improving the efficiency of tasks and reducing the loss within any project. Example, something like AutoML which can reduce the effort needed for getting the quick estimate.\n",
        "\n",
        "I want to work on Data Science projects which have positive impact on people's lives and give them more free time. For example, something like Google Maps which has drastically improved the efficiency of travel to people.\n",
        "\n",
        "- Where do you think Data Science will be in 5 years?\n",
        "\n",
        "**Answer**\n",
        "\n",
        "I see the future of Data Science both exiting and challenging for the next few years. This is due to the fact that incubent companies with huge cash reserves in fields other than software are yet to catch up and incorporate Machine Learning or AI into their existing products.\n",
        "\n",
        "Also now we have many ecosystems of Data Science service providers or platforms evolving in parallel for example, Google, Amazon, IBM, Windows...\n",
        "\n",
        "5 years from now the companies would have gone through several cycles of product evalution based on ML/AI and the delivery would be stable with improved results. Similarly the platforms providing ML/AI solutions would be stablized and market would have few leaders.\n",
        "\n",
        "A few sentences per answer is fine - only elaborate if time allows."
      ]
    },
    {
      "metadata": {
        "id": "_Hoqe3mM_Mtc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Thank you for your hard work, and congratulations! You've learned a lot, and should proudly call yourself a Data Scientist."
      ]
    }
  ]
}