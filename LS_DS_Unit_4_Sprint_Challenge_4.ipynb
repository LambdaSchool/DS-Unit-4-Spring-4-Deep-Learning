{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 4*\n",
    "\n",
    "# Sprint Challenge\n",
    "### RNNs, CNNs, GANS, and AutoML\n",
    "\n",
    "In this Sprint Challenge, you'll explore some of the cutting edge of Data Science. *Caution* - these approaches can be pretty heavy computationally. All problems are designed to completed with 5-10 minutes of run time on most machines. If you approach takes longer, please double check your work. \n",
    "\n",
    "## Part 1 - RNNs\n",
    "\n",
    "Use an RNN to fit a classification model on tweets to distinguish from tweets from any two accounts. The following code sample illustrates how to access data from an account (no API auth needed, uses [twitterscraper](https://github.com/taspinar/twitterscraper): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T17:48:38.483469Z",
     "start_time": "2019-05-17T17:48:35.713050Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: twitterscraper in /home/superio/anaconda3/lib/python3.7/site-packages (0.9.3)\n",
      "Requirement already satisfied: requests in /home/superio/anaconda3/lib/python3.7/site-packages (from twitterscraper) (2.21.0)\n",
      "Requirement already satisfied: coala-utils~=0.5.0 in /home/superio/anaconda3/lib/python3.7/site-packages (from twitterscraper) (0.5.1)\n",
      "Requirement already satisfied: bs4 in /home/superio/anaconda3/lib/python3.7/site-packages (from twitterscraper) (0.0.1)\n",
      "Requirement already satisfied: lxml in /home/superio/anaconda3/lib/python3.7/site-packages (from twitterscraper) (4.3.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/superio/anaconda3/lib/python3.7/site-packages (from requests->twitterscraper) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/superio/anaconda3/lib/python3.7/site-packages (from requests->twitterscraper) (2019.3.9)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/superio/anaconda3/lib/python3.7/site-packages (from requests->twitterscraper) (2.8)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/superio/anaconda3/lib/python3.7/site-packages (from requests->twitterscraper) (1.24.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/superio/anaconda3/lib/python3.7/site-packages (from bs4->twitterscraper) (4.7.1)\n",
      "Requirement already satisfied: soupsieve>=1.2 in /home/superio/anaconda3/lib/python3.7/site-packages (from beautifulsoup4->bs4->twitterscraper) (1.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install twitterscraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T17:48:44.143284Z",
     "start_time": "2019-05-17T17:48:38.807795Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: queries: ['from:austen since:2006-03-21 until:2006-11-16', 'from:austen since:2006-11-16 until:2007-07-14', 'from:austen since:2007-07-14 until:2008-03-10', 'from:austen since:2008-03-10 until:2008-11-06', 'from:austen since:2008-11-06 until:2009-07-04', 'from:austen since:2009-07-04 until:2010-03-01', 'from:austen since:2010-03-01 until:2010-10-27', 'from:austen since:2010-10-27 until:2011-06-25', 'from:austen since:2011-06-25 until:2012-02-20', 'from:austen since:2012-02-20 until:2012-10-17', 'from:austen since:2012-10-17 until:2013-06-14', 'from:austen since:2013-06-14 until:2014-02-10', 'from:austen since:2014-02-10 until:2014-10-08', 'from:austen since:2014-10-08 until:2015-06-05', 'from:austen since:2015-06-05 until:2016-01-31', 'from:austen since:2016-01-31 until:2016-09-28', 'from:austen since:2016-09-28 until:2017-05-26', 'from:austen since:2017-05-26 until:2018-01-21', 'from:austen since:2018-01-21 until:2018-09-18', 'from:austen since:2018-09-18 until:2019-05-17']\n",
      "INFO: Querying from:austen since:2006-03-21 until:2006-11-16\n",
      "INFO: Querying from:austen since:2006-11-16 until:2007-07-14\n",
      "INFO: Querying from:austen since:2010-03-01 until:2010-10-27\n",
      "INFO: Querying from:austen since:2009-07-04 until:2010-03-01\n",
      "INFO: Querying from:austen since:2008-03-10 until:2008-11-06\n",
      "INFO: Querying from:austen since:2008-11-06 until:2009-07-04\n",
      "INFO: Querying from:austen since:2016-09-28 until:2017-05-26\n",
      "INFO: Querying from:austen since:2012-10-17 until:2013-06-14\n",
      "INFO: Querying from:austen since:2013-06-14 until:2014-02-10\n",
      "INFO: Querying from:austen since:2007-07-14 until:2008-03-10\n",
      "INFO: Querying from:austen since:2011-06-25 until:2012-02-20\n",
      "INFO: Querying from:austen since:2016-01-31 until:2016-09-28\n",
      "INFO: Querying from:austen since:2014-02-10 until:2014-10-08\n",
      "INFO: Querying from:austen since:2015-06-05 until:2016-01-31\n",
      "INFO: Querying from:austen since:2018-01-21 until:2018-09-18\n",
      "INFO: Querying from:austen since:2017-05-26 until:2018-01-21\n",
      "INFO: Querying from:austen since:2014-10-08 until:2015-06-05\n",
      "INFO: Querying from:austen since:2010-10-27 until:2011-06-25\n",
      "INFO: Querying from:austen since:2018-09-18 until:2019-05-17\n",
      "INFO: Querying from:austen since:2012-02-20 until:2012-10-17\n",
      "INFO: Got 0 tweets for from%3Aausten%20since%3A2010-10-27%20until%3A2011-06-25.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 1 tweets for from%3Aausten%20since%3A2012-02-20%20until%3A2012-10-17.\n",
      "INFO: Got 1 tweets (1 new).\n",
      "INFO: Got 0 tweets for from%3Aausten%20since%3A2016-09-28%20until%3A2017-05-26.\n",
      "INFO: Got 1 tweets (0 new).\n",
      "INFO: Got 0 tweets for from%3Aausten%20since%3A2012-10-17%20until%3A2013-06-14.\n",
      "INFO: Got 1 tweets (0 new).\n",
      "INFO: Got 0 tweets for from%3Aausten%20since%3A2006-03-21%20until%3A2006-11-16.\n",
      "INFO: Got 1 tweets (0 new).\n",
      "INFO: Got 0 tweets for from%3Aausten%20since%3A2011-06-25%20until%3A2012-02-20.\n",
      "INFO: Got 1 tweets (0 new).\n",
      "INFO: Got 0 tweets for from%3Aausten%20since%3A2008-03-10%20until%3A2008-11-06.\n",
      "INFO: Got 1 tweets (0 new).\n",
      "INFO: Got 0 tweets for from%3Aausten%20since%3A2016-01-31%20until%3A2016-09-28.\n",
      "INFO: Got 1 tweets (0 new).\n",
      "INFO: Got 0 tweets for from%3Aausten%20since%3A2014-10-08%20until%3A2015-06-05.\n",
      "INFO: Got 0 tweets for from%3Aausten%20since%3A2014-02-10%20until%3A2014-10-08.\n",
      "INFO: Got 1 tweets (0 new).\n",
      "INFO: Got 1 tweets (0 new).\n",
      "INFO: Got 0 tweets for from%3Aausten%20since%3A2007-07-14%20until%3A2008-03-10.\n",
      "INFO: Got 0 tweets for from%3Aausten%20since%3A2013-06-14%20until%3A2014-02-10.\n",
      "INFO: Got 1 tweets (0 new).\n",
      "INFO: Got 0 tweets for from%3Aausten%20since%3A2010-03-01%20until%3A2010-10-27.\n",
      "INFO: Got 1 tweets (0 new).\n",
      "INFO: Got 1 tweets (0 new).\n",
      "INFO: Got 0 tweets for from%3Aausten%20since%3A2015-06-05%20until%3A2016-01-31.\n",
      "INFO: Got 1 tweets (0 new).\n",
      "INFO: Got 0 tweets for from%3Aausten%20since%3A2006-11-16%20until%3A2007-07-14.\n",
      "INFO: Got 1 tweets (0 new).\n",
      "INFO: Got 0 tweets for from%3Aausten%20since%3A2008-11-06%20until%3A2009-07-04.\n",
      "INFO: Got 1 tweets (0 new).\n",
      "INFO: Got 0 tweets for from%3Aausten%20since%3A2009-07-04%20until%3A2010-03-01.\n",
      "INFO: Got 1 tweets (0 new).\n",
      "INFO: Got 60 tweets for from%3Aausten%20since%3A2018-09-18%20until%3A2019-05-17.\n",
      "INFO: Got 61 tweets (60 new).\n",
      "INFO: Got 60 tweets for from%3Aausten%20since%3A2018-01-21%20until%3A2018-09-18.\n",
      "INFO: Got 121 tweets (60 new).\n",
      "INFO: Got 60 tweets for from%3Aausten%20since%3A2017-05-26%20until%3A2018-01-21.\n",
      "INFO: Got 181 tweets (60 new).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "181"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from twitterscraper import query_tweets\n",
    "\n",
    "austen_tweets = query_tweets('from:austen', 1000)\n",
    "len(austen_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T17:48:45.219278Z",
     "start_time": "2019-05-17T17:48:45.213196Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love love love working with great people.pic.twitter.com/fCKOm6Vl'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "austen_tweets[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your Tasks:\n",
    "* Select two twitter accounts to gather data from\n",
    "* Use twitterscraper to get ~1,000 tweets from each account\n",
    "* Encode the characters to a sequence of integers for the model\n",
    "* Get the data into the appropriate shape/format, including labels and a train/test split\n",
    "* Use Keras to fit a predictive model, classying tweets as being from one acount or the other\n",
    "* Report your overall score and accuracy\n",
    "\n",
    "For reference, the [Keras IMDB classification example](https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py) will be useful, as well as the RNN code we used in class.\n",
    "\n",
    "Note - focus on getting a running model, not on making accuracy with extreme data size or epoch numbers. Fit a baseline model based on tweet text. Only revisit and push accuracy or incorporate additional features if you get everything else done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T17:48:56.128660Z",
     "start_time": "2019-05-17T17:48:47.908898Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: queries: ['from:elonmusk since:2006-03-21 until:2006-11-16', 'from:elonmusk since:2006-11-16 until:2007-07-14', 'from:elonmusk since:2007-07-14 until:2008-03-10', 'from:elonmusk since:2008-03-10 until:2008-11-06', 'from:elonmusk since:2008-11-06 until:2009-07-04', 'from:elonmusk since:2009-07-04 until:2010-03-01', 'from:elonmusk since:2010-03-01 until:2010-10-27', 'from:elonmusk since:2010-10-27 until:2011-06-25', 'from:elonmusk since:2011-06-25 until:2012-02-20', 'from:elonmusk since:2012-02-20 until:2012-10-17', 'from:elonmusk since:2012-10-17 until:2013-06-14', 'from:elonmusk since:2013-06-14 until:2014-02-10', 'from:elonmusk since:2014-02-10 until:2014-10-08', 'from:elonmusk since:2014-10-08 until:2015-06-05', 'from:elonmusk since:2015-06-05 until:2016-01-31', 'from:elonmusk since:2016-01-31 until:2016-09-28', 'from:elonmusk since:2016-09-28 until:2017-05-26', 'from:elonmusk since:2017-05-26 until:2018-01-21', 'from:elonmusk since:2018-01-21 until:2018-09-18', 'from:elonmusk since:2018-09-18 until:2019-05-17']\n",
      "INFO: Querying from:elonmusk since:2006-03-21 until:2006-11-16\n",
      "INFO: Querying from:elonmusk since:2008-03-10 until:2008-11-06\n",
      "INFO: Querying from:elonmusk since:2008-11-06 until:2009-07-04\n",
      "INFO: Querying from:elonmusk since:2010-03-01 until:2010-10-27\n",
      "INFO: Querying from:elonmusk since:2007-07-14 until:2008-03-10\n",
      "INFO: Querying from:elonmusk since:2014-10-08 until:2015-06-05\n",
      "INFO: Querying from:elonmusk since:2010-10-27 until:2011-06-25\n",
      "INFO: Querying from:elonmusk since:2006-11-16 until:2007-07-14\n",
      "INFO: Querying from:elonmusk since:2009-07-04 until:2010-03-01\n",
      "INFO: Querying from:elonmusk since:2012-10-17 until:2013-06-14\n",
      "INFO: Querying from:elonmusk since:2011-06-25 until:2012-02-20\n",
      "INFO: Querying from:elonmusk since:2013-06-14 until:2014-02-10\n",
      "INFO: Querying from:elonmusk since:2015-06-05 until:2016-01-31\n",
      "INFO: Querying from:elonmusk since:2018-09-18 until:2019-05-17\n",
      "INFO: Querying from:elonmusk since:2012-02-20 until:2012-10-17\n",
      "INFO: Querying from:elonmusk since:2017-05-26 until:2018-01-21\n",
      "INFO: Querying from:elonmusk since:2014-02-10 until:2014-10-08\n",
      "INFO: Querying from:elonmusk since:2018-01-21 until:2018-09-18\n",
      "INFO: Querying from:elonmusk since:2016-09-28 until:2017-05-26\n",
      "INFO: Querying from:elonmusk since:2016-01-31 until:2016-09-28\n",
      "INFO: Got 0 tweets for from%3Aelonmusk%20since%3A2006-03-21%20until%3A2006-11-16.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets for from%3Aelonmusk%20since%3A2008-03-10%20until%3A2008-11-06.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets for from%3Aelonmusk%20since%3A2006-11-16%20until%3A2007-07-14.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets for from%3Aelonmusk%20since%3A2010-10-27%20until%3A2011-06-25.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets for from%3Aelonmusk%20since%3A2008-11-06%20until%3A2009-07-04.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets for from%3Aelonmusk%20since%3A2007-07-14%20until%3A2008-03-10.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets for from%3Aelonmusk%20since%3A2009-07-04%20until%3A2010-03-01.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 1 tweets for from%3Aelonmusk%20since%3A2010-03-01%20until%3A2010-10-27.\n",
      "INFO: Got 1 tweets (1 new).\n",
      "INFO: Got 60 tweets for from%3Aelonmusk%20since%3A2014-02-10%20until%3A2014-10-08.\n",
      "INFO: Got 61 tweets (60 new).\n",
      "INFO: Got 60 tweets for from%3Aelonmusk%20since%3A2013-06-14%20until%3A2014-02-10.\n",
      "INFO: Got 121 tweets (60 new).\n",
      "INFO: Got 60 tweets for from%3Aelonmusk%20since%3A2016-09-28%20until%3A2017-05-26.\n",
      "INFO: Got 181 tweets (60 new).\n",
      "INFO: Got 60 tweets for from%3Aelonmusk%20since%3A2012-10-17%20until%3A2013-06-14.\n",
      "INFO: Got 241 tweets (60 new).\n",
      "INFO: Got 60 tweets for from%3Aelonmusk%20since%3A2016-01-31%20until%3A2016-09-28.\n",
      "INFO: Got 301 tweets (60 new).\n",
      "INFO: Got 60 tweets for from%3Aelonmusk%20since%3A2015-06-05%20until%3A2016-01-31.\n",
      "INFO: Got 361 tweets (60 new).\n",
      "INFO: Got 60 tweets for from%3Aelonmusk%20since%3A2014-10-08%20until%3A2015-06-05.\n",
      "INFO: Got 60 tweets for from%3Aelonmusk%20since%3A2012-02-20%20until%3A2012-10-17.\n",
      "INFO: Got 421 tweets (60 new).\n",
      "INFO: Got 481 tweets (60 new).\n",
      "INFO: Got 60 tweets for from%3Aelonmusk%20since%3A2018-01-21%20until%3A2018-09-18.\n",
      "INFO: Got 541 tweets (60 new).\n",
      "INFO: Got 60 tweets for from%3Aelonmusk%20since%3A2011-06-25%20until%3A2012-02-20.\n",
      "INFO: Got 60 tweets for from%3Aelonmusk%20since%3A2018-09-18%20until%3A2019-05-17.\n",
      "INFO: Got 601 tweets (60 new).\n",
      "INFO: Got 661 tweets (60 new).\n",
      "INFO: Got 60 tweets for from%3Aelonmusk%20since%3A2017-05-26%20until%3A2018-01-21.\n",
      "INFO: Got 721 tweets (60 new).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "721"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elon_tweets = query_tweets('from:elonmusk', 1000)\n",
    "len(elon_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T17:48:56.459136Z",
     "start_time": "2019-05-17T17:48:56.445755Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love love love working with great people.pic.twitter.com/fCKOm6Vl\n",
      "Today for all-hands we watched a video of Aaron, a 47-year-old military chaplain now turned software engineer.\n",
      "\n",
      "The company that hired him less than a year ago has now hired 8 more Lambda School students.\n",
      "Other people who were creating wealth. And we exchange wealth with trade.\n",
      "Sounds like by definition it's not a relative measure then\n",
      "Is the world more wealthy than it was 10 million years ago? Obviously, yes.\n",
      "\n",
      "How is that possible?\n",
      "I'd love to see data around what robocalling has done to phone pickups generally. \n",
      "\n",
      "I just never answer the phone anymore. Telemarketing must be getting hammered.\n",
      "The tweet wasn’t about Lambda School\n",
      "You’re assuming I’m talking about myself\n",
      "This ^\n",
      "False\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Please ignore prior tweets, as that was someone pretending to be me :)  This is actually me.\n",
      "I love the Internet. Comments had me literally ROFL. No, it wasn't intentional. Glad I didn't mention the other letter!\n",
      "About time to unveil the D and something elsepic.twitter.com/qp23yi59i6\n",
      "“@TheDailyShow: The House Science, Space and Technology Committee hearing on global warming. http://on.cc.com/1vcjuzt ”\n",
      "@TalulahRiley Good suggestion :)\n",
      "Calendar app w tap to nav & traffic predictor in Tesla V6.0 release will radically improve how the car adapts to the owner over time\n",
      "Would also like to congratulate @Boeing, fellow winner of the @NASA commercial crew program\n",
      "This is the Crew Dragon spacecraft design that we unveiled earlier this year:http://www.youtube.com/watch?v=yEQrmDoIRO8 …\n",
      "Deeply honored and appreciative of the trust that @NASA has placed in @SpaceX for the future of human spaceflight\n",
      "Official Gigafactory address to be: Electric Avenue, McCarran, Nevada\n"
     ]
    }
   ],
   "source": [
    "for i, j in enumerate(austen_tweets):\n",
    "    if i < 10:\n",
    "        print(austen_tweets[i].text)\n",
    "\n",
    "print(\"-\"*100)\n",
    "for i, j in enumerate(elon_tweets):\n",
    "    if i < 10:\n",
    "        print(elon_tweets[i].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T17:49:29.435486Z",
     "start_time": "2019-05-17T17:49:29.395793Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181 721\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[[117, 115, 25, 79, 45, 94, 123, 115, 13, 94, 79, 90, 90, 46, 61, 79, 40, 25, 93, 94, 63, 32, 94, 63, 79, 110, 102, 61, 32, 25, 94, 79, 94, 16, 122, 25, 32, 115, 94, 115, 123, 94, 24, 79, 13, 115, 40, 30, 94, 79, 94, 109, 42, 46, 45, 32, 79, 13, 46, 115, 90, 25, 94, 38, 122, 90, 122, 110, 79, 13, 45, 94, 102, 61, 79, 116, 90, 79, 122, 40, 94, 40, 115, 63, 94, 110, 12, 13, 40, 32, 25, 94, 93, 115, 123, 110, 63, 79, 13, 32, 94, 32, 40, 21, 122, 40, 32, 32, 13, 22, 52, 52, 117, 61, 32, 94, 102, 115, 38, 116, 79, 40, 45, 94, 110, 61, 79, 110, 94, 61, 122, 13, 32, 25, 94, 61, 122, 38, 94, 90, 32, 93, 93, 94, 110, 61, 79, 40, 94, 79, 94, 45, 32, 79, 13, 94, 79, 21, 115, 94, 61, 79, 93, 94, 40, 115, 63, 94, 61, 122, 13, 32, 25, 94, 68, 94, 38, 115, 13, 32, 94, 15, 79, 38, 57, 25, 79, 94, 35, 102, 61, 115, 115, 90, 94, 93, 110, 12, 25, 32, 40, 110, 93, 22], [8, 110, 61, 32, 13, 94, 116, 32, 115, 116, 90, 32, 94, 63, 61, 115, 94, 63, 32, 13, 32, 94, 102, 13, 32, 79, 110, 122, 40, 21, 94, 63, 32, 79, 90, 110, 61, 22, 94, 24, 40, 25, 94, 63, 32, 94, 32, 67, 102, 61, 79, 40, 21, 32, 94, 63, 32, 79, 90, 110, 61, 94, 63, 122, 110, 61, 94, 110, 13, 79, 25, 32, 22]]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[[91, 94, 90, 115, 16, 32, 94, 110, 61, 32, 94, 91, 40, 110, 32, 13, 40, 32, 110, 22, 94, 23, 115, 38, 38, 32, 40, 110, 93, 94, 61, 79, 25, 94, 38, 32, 94, 90, 122, 110, 32, 13, 79, 90, 90, 45, 94, 48, 8, 85, 15, 22, 94, 104, 115, 30, 94, 122, 110, 94, 63, 79, 93, 40, 29, 110, 94, 122, 40, 110, 32, 40, 110, 122, 115, 40, 79, 90, 22, 94, 77, 90, 79, 25, 94, 91, 94, 25, 122, 25, 40, 29, 110, 94, 38, 32, 40, 110, 122, 115, 40, 94, 110, 61, 32, 94, 115, 110, 61, 32, 13, 94, 90, 32, 110, 110, 32, 13, 111], [24, 57, 115, 12, 110, 94, 110, 122, 38, 32, 94, 110, 115, 94, 12, 40, 16, 32, 122, 90, 94, 110, 61, 32, 94, 120, 94, 79, 40, 25, 94, 93, 115, 38, 32, 110, 61, 122, 40, 21, 94, 32, 90, 93, 32, 116, 122, 102, 22, 110, 63, 122, 110, 110, 32, 13, 22, 102, 115, 38, 100, 95, 116, 18, 75, 45, 122, 101, 70, 122, 27]]\n"
     ]
    }
   ],
   "source": [
    "# get all tweet texts\n",
    "both_tweets = ''\n",
    "for i in austen_tweets:\n",
    "    both_tweets = both_tweets + i.text\n",
    "for i in elon_tweets:\n",
    "    both_tweets = both_tweets + i.text\n",
    "\n",
    "# Convert all tweet texts to numeric\n",
    "chars = list(set(both_tweets))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "\n",
    "# Convert austen tweet to numeric set based on all tweets\n",
    "austen_tweets_num = []\n",
    "for i, j in enumerate(austen_tweets):\n",
    "    num_list = [char_indices[char] for char in j.text]\n",
    "    austen_tweets_num.append(num_list)\n",
    "\n",
    "# Convert elon tweet to numeric set based on all tweets\n",
    "elon_tweets_num = []\n",
    "for i, j in enumerate(elon_tweets):\n",
    "    num_list = [char_indices[char] for char in j.text]\n",
    "    elon_tweets_num.append(num_list)\n",
    "\n",
    "print(len(austen_tweets_num), len(elon_tweets_num))\n",
    "print(\"-\"*100)\n",
    "print(austen_tweets_num[1:3])\n",
    "print(\"-\"*100)\n",
    "print(elon_tweets_num[1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T15:14:44.529320Z",
     "start_time": "2019-05-17T15:14:44.500869Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((902,), (902,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to np array for machine learning\n",
    "X = np.array(austen_tweets_num + elon_tweets_num)\n",
    "\n",
    "austen_y = np.zeros((len(austen_tweets_num),), dtype=np.int)\n",
    "elon_y = np.ones((len(elon_tweets_num),), dtype=np.int)\n",
    "y = np.concatenate((austen_y,elon_y), axis=0)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T15:15:03.892313Z",
     "start_time": "2019-05-17T15:15:02.497710Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((721,), (181,), (721,), (181,))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T15:18:43.936552Z",
     "start_time": "2019-05-17T15:18:40.090125Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_train shape: (721, 80)\n",
      "x_test shape: (181, 80)\n",
      "WARNING:tensorflow:From /home/superio/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/superio/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 128)         256000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 387,713\n",
      "Trainable params: 387,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "max_features = 2000\n",
    "maxlen = 80\n",
    "epochs = 10\n",
    "batch_size = 20\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "print('x_train shape:', X_train.shape)\n",
    "print('x_test shape:', X_test.shape)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T15:23:17.369548Z",
     "start_time": "2019-05-17T15:20:33.136173Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/superio/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 721 samples, validate on 181 samples\n",
      "Epoch 1/10\n",
      "721/721 [==============================] - 21s 29ms/step - loss: 0.5311 - acc: 0.8086 - val_loss: 0.5593 - val_acc: 0.7569\n",
      "Epoch 2/10\n",
      "721/721 [==============================] - 14s 20ms/step - loss: 0.4708 - acc: 0.8100 - val_loss: 0.5344 - val_acc: 0.7569\n",
      "Epoch 3/10\n",
      "721/721 [==============================] - 13s 18ms/step - loss: 0.4480 - acc: 0.8100 - val_loss: 0.5231 - val_acc: 0.7569\n",
      "Epoch 4/10\n",
      "721/721 [==============================] - 13s 18ms/step - loss: 0.4286 - acc: 0.8100 - val_loss: 0.5050 - val_acc: 0.7680\n",
      "Epoch 5/10\n",
      "721/721 [==============================] - 14s 19ms/step - loss: 0.4209 - acc: 0.8128 - val_loss: 0.5056 - val_acc: 0.7790\n",
      "Epoch 6/10\n",
      "721/721 [==============================] - 14s 20ms/step - loss: 0.4222 - acc: 0.8252 - val_loss: 0.5022 - val_acc: 0.7735\n",
      "Epoch 7/10\n",
      "721/721 [==============================] - 16s 22ms/step - loss: 0.4113 - acc: 0.8280 - val_loss: 0.4942 - val_acc: 0.7845\n",
      "Epoch 8/10\n",
      "721/721 [==============================] - 27s 37ms/step - loss: 0.4094 - acc: 0.8308 - val_loss: 0.5187 - val_acc: 0.7735\n",
      "Epoch 9/10\n",
      "721/721 [==============================] - 16s 22ms/step - loss: 0.3994 - acc: 0.8280 - val_loss: 0.4854 - val_acc: 0.7901\n",
      "Epoch 10/10\n",
      "721/721 [==============================] - 15s 20ms/step - loss: 0.3923 - acc: 0.8197 - val_loss: 0.4980 - val_acc: 0.7956\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa43ee2f940>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - CNNs\n",
    "Time to play \"find the frog!\" Use Keras and ResNet50 to detect which of the following images contain frogs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T15:29:19.756860Z",
     "start_time": "2019-05-17T15:29:16.485181Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google_images_download in /home/superio/anaconda3/lib/python3.7/site-packages (2.7.1)\r\n",
      "Requirement already satisfied: selenium in /home/superio/anaconda3/lib/python3.7/site-packages (from google_images_download) (3.141.0)\r\n",
      "Requirement already satisfied: urllib3 in /home/superio/anaconda3/lib/python3.7/site-packages (from selenium->google_images_download) (1.24.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install google_images_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T15:29:32.269265Z",
     "start_time": "2019-05-17T15:29:26.410498Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Item no.: 1 --> Item name = animal pond\n",
      "Evaluating...\n",
      "Starting Download...\n",
      "Image URL: https://www.enchantedlearning.com/pgifs/Pondanimals.GIF\n",
      "Completed Image ====> 1.Pondanimals.GIF\n",
      "Image URL: https://i.ytimg.com/vi/NCbu0TND9vE/hqdefault.jpg\n",
      "Completed Image ====> 2.hqdefault.jpg\n",
      "Image URL: https://pklifescience.com/staticfiles/articles/images/PKLS4116_inline.png\n",
      "Completed Image ====> 3.PKLS4116_inline.png\n",
      "Image URL: https://get.pxhere.com/photo/water-animal-pond-wildlife-mammal-fish-eat-fauna-whiskers-vertebrate-otter-mink-marmot-sea-otter-mustelidae-1383482.jpg\n",
      "Completed Image ====> 4.water-animal-pond-wildlife-mammal-fish-eat-fauna-whiskers-vertebrate-otter-mink-marmot-sea-otter-mustelidae-1383482.jpg\n",
      "Image URL: http://images.animalpicturesociety.com/images/5d/alligator_animal_on_pond.jpg\n",
      "Completed Image ====> 5.alligator_animal_on_pond.jpg\n",
      "\n",
      "Errors: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from google_images_download import google_images_download\n",
    "\n",
    "response = google_images_download.googleimagesdownload()\n",
    "arguments = {'keywords': \"animal pond\", \"limit\": 5, \"print_urls\": True}\n",
    "absolute_image_paths = response.download(arguments)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the time of writing at least a few do, but since the internet changes - it is possible your 5 won't. You can easily verify yourself, and (once you have working code) increase the number of images you pull to be more sure of getting a frog. Your goal is validly run ResNet50 on the input images - don't worry about tuning or improving the model. \n",
    "\n",
    "*Hint:* ResNet 50 doesn't just return \"frog\". The three labels it has for frogs are bullfrog, tree frog, and tailed frog.\n",
    "\n",
    "Stretch goal - also check for fish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T15:29:46.831567Z",
     "start_time": "2019-05-17T15:29:46.823789Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1.Pondanimals.GIF']\n",
      "['2.hqdefault.jpg']\n",
      "['3.PKLS4116_inline.png']\n",
      "['4.water-animal-pond-wildlife-mammal-fish-eat-fauna-whiskers-vertebrate-otter-mink-marmot-sea-otter-mustelidae-1383482.jpg']\n",
      "['5.alligator_animal_on_pond.jpg']\n"
     ]
    }
   ],
   "source": [
    "for i in absolute_image_paths[0]['animal pond']:\n",
    "    print(i.split(\"/\")[-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T15:56:03.196477Z",
     "start_time": "2019-05-17T15:42:43.460439Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1.Pondanimals.GIF']\n",
      "[('n03598930', 'jigsaw_puzzle', 0.8680317), ('n06359193', 'web_site', 0.064100206), ('n02834397', 'bib', 0.021264251)]\n",
      "Frog is not in this picture\n",
      "--------------------------------------------------\n",
      "['2.hqdefault.jpg']\n",
      "[('n01443537', 'goldfish', 0.8495909), ('n01631663', 'eft', 0.067602254), ('n02536864', 'coho', 0.03516372)]\n",
      "Frog is not in this picture\n",
      "--------------------------------------------------\n",
      "['3.PKLS4116_inline.png']\n",
      "[('n04243546', 'slot', 0.8712449), ('n04476259', 'tray', 0.049936026), ('n03908618', 'pencil_box', 0.023072386)]\n",
      "Frog is not in this picture\n",
      "--------------------------------------------------\n",
      "['4.water-animal-pond-wildlife-mammal-fish-eat-fauna-whiskers-vertebrate-otter-mink-marmot-sea-otter-mustelidae-1383482.jpg']\n",
      "[('n02442845', 'mink', 0.30976456), ('n02363005', 'beaver', 0.23399007), ('n02361337', 'marmot', 0.20796947)]\n",
      "Frog is not in this picture\n",
      "--------------------------------------------------\n",
      "['5.alligator_animal_on_pond.jpg']\n",
      "[('n01698640', 'American_alligator', 0.963947), ('n01697457', 'African_crocodile', 0.026759941), ('n01737021', 'water_snake', 0.005964684)]\n",
      "Frog is not in this picture\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "def process_img_path(img_path):\n",
    "  return image.load_img(img_path, target_size=(224, 224))\n",
    "\n",
    "def img_contains_frog(img):\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    model = ResNet50(weights='imagenet')\n",
    "    features = model.predict(x)\n",
    "    results = decode_predictions(features, top=3)[0]\n",
    "    print(results)\n",
    "    for entry in results:\n",
    "        if 'frog' in entry[1]:\n",
    "            return entry[2], 'Frog is in this picture'\n",
    "        else:\n",
    "            return 'Frog is not in this picture'\n",
    "\n",
    "for i in absolute_image_paths[0]['animal pond']:\n",
    "    print(i.split(\"/\")[-1:])\n",
    "    print(img_contains_frog(process_img_path(i)))\n",
    "    print(\"-\"*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - AutoML\n",
    "\n",
    "Use [TPOT](https://epistasislab.github.io/tpot/) to fit a predictive model for the King County housing data, with `price` as the target output variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T16:01:15.219108Z",
     "start_time": "2019-05-17T16:01:11.326941Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tpot in /home/superio/anaconda3/lib/python3.7/site-packages (0.10.1)\n",
      "Requirement already satisfied: numpy>=1.12.1 in /home/superio/anaconda3/lib/python3.7/site-packages (from tpot) (1.16.2)\n",
      "Requirement already satisfied: stopit>=1.1.1 in /home/superio/anaconda3/lib/python3.7/site-packages (from tpot) (1.1.2)\n",
      "Requirement already satisfied: tqdm>=4.26.0 in /home/superio/anaconda3/lib/python3.7/site-packages (from tpot) (4.31.1)\n",
      "Requirement already satisfied: deap>=1.0 in /home/superio/anaconda3/lib/python3.7/site-packages (from tpot) (1.2.2)\n",
      "Requirement already satisfied: scipy>=0.19.0 in /home/superio/anaconda3/lib/python3.7/site-packages (from tpot) (1.2.1)\n",
      "Requirement already satisfied: update-checker>=0.16 in /home/superio/anaconda3/lib/python3.7/site-packages (from tpot) (0.16)\n",
      "Requirement already satisfied: pandas>=0.20.2 in /home/superio/anaconda3/lib/python3.7/site-packages (from tpot) (0.24.2)\n",
      "Requirement already satisfied: scikit-learn>=0.18.1 in /home/superio/anaconda3/lib/python3.7/site-packages (from tpot) (0.20.3)\n",
      "Requirement already satisfied: requests>=2.3.0 in /home/superio/anaconda3/lib/python3.7/site-packages (from update-checker>=0.16->tpot) (2.21.0)\n",
      "Requirement already satisfied: pytz>=2011k in /home/superio/anaconda3/lib/python3.7/site-packages (from pandas>=0.20.2->tpot) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /home/superio/anaconda3/lib/python3.7/site-packages (from pandas>=0.20.2->tpot) (2.8.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/superio/anaconda3/lib/python3.7/site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (2019.3.9)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/superio/anaconda3/lib/python3.7/site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (1.24.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/superio/anaconda3/lib/python3.7/site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/superio/anaconda3/lib/python3.7/site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (2.8)\n",
      "Requirement already satisfied: six>=1.5 in /home/superio/anaconda3/lib/python3.7/site-packages (from python-dateutil>=2.5.0->pandas>=0.20.2->tpot) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tpot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T16:01:20.909316Z",
     "start_time": "2019-05-17T16:01:18.383100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>...</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6414100192</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>400</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5631500400</td>\n",
       "      <td>20150225T000000</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>0</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2487200875</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>910</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1954400510</td>\n",
       "      <td>20150218T000000</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1680</td>\n",
       "      <td>0</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id             date     price  bedrooms  bathrooms  sqft_living  \\\n",
       "0  7129300520  20141013T000000  221900.0         3       1.00         1180   \n",
       "1  6414100192  20141209T000000  538000.0         3       2.25         2570   \n",
       "2  5631500400  20150225T000000  180000.0         2       1.00          770   \n",
       "3  2487200875  20141209T000000  604000.0         4       3.00         1960   \n",
       "4  1954400510  20150218T000000  510000.0         3       2.00         1680   \n",
       "\n",
       "   sqft_lot  floors  waterfront  view  ...  grade  sqft_above  sqft_basement  \\\n",
       "0      5650     1.0           0     0  ...      7        1180              0   \n",
       "1      7242     2.0           0     0  ...      7        2170            400   \n",
       "2     10000     1.0           0     0  ...      6         770              0   \n",
       "3      5000     1.0           0     0  ...      7        1050            910   \n",
       "4      8080     1.0           0     0  ...      8        1680              0   \n",
       "\n",
       "   yr_built  yr_renovated  zipcode      lat     long  sqft_living15  \\\n",
       "0      1955             0    98178  47.5112 -122.257           1340   \n",
       "1      1951          1991    98125  47.7210 -122.319           1690   \n",
       "2      1933             0    98028  47.7379 -122.233           2720   \n",
       "3      1965             0    98136  47.5208 -122.393           1360   \n",
       "4      1987             0    98074  47.6168 -122.045           1800   \n",
       "\n",
       "   sqft_lot15  \n",
       "0        5650  \n",
       "1        7639  \n",
       "2        8062  \n",
       "3        5000  \n",
       "4        7503  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/ryanleeallred/datasets/master/kc_house_data.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T16:01:23.264211Z",
     "start_time": "2019-05-17T16:01:23.219635Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21613, 21)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "id               0\n",
       "date             0\n",
       "price            0\n",
       "bedrooms         0\n",
       "bathrooms        0\n",
       "sqft_living      0\n",
       "sqft_lot         0\n",
       "floors           0\n",
       "waterfront       0\n",
       "view             0\n",
       "condition        0\n",
       "grade            0\n",
       "sqft_above       0\n",
       "sqft_basement    0\n",
       "yr_built         0\n",
       "yr_renovated     0\n",
       "zipcode          0\n",
       "lat              0\n",
       "long             0\n",
       "sqft_living15    0\n",
       "sqft_lot15       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with previous questions, your goal is to run TPOT and successfully run and report error at the end. Also, in the interest of time, feel free to choose small `generation=1`and `population_size=10` parameters, so your pipeline runs efficiently. You will want to be able to iterate and test. \n",
    "\n",
    "*Hint:* You will have to drop and/or type coerce at least a few variables to get things working. It's fine to err on the side of dropping to get things running - as long as you still get a valid model with reasonable predictive power. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T16:01:34.900387Z",
     "start_time": "2019-05-17T16:01:33.266035Z"
    }
   },
   "outputs": [],
   "source": [
    "from tpot import TPOTRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(['price','id','date'], axis=1).values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df['price'].values, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T16:15:40.274570Z",
     "start_time": "2019-05-17T16:08:13.823795Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Optimization Progress', max=30, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: -23246771549.915924\n",
      "Generation 2 - Current best internal CV score: -18753456012.815598\n",
      "\n",
      "Best pipeline: LassoLarsCV(RandomForestRegressor(input_matrix, bootstrap=False, max_features=0.5, min_samples_leaf=18, min_samples_split=7, n_estimators=100), normalize=False)\n",
      "-23411405727.61556\n"
     ]
    }
   ],
   "source": [
    "tpot = TPOTRegressor(generations=2, population_size=10, verbosity=2)\n",
    "tpot.fit(X_train, y_train)\n",
    "print(tpot.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - More... \n",
    "\n",
    "Answer the following questions, with a target audience of a fellow Data Scientist:\n",
    "* What do you consider your strongest area as a Data Scientist? \n",
    "    * My strongest area as a Data Scientist are domain knowledge (business & healthcare) and applying machine learning models to solve real-world problem.\n",
    "* What area of Data Science would you most like to learn more about and why? \n",
    "    * I would probably spend more time learning more about natural language processing and cognitive computing development since their usefulness are applicable to most industries.\n",
    "* Where do you think Data Science will be in 5 years? \n",
    "    * Similar to a boom and bust cycle, I feel that data science is in a booming cycle where its popularity is trending up.  I hope the good time continues for at least 10 years so we can build better and diverse technology as more people getting into the field.\n",
    "\n",
    "A few sentences per answer is fine. Only elaborate if time allows. Use markdown to format your answers.\n",
    "\n",
    "Thank you for your hard, and congratulations!! You've learned a lot, and you should proudly call yourself a Data Scientist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
