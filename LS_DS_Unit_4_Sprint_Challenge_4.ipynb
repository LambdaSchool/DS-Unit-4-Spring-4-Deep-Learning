{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 4*\n",
    "\n",
    "# Sprint Challenge\n",
    "### RNNs, CNNs, GANS, and AutoML\n",
    "\n",
    "In this Sprint Challenge, you'll explore some of the cutting edge of Data Science. *Caution* - these approaches can be pretty heavy computationally. All problems are designed to completed with 5-10 minutes of run time on most machines. If you approach takes longer, please double check your work. \n",
    "\n",
    "## Part 1 - RNNs\n",
    "\n",
    "Use an RNN to fit a classification model on tweets to distinguish from tweets from any two accounts. The following code sample illustrates how to access data from an account (no API auth needed, uses [twitterscraper](https://github.com/taspinar/twitterscraper): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting twitterscraper\n",
      "  Downloading https://files.pythonhosted.org/packages/38/7d/0bf84247b78d7d223914cbf410e1160203a65d39086aaf8c6cad521cec74/twitterscraper-0.9.3.tar.gz\n",
      "Collecting coala-utils~=0.5.0 (from twitterscraper)\n",
      "  Downloading https://files.pythonhosted.org/packages/54/00/74ec750cfc4e830f9d1cfdd4d559f3d2d4ba1b834b78d5266446db3fd1d6/coala_utils-0.5.1-py3-none-any.whl\n",
      "Collecting bs4 (from twitterscraper)\n",
      "  Downloading https://files.pythonhosted.org/packages/10/ed/7e8b97591f6f456174139ec089c769f89a94a1a4025fe967691de971f314/bs4-0.0.1.tar.gz\n",
      "Requirement already satisfied: lxml in /Users/jonathansokoll/anaconda3/lib/python3.7/site-packages (from twitterscraper) (4.3.2)\n",
      "Requirement already satisfied: requests in /Users/jonathansokoll/anaconda3/lib/python3.7/site-packages (from twitterscraper) (2.21.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/jonathansokoll/anaconda3/lib/python3.7/site-packages (from bs4->twitterscraper) (4.7.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/jonathansokoll/anaconda3/lib/python3.7/site-packages (from requests->twitterscraper) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/jonathansokoll/anaconda3/lib/python3.7/site-packages (from requests->twitterscraper) (2.8)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /Users/jonathansokoll/anaconda3/lib/python3.7/site-packages (from requests->twitterscraper) (1.24.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jonathansokoll/anaconda3/lib/python3.7/site-packages (from requests->twitterscraper) (2019.3.9)\n",
      "Requirement already satisfied: soupsieve>=1.2 in /Users/jonathansokoll/anaconda3/lib/python3.7/site-packages (from beautifulsoup4->bs4->twitterscraper) (1.8)\n",
      "Building wheels for collected packages: twitterscraper, bs4\n",
      "  Building wheel for twitterscraper (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/jonathansokoll/Library/Caches/pip/wheels/45/50/9b/70128bca07e2bf8b5ed3f504002e9e74a6eaa5e756341b6931\n",
      "  Building wheel for bs4 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/jonathansokoll/Library/Caches/pip/wheels/a0/b0/b2/4f80b9456b87abedbc0bf2d52235414c3467d8889be38dd472\n",
      "Successfully built twitterscraper bs4\n",
      "Installing collected packages: coala-utils, bs4, twitterscraper\n",
      "Successfully installed bs4-0.0.1 coala-utils-0.5.1 twitterscraper-0.9.3\n"
     ]
    }
   ],
   "source": [
    "!pip install twitterscraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: queries: ['from:austen since:2006-03-21 until:2006-11-16', 'from:austen since:2006-11-16 until:2007-07-14', 'from:austen since:2007-07-14 until:2008-03-10', 'from:austen since:2008-03-10 until:2008-11-05', 'from:austen since:2008-11-05 until:2009-07-04', 'from:austen since:2009-07-04 until:2010-03-01', 'from:austen since:2010-03-01 until:2010-10-27', 'from:austen since:2010-10-27 until:2011-06-24', 'from:austen since:2011-06-24 until:2012-02-19', 'from:austen since:2012-02-19 until:2012-10-17', 'from:austen since:2012-10-17 until:2013-06-14', 'from:austen since:2013-06-14 until:2014-02-09', 'from:austen since:2014-02-09 until:2014-10-07', 'from:austen since:2014-10-07 until:2015-06-04', 'from:austen since:2015-06-04 until:2016-01-31', 'from:austen since:2016-01-31 until:2016-09-27', 'from:austen since:2016-09-27 until:2017-05-25', 'from:austen since:2017-05-25 until:2018-01-20', 'from:austen since:2018-01-20 until:2018-09-17', 'from:austen since:2018-09-17 until:2019-05-16']\n",
      "INFO: Querying from:austen since:2008-03-10 until:2008-11-05\n",
      "INFO: Querying from:austen since:2006-03-21 until:2006-11-16\n",
      "INFO: Querying from:austen since:2006-11-16 until:2007-07-14\n",
      "INFO: Querying from:austen since:2007-07-14 until:2008-03-10\n",
      "INFO: Querying from:austen since:2008-11-05 until:2009-07-04\n",
      "INFO: Querying from:austen since:2009-07-04 until:2010-03-01\n",
      "INFO: Querying from:austen since:2010-10-27 until:2011-06-24\n",
      "INFO: Querying from:austen since:2010-03-01 until:2010-10-27\n",
      "INFO: Querying from:austen since:2011-06-24 until:2012-02-19\n",
      "INFO: Querying from:austen since:2012-02-19 until:2012-10-17\n",
      "INFO: Querying from:austen since:2012-10-17 until:2013-06-14\n",
      "INFO: Querying from:austen since:2013-06-14 until:2014-02-09\n",
      "INFO: Querying from:austen since:2014-02-09 until:2014-10-07\n",
      "INFO: Querying from:austen since:2014-10-07 until:2015-06-04\n",
      "INFO: Querying from:austen since:2015-06-04 until:2016-01-31\n",
      "INFO: Querying from:austen since:2017-05-25 until:2018-01-20\n",
      "INFO: Querying from:austen since:2016-09-27 until:2017-05-25\n",
      "INFO: Querying from:austen since:2018-01-20 until:2018-09-17\n",
      "INFO: Querying from:austen since:2016-01-31 until:2016-09-27\n",
      "INFO: Querying from:austen since:2018-09-17 until:2019-05-16\n",
      "INFO: Got 0 tweets for from%3Aausten%20since%3A2009-07-04%20until%3A2010-03-01.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets for from%3Aausten%20since%3A2010-03-01%20until%3A2010-10-27.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets for from%3Aausten%20since%3A2006-11-16%20until%3A2007-07-14.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets for from%3Aausten%20since%3A2014-02-09%20until%3A2014-10-07.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets for from%3Aausten%20since%3A2015-06-04%20until%3A2016-01-31.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets for from%3Aausten%20since%3A2006-03-21%20until%3A2006-11-16.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets for from%3Aausten%20since%3A2008-11-05%20until%3A2009-07-04.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets for from%3Aausten%20since%3A2010-10-27%20until%3A2011-06-24.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets for from%3Aausten%20since%3A2011-06-24%20until%3A2012-02-19.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets for from%3Aausten%20since%3A2016-09-27%20until%3A2017-05-25.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets for from%3Aausten%20since%3A2014-10-07%20until%3A2015-06-04.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets for from%3Aausten%20since%3A2012-10-17%20until%3A2013-06-14.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets for from%3Aausten%20since%3A2013-06-14%20until%3A2014-02-09.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets for from%3Aausten%20since%3A2007-07-14%20until%3A2008-03-10.\n",
      "INFO: Got 0 tweets for from%3Aausten%20since%3A2008-03-10%20until%3A2008-11-05.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 1 tweets for from%3Aausten%20since%3A2012-02-19%20until%3A2012-10-17.\n",
      "INFO: Got 1 tweets (1 new).\n",
      "INFO: Got 0 tweets for from%3Aausten%20since%3A2016-01-31%20until%3A2016-09-27.\n",
      "INFO: Got 1 tweets (0 new).\n",
      "INFO: Got 60 tweets for from%3Aausten%20since%3A2017-05-25%20until%3A2018-01-20.\n",
      "INFO: Got 61 tweets (60 new).\n",
      "INFO: Got 60 tweets for from%3Aausten%20since%3A2018-09-17%20until%3A2019-05-16.\n",
      "INFO: Got 121 tweets (60 new).\n",
      "INFO: Got 60 tweets for from%3Aausten%20since%3A2018-01-20%20until%3A2018-09-17.\n",
      "INFO: Got 181 tweets (60 new).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "181"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from twitterscraper import query_tweets\n",
    "\n",
    "austen_tweets = query_tweets('from:austen',1000)\n",
    "len(austin_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love love love working with great people.pic.twitter.com/fCKOm6Vl'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "austen_tweets[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your Tasks:\n",
    "* Select two twitter accounts to gather data from\n",
    "* Use twitterscraper to get ~1,000 tweets from each account\n",
    "* Encode the characters to a sequence of integers for the model\n",
    "* Get the data into the appropriate shape/format, including labels and a train/test split\n",
    "* Use Keras to fit a predictive model, classying tweets as being from one acount or the other\n",
    "* Report your overall score and accuracy\n",
    "\n",
    "For reference, the [Keras IMDB classification example](https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py) will be useful, as well as the RNN code we used in class.\n",
    "\n",
    "Note - focus on getting a running model, not on making accuracy with extreme data size or epoch numbers. Fit a baseline model based on tweet text. Only revisit and push accuracy or incorporate additional features if you get everything else done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "This task is incomplete. \nReplace this line with your code.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-9ede0649e9f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# TODO - your code!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"This task is incomplete. \\nReplace this line with your code.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mException\u001b[0m: This task is incomplete. \nReplace this line with your code."
     ]
    }
   ],
   "source": [
    "# TODO - your code!\n",
    "raise Exception(\"This task is incomplete. \\nReplace this line with your code.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - CNNs\n",
    "Time to play \"find the frog!\" Use Keras and ResNet50 to detect which of the following images contain frogs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google_images_download\n",
      "  Downloading https://files.pythonhosted.org/packages/a5/a7/1dc0de31c24d6b93469f46d743f36a7f51406d4e452690706ac5be2a5eab/google_images_download-2.7.1.tar.gz\n",
      "Collecting selenium (from google_images_download)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n",
      "\u001b[K    100% |████████████████████████████████| 911kB 3.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: urllib3 in /Users/jonathansokoll/anaconda3/lib/python3.7/site-packages (from selenium->google_images_download) (1.24.1)\n",
      "Building wheels for collected packages: google-images-download\n",
      "  Building wheel for google-images-download (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/jonathansokoll/Library/Caches/pip/wheels/22/f3/7b/d1d7a18d9784458622ef3f9702c0bdbc179b431adde169c1a0\n",
      "Successfully built google-images-download\n",
      "Installing collected packages: selenium, google-images-download\n",
      "Successfully installed google-images-download-2.7.1 selenium-3.141.0\n"
     ]
    }
   ],
   "source": [
    "!pip install google_images_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Item no.: 1 --> Item name = animal pond\n",
      "Evaluating...\n",
      "Starting Download...\n",
      "Image URL: https://www.enchantedlearning.com/pgifs/Pondanimals.GIF\n",
      "Completed Image ====> 1.Pondanimals.GIF\n",
      "Image URL: https://i.ytimg.com/vi/NCbu0TND9vE/hqdefault.jpg\n",
      "Completed Image ====> 2.hqdefault.jpg\n",
      "Image URL: https://get.pxhere.com/photo/water-animal-pond-wildlife-mammal-fish-eat-fauna-whiskers-vertebrate-otter-mink-marmot-sea-otter-mustelidae-1383482.jpg\n",
      "Completed Image ====> 3.water-animal-pond-wildlife-mammal-fish-eat-fauna-whiskers-vertebrate-otter-mink-marmot-sea-otter-mustelidae-1383482.jpg\n",
      "Image URL: https://pklifescience.com/staticfiles/articles/images/PKLS4116_inline.png\n",
      "Completed Image ====> 4.PKLS4116_inline.png\n",
      "Image URL: http://images.animalpicturesociety.com/images/5d/alligator_animal_on_pond.jpg\n",
      "Completed Image ====> 5.alligator_animal_on_pond.jpg\n",
      "\n",
      "Errors: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from google_images_download import google_images_download\n",
    "\n",
    "response = google_images_download.googleimagesdownload()\n",
    "arguments = {'keywords': \"animal pond\", \"limit\": 5, \"print_urls\": True}\n",
    "absolute_image_paths = response.download(arguments)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the time of writing at least a few do, but since the internet changes - it is possible your 5 won't. You can easily verify yourself, and (once you have working code) increase the number of images you pull to be more sure of getting a frog. Your goal is validly run ResNet50 on the input images - don't worry about tuning or improving the model. \n",
    "\n",
    "*Hint:* ResNet 50 doesn't just return \"frog\". The three labels it has for frogs are bullfrog, tree frog, and tailed frog.\n",
    "\n",
    "Stretch goal - also check for fish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "This task is incomplete. \nReplace this line with your code.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-9ede0649e9f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# TODO - your code!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"This task is incomplete. \\nReplace this line with your code.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mException\u001b[0m: This task is incomplete. \nReplace this line with your code."
     ]
    }
   ],
   "source": [
    "# TODO - your code!\n",
    "raise Exception(\"This task is incomplete. \\nReplace this line with your code.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - AutoML\n",
    "\n",
    "Use [TPOT](https://epistasislab.github.io/tpot/) to fit a predictive model for the King County housing data, with `price` as the target output variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tpot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>...</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6414100192</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>400</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5631500400</td>\n",
       "      <td>20150225T000000</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>0</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2487200875</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>910</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1954400510</td>\n",
       "      <td>20150218T000000</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1680</td>\n",
       "      <td>0</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id             date     price  bedrooms  bathrooms  sqft_living  \\\n",
       "0  7129300520  20141013T000000  221900.0         3       1.00         1180   \n",
       "1  6414100192  20141209T000000  538000.0         3       2.25         2570   \n",
       "2  5631500400  20150225T000000  180000.0         2       1.00          770   \n",
       "3  2487200875  20141209T000000  604000.0         4       3.00         1960   \n",
       "4  1954400510  20150218T000000  510000.0         3       2.00         1680   \n",
       "\n",
       "   sqft_lot  floors  waterfront  view  ...  grade  sqft_above  sqft_basement  \\\n",
       "0      5650     1.0           0     0  ...      7        1180              0   \n",
       "1      7242     2.0           0     0  ...      7        2170            400   \n",
       "2     10000     1.0           0     0  ...      6         770              0   \n",
       "3      5000     1.0           0     0  ...      7        1050            910   \n",
       "4      8080     1.0           0     0  ...      8        1680              0   \n",
       "\n",
       "   yr_built  yr_renovated  zipcode      lat     long  sqft_living15  \\\n",
       "0      1955             0    98178  47.5112 -122.257           1340   \n",
       "1      1951          1991    98125  47.7210 -122.319           1690   \n",
       "2      1933             0    98028  47.7379 -122.233           2720   \n",
       "3      1965             0    98136  47.5208 -122.393           1360   \n",
       "4      1987             0    98074  47.6168 -122.045           1800   \n",
       "\n",
       "   sqft_lot15  \n",
       "0        5650  \n",
       "1        7639  \n",
       "2        8062  \n",
       "3        5000  \n",
       "4        7503  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/ryanleeallred/datasets/master/kc_house_data.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with previous questions, your goal is to run TPOT and successfully run and report error at the end. Also, in the interest of time, feel free to choose small `generation=1`and `population_size=10` parameters, so your pipeline runs efficiently. You will want to be able to iterate and test. \n",
    "\n",
    "*Hint:* You will have to drop and/or type coerce at least a few variables to get things working. It's fine to err on the side of dropping to get things running - as long as you still get a valid model with reasonable predictive power. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "This task is incomplete. \nReplace this line with your code.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-9ede0649e9f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# TODO - your code!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"This task is incomplete. \\nReplace this line with your code.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mException\u001b[0m: This task is incomplete. \nReplace this line with your code."
     ]
    }
   ],
   "source": [
    "# TODO - your code!\n",
    "raise Exception(\"This task is incomplete. \\nReplace this line with your code.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - More... \n",
    "\n",
    "Answer the following questions, with a target audience of a fellow Data Scientist:\n",
    "* What do you consider your strongest area as a Data Scientist? \n",
    "* What area of Data Science would you most like to learn more about and why? \n",
    "* Where do you think Data Science will be in 5 years? \n",
    "\n",
    "A few sentences per answer is fine. Only elaborate if time allows. Use markdown to format your answers.\n",
    "\n",
    "Thank you for your hard, and congratulations!! You've learned a lot, and you should proudly call yourself a Data Scientist. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
