{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xhU3R-8dzk5z"
   },
   "source": [
    "# Lambda School Data Science Unit 4 Sprint Challenge 4\n",
    "\n",
    "## RNNs, CNNs, AutoML, and more...\n",
    "\n",
    "In this sprint challenge, you'll explore some of the cutting edge of Data Science.\n",
    "\n",
    "*Caution* - these approaches can be pretty heavy computationally. All problems were designed so that you should be able to achieve results within at most 5-10 minutes of runtime on Colab or a comparable environment. If something is running longer, doublecheck your approach!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-5UwGRnJOmD4"
   },
   "source": [
    "## Part 1 - RNNs\n",
    "\n",
    "Use an RNN to fit a simple classification model on tweets to distinguish from tweets from Austen Allred and tweets from Weird Al Yankovic.\n",
    "\n",
    "Following is code to scrape the needed data (no API auth needed, uses [twitterscraper](https://github.com/taspinar/twitterscraper)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "colab_type": "code",
    "id": "3if1yTMUoG3U",
    "outputId": "6075f571-d8b1-4e91-bfb9-26ad7a467261"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting twitterscraper\n",
      "  Downloading https://files.pythonhosted.org/packages/38/7d/0bf84247b78d7d223914cbf410e1160203a65d39086aaf8c6cad521cec74/twitterscraper-0.9.3.tar.gz\n",
      "Collecting coala-utils~=0.5.0 (from twitterscraper)\n",
      "  Downloading https://files.pythonhosted.org/packages/54/00/74ec750cfc4e830f9d1cfdd4d559f3d2d4ba1b834b78d5266446db3fd1d6/coala_utils-0.5.1-py3-none-any.whl\n",
      "Collecting bs4 (from twitterscraper)\n",
      "  Downloading https://files.pythonhosted.org/packages/10/ed/7e8b97591f6f456174139ec089c769f89a94a1a4025fe967691de971f314/bs4-0.0.1.tar.gz\n",
      "Requirement already satisfied: lxml in c:\\users\\jhump\\anaconda3\\lib\\site-packages (from twitterscraper) (4.3.2)\n",
      "Requirement already satisfied: requests in c:\\users\\jhump\\anaconda3\\lib\\site-packages (from twitterscraper) (2.21.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\jhump\\anaconda3\\lib\\site-packages (from bs4->twitterscraper) (4.7.1)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in c:\\users\\jhump\\anaconda3\\lib\\site-packages (from requests->twitterscraper) (1.24.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jhump\\anaconda3\\lib\\site-packages (from requests->twitterscraper) (2019.3.9)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\jhump\\anaconda3\\lib\\site-packages (from requests->twitterscraper) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\jhump\\anaconda3\\lib\\site-packages (from requests->twitterscraper) (3.0.4)\n",
      "Requirement already satisfied: soupsieve>=1.2 in c:\\users\\jhump\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4->twitterscraper) (1.8)\n",
      "Building wheels for collected packages: twitterscraper, bs4\n",
      "  Building wheel for twitterscraper (setup.py): started\n",
      "  Building wheel for twitterscraper (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\jhump\\AppData\\Local\\pip\\Cache\\wheels\\45\\50\\9b\\70128bca07e2bf8b5ed3f504002e9e74a6eaa5e756341b6931\n",
      "  Building wheel for bs4 (setup.py): started\n",
      "  Building wheel for bs4 (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\jhump\\AppData\\Local\\pip\\Cache\\wheels\\a0\\b0\\b2\\4f80b9456b87abedbc0bf2d52235414c3467d8889be38dd472\n",
      "Successfully built twitterscraper bs4\n",
      "Installing collected packages: coala-utils, bs4, twitterscraper\n",
      "Successfully installed bs4-0.0.1 coala-utils-0.5.1 twitterscraper-0.9.3\n"
     ]
    }
   ],
   "source": [
    "!pip install twitterscraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1114
    },
    "colab_type": "code",
    "id": "DS-9ksWjoJit",
    "outputId": "0c3512e4-5cd4-4dc6-9cda-baf00c835f59"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: queries: ['from:austen since:2006-03-21 until:2006-11-14', 'from:austen since:2006-11-14 until:2007-07-11', 'from:austen since:2007-07-11 until:2008-03-05', 'from:austen since:2008-03-05 until:2008-10-30', 'from:austen since:2008-10-30 until:2009-06-25', 'from:austen since:2009-06-25 until:2010-02-19', 'from:austen since:2010-02-19 until:2010-10-15', 'from:austen since:2010-10-15 until:2011-06-11', 'from:austen since:2011-06-11 until:2012-02-04', 'from:austen since:2012-02-04 until:2012-09-30', 'from:austen since:2012-09-30 until:2013-05-26', 'from:austen since:2013-05-26 until:2014-01-20', 'from:austen since:2014-01-20 until:2014-09-15', 'from:austen since:2014-09-15 until:2015-05-12', 'from:austen since:2015-05-12 until:2016-01-05', 'from:austen since:2016-01-05 until:2016-08-31', 'from:austen since:2016-08-31 until:2017-04-26', 'from:austen since:2017-04-26 until:2017-12-21', 'from:austen since:2017-12-21 until:2018-08-16', 'from:austen since:2018-08-16 until:2019-04-12']\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 1 tweets (1 new).\n",
      "INFO: Got 1 tweets (0 new).\n",
      "INFO: Got 1 tweets (0 new).\n",
      "INFO: Got 1 tweets (0 new).\n",
      "INFO: Got 1 tweets (0 new).\n",
      "INFO: Got 1 tweets (0 new).\n",
      "INFO: Got 1 tweets (0 new).\n",
      "INFO: Got 1 tweets (0 new).\n",
      "INFO: Got 1 tweets (0 new).\n",
      "INFO: Got 61 tweets (60 new).\n",
      "INFO: Got 121 tweets (60 new).\n",
      "INFO: Got 181 tweets (60 new).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "181"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from twitterscraper import query_tweets\n",
    "\n",
    "austen_tweets = query_tweets('from:austen', 1000)\n",
    "len(austen_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fLKqFh8DovaN",
    "outputId": "64b0d621-7e74-4181-9116-406e8c518465"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love love love working with great people.pic.twitter.com/fCKOm6Vl'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "austen_tweets[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1114
    },
    "colab_type": "code",
    "id": "MRQeIIf1orCS",
    "outputId": "44b57b5e-2a0e-4656-ca06-d77637caf593"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: queries: ['from:AlYankovic since:2006-03-21 until:2006-11-14', 'from:AlYankovic since:2006-11-14 until:2007-07-11', 'from:AlYankovic since:2007-07-11 until:2008-03-05', 'from:AlYankovic since:2008-03-05 until:2008-10-30', 'from:AlYankovic since:2008-10-30 until:2009-06-25', 'from:AlYankovic since:2009-06-25 until:2010-02-19', 'from:AlYankovic since:2010-02-19 until:2010-10-15', 'from:AlYankovic since:2010-10-15 until:2011-06-11', 'from:AlYankovic since:2011-06-11 until:2012-02-04', 'from:AlYankovic since:2012-02-04 until:2012-09-30', 'from:AlYankovic since:2012-09-30 until:2013-05-26', 'from:AlYankovic since:2013-05-26 until:2014-01-20', 'from:AlYankovic since:2014-01-20 until:2014-09-15', 'from:AlYankovic since:2014-09-15 until:2015-05-12', 'from:AlYankovic since:2015-05-12 until:2016-01-05', 'from:AlYankovic since:2016-01-05 until:2016-08-31', 'from:AlYankovic since:2016-08-31 until:2017-04-26', 'from:AlYankovic since:2017-04-26 until:2017-12-21', 'from:AlYankovic since:2017-12-21 until:2018-08-16', 'from:AlYankovic since:2018-08-16 until:2019-04-12']\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 60 tweets (60 new).\n",
      "INFO: Got 120 tweets (60 new).\n",
      "INFO: Got 180 tweets (60 new).\n",
      "INFO: Got 240 tweets (60 new).\n",
      "INFO: Got 300 tweets (60 new).\n",
      "INFO: Got 360 tweets (60 new).\n",
      "INFO: Got 420 tweets (60 new).\n",
      "INFO: Got 480 tweets (60 new).\n",
      "INFO: Got 540 tweets (60 new).\n",
      "INFO: Got 540 tweets (0 new).\n",
      "INFO: Got 600 tweets (60 new).\n",
      "INFO: Got 660 tweets (60 new).\n",
      "INFO: Got 720 tweets (60 new).\n",
      "INFO: Got 780 tweets (60 new).\n",
      "INFO: Got 840 tweets (60 new).\n",
      "INFO: Got 900 tweets (60 new).\n",
      "INFO: Got 960 tweets (60 new).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "960"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "al_tweets = query_tweets('from:AlYankovic', 1000)\n",
    "len(al_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_dB7I87ty8f1",
    "outputId": "38e7dffb-92bb-4a4c-8bbd-f1ddd951cc85"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hey @suzanneyankovic, where'd I leave my shoes?\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "al_tweets[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0mrcjEu_zRl4",
    "outputId": "cdea5ac9-bf26-434f-d2aa-fbc251c8ceef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1141"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(austen_tweets + al_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WYCVJX6ep8iO"
   },
   "source": [
    "Your tasks:\n",
    "\n",
    "- Encode the characters to a sequence of integers for the model\n",
    "- Get the data into the appropriate shape/format, including labels and a train/test split\n",
    "- Use Keras to fit a predictive model, classifying tweets as being from Austen versus Weird Al\n",
    "- Report your overall score and accuracy\n",
    "\n",
    "For reference, the [Keras IMDB sentiment classification example](https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py) will be useful, as well the RNN code we used in class.\n",
    "\n",
    "*Note* - focus on getting a running model, not on maxing accuracy with extreme data size or epoch numbers. Only revisit and push accuracy if you get everything else done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_QVSlFEAqWJM"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "# from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from random import sample\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data: encoding, shaping, train/test split\n",
    "# encoding\n",
    "austen_text = ''\n",
    "al_text = ''\n",
    "\n",
    "\n",
    "def process_tweets(text, tweets):\n",
    "    for tweet in tweets:  # austen_tweets, al_tweets\n",
    "        try:\n",
    "            text += '\\n\\n' + tweet.text  # austen_text, al_text\n",
    "        except:\n",
    "            print('Failed: ' + tweet.text)\n",
    "\n",
    "    text = text.split('\\n\\n')\n",
    "    return text\n",
    "\n",
    "\n",
    "def encode_tweets(text):    \n",
    "    chars = list(set(text)) # split and remove duplicate characters. convert to list.\n",
    "\n",
    "    num_chars = len(chars) # the number of unique characters\n",
    "    txt_data_size = len(text)\n",
    "    print(\"unique characters : \", num_chars)\n",
    "    print(\"txt_data_size : \", txt_data_size)\n",
    "    # one hot encode\n",
    "    char_to_int = dict((c, i) for i, c in enumerate(chars)) # \"enumerate\" retruns index and value. Convert it to dictionary\n",
    "    int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "    # print(char_to_int)\n",
    "    # print(\"----------------------------------------------------\")\n",
    "    # print(int_to_char)\n",
    "    # print(\"----------------------------------------------------\")\n",
    "    # integer encode input data\n",
    "    integer_encoded = [char_to_int[i] for i in text] # \"integer_encoded\" is a list which has a sequence converted from an original data to integers.\n",
    "    print(integer_encoded)\n",
    "    print(\"----------------------------------------------------\")\n",
    "    print(\"data length : \", len(integer_encoded))\n",
    "    return integer_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique characters :  224\n",
      "txt_data_size :  227\n",
      "[0, 197, 195, 51, 53, 206, 215, 67, 120, 17, 144, 212, 182, 58, 205, 94, 57, 171, 209, 127, 78, 156, 121, 11, 49, 43, 148, 15, 160, 133, 64, 214, 80, 165, 83, 19, 219, 128, 25, 84, 181, 211, 44, 9, 6, 183, 213, 192, 33, 56, 103, 75, 169, 70, 24, 31, 185, 199, 98, 172, 8, 88, 81, 79, 161, 131, 223, 190, 69, 35, 196, 122, 76, 28, 125, 194, 191, 77, 26, 104, 222, 198, 20, 61, 36, 102, 30, 38, 123, 167, 27, 105, 173, 117, 29, 47, 62, 207, 153, 54, 39, 65, 157, 21, 177, 22, 193, 158, 40, 154, 108, 217, 166, 73, 200, 129, 170, 146, 3, 201, 63, 164, 18, 124, 184, 145, 149, 179, 140, 109, 147, 96, 87, 175, 142, 187, 174, 135, 100, 48, 130, 155, 85, 150, 7, 141, 37, 12, 186, 113, 151, 168, 218, 136, 4, 55, 134, 106, 101, 41, 55, 1, 46, 111, 92, 60, 132, 137, 202, 107, 72, 178, 52, 143, 112, 16, 32, 114, 216, 203, 13, 188, 180, 50, 34, 90, 210, 82, 86, 185, 89, 176, 10, 59, 42, 95, 66, 93, 23, 115, 68, 159, 45, 91, 97, 99, 110, 138, 126, 208, 204, 189, 152, 221, 14, 119, 118, 71, 2, 5, 220, 162, 163, 139, 116, 74, 112]\n",
      "----------------------------------------------------\n",
      "data length :  227\n"
     ]
    }
   ],
   "source": [
    "austen_text_procd = encode_tweets(process_tweets(austen_text, austen_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique characters :  965\n",
      "txt_data_size :  965\n",
      "[0, 390, 718, 222, 528, 777, 508, 525, 844, 819, 752, 684, 232, 69, 64, 695, 223, 754, 932, 149, 787, 649, 538, 757, 935, 229, 270, 770, 885, 676, 882, 734, 839, 706, 636, 906, 243, 705, 828, 724, 266, 673, 391, 507, 872, 311, 604, 888, 586, 904, 668, 891, 729, 37, 740, 75, 107, 903, 66, 873, 3, 307, 373, 163, 142, 601, 370, 805, 769, 755, 510, 472, 812, 847, 289, 698, 483, 468, 67, 776, 514, 282, 335, 86, 792, 588, 931, 843, 389, 521, 914, 55, 433, 253, 4, 585, 244, 167, 591, 30, 296, 432, 572, 58, 574, 849, 399, 238, 334, 553, 958, 661, 527, 87, 106, 499, 362, 738, 663, 118, 458, 463, 386, 859, 245, 273, 170, 465, 346, 322, 141, 297, 17, 954, 693, 343, 303, 437, 15, 394, 579, 832, 120, 947, 41, 730, 629, 201, 52, 46, 915, 435, 824, 126, 43, 496, 451, 924, 702, 547, 818, 841, 295, 895, 456, 164, 952, 236, 890, 852, 664, 21, 33, 351, 719, 925, 772, 549, 745, 721, 962, 161, 949, 780, 471, 900, 226, 422, 774, 239, 204, 74, 227, 529, 596, 762, 732, 893, 960, 177, 434, 487, 39, 261, 530, 71, 504, 652, 696, 503, 354, 330, 502, 927, 647, 486, 936, 796, 172, 76, 733, 138, 121, 785, 62, 376, 146, 589, 597, 607, 963, 436, 331, 210, 691, 683, 714, 624, 764, 741, 870, 580, 793, 132, 364, 853, 169, 866, 765, 686, 205, 198, 80, 70, 763, 505, 863, 450, 526, 933, 632, 115, 626, 184, 851, 513, 28, 608, 185, 928, 913, 206, 452, 292, 653, 278, 114, 249, 671, 193, 781, 371, 337, 242, 318, 794, 457, 655, 808, 543, 896, 300, 224, 797, 795, 748, 130, 815, 555, 265, 522, 862, 709, 294, 783, 953, 865, 251, 583, 2, 536, 441, 898, 454, 654, 342, 926, 494, 284, 909, 860, 140, 516, 771, 42, 827, 929, 220, 533, 425, 150, 225, 122, 641, 319, 830, 443, 228, 825, 737, 881, 756, 644, 119, 352, 82, 534, 344, 675, 365, 230, 414, 155, 143, 455, 7, 918, 136, 637, 711, 277, 374, 887, 474, 488, 418, 840, 397, 710, 77, 314, 383, 158, 590, 88, 790, 816, 923, 874, 359, 551, 704, 897, 884, 563, 609, 315, 412, 568, 332, 665, 789, 821, 678, 485, 133, 877, 190, 144, 687, 560, 835, 660, 722, 798, 269, 831, 700, 424, 246, 357, 237, 546, 713, 13, 35, 116, 195, 98, 16, 304, 405, 659, 20, 103, 921, 605, 856, 606, 699, 85, 498, 124, 767, 145, 799, 186, 392, 462, 305, 778, 941, 108, 258, 268, 779, 666, 191, 338, 404, 535, 728, 564, 614, 648, 599, 252, 640, 627, 211, 18, 758, 930, 420, 327, 672, 73, 784, 54, 180, 36, 464, 518, 395, 723, 820, 428, 934, 271, 800, 838, 430, 697, 23, 578, 916, 72, 32, 213, 743, 123, 1, 127, 850, 372, 281, 650, 690, 385, 96, 573, 491, 667, 367, 768, 511, 44, 83, 619, 14, 439, 489, 461, 63, 716, 570, 581, 446, 209, 766, 447, 356, 942, 263, 773, 416, 886, 575, 506, 920, 324, 905, 293, 316, 308, 611, 369, 554, 99, 288, 254, 426, 957, 247, 100, 336, 692, 148, 720, 151, 459, 317, 396, 368, 955, 109, 438, 291, 694, 196, 753, 259, 469, 643, 576, 656, 616, 45, 725, 466, 218, 685, 823, 595, 806, 867, 431, 907, 804, 320, 187, 557, 651, 47, 283, 612, 290, 688, 587, 846, 911, 48, 515, 345, 495, 444, 89, 207, 861, 57, 347, 31, 165, 726, 727, 532, 467, 366, 544, 427, 312, 826, 276, 561, 221, 93, 241, 662, 519, 423, 630, 260, 473, 565, 537, 620, 380, 215, 152, 622, 299, 657, 202, 453, 176, 166, 566, 837, 829, 328, 801, 600, 807, 524, 883, 545, 333, 742, 173, 788, 594, 192, 341, 669, 912, 6, 8, 731, 323, 919, 868, 802, 22, 25, 381, 125, 188, 387, 610, 429, 137, 959, 111, 181, 340, 598, 235, 493, 27, 899, 559, 497, 746, 717, 633, 51, 53, 417, 577, 348, 449, 582, 110, 703, 84, 182, 550, 876, 313, 833, 512, 406, 306, 689, 854, 413, 339, 617, 105, 189, 603, 484, 9, 950, 233, 531, 869, 951, 128, 203, 809, 440, 375, 350, 159, 749, 635, 134, 326, 701, 739, 212, 256, 384, 11, 855, 480, 442, 10, 481, 360, 628, 901, 509, 615, 214, 321, 410, 842, 112, 50, 325, 78, 231, 101, 147, 613, 477, 139, 286, 102, 759, 917, 945, 194, 889, 490, 674, 377, 964, 160, 681, 857, 298, 858, 708, 131, 272, 871, 937, 274, 677, 864, 938, 642, 625, 301, 775, 475, 407, 275, 712, 782, 939, 817, 287, 944, 548, 65, 803, 168, 478, 460, 523, 658, 811, 34, 813, 448, 24, 639, 329, 90, 645, 310, 584, 592, 12, 178, 353, 750, 285, 751, 445, 593, 836, 539, 255, 361, 174, 791, 153, 569, 97, 908, 786, 571, 894, 38, 117, 91, 810, 234, 61, 248, 602, 540, 175, 363, 162, 848, 208, 95, 411, 482, 79, 262, 834, 19, 902, 500, 541, 492, 378, 400, 56, 956, 257, 408, 267, 479, 60, 171, 562, 157, 5, 355, 910, 279, 217, 744, 219, 682, 26, 946, 129, 240, 216, 409, 747, 618, 415, 250, 197, 68, 382, 358, 542, 154, 156, 40, 179, 517, 670, 81, 199, 421, 470, 875, 878, 398, 104, 558, 401, 631, 552, 845, 419, 29, 880, 200, 280, 349, 183, 814, 760, 922, 113, 59, 476, 822, 403, 402, 948, 715, 638, 943, 879, 736, 646, 940, 302, 264, 961, 634, 556, 92, 520, 707, 623, 735, 379, 393, 309, 892, 49, 679, 680, 567, 94, 135, 761, 501, 388, 621]\n",
      "----------------------------------------------------\n",
      "data length :  965\n"
     ]
    }
   ],
   "source": [
    "al_text_procd = encode_tweets(process_tweets(al_text, al_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(austen_text_procd, sample(al_text_procd, 227),\n",
    "                                                    test_size=0.25, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "WARNING:tensorflow:From C:\\Users\\jhump\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\jhump\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Train...\n",
      "WARNING:tensorflow:From C:\\Users\\jhump\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 170 samples, validate on 57 samples\n",
      "Epoch 1/15\n",
      "170/170 [==============================] - 2s 13ms/step - loss: -3.4498 - acc: 0.0000e+00 - val_loss: -11.6550 - val_acc: 0.0000e+00\n",
      "Epoch 2/15\n",
      "170/170 [==============================] - 0s 500us/step - loss: -20.1962 - acc: 0.0000e+00 - val_loss: -25.4989 - val_acc: 0.0000e+00\n",
      "Epoch 3/15\n",
      "170/170 [==============================] - 0s 509us/step - loss: -39.3440 - acc: 0.0000e+00 - val_loss: -40.7772 - val_acc: 0.0000e+00\n",
      "Epoch 4/15\n",
      "170/170 [==============================] - 0s 618us/step - loss: -64.6921 - acc: 0.0000e+00 - val_loss: -57.4691 - val_acc: 0.0000e+00\n",
      "Epoch 5/15\n",
      "170/170 [==============================] - 0s 547us/step - loss: -98.7000 - acc: 0.0000e+00 - val_loss: -76.0985 - val_acc: 0.0000e+00\n",
      "Epoch 6/15\n",
      "170/170 [==============================] - 0s 568us/step - loss: -143.5063 - acc: 0.0000e+00 - val_loss: -97.0660 - val_acc: 0.0000e+00\n",
      "Epoch 7/15\n",
      "170/170 [==============================] - 0s 579us/step - loss: -208.3925 - acc: 0.0000e+00 - val_loss: -120.2635 - val_acc: 0.0000e+00\n",
      "Epoch 8/15\n",
      "170/170 [==============================] - 0s 529us/step - loss: -297.9400 - acc: 0.0000e+00 - val_loss: -145.9677 - val_acc: 0.0000e+00\n",
      "Epoch 9/15\n",
      "170/170 [==============================] - 0s 526us/step - loss: -425.1488 - acc: 0.0000e+00 - val_loss: -175.0642 - val_acc: 0.0000e+00\n",
      "Epoch 10/15\n",
      "170/170 [==============================] - 0s 568us/step - loss: -593.5693 - acc: 0.0000e+00 - val_loss: -207.5422 - val_acc: 0.0000e+00\n",
      "Epoch 11/15\n",
      "170/170 [==============================] - 0s 444us/step - loss: -844.0970 - acc: 0.0000e+00 - val_loss: -243.8584 - val_acc: 0.0000e+00\n",
      "Epoch 12/15\n",
      "170/170 [==============================] - 0s 526us/step - loss: -1159.6451 - acc: 0.0000e+00 - val_loss: -284.0924 - val_acc: 0.0000e+00\n",
      "Epoch 13/15\n",
      "170/170 [==============================] - 0s 432us/step - loss: -1574.5516 - acc: 0.0000e+00 - val_loss: -328.4977 - val_acc: 0.0000e+00\n",
      "Epoch 14/15\n",
      "170/170 [==============================] - 0s 497us/step - loss: -2103.6997 - acc: 0.0000e+00 - val_loss: -376.7606 - val_acc: 0.0000e+00\n",
      "Epoch 15/15\n",
      "170/170 [==============================] - 0s 418us/step - loss: -2769.7959 - acc: 0.0000e+00 - val_loss: -429.4204 - val_acc: 0.0000e+00\n",
      "57/57 [==============================] - 0s 114us/step\n",
      "Test score: -429.42041551021106\n",
      "Test accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Fit keras model and report score, accuracy\n",
    "'''\n",
    "Proviso: The dataset is actually too small for LSTM to be of any advantage\n",
    "compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
    "**Notes**\n",
    "- Choice of batch size is important; choice of loss and optimizer is critical, etc.\n",
    "Some configurations won't converge.\n",
    "- LSTM loss decrease patterns during training can be quite different\n",
    "from what you see with CNNs/MLPs/etc.\n",
    "'''\n",
    "\n",
    "max_features = 2000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "\n",
    "# print('Loading data...')\n",
    "\n",
    "# print(len(x_train), 'train sequences')\n",
    "# print(len(x_test), 'test sequences')\n",
    "\n",
    "# print('Pad sequences (samples x time)')\n",
    "# x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "# x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "# print('x_train shape:', x_train.shape)\n",
    "# print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lPn6c0x21gu1"
   },
   "source": [
    "Conclusion - RNN runs, and gives pretty decent improvement over a naive \"It's Al!\" model. To *really* improve the model, more playing with parameters, and just getting more data (particularly Austen tweets), would help. Also - RNN may well not be the best approach here, but it is at least a valid one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yz0LCZd_O4IG"
   },
   "source": [
    "## Part 2- CNNs\n",
    "\n",
    "Time to play \"find the frog!\" Use Keras and ResNet50 to detect which of the following images contain frogs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "colab_type": "code",
    "id": "whIqEWR236Af",
    "outputId": "7a74e30d-310d-4a3a-9ae4-5bf52d137bda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google_images_download\n",
      "  Downloading https://files.pythonhosted.org/packages/43/51/49ebfd3a02945974b1d93e34bb96a1f9530a0dde9c2bc022b30fd658edd6/google_images_download-2.5.0.tar.gz\n",
      "Collecting selenium (from google_images_download)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n",
      "\u001b[K    100% |████████████████████████████████| 911kB 9.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium->google_images_download) (1.22)\n",
      "Building wheels for collected packages: google-images-download\n",
      "  Building wheel for google-images-download (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/d2/23/84/3cec6d566b88bef64ad727a7e805f6544b8af4a8f121f9691c\n",
      "Successfully built google-images-download\n",
      "Installing collected packages: selenium, google-images-download\n",
      "Successfully installed google-images-download-2.5.0 selenium-3.141.0\n"
     ]
    }
   ],
   "source": [
    "!pip install google_images_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "colab_type": "code",
    "id": "EKnnnM8k38sN",
    "outputId": "59f477e9-0b25-4a38-9678-af24e0176535"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Item no.: 1 --> Item name = animal pond\n",
      "Evaluating...\n",
      "Starting Download...\n",
      "Image URL: https://www.enchantedlearning.com/pgifs/Pondanimals.GIF\n",
      "Completed Image ====> 1. pondanimals.gif\n",
      "Image URL: https://i.ytimg.com/vi/NCbu0TND9vE/hqdefault.jpg\n",
      "Completed Image ====> 2. hqdefault.jpg\n",
      "Image URL: https://pklifescience.com/staticfiles/articles/images/PKLS4116_inline.png\n",
      "Completed Image ====> 3. pkls4116_inline.png\n",
      "Image URL: https://pixnio.com/free-images/fauna-animals/reptiles-and-amphibians/alligators-and-crocodiles-pictures/alligator-animal-on-pond.jpg\n",
      "Completed Image ====> 4. alligator-animal-on-pond.jpg\n",
      "Image URL: https://www.nwf.org/-/media/NEW-WEBSITE/Programs/Garden-for-Wildlife/amphibian_bronze-frog_Julia-Bartosh_400x267.ashx\n",
      "Completed Image ====> 5. amphibian_bronze-frog_julia-bartosh_400x267.ash\n",
      "\n",
      "Errors: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from google_images_download import google_images_download\n",
    "\n",
    "response = google_images_download.googleimagesdownload()\n",
    "arguments = {\"keywords\": \"animal pond\", \"limit\": 5, \"print_urls\": True}\n",
    "absolute_image_paths = response.download(arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "si5YfNqS50QU"
   },
   "source": [
    "At time of writing at least a few do, but since the Internet changes - it is possible your 5 won't. You can easily verify yourself, and (once you have working code) increase the number of images you pull to be more sure of getting a frog. Your goal is to validly run ResNet50 on the input images - don't worry about tuning or improving the model.\n",
    "\n",
    "*Hint* - ResNet 50 doesn't just return \"frog\". The three labels it has for frogs are: `bullfrog, tree frog, tailed frog`\n",
    "\n",
    "*Stretch goal* - also check for fish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FaT07ddW3nHz"
   },
   "outputs": [],
   "source": [
    "# TODO - your code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XEuhvSu7O5Rf"
   },
   "source": [
    "## Part 3 - AutoML\n",
    "\n",
    "Use [TPOT](https://github.com/EpistasisLab/tpot) to fit a predictive model for the King County housing data, with `price` as the target output variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 610
    },
    "colab_type": "code",
    "id": "pz0e_7Ve60pM",
    "outputId": "7d7f644d-2edc-417b-bd7e-14d7044ef032"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tpot\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/35/a6cc358b6bb2749d6dffa1ae2427143211f3092904bbb15d1ad317ebe051/TPOT-0.9.6.tar.gz (892kB)\n",
      "\u001b[K    100% |████████████████████████████████| 901kB 18.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tpot) (1.14.6)\n",
      "Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from tpot) (1.1.0)\n",
      "Requirement already satisfied: scikit-learn>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from tpot) (0.20.3)\n",
      "Collecting deap>=1.0 (from tpot)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/29/e7f2ecbe02997b16a768baed076f5fc4781d7057cd5d9adf7c94027845ba/deap-1.2.2.tar.gz (936kB)\n",
      "\u001b[K    100% |████████████████████████████████| 942kB 18.9MB/s \n",
      "\u001b[?25hCollecting update_checker>=0.16 (from tpot)\n",
      "  Downloading https://files.pythonhosted.org/packages/17/c9/ab11855af164d03be0ff4fddd4c46a5bd44799a9ecc1770e01a669c21168/update_checker-0.16-py2.py3-none-any.whl\n",
      "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.6/dist-packages (from tpot) (4.28.1)\n",
      "Collecting stopit>=1.1.1 (from tpot)\n",
      "  Downloading https://files.pythonhosted.org/packages/35/58/e8bb0b0fb05baf07bbac1450c447d753da65f9701f551dca79823ce15d50/stopit-1.1.2.tar.gz\n",
      "Requirement already satisfied: pandas>=0.20.2 in /usr/local/lib/python3.6/dist-packages (from tpot) (0.22.0)\n",
      "Requirement already satisfied: requests>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from update_checker>=0.16->tpot) (2.18.4)\n",
      "Requirement already satisfied: python-dateutil>=2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.20.2->tpot) (2.5.3)\n",
      "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.20.2->tpot) (2018.9)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update_checker>=0.16->tpot) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update_checker>=0.16->tpot) (1.22)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update_checker>=0.16->tpot) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update_checker>=0.16->tpot) (2019.3.9)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2->pandas>=0.20.2->tpot) (1.11.0)\n",
      "Building wheels for collected packages: tpot, deap, stopit\n",
      "  Building wheel for tpot (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/86/5c/dd/c7673fbaccb901ec1a4eb79017fa5b65766805d2a98f954b9a\n",
      "  Building wheel for deap (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/22/ea/bf/dc7c8a2262025a0ab5da9ef02282c198be88902791ca0c6658\n",
      "  Building wheel for stopit (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/3c/85/2b/2580190404636bfc63e8de3dff629c03bb795021e1983a6cc7\n",
      "Successfully built tpot deap stopit\n",
      "Installing collected packages: deap, update-checker, stopit, tpot\n",
      "Successfully installed deap-1.2.2 stopit-1.1.2 tpot-0.9.6 update-checker-0.16\n"
     ]
    }
   ],
   "source": [
    "!pip install tpot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "GflK25wp7jnf",
    "outputId": "a3e65568-5bfa-4b33-ca10-da515d8b418d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-04-12 05:27:55--  https://raw.githubusercontent.com/ryanleeallred/datasets/master/kc_house_data.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2515206 (2.4M) [text/plain]\n",
      "Saving to: ‘kc_house_data.csv’\n",
      "\n",
      "\r",
      "kc_house_data.csv     0%[                    ]       0  --.-KB/s               \r",
      "kc_house_data.csv   100%[===================>]   2.40M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2019-04-12 05:27:56 (24.1 MB/s) - ‘kc_house_data.csv’ saved [2515206/2515206]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/ryanleeallred/datasets/master/kc_house_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "colab_type": "code",
    "id": "7G5-Gqyg9A-V",
    "outputId": "1140fe22-952d-4745-d628-d91766af65b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,date,price,bedrooms,bathrooms,sqft_living,sqft_lot,floors,waterfront,view,condition,grade,sqft_above,sqft_basement,yr_built,yr_renovated,zipcode,lat,long,sqft_living15,sqft_lot15\n",
      "\"7129300520\",\"20141013T000000\",221900,3,1,1180,5650,\"1\",0,0,3,7,1180,0,1955,0,\"98178\",47.5112,-122.257,1340,5650\n",
      "\"6414100192\",\"20141209T000000\",538000,3,2.25,2570,7242,\"2\",0,0,3,7,2170,400,1951,1991,\"98125\",47.721,-122.319,1690,7639\n",
      "\"5631500400\",\"20150225T000000\",180000,2,1,770,10000,\"1\",0,0,3,6,770,0,1933,0,\"98028\",47.7379,-122.233,2720,8062\n",
      "\"2487200875\",\"20141209T000000\",604000,4,3,1960,5000,\"1\",0,0,5,7,1050,910,1965,0,\"98136\",47.5208,-122.393,1360,5000\n",
      "\"1954400510\",\"20150218T000000\",510000,3,2,1680,8080,\"1\",0,0,3,8,1680,0,1987,0,\"98074\",47.6168,-122.045,1800,7503\n",
      "\"7237550310\",\"20140512T000000\",1.225e+006,4,4.5,5420,101930,\"1\",0,0,3,11,3890,1530,2001,0,\"98053\",47.6561,-122.005,4760,101930\n",
      "\"1321400060\",\"20140627T000000\",257500,3,2.25,1715,6819,\"2\",0,0,3,7,1715,0,1995,0,\"98003\",47.3097,-122.327,2238,6819\n",
      "\"2008000270\",\"20150115T000000\",291850,3,1.5,1060,9711,\"1\",0,0,3,7,1060,0,1963,0,\"98198\",47.4095,-122.315,1650,9711\n",
      "\"2414600126\",\"20150415T000000\",229500,3,1,1780,7470,\"1\",0,0,3,7,1050,730,1960,0,\"98146\",47.5123,-122.337,1780,8113\n"
     ]
    }
   ],
   "source": [
    "!head kc_house_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KynXZjOY8hBL"
   },
   "source": [
    "As with previous questions, your goal is to run TPOT and successfully run and report error at the end.  Also, in the interest of time, feel free to choose small `generation=1` and `population_size=10` parameters so your pipeline runs efficiently and you are able to iterate and test.\n",
    "\n",
    "*Hint* - you'll have to drop and/or type coerce at least a few variables to get things working. It's fine to err on the side of dropping to get things running, as long as you still get a valid model with reasonable predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BOREO8VJO7MZ"
   },
   "outputs": [],
   "source": [
    "# TODO - your code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "626zYgjkO7Vq"
   },
   "source": [
    "## Part 4 - More..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "__lDWfcUO8oo"
   },
   "source": [
    "Answer the following questions, with a target audience of a fellow Data Scientist:\n",
    "\n",
    "- What do you consider your strongest area, as a Data Scientist?\n",
    "- What area of Data Science would you most like to learn more about, and why?\n",
    "- Where do you think Data Science will be in 5 years?\n",
    "\n",
    "A few sentences per answer is fine - only elaborate if time allows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Hoqe3mM_Mtc"
   },
   "source": [
    "Thank you for your hard work, and congratulations! You've learned a lot, and should proudly call yourself a Data Scientist."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_Unit_4_Sprint_Challenge_4.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
